import requests
import json
import time
import os
import csv
import random

def get_rate_limit(headers):
    """Controlla il rate limit delle API di GitHub."""
    url = f"{headers[1]}/rate_limit"
    response = requests.get(url, headers=headers[2]).json()
    remaining = response["rate"]["remaining"]
    reset_time = response["rate"]["reset"]
    return remaining, reset_time

def wait_if_rate_limited(headers):
    """Attende se il rate limit √® stato superato."""
    remaining, reset_time = get_rate_limit(headers)
    if remaining == 0:
        wait_time = reset_time - time.time()
        if wait_time > 0:
            print(f"‚ö†Ô∏è Rate limit superato. Attendo {int(wait_time)} secondi...")
            time.sleep(wait_time)

def collect_random_commits(headers, output_jsonl_path, csv_output_path, min_lines, max_lines, valid_extensions, max_commits=20):
    """Colleziona commit casuali pre-2020 con metadati strutturati e salva sia JSONL che CSV."""

    search_url = f"{headers[1]}/search/commits"
    collected = 0
    page = 1

    # Ensure directories exist
    os.makedirs(os.path.dirname(output_jsonl_path), exist_ok=True)
    os.makedirs(os.path.dirname(csv_output_path), exist_ok=True)

    commit_labels = [
        "FIRST COMMIT", "SECOND COMMIT", "THIRD COMMIT", "FOURTH COMMIT", "FIFTH COMMIT",
        "SIXTH COMMIT", "SEVENTH COMMIT", "EIGHTH COMMIT", "NINTH COMMIT", "TENTH COMMIT",
        "ELEVENTH COMMIT", "TWELFTH COMMIT", "THIRTEENTH COMMIT", "FOURTEENTH COMMIT",
        "FIFTEENTH COMMIT", "SIXTEENTH COMMIT", "SEVENTEENTH COMMIT", "EIGHTEENTH COMMIT",
        "NINETEENTH COMMIT", "TWENTIETH COMMIT"
    ]

    with open(output_jsonl_path, "w", encoding="utf-8") as jsonl_out, \
         open(csv_output_path, "w", encoding="utf-8", newline="") as csv_out:

        csv_writer = csv.writer(csv_out)

        while collected < max_commits:
            wait_if_rate_limited(headers)
            query = "pushed:<2020-01-01"
            params = {
                "q": query,
                "sort": "committer-date",
                "order": "desc",
                "per_page": 100,
                "page": page
            }
            response = requests.get(search_url, headers=headers[2], params=params)

            if response.status_code != 200:
                print(f"‚ùå Errore nella richiesta: {response.json()}")
                break

            data = response.json()
            items = data.get("items", [])
            random.shuffle(items)

            for item in items:
                sha = item["sha"]
                repo_url = item["repository"]["url"]
                commit_url = f"{repo_url}/commits/{sha}"

                wait_if_rate_limited(headers)
                commit_resp = requests.get(commit_url, headers=headers[2])
                if commit_resp.status_code != 200:
                    continue

                commit_data = commit_resp.json()
                files = commit_data.get("files", [])
                valid_files = [
                    f for f in files
                    if "patch" in f and
                    any(f["filename"].endswith(ext) for ext in valid_extensions)
                ]

                total_lines = sum(f.get("changes", 0) for f in valid_files)
                if min_lines <= total_lines <= max_lines and valid_files:
                    longest_chunk = []
                    max_chunk_len = 0
                    selected_file = None

                    for f in valid_files:
                        lines = f.get("patch", "").split('\n')
                        added_chunk = []
                        current_chunk = []

                        for line in lines:
                            if line.startswith('+') and not line.startswith('+++'):
                                current_chunk.append(line[1:])
                            else:
                                if len(current_chunk) > len(added_chunk):
                                    added_chunk = current_chunk
                                current_chunk = []

                        if len(current_chunk) > len(added_chunk):
                            added_chunk = current_chunk

                        if len(added_chunk) > max_chunk_len:
                            longest_chunk = added_chunk
                            max_chunk_len = len(added_chunk)
                            selected_file = f["filename"]

                    label = commit_labels[collected] if collected < len(commit_labels) else f"COMMIT #{collected+1}"

                    record = {
                        "Link_to_commit": commit_url,
                        "n-gram matched": "generated by copilot",
                        "n_lines_longer_change": total_lines,
                        "n_files_impacted": len(valid_files),
                        "longest_chunk": longest_chunk,
                        "file_path": selected_file
                    }

                    jsonl_out.write(json.dumps(record) + "\n")

                    csv_writer.writerow([label])
                    csv_writer.writerow([
                        "generated by copilot",
                        total_lines,
                        len(valid_files),
                        selected_file,
                        "\n".join(longest_chunk)
                    ])
                    csv_writer.writerow([])

                    collected += 1
                    print(f"‚úÖ Commit salvato ({collected}/{max_commits})")
                    if collected >= max_commits:
                        break

            page += 1

    print(f"üéØ Raccolta completata: {collected} commit salvati in {output_jsonl_path} e {csv_output_path}")
