{"Link_to_commit": "https://github.com/jimmylewis/libman.audit/commit/91bef447774e61cd70318a523aa84a939e369c63", "n-gram matched": "generated by copilot", "n_lines_longer_change": 268, "n_files_impacted": 8, "longest_chunk": ["using System;", "using System.Collections.Generic;", "using System.IO;", "using System.Linq;", "using System.Net.Http;", "using System.Text;", "using System.Text.Json;", "using System.Text.Json.Serialization;", "using System.Threading.Tasks;", "using Microsoft.Build.Framework;", "using Microsoft.Build.Utilities;", "", "using Task = Microsoft.Build.Utilities.Task;", "", "namespace Libman.Audit", "{", "    public class LibmanAuditTask : Task", "    {", "        [Required]", "        public string LibmanJsonPath { get; set; }", "", "        [Output]", "        public ITaskItem[] VulnerablePackages { get; private set; }", "", "        private readonly HttpClient _httpClient;", "        private const string SonatypeApiBaseUrl = \"https://ossindex.sonatype.org/api/v3/component-report\";", "        private static readonly JsonSerializerOptions _jsonOptions = new JsonSerializerOptions", "        {", "            PropertyNameCaseInsensitive = true,", "            PropertyNamingPolicy = JsonNamingPolicy.CamelCase", "        };", "", "        public LibmanAuditTask()", "        {", "            _httpClient = new HttpClient();", "            // initialize non-null values", "            LibmanJsonPath = \"\";", "            VulnerablePackages = [];", "        }", "", "        public override bool Execute()", "        {", "            try", "            {", "                Log.LogMessage(MessageImportance.Normal, \"Starting Libman audit task...\");", "", "                if (!File.Exists(LibmanJsonPath))", "                {", "                    Log.LogError($\"Libman.json file not found at: {LibmanJsonPath}\");", "                    return false;", "                }", "", "                string jsonContent = File.ReadAllText(LibmanJsonPath);", "                List<LibmanPackage> libmanPackages = ParseLibmanJson(jsonContent);", "", "                if (libmanPackages.Count == 0)", "                {", "                    Log.LogMessage(MessageImportance.Normal, \"No packages found in libman.json\");", "                    VulnerablePackages = new TaskItem[0];", "                    return true;", "                }", "", "                List<VulnerablePackage> vulnerablePackages = AuditPackagesAsync(libmanPackages).GetAwaiter().GetResult();", "                VulnerablePackages = ConvertToTaskItems(vulnerablePackages);", "", "                if (vulnerablePackages.Count > 0)", "                {", "                    Log.LogWarning($\"Found {vulnerablePackages.Count} vulnerable packages in libman.json\");", "                    foreach (VulnerablePackage package in vulnerablePackages)", "                    {", "                        Log.LogWarning($\"Vulnerable package: {package.Name} {package.Version}, Vulnerability count: {package.VulnerabilityCount}\");", "                    }", "                }", "                else", "                {", "                    Log.LogMessage(MessageImportance.Normal, \"No vulnerable packages found\");", "                }", "", "                return true;", "            }", "            catch (Exception ex)", "            {", "                Log.LogErrorFromException(ex);", "                return false;", "            }", "        }", "", "        private List<LibmanPackage> ParseLibmanJson(string jsonContent)", "        {", "            List<LibmanPackage> packages = new List<LibmanPackage>();", "", "            try", "            {", "                using (JsonDocument doc = JsonDocument.Parse(jsonContent))", "                {", "                    JsonElement root = doc.RootElement;", "", "                    if (!root.TryGetProperty(\"libraries\", out JsonElement librariesElement) ||", "                        librariesElement.ValueKind != JsonValueKind.Array)", "                    {", "                        Log.LogMessage(MessageImportance.Normal, \"No libraries found in libman.json\");", "                        return packages;", "                    }", "", "                    foreach (JsonElement library in librariesElement.EnumerateArray())", "                    {", "                        if (!library.TryGetProperty(\"provider\", out JsonElement providerElement) ||", "                            !library.TryGetProperty(\"library\", out JsonElement nameElement))", "                        {", "                            continue;", "                        }", "", "                        string provider = providerElement.GetString() ?? string.Empty;", "                        string name = nameElement.GetString() ?? string.Empty;", "", "                        if (string.IsNullOrEmpty(provider) || string.IsNullOrEmpty(name))", "                        {", "                            continue;", "                        }", "", "                        // Parse package name and version (format varies by provider)", "                        string packageName;", "                        string packageVersion;", "", "                        if (name.Contains(\"@\"))", "                        {", "                            string[] parts = name.Split(new[] { '@' }, 2);", "                            packageName = parts[0];", "                            packageVersion = parts[1];", "                        }", "                        else", "                        {", "                            // If no version is specified, use the name as-is and leave version empty", "                            packageName = name;", "                            packageVersion = string.Empty;", "                        }", "", "                        packages.Add(new LibmanPackage", "                        {", "                            Name = packageName,", "                            Version = packageVersion,", "                            Provider = provider", "                        });", "", "                        Log.LogMessage(MessageImportance.Low, $\"Found package: {packageName} {packageVersion} (Provider: {provider})\");", "                    }", "                }", "            }", "            catch (JsonException ex)", "            {", "                Log.LogError($\"Failed to parse libman.json: {ex.Message}\");", "            }", "", "            return packages;", "        }", "", "        private async Task<List<VulnerablePackage>> AuditPackagesAsync(List<LibmanPackage> packages)", "        {", "            List<VulnerablePackage> vulnerablePackages = new List<VulnerablePackage>();", "", "            try", "            {", "                // Group packages in batches to avoid large requests", "                for (int i = 0; i < packages.Count; i += 20)", "                {", "                    List<LibmanPackage> batch = packages.Skip(i).Take(20).ToList();", "                    List<string> components = new List<string>();", "", "                    foreach (LibmanPackage package in batch)", "                    {", "                        string packageId = GetPackageCoordinates(package);", "                        if (!string.IsNullOrEmpty(packageId))", "                        {", "                            components.Add(packageId);", "                        }", "                    }", "", "                    if (components.Count > 0)", "                    {", "                        SonatypeRequest requestData = new SonatypeRequest", "                        {", "                            Coordinates = components.ToArray()", "                        };", "", "                        StringContent content = new StringContent(", "                            JsonSerializer.Serialize(requestData, _jsonOptions),", "                            Encoding.UTF8,", "                            \"application/json\");", "", "                        HttpResponseMessage response = await _httpClient.PostAsync(SonatypeApiBaseUrl, content);", "", "                        if (response.IsSuccessStatusCode)", "                        {", "                            string responseContent = await response.Content.ReadAsStringAsync();", "                            List<SonatypeResult>? results = JsonSerializer.Deserialize<List<SonatypeResult>>(responseContent, _jsonOptions);", "", "                            if (results != null)", "                            {", "                                foreach (SonatypeResult result in results)", "                                {", "                                    if (result.Vulnerabilities != null && result.Vulnerabilities.Count > 0)", "                                    {", "                                        LibmanPackage? package = batch.FirstOrDefault(p => GetPackageCoordinates(p) == result.Coordinates);", "                                        if (package != null)", "                                        {", "                                            vulnerablePackages.Add(new VulnerablePackage", "                                            {", "                                                Name = package.Name,", "                                                Version = package.Version,", "                                                Provider = package.Provider,", "                                                VulnerabilityCount = result.Vulnerabilities.Count,", "                                                Description = string.Join(\"; \", result.Vulnerabilities.Select(v => v.Title))", "                                            });", "                                        }", "                                    }", "                                }", "                            }", "                        }", "                        else", "                        {", "                            Log.LogWarning($\"Failed to get vulnerability data: {response.StatusCode} {await response.Content.ReadAsStringAsync()}\");", "                        }", "                    }", "                }", "            }", "            catch (Exception ex)", "            {", "                Log.LogWarning($\"Error checking for vulnerabilities: {ex.Message}\");", "            }", "", "            return vulnerablePackages;", "        }", "", "        private string GetPackageCoordinates(LibmanPackage package)", "        {", "            // Map libman providers to Sonatype coordinate formats", "            switch (package.Provider.ToLowerInvariant())", "            {", "                case \"cdnjs\":", "                    return $\"pkg:npm/{package.Name}@{package.Version}\";", "                case \"unpkg\":", "                    return $\"pkg:npm/{package.Name}@{package.Version}\";", "                case \"jsdelivr\":", "                    return $\"pkg:npm/{package.Name}@{package.Version}\";", "                default:", "                    Log.LogWarning($\"Unsupported provider: {package.Provider}\");", "                    return \"\";", "            }", "        }", "", "        private ITaskItem[] ConvertToTaskItems(List<VulnerablePackage> vulnerablePackages)", "        {", "            List<TaskItem> taskItems = new List<TaskItem>();", "", "            foreach (VulnerablePackage package in vulnerablePackages)", "            {", "                TaskItem taskItem = new TaskItem(package.Name);", "                taskItem.SetMetadata(\"Version\", package.Version);", "                taskItem.SetMetadata(\"Provider\", package.Provider);", "                taskItem.SetMetadata(\"VulnerabilityCount\", package.VulnerabilityCount.ToString());", "                taskItem.SetMetadata(\"Description\", package.Description);", "                taskItems.Add(taskItem);", "            }", "", "            return taskItems.ToArray();", "        }", "    }", "}"], "file_path": "Libman.Audit/LibmanPackage.cs"}
{"Link_to_commit": "https://github.com/luis-rodriguezfernandez-sonarsource/maven-basic/commit/177684f4cbcef2fd26f63623cac2161f584c0496", "n-gram matched": "generated by copilot", "n_lines_longer_change": 27, "n_files_impacted": 1, "longest_chunk": ["package com.acme.basic;", "", "import org.junit.jupiter.api.Test;", "import org.springframework.boot.test.context.SpringBootTest;", "import org.springframework.boot.test.web.server.LocalServerPort;", "import org.springframework.http.ResponseEntity;", "import org.springframework.web.client.RestTemplate;", "", "import static org.assertj.core.api.Assertions.assertThat;", "", "@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)", "public class FibonacciControllerIT {", "", "    @LocalServerPort", "    private int port;", "", "    private final RestTemplate restTemplate = new RestTemplate();", "", "    @Test", "    public void testFibonacciEndpoint() {", "        String url = \"http://localhost:\" + port + \"/fibonacci?n=5\";", "        ResponseEntity<String> response = restTemplate.getForEntity(url, String.class);", "", "        assertThat(response.getStatusCode().is2xxSuccessful()).isTrue();", "        assertThat(response.getBody()).isEqualTo(\"5\");", "    }", "}"], "file_path": "src/test/java/com/acme/basic/FibonacciControllerIT.java"}
{"Link_to_commit": "https://github.com/professor-tucker/ai-prompts/commit/a023769eb9d4881bd696a1e52cad31a5c3967dc2", "n-gram matched": "generated by copilot", "n_lines_longer_change": 141, "n_files_impacted": 1, "longest_chunk": ["import requests", "import pandas as pd", "import re", "import os", "from bs4 import BeautifulSoup", "from datetime import datetime, timedelta", "import pickle", "from google.colab import drive", "from google_auth_oauthlib.flow import InstalledAppFlow", "from googleapiclient.discovery import build", "from docx import Document", "import nltk", "from nltk.tokenize import word_tokenize", "from nltk.corpus import stopwords", "from sklearn.feature_extraction.text import TfidfVectorizer", "", "# Initialization", "def initialize():", "    drive.mount('/content/drive')", "    nltk.download('punkt')", "    nltk.download('stopwords')", "", "# Job Retriever Class", "class JobRetriever:", "    def __init__(self):", "        self.jobs_df = pd.DataFrame(columns=['Title', 'Company', 'Location', 'Description', 'URL', 'Date_Posted', 'Keywords_Match'])", "", "    def search_indeed(self, keywords, location, pages=5):", "        base_url = \"https://www.indeed.com/jobs\"", "        all_jobs = []", "", "        for page in range(pages):", "            params = {'q': keywords, 'l': location, 'start': page * 10}", "            headers = {'User-Agent': 'Mozilla/5.0'}", "", "            try:", "                response = requests.get(base_url, params=params, headers=headers)", "                soup = BeautifulSoup(response.text, 'html.parser')", "                job_cards = soup.find_all('div', class_='jobsearch-SerpJobCard')", "", "                for card in job_cards:", "                    job_title_elem = card.find('a', class_='jobtitle')", "                    company_elem = card.find('span', class_='company')", "                    location_elem = card.find('div', class_='recJobLoc')", "                    description_elem = card.find('div', class_='summary')", "", "                    if job_title_elem and company_elem:", "                        job_title = job_title_elem.text.strip()", "                        company = company_elem.text.strip()", "                        location = location_elem['data-rc-loc'] if location_elem else \"N/A\"", "                        description = description_elem.text.strip() if description_elem else \"N/A\"", "                        url = \"https://www.indeed.com\" + job_title_elem['href']", "", "                        keywords_list = keywords.lower().split()", "                        match_score = sum(1 for keyword in keywords_list if keyword.lower() in (job_title.lower() + \" \" + description.lower()))", "", "                        job_data = {'Title': job_title, 'Company': company, 'Location': location, 'Description': description, 'URL': url, 'Date_Posted': datetime.now().strftime(\"%Y-%m-%d\"), 'Keywords_Match': match_score}", "                        all_jobs.append(job_data)", "", "            except requests.RequestException as e:", "                print(f\"Error scraping Indeed page {page}: {str(e)}\")", "", "        self.jobs_df = pd.concat([self.jobs_df, pd.DataFrame(all_jobs)], ignore_index=True)", "", "    def filter_jobs(self, min_keywords_match=2):", "        return self.jobs_df[self.jobs_df['Keywords_Match'] >= min_keywords_match].sort_values('Keywords_Match', ascending=False)", "", "    def save_jobs(self, filename='job_listings.csv'):", "        path = '/content/drive/My Drive/' + filename", "        self.jobs_df.to_csv(path, index=False)", "        print(f\"Saved {len(self.jobs_df)} jobs to {path}\")", "        return path", "", "# Document Customizer Class", "class DocumentCustomizer:", "    def __init__(self, resume_path, cover_letter_path):", "        self.resume_path = resume_path", "        self.cover_letter_path = cover_letter_path", "        self.resume_doc = self.load_document(resume_path)", "        self.cover_letter_doc = self.load_document(cover_letter_path)", "", "    def load_document(self, path):", "        try:", "            return Document(path)", "        except Exception as e:", "            print(f\"Error loading document from {path}: {str(e)}\")", "            return None", "", "    def extract_job_keywords(self, job_description):", "        tokens = word_tokenize(job_description.lower())", "        filtered_tokens = [word for word in tokens if word.isalnum() and word not in stopwords.words('english')]", "        vectorizer = TfidfVectorizer(max_features=20)", "        tfidf_matrix = vectorizer.fit_transform([' '.join(filtered_tokens)])", "        feature_names = vectorizer.get_feature_names_out()", "        word_scores = [(word, tfidf_matrix[0, i]) for i, word in enumerate(feature_names)]", "        word_scores.sort(key=lambda x: x[1], reverse=True)", "        return [word for word, score in word_scores[:10]]", "", "    def customize_resume(self, job_title, company_name, job_description):", "        if not self.resume_doc:", "            return None", "", "        custom_resume = Document()", "        keywords = self.extract_job_keywords(job_description)", "        print(f\"Keywords extracted: {keywords}\")", "", "        for para in self.resume_doc.paragraphs:", "            if \"[OBJECTIVE]\" in para.text:", "                custom_text = para.text.replace(\"[OBJECTIVE]\", f\"Experienced professional seeking the {job_title} position at {company_name}, bringing expertise in {', '.join(keywords[:3])}.\")", "                custom_resume.add_paragraph(custom_text, para.style)", "            else:", "                text = para.text", "                for keyword in keywords:", "                    if keyword.lower() in text.lower() and len(keyword) > 3:", "                        text = re.sub(re.escape(keyword), f\"**{keyword}**\", text, flags=re.IGNORECASE)", "                custom_resume.add_paragraph(text, para.style)", "", "        filename = f\"Custom_Resume_{company_name}_{datetime.now().strftime('%Y%m%d')}.docx\"", "        save_path = f\"/content/drive/My Drive/{filename}\"", "        custom_resume.save(save_path)", "        print(f\"Customized resume saved to {save_path}\")", "        return save_path", "", "# Initialization", "initialize()", "", "# Example usage", "def run_job_application_system():", "    resume_path = \"/content/drive/My Drive/Resume_Template.docx\"", "    cover_letter_path = \"/content/drive/My Drive/Cover_Letter_Template.docx\"", "    manager = JobApplicationManager(resume_path, cover_letter_path)", "    job_keywords = \"cybersecurity IT project management CISSP PMP\"", "    locations = [\"New York, NY\", \"Remote\"]", "    jobs_found = manager.search_jobs(job_keywords, locations)", "    print(\"\\nTop matching jobs:\")", "    for i, (_, job) in enumerate(jobs_found.head(10).iterrows()):", "        print(f\"{i+1}. {job['Title']} at {job['Company']} ({job['Location']}) - Match Score: {job['Keywords_Match']}\")", "    manager.batch_process_jobs(num_jobs=3)", "    print(\"\\nJob application automation completed!\")", "", "run_job_application_system()"], "file_path": "03.03.2025.claude-auto-app-copilot-mod.py"}
{"Link_to_commit": "https://github.com/SimTimms/mappi/commit/7f511bd95ddf7066fd40a729a1155b95b8eabe02", "n-gram matched": "generated by copilot", "n_lines_longer_change": 8, "n_files_impacted": 3, "longest_chunk": ["/**", " * Fetch tide data for a given location.", " *", " * @param locationName - The name of the location to fetch tide data for.", " * @param apiKey - The API key for authenticating with the weather API.", " * @returns A promise that resolves to the tide data for the specified location.", " */", ""], "file_path": "mappi-api/src/services/fetchTideData.ts"}
{"Link_to_commit": "https://github.com/MoleskiCoder/EightBitNet/commit/caca3467d9c2b627b824a80601b1ccee4221dd13", "n-gram matched": "generated by copilot", "n_lines_longer_change": 105, "n_files_impacted": 6, "longest_chunk": ["namespace EightBit.UnitTest", "{", "    using Microsoft.VisualStudio.TestTools.UnitTesting;", "    using EightBit;", "", "    [TestClass]", "    public class ChipTests", "    {", "        [TestMethod]", "        public void Bit_ReturnsCorrectBit()", "        {", "            Assert.AreEqual(0x01, Chip.Bit(0));", "            Assert.AreEqual(0x02, Chip.Bit(1));", "            Assert.AreEqual(0x80, Chip.Bit(7));", "            Assert.AreEqual(0x08, Chip.Bit((byte)3));", "        }", "", "        [TestMethod]", "        public void SetBit_SetsBitCorrectly()", "        {", "            Assert.AreEqual(0b00001101, Chip.SetBit(0b00001001, 0b00000100));", "            Assert.AreEqual(0b00001101, Chip.SetBit(0b00001101, 0b00000100));", "            Assert.AreEqual(0b00001101, Chip.SetBit(0b00001101, 0b00000100, true));", "            Assert.AreEqual(0b00001001, Chip.SetBit(0b00001101, 0b00000100, false));", "        }", "", "        [TestMethod]", "        public void ClearBit_ClearsBitCorrectly()", "        {", "            Assert.AreEqual(0b00001001, Chip.ClearBit(0b00001101, 0b00000100));", "            Assert.AreEqual(0b00001101, Chip.ClearBit(0b00001101, 0b00000100, false));", "            Assert.AreEqual(0b00001001, Chip.ClearBit(0b00001101, 0b00000100, true));", "        }", "", "        [TestMethod]", "        public void HighByte_LowByte_WorkCorrectly()", "        {", "            ushort value = 0xABCD;", "            Assert.AreEqual(0xAB, Chip.HighByte(value));", "            Assert.AreEqual(0xCD, Chip.LowByte(value));", "            int intValue = 0x1234;", "            Assert.AreEqual(0x12, Chip.HighByte(intValue));", "            Assert.AreEqual(0x34, Chip.LowByte(intValue));", "        }", "", "        [TestMethod]", "        public void PromoteByte_DemoteByte_WorkCorrectly()", "        {", "            Assert.AreEqual(0x3400, Chip.PromoteByte(0x34));", "            Assert.AreEqual(0x12, Chip.DemoteByte(0x1234));", "        }", "", "        [TestMethod]", "        public void HigherPart_LowerPart_WorkCorrectly()", "        {", "            ushort value = 0xABCD;", "            Assert.AreEqual(0xAB00, Chip.HigherPart(value));", "            Assert.AreEqual(0xCD, Chip.LowerPart(value));", "        }", "", "        [TestMethod]", "        public void MakeWord_CreatesCorrectWord()", "        {", "            Assert.AreEqual(0x1234, Chip.MakeWord(0x34, 0x12));", "        }", "", "        [TestMethod]", "        public void NibbleMethods_WorkCorrectly()", "        {", "            byte value = 0xAB;", "            Assert.AreEqual(0xA, Chip.HighNibble(value));", "            Assert.AreEqual(0xB, Chip.LowNibble(value));", "            Assert.AreEqual(0xA0, Chip.HigherNibble(value));", "            Assert.AreEqual(0xB, Chip.LowerNibble(value));", "            Assert.AreEqual(0xB0, Chip.PromoteNibble(value));", "            Assert.AreEqual(0xA, Chip.DemoteNibble(value));", "        }", "", "        [TestMethod]", "        public void CountBits_ReturnsCorrectCount()", "        {", "            Assert.AreEqual(0, Chip.CountBits(0));", "            Assert.AreEqual(1, Chip.CountBits(1));", "            Assert.AreEqual(8, Chip.CountBits(0xFF));", "        }", "", "        [TestMethod]", "        public void EvenParity_ReturnsCorrectParity()", "        {", "            Assert.IsTrue(Chip.EvenParity(0)); // 0 bits set", "            Assert.IsFalse(Chip.EvenParity(1)); // 1 bit set", "            Assert.IsTrue(Chip.EvenParity(3)); // 2 bits set", "        }", "", "        [TestMethod]", "        public void FindFirstSet_ReturnsCorrectIndex()", "        {", "            Assert.AreEqual(0, Chip.FindFirstSet(0));", "            Assert.AreEqual(1, Chip.FindFirstSet(1));", "            Assert.AreEqual(2, Chip.FindFirstSet(2));", "            Assert.AreEqual(3, Chip.FindFirstSet(4));", "            Assert.AreEqual(5, Chip.FindFirstSet(0b10000));", "        }", "    }", "}"], "file_path": "EightBit/EightBit.UnitTest/DeviceTests.cs"}
{"Link_to_commit": "https://github.com/jakobatgithub/notification_test/commit/db2954f478a1cebc63fad4a320bda7039de35838", "n-gram matched": "generated by copilot", "n_lines_longer_change": 15, "n_files_impacted": 8, "longest_chunk": ["    \"\"\"", "    Send a data message via Firebase Cloud Messaging (FCM).", "", "    Args:", "        token (str): The recipient's FCM device token.", "        msg_id (str): The unique message ID.", "        title (str): The title of the message.", "        body (str): The body content of the message.", "", "    Returns:", "        str: The response from the Firebase messaging service.", "", "    Raises:", "        ImportError: If the Firebase Admin SDK is not installed.", "    \"\"\""], "file_path": "backend/django_emqx/utils.py"}
{"Link_to_commit": "https://github.com/depromeet/depromeet-makers-fe/commit/e6e80a27020c304f49d511e4fbebd73db3aa6a11", "n-gram matched": "generated by copilot", "n_lines_longer_change": 37, "n_files_impacted": 10, "longest_chunk": ["import { CURRENT_GENERATION } from '@depromeet-makers/constant';", "import type { UseMutationOptions } from '@tanstack/react-query';", "import { useMutation, useQueryClient } from '@tanstack/react-query';", "", "import type { CustomError } from '../base';", "import { api } from '../base';", "import type { Session } from '../types';", "", "interface EditSessionRequest extends Omit<Session, 'sessionId' | 'generation'> {}", "", "interface EditSessionResponse extends Session {}", "", "const editSession = (sessionId: Session['sessionId'], request: EditSessionRequest) => {", "  return api.put<EditSessionResponse>(`/v1/sessions/${sessionId}`, {", "    ...request,", "    generation: CURRENT_GENERATION,", "  });", "};", "", "export const useEditSession = (", "  sessionId: Session['sessionId'],", "  options?: UseMutationOptions<EditSessionResponse, CustomError, EditSessionRequest>,", ") => {", "  const queryClient = useQueryClient();", "", "  return useMutation({", "    mutationFn: (request: EditSessionRequest) => editSession(sessionId, request),", "    ...options,", "    onSuccess: (...params) => {", "      options?.onSuccess?.(...params);", "", "      queryClient.invalidateQueries({", "        queryKey: ['sessions'],", "      });", "    },", "  });", "};"], "file_path": "packages/api/src/sessions/useGetSessionDetail.ts"}
{"Link_to_commit": "https://github.com/ikedam/udpredirector/commit/e027a115f6670d96ce526a615d320c3a1b88e9bc", "n-gram matched": "generated by copilot", "n_lines_longer_change": 10, "n_files_impacted": 1, "longest_chunk": ["\tdefer syscall.Close(fd)", "", "\t// \u30bd\u30b1\u30c3\u30c8\u3092\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u306b\u30d0\u30a4\u30f3\u30c9", "\taddr := syscall.SockaddrLinklayer{", "\t\tProtocol: syscall.ETH_P_ALL,", "\t\tIfindex:  iface.Index,", "\t}", "\tif err := syscall.Bind(fd, &addr); err != nil {", "\t\tlog.Fatalf(\"Failed to bind raw socket: %v\", err)", "\t}"], "file_path": "main.go"}
{"Link_to_commit": "https://github.com/depromeet/depromeet-makers-fe/commit/724bed64d2071c62797209a4dffeccf6986353bf", "n-gram matched": "generated by copilot", "n_lines_longer_change": 7, "n_files_impacted": 7, "longest_chunk": ["export const ATTENDANCE_STATUS_TEXT_COLOR: Record<ATTENDANCE_STATUS, string> = {", "  [ATTENDANCE_STATUS.\ucd9c\uc11d\ub300\uae30]: 'text-gray-300',", "  [ATTENDANCE_STATUS.\ucd9c\uc11d]: 'text-green-300',", "  [ATTENDANCE_STATUS.\uc9c0\uac01]: 'text-yellow-300',", "  [ATTENDANCE_STATUS.\uacb0\uc11d]: 'text-red-300',", "};", ""], "file_path": "apps/admin/src/constants/attendance.ts"}
{"Link_to_commit": "https://github.com/Pablo-Aliskevicius/cpp8Queens/commit/26dfaacf128b0dfb554e5cf2e7fa6c9890f4a44b", "n-gram matched": "generated by copilot", "n_lines_longer_change": 144, "n_files_impacted": 6, "longest_chunk": ["// #include <iostream>", "// include <vector>", "// include <chrono>", "", "import <iostream>;", "import <vector>;", "import <chrono>;", "", "constexpr int N = 8;", "using Board = uint64_t;", "", "constexpr Board col_mask = (1ULL << N) - 1;", "constexpr Board diag1_mask = (1ULL << (2 * N - 1)) - 1;", "constexpr Board diag2_mask = (1ULL << (2 * N - 1)) - 1;", "", "/*", "constexpr Board col_threats[N] = {", "    0x0101010101010101ULL,", "    0x0202020202020202ULL,", "    0x0404040404040404ULL,", "    0x0808080808080808ULL,", "    0x1010101010101010ULL,", "    0x2020202020202020ULL,", "    0x4040404040404040ULL,", "    0x8080808080808080ULL", "};", "", "constexpr Board diag1_threats[2 * N - 1] = {", "    0x0000000000000080ULL,", "    0x0000000000008040ULL,", "    0x0000000000804020ULL,", "    0x0000000080402010ULL,", "    0x0000008040201008ULL,", "    0x0000804020100804ULL,", "    0x0080402010080402ULL,", "    0x8040201008040201ULL,", "    0x4020100804020100ULL,", "    0x2010080402010000ULL,", "    0x1008040201000000ULL,", "    0x0804020100000000ULL,", "    0x0402010000000000ULL,", "    0x0201000000000000ULL,", "    0x0100000000000000ULL", "};", "", "constexpr Board diag2_threats[2 * N - 1] = {", "    0x0000000000000001ULL,", "    0x0000000000000102ULL,", "    0x0000000000010204ULL,", "    0x0000000001020408ULL,", "    0x0000000102040810ULL,", "    0x0000010204081020ULL,", "    0x0001020408102040ULL,", "    0x0102040810204080ULL,", "    0x0204081020408000ULL,", "    0x0408102040800000ULL,", "    0x0810204080000000ULL,", "    0x1020408000000000ULL,", "    0x2040800000000000ULL,", "    0x4080000000000000ULL,", "    0x8000000000000000ULL", "};", "//*/", "", "void solve(Board col, Board diag1, Board diag2, int row, std::vector<Board>& solutions, Board board) {", "    if (row == N) [[unlikely]] {", "        solutions.push_back(board);", "        return;", "    }", "", "    Board safe = ~(col | diag1 | diag2) & col_mask;", "", "    while (safe) {", "        Board p = safe & (-safe);", "        safe -= p;", "        solve(col | p, (diag1 | p) << 1, (diag2 | p) >> 1, row + 1, solutions, board | (p << (row * N)));", "    }", "}", "", "void print_solution(Board board) {", "    for (int i = 0; i < N; ++i) {", "        for (int j = 0; j < N; ++j) {", "            std::cout << ((board & (1ULL << (i * N + j))) ? \"1 \" : \"0 \");", "        }", "        std::cout << \"\\n\";", "    }", "    std::cout << \"\\n\";", "}", "", "int main() {", "    std::vector<Board> solutions;", "    auto start = std::chrono::high_resolution_clock::now();", "    solve(0, 0, 0, 0, solutions, 0);", "    auto end = std::chrono::high_resolution_clock::now();", "    std::chrono::duration<double> elapsed = end - start;", "", "    std::cout << \"Time taken: \" << elapsed.count() << \" seconds\\n\";", "    std::cout << \"Number of solutions found: \" << solutions.size() << \"\\n\";", "", "    for (int i = 0; i < std::min(3, static_cast<int>(solutions.size())); ++i) {", "        std::cout << \"Solution \" << i + 1 << \":\\n\";", "        print_solution(solutions[i]);", "    }", "", "    return 0;", "}", "", "/*", "Time taken: 2.68e-05 seconds", "Number of solutions found: 92", "Solution 1:", "1 0 0 0 0 0 0 0", "0 0 0 0 1 0 0 0", "0 0 0 0 0 0 0 1", "0 0 0 0 0 1 0 0", "0 0 1 0 0 0 0 0", "0 0 0 0 0 0 1 0", "0 1 0 0 0 0 0 0", "0 0 0 1 0 0 0 0", "", "Solution 2:", "1 0 0 0 0 0 0 0", "0 0 0 0 0 1 0 0", "0 0 0 0 0 0 0 1", "0 0 1 0 0 0 0 0", "0 0 0 0 0 0 1 0", "0 0 0 1 0 0 0 0", "0 1 0 0 0 0 0 0", "0 0 0 0 1 0 0 0", "", "Solution 3:", "1 0 0 0 0 0 0 0", "0 0 0 0 0 0 1 0", "0 0 0 1 0 0 0 0", "0 0 0 0 0 1 0 0", "0 0 0 0 0 0 0 1", "0 1 0 0 0 0 0 0", "0 0 0 0 1 0 0 0", "0 0 1 0 0 0 0 0", "", "", "E:\\Documents and Settings\\Pablo\\My Documents\\My Sources\\Cpp8Queens\\cpp8Queens\\x64\\Release\\CopilotCpp8Qeens.exe (process 10280) exited with code 0.", "Press any key to close this window . . .", "*/"], "file_path": "CopilotCpp8Qeens/CopilotCpp8Qeens.cpp"}
{"Link_to_commit": "https://github.com/MirkoMilenkovic/QuickQuiz/commit/c3156d98a929ea805d4c5d72764fa274a2cf1a02", "n-gram matched": "generated by copilot", "n_lines_longer_change": 134, "n_files_impacted": 2, "longest_chunk": ["using System;", "using System.Collections.Generic;", "using System.Linq;", "using Microsoft.VisualStudio.TestTools.UnitTesting;", "using QuickQuiz.QuestionLogic.Model;", "using QuickQuiz.QuizLogic.Exceptions;", "using QuickQuiz.QuizLogic.Model;", "", "namespace QuickQuiz.Tests", "{", "    [TestClass]", "    public class QuizTests", "    {", "        [TestMethod]", "        public void CreateQuiz_ShouldInitializeQuizWithPlayerName()", "        {", "            // Arrange", "            string playerName = \"Test Player\";", "            var questions = new List<Question>", "            {", "                new Question(\"Question 1\"),", "                new Question(\"Question 2\")", "            };", "", "            // Act", "            Quiz quiz = Quiz.Create(playerName, questions);", "", "            // Assert", "            Assert.AreEqual(playerName, quiz.PlayerName);", "            Assert.IsNotNull(quiz.QuizId);", "            Assert.IsTrue(quiz.QuestionListReadOnly.Any());", "        }", "", "        [TestMethod]", "        public void GetNextQuestion_ShouldReturnNextUnansweredQuestion()", "        {", "            // Arrange", "            string playerName = \"Test Player\";", "            var questions = new List<Question>", "            {", "                new Question(\"Question 1\"),", "                new Question(\"Question 2\")", "            };", "            Quiz quiz = Quiz.Create(playerName, questions);", "", "            // Act", "            QuizQuestion nextQuestion = quiz.GetNextQuestion();", "", "            // Assert", "            Assert.IsNotNull(nextQuestion);", "            Assert.AreEqual(\"Question 1\", nextQuestion.OriginalQuestion.Text);", "        }", "", "        [TestMethod]", "        [ExpectedException(typeof(ActiveQuestionNotAnsweredException))]", "        public void GetNextQuestion_ShouldThrowExceptionIfActiveQuestionNotAnswered()", "        {", "            // Arrange", "            string playerName = \"Test Player\";", "            var questions = new List<Question>", "            {", "                new Question(\"Question 1\"),", "                new Question(\"Question 2\")", "            };", "            Quiz quiz = Quiz.Create(playerName, questions);", "            QuizQuestion nextQuestion = quiz.GetNextQuestion();", "", "            // Act", "            quiz.GetNextQuestion();", "        }", "", "        [TestMethod]", "        public void AnswerQuestion_ShouldReturnPlayersAnswer()", "        {", "            // Arrange", "            string playerName = \"Test Player\";", "            var questions = new List<Question>();", "", "            Question question = new Question(\"Question 1\");", "            questions.Add(question);", "            question.AddAnswer(new Answer(\"Answer 1\", true));", "            question.AddAnswer(new Answer(\"Answer 2\", false));", "", "            Quiz quiz = Quiz.Create(playerName, questions);", "            QuizQuestion nextQuestion = quiz.GetNextQuestion();", "", "            // Act", "            Answer answer = quiz.AnswerQuestion(nextQuestion.QuizQuestionId, nextQuestion.OriginalQuestion.Answers.First().AnswerId);", "", "            // Assert", "            Assert.IsNotNull(answer);", "            Assert.AreEqual(\"Answer 1\", answer.Text);", "        }", "", "        [TestMethod]", "        [ExpectedException(typeof(Exception), \"There is no Active Question\")]", "        public void AnswerQuestion_ShouldThrowExceptionIfNoActiveQuestion()", "        {", "            // Arrange", "            string playerName = \"Test Player\";", "            var questions = new List<Question>();", "", "            Question question = new Question(\"Question 1\");", "            questions.Add(question);", "            question.AddAnswer(new Answer(\"Answer 1\", true));", "            question.AddAnswer(new Answer(\"Answer 2\", false));", "", "            Quiz quiz = Quiz.Create(playerName, questions);", "", "            // Act", "            quiz.AnswerQuestion(\"invalidQuizQuestionId\", \"invalidAnswerId\");", "        }", "", "        [TestMethod]", "        [ExpectedException(typeof(Exception), \"You are not answering Active Question\")]", "        public void AnswerQuestion_ShouldThrowExceptionIfAnsweringNonActiveQuestion()", "        {", "            // Arrange", "            string playerName = \"Test Player\";", "            var questions = new List<Question>();", "", "            Question question = new Question(\"Question 1\");", "            questions.Add(question);", "            question.AddAnswer(new Answer(\"Answer 1\", true));", "            question.AddAnswer(new Answer(\"Answer 2\", false));", "", "            Quiz quiz = Quiz.Create(playerName, questions);", "            QuizQuestion nextQuestion = quiz.GetNextQuestion();", "", "            // Act", "            quiz.AnswerQuestion(\"invalidQuizQuestionId\", \"invalidAnswerId\");", "        }", "    }", "}"], "file_path": "QuickQuiz/QuickQuiz/QuizLogic/Model/QuizTests.cs"}
{"Link_to_commit": "https://github.com/paul356/agent-workspace/commit/eb91104c74d5354477325eb8b172f12135d6c9e2", "n-gram matched": "generated by copilot", "n_lines_longer_change": 15, "n_files_impacted": 10, "longest_chunk": ["<template>", "  <div>", "    <h1>Hello World</h1>", "  </div>", "</template>", "", "<script>", "export default {", "  name: 'MainView',", "};", "</script>", "", "<style scoped>", "/* Add any styles specific to MainView here */", "</style>"], "file_path": "vue.config.js"}
{"Link_to_commit": "https://github.com/Elkozel/WooODM/commit/883310ca61e772e072d329b5901884d2d8bd84db", "n-gram matched": "generated by copilot", "n_lines_longer_change": 103, "n_files_impacted": 1, "longest_chunk": ["import unittest", "import unittest", "from wooODM.products.category import Category", "from wooODM.core import WooCommerce", "", "class TestCategoryModel(unittest.TestCase):", "", "    @classmethod", "    def setUpClass(cls):", "        # Initialize WooCommerce API with dummy credentials for testing", "        WooCommerce.init(", "            url=\"\",", "            consumer_key=\"\",", "            consumer_secret=\"\"", "        )", "    @classmethod", "    def create_test_category(cls, name=\"Test Category\", slug=\"test-category\", description=\"A category for testing\"):", "        category = Category(", "            name=name,", "            slug=slug,", "            description=description", "        )", "        return category.save()", "", "    @classmethod", "    def delete_test_category(cls, category: Category):", "        return category.delete()", "", "    def test_create_category(self):", "        category = Category(", "            name=\"Test Category\",", "            slug=\"test-category\",", "            description=\"A category for testing\"", "        )", "        saved_category = category.save()", "        self.assertIsNotNone(saved_category.id)", "        self.assertEqual(saved_category.name, \"Test Category\")", "", "    def test_get_category(self):", "        test_category = TestCategoryModel.create_test_category(name=\"Get Test Category\", slug=\"get-test-category\")", "        category = Category.get(test_category.id)", "        self.assertEqual(category.id, test_category.id)", "        self.assertEqual(category, test_category)", "", "        test_category.delete()", "        with self.assertRaises(Exception):", "            Category.get(test_category.id)", "", "    def test_get_all_categories(self):", "        categories = Category.all(per_page=5, page=1)", "        self.assertIsInstance(categories, list)", "        self.assertGreaterEqual(len(categories), 1)", "        self.assertIsInstance(categories[0], Category)", "", "    def test_create_incomplete_category(self):", "        with self.assertRaises(ValueError):", "            Category(", "                slug=\"incomplete-category\"", "            ).save()", "", "    def test_create_category_without_slug(self):", "        with self.assertRaises(ValueError):", "            Category(", "                name=\"Category without Slug\"", "            ).save()", "", "    def test_create_category_with_invalid_data(self):", "        with self.assertRaises(ValueError):", "            Category(", "                name=\"Invalid Category\",", "                slug=\"invalid-category\",", "                description=123  # Invalid type for description", "            ).save()", "", "    def test_update_category_with_invalid_data(self):", "        category_id = 1  # Update with a valid category ID", "        category = Category.get(category_id)", "        category.description = 123  # Invalid type for description", "        with self.assertRaises(ValueError):", "            category.save()", "", "    def test_smoke(self):", "        # Create a category", "        category = TestCategoryModel.create_test_category(", "            name=\"Smoke Test Category\",", "            slug=\"smoke-test-category\",", "            description=\"A category for smoke testing\"", "        )", "        self.assertIsNotNone(category.id)", "        self.assertEqual(category.name, \"Smoke Test Category\")", "        self.assertEqual(category.description, \"A category for smoke testing\")", "", "        # Update the category", "        category.name = \"Updated Smoke Test Category\"", "        updated_category = category.save()", "        self.assertEqual(updated_category.name, \"Updated Smoke Test Category\")", "", "        # Delete the category", "        delete_response = updated_category.delete()", "        self.assertEqual(delete_response, updated_category)", "", "if __name__ == '__main__':", "    unittest.main()"], "file_path": "tests/wooODM/product/test_category.py"}
{"Link_to_commit": "https://github.com/coolsheets/takeStock/commit/5f90da0c46e6cd5072f35816e7aa011590cdcdb3", "n-gram matched": "generated by copilot", "n_lines_longer_change": 36, "n_files_impacted": 6, "longest_chunk": ["const express = require('express');", "const axios = require('axios');", "const router = express.Router();", "", "// Replace with your Alpha Vantage API key", "const API_KEY = 'YOUR_API_KEY';", "", "router.get('/:symbol', async (req, res) => {", "  const symbol = req.params.symbol;", "  const url = `https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=${symbol}&interval=5min&apikey=${API_KEY}`;", "", "  try {", "    const response = await axios.get(url);", "    const data = response.data;", "", "    if (data['Error Message']) {", "      return res.status(404).render('stock', { error: 'Stock symbol not found' });", "    }", "", "    const timeSeries = data['Time Series (5min)'];", "    const latestTime = Object.keys(timeSeries)[0];", "    const latestData = timeSeries[latestTime];", "", "    const stockInfo = {", "      symbol,", "      price: latestData['1. open'],", "      time: latestTime", "    };", "", "    res.render('stock', { stock: stockInfo });", "  } catch (error) {", "    res.status(500).render('stock', { error: 'Error fetching stock data' });", "  }", "});", "", "module.exports = router;"], "file_path": "routes/stock.js"}
{"Link_to_commit": "https://github.com/AnkiBhatia/copilot_tdd_experiment/commit/6e396755d92ae84d51751459a1bb4215a1338de3", "n-gram matched": "generated by copilot", "n_lines_longer_change": 107, "n_files_impacted": 16, "longest_chunk": ["# \ud83d\ude9c Smart Fleet Maintenance API", "", "## \ud83d\udccc Overview", "The **Smart Fleet Maintenance API** is designed to help track maintenance schedules, predict equipment failures, and log service records for agricultural machinery.", "", "## \ud83d\udd25 Features", "- **Register Equipment** \u2013 Store machine details (model, purchase date, usage hours).", "- **Maintenance Scheduler** \u2013 Predict maintenance needs based on usage.", "- **Service Logging** \u2013 Track service records and update machine status.", "- **Alerts & Notifications** \u2013 Notify users when maintenance is due.", "", "## \ud83c\udfd7\ufe0f Tech Stack", "- **Backend:** FastAPI (Python)", "- **Database:** SQLite / PostgreSQL", "- **Testing:** Pytest", "- **Data Validation:** Pydantic", "- **CI/CD:** GitHub Actions (for automated testing)", "", "## \ud83c\udfc1 Getting Started", "", "### **\ud83d\udd39 Prerequisites**", "Ensure you have the following installed:", "- Python 3.9+", "- pip", "- Virtual environment tool (venv or conda)", "", "### **\ud83d\udd39 Installation**", "```bash", "# Clone the repository", "git clone https://github.com/yourusername/smart-fleet-maintenance-api.git", "cd smart-fleet-maintenance-api", "", "# Set up a virtual environment", "python -m venv venv", "source venv/bin/activate  # On Windows: venv\\Scripts\\activate", "", "# Install dependencies", "pip install -r requirements.txt", "```", "", "### **\ud83d\udd39 Running the API**", "```bash", "uvicorn main:app --reload", "```", "API will be available at: `http://127.0.0.1:8000`", "", "### **\ud83d\udd39 Running Tests**", "```bash", "pytest tests/", "```", "", "## \ud83d\udcc2 Project Structure", "```", "smart-fleet-maintenance-api/", "\u2502\u2500\u2500 main.py          # Entry point for FastAPI application", "\u2502\u2500\u2500 database.py      # Database connection setup", "\u2502\u2500\u2500 requirements.txt # List of dependencies", "|\u2500\u2500 models/", "    |\u2500\u2500 models.py      # Pydantic models for request validation", "\u2502\u2500\u2500 tests/           # Pytest test cases", "\u2502\u2500\u2500 routers/         # API route handlers", "\u2502   \u251c\u2500\u2500 equipment.py", "\u2502   \u251c\u2500\u2500 maintenance.py", "\u2502\u2500\u2500 .github/workflows/ci.yml  # GitHub Actions for CI/CD", "\u2502\u2500\u2500 README.md        # Project documentation", "```", "", "## \ud83d\ude80 API Endpoints", "### 1\ufe0f\u20e3 Register Equipment", "**POST** `/equipment/`", "#### Request Body:", "```json", "{", "  \"name\": \"Tractor X\",", "  \"model\": \"TX-500\",", "  \"purchase_date\": \"2023-01-15\",", "  \"usage_hours\": 100", "}", "```", "#### Response:", "```json", "{", "  \"id\": \"UUID\",", "  \"name\": \"Tractor X\",", "  \"model\": \"TX-500\",", "  \"purchase_date\": \"2023-01-15\",", "  \"usage_hours\": 100", "}", "```", "", "### 2\ufe0f\u20e3 Get Equipment List", "**GET** `/equipment/`", "", "More endpoints will be added as features are implemented.", "", "## \ud83d\udccc Future Enhancements", "- **Machine Learning for Predictive Maintenance**", "- **Fleet Analytics Dashboard (Power BI)**", "- **Real-Time Alerts via WebSockets**", "", "---", "### \ud83d\udca1 Contributing", "Feel free to open an issue or submit a pull request.", "", "### \ud83d\udcdc License", "MIT License. See `LICENSE` for details.", ""], "file_path": "app/__init__.py"}
{"Link_to_commit": "https://github.com/depromeet/depromeet-makers-fe/commit/e2d541463d9e1388b2854e4f6f9bf4df37a029ea", "n-gram matched": "generated by copilot", "n_lines_longer_change": 44, "n_files_impacted": 9, "longest_chunk": ["import type { Dispatch, PropsWithChildren, SetStateAction } from 'react';", "import { createContext, useContext, useState } from 'react';", "", "export interface MarkerType {", "  id: string;", "  position: {", "    lat: number;", "    lng: number;", "  };", "  placeName: string;", "  addressName: string;", "}", "", "interface UsersContextType {", "  markers: MarkerType[];", "  setMarkers: Dispatch<SetStateAction<MarkerType[]>>;", "  selectedPlace?: MarkerType;", "  setSelectedPlace: Dispatch<SetStateAction<MarkerType | undefined>>;", "}", "", "const KaKaoMapContext = createContext<UsersContextType | null>(null);", "", "const KaKaoMapProvider = ({ children }: PropsWithChildren) => {", "  const [markers, setMarkers] = useState<MarkerType[]>([]);", "  const [selectedPlace, setSelectedPlace] = useState<MarkerType>();", "", "  return (", "    <KaKaoMapContext.Provider value={{ markers, setMarkers, selectedPlace, setSelectedPlace }}>", "      {children}", "    </KaKaoMapContext.Provider>", "  );", "};", "", "export const useKaKaoMap = () => {", "  const kaKaoMapContext = useContext(KaKaoMapContext);", "", "  if (!kaKaoMapContext) {", "    throw new Error('<kaKaoMapContext /> \ub0b4\ubd80\uc5d0\uc11c useKaKaoMap\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc5b4\uc694.');", "  }", "", "  return kaKaoMapContext;", "};", "", "export default KaKaoMapProvider;"], "file_path": "apps/admin/src/app/(admin)/session/(data)/session.ts"}
{"Link_to_commit": "https://github.com/depromeet/depromeet-makers-fe/commit/03dcbae7a9530ee260ae429c30cf198e0883854b", "n-gram matched": "generated by copilot", "n_lines_longer_change": 30, "n_files_impacted": 6, "longest_chunk": ["import type { UseMutationOptions } from '@tanstack/react-query';", "import { useMutation, useQueryClient } from '@tanstack/react-query';", "", "import type { CustomError } from '../base';", "import { api } from '../base';", "import type { Session } from '../types';", "", "interface DeleteSessionRequest extends Pick<Session, 'sessionId'> {}", "", "interface DeleteSessionResponse extends Session {}", "", "const deleteSession = ({ sessionId }: DeleteSessionRequest) => {", "  return api.delete<DeleteSessionResponse>(`/v1/sessions/${sessionId}`);", "};", "", "export const useDeleteSession = (", "  options?: UseMutationOptions<DeleteSessionResponse, CustomError, DeleteSessionRequest>,", ") => {", "  const queryClient = useQueryClient();", "", "  return useMutation({", "    mutationFn: ({ sessionId }: DeleteSessionRequest) => deleteSession({ sessionId }),", "    ...options,", "    onSuccess: (...params) => {", "      options?.onSuccess?.(...params);", "", "      queryClient.invalidateQueries({ queryKey: ['sessions'] });", "    },", "  });", "};"], "file_path": "packages/api/src/sessions/useDeleteSession.ts"}
{"Link_to_commit": "https://github.com/Axolotls7/Axolotls7/commit/32e57ed021a49360d53357f32650383944bcae39", "n-gram matched": "generated by copilot", "n_lines_longer_change": 13, "n_files_impacted": 1, "longest_chunk": ["for i in range(0,len(tape)):", "\ttry:", "\t\ttape[i] = int(tape[i])", "\texcept ValueError:", "\t\ttry:", "\t\t\tassert tape[i] in symbols", "\t\texcept AssertionError:", "\t\t\ttape[i] = \"A\"", "\telse:", "\t\ttry:", "\t\t\tassert tape[i] in symbols", "\t\texcept AssertionError:", "\t\t\ttape[i] = 0"], "file_path": "turingmachine.py"}
{"Link_to_commit": "https://github.com/mg3-codes/d-d-spell-finder/commit/8222734bd8a9ff1118e8dfa578a6515120f8ff5e", "n-gram matched": "generated by copilot", "n_lines_longer_change": 15, "n_files_impacted": 33, "longest_chunk": ["\t/**", "\t * Maps the value of the proficiency die to the corresponding array of `EdgeOfTheEmpireDiceSymbol` results.", "\t *", "\t * @returns {EdgeOfTheEmpireDiceSymbol[]} An array of `EdgeOfTheEmpireDiceSymbol` representing the result of the die roll.", "\t *", "\t * The mapping is as follows:", "\t * - 1: Blank", "\t * - 2, 3: Success", "\t * - 4, 5: Success, Success", "\t * - 6: Advantage", "\t * - 7, 8, 9: Success, Advantage", "\t * - 10, 11: Advantage, Advantage", "\t * - 12: Triumph", "\t * - Default: Empty array", "\t */"], "file_path": "src/classes/edge-of-the-empire-dice/proficiency-die.ts"}
{"Link_to_commit": "https://github.com/depromeet/depromeet-makers-fe/commit/c06284175a9ca0a7d14ecb120520f8a80e791952", "n-gram matched": "generated by copilot", "n_lines_longer_change": 27, "n_files_impacted": 5, "longest_chunk": ["import type { UseMutationOptions } from '@tanstack/react-query';", "import { useMutation } from '@tanstack/react-query';", "", "import type { CustomError } from '../base';", "import { api } from '../base';", "import { setAccessToken, setRefreshToken } from '../base/token';", "", "interface PostAuthTestResponse {", "  accessToken: string;", "  refreshToken: string;", "}", "", "const postAuthTest = () => {", "  return api.post<PostAuthTestResponse>('/v1/auth/test');", "};", "", "export const useAuthTest = (options?: UseMutationOptions<PostAuthTestResponse, CustomError>) =>", "  useMutation({", "    mutationFn: postAuthTest,", "    ...options,", "    onSuccess: async (data, ...params) => {", "      await setAccessToken(data.accessToken);", "      await setRefreshToken(data.refreshToken);", "", "      options?.onSuccess?.(data, ...params);", "    },", "  });"], "file_path": "packages/api/src/base/token.ts"}
{"Link_to_commit": "https://github.com/shinyay/cobol-sample-app/commit/5ec286b12bd10e8186aabc65ebb0d3bef996313a", "n-gram matched": "generated by copilot", "n_lines_longer_change": 17, "n_files_impacted": 1, "longest_chunk": ["/**", " * Encapsulates the logic for calculating bonuses based on gross salary.", " */", "public class BonusCalculator {", "    // Constants", "    private static final double BONUS_RATE = 0.10;", "", "    /**", "     * Calculates the bonus based on the given gross salary.", "     *", "     * @param grossSalary the gross salary of the employee", "     * @return the calculated bonus", "     */", "    public double calculateBonus(double grossSalary) {", "        return grossSalary * BONUS_RATE;", "    }", "}"], "file_path": "java/BonusCalculator.java"}
{"Link_to_commit": "https://github.com/shinyay/cobol-sample-app/commit/4cce4a13f3f12f369c7fcd3e65ae6c88faa2305d", "n-gram matched": "generated by copilot", "n_lines_longer_change": 121, "n_files_impacted": 1, "longest_chunk": ["import java.util.ArrayList;", "import java.util.Collections;", "import java.util.Comparator;", "import java.util.List;", "", "/**", " * Manages the overall payroll system, including initializing employees, sorting employees, calculating net salaries,", " * calculating department totals, and displaying information.", " */", "public class PayrollSystem {", "    // Constants", "    private static final int MAX_EMPLOYEES = 5;", "    private static final double TAX_RATE = 0.20;", "    private static final double BONUS_RATE = 0.10;", "    private static final double DEDUCTION_RATE = 0.05;", "", "    // Attributes", "    private List<Employee> employees = new ArrayList<>();", "    private List<DepartmentTotal> departmentTotals = new ArrayList<>();", "", "    /**", "     * Initializes the employee data with hardcoded values.", "     */", "    public void initializeEmployees() {", "        employees.add(new Employee(\"E001\", \"Alice Johnson\", \"HR\", 70000.00));", "        employees.add(new Employee(\"E002\", \"Bob Smith\", \"IT\", 85000.00));", "        employees.add(new Employee(\"E003\", \"Charlie Brown\", \"Finance\", 60000.00));", "        employees.add(new Employee(\"E004\", \"David Wilson\", \"IT\", 95000.00));", "        employees.add(new Employee(\"E005\", \"Eve Davis\", \"HR\", 75000.00));", "    }", "", "    /**", "     * Sorts the employees by their IDs.", "     */", "    public void sortEmployees() {", "        Collections.sort(employees, Comparator.comparing(Employee::getId));", "    }", "", "    /**", "     * Calculates the net salaries for all employees.", "     */", "    public void calculateNetSalaries() {", "        BonusCalculator bonusCalculator = new BonusCalculator();", "        for (Employee employee : employees) {", "            double bonus = bonusCalculator.calculateBonus(employee.getGrossSalary());", "            double deductions = employee.getGrossSalary() * DEDUCTION_RATE;", "            double taxDeduction = employee.getGrossSalary() * TAX_RATE;", "            double netSalary = employee.getGrossSalary() + bonus - taxDeduction - deductions;", "", "            employee.setBonus(bonus);", "            employee.setDeductions(deductions);", "            employee.setTaxDeduction(taxDeduction);", "            employee.setNetSalary(netSalary);", "        }", "    }", "", "    /**", "     * Calculates the total salaries for each department.", "     */", "    public void calculateDepartmentTotals() {", "        for (Employee employee : employees) {", "            DepartmentTotal departmentTotal = departmentTotals.stream()", "                .filter(dt -> dt.getDepartmentName().equals(employee.getDepartment()))", "                .findFirst()", "                .orElseGet(() -> {", "                    DepartmentTotal newDeptTotal = new DepartmentTotal(employee.getDepartment());", "                    departmentTotals.add(newDeptTotal);", "                    return newDeptTotal;", "                });", "", "            departmentTotal.setTotalSalary(departmentTotal.getTotalSalary() + employee.getNetSalary());", "        }", "    }", "", "    /**", "     * Displays the employee payroll information.", "     */", "    public void displayEmployees() {", "        System.out.println(\"Employee Payroll Information\");", "        System.out.println(\"-----------------------------\");", "        for (Employee employee : employees) {", "            System.out.println(\"Employee ID: \" + employee.getId());", "            System.out.println(\"Name: \" + employee.getName());", "            System.out.println(\"Department: \" + employee.getDepartment());", "            System.out.println(\"Gross Salary: $\" + employee.getGrossSalary());", "            System.out.println(\"Bonus: $\" + employee.getBonus());", "            System.out.println(\"Deductions: $\" + employee.getDeductions());", "            System.out.println(\"Tax Deduction: $\" + employee.getTaxDeduction());", "            System.out.println(\"Net Salary: $\" + employee.getNetSalary());", "            System.out.println(\"-----------------------------\");", "        }", "    }", "", "    /**", "     * Displays the department salary totals.", "     */", "    public void displayDepartmentTotals() {", "        System.out.println(\"Department Salary Totals\");", "        System.out.println(\"-----------------------------\");", "        for (DepartmentTotal departmentTotal : departmentTotals) {", "            System.out.println(\"Department: \" + departmentTotal.getDepartmentName());", "            System.out.println(\"Total Salary: $\" + departmentTotal.getTotalSalary());", "            System.out.println(\"-----------------------------\");", "        }", "    }", "", "    /**", "     * The main method to run the payroll system.", "     *", "     * @param args command-line arguments", "     */", "    public static void main(String[] args) {", "        PayrollSystem payrollSystem = new PayrollSystem();", "        payrollSystem.initializeEmployees();", "        payrollSystem.sortEmployees();", "        payrollSystem.calculateNetSalaries();", "        payrollSystem.calculateDepartmentTotals();", "        payrollSystem.displayEmployees();", "        payrollSystem.displayDepartmentTotals();", "    }", "}"], "file_path": "java/PayrollSystem.java"}
{"Link_to_commit": "https://github.com/shinyay/cobol-sample-app/commit/c0b86f798bfb3b061c1622c6e5b808b4ba6a1c92", "n-gram matched": "generated by copilot", "n_lines_longer_change": 102, "n_files_impacted": 1, "longest_chunk": ["package java;", "", "/**", " * Represents an employee with attributes like ID, name, department, gross salary, bonus, deductions, net salary, and tax deduction.", " */", "public class Employee {", "    // Attributes", "    private String id;", "    private String name;", "    private String department;", "    private double grossSalary;", "    private double bonus;", "    private double deductions;", "    private double netSalary;", "    private double taxDeduction;", "", "    /**", "     * Initializes the employee with the given ID, name, department, and gross salary.", "     * Other attributes are initialized to default values.", "     *", "     * @param id          the employee's ID", "     * @param name        the employee's name", "     * @param department  the department the employee belongs to", "     * @param grossSalary the employee's gross salary", "     */", "    public Employee(String id, String name, String department, double grossSalary) {", "        this.id = id;", "        this.name = name;", "        this.department = department;", "        this.grossSalary = grossSalary;", "        this.bonus = 0.0;", "        this.deductions = 0.0;", "        this.netSalary = 0.0;", "        this.taxDeduction = 0.0;", "    }", "", "    // Getters and setters for all attributes", "", "    public String getId() {", "        return id;", "    }", "", "    public void setId(String id) {", "        this.id = id;", "    }", "", "    public String getName() {", "        return name;", "    }", "", "    public void setName(String name) {", "        this.name = name;", "    }", "", "    public String getDepartment() {", "        return department;", "    }", "", "    public void setDepartment(String department) {", "        this.department = department;", "    }", "", "    public double getGrossSalary() {", "        return grossSalary;", "    }", "", "    public void setGrossSalary(double grossSalary) {", "        this.grossSalary = grossSalary;", "    }", "", "    public double getBonus() {", "        return bonus;", "    }", "", "    public void setBonus(double bonus) {", "        this.bonus = bonus;", "    }", "", "    public double getDeductions() {", "        return deductions;", "    }", "", "    public void setDeductions(double deductions) {", "        this.deductions = deductions;", "    }", "", "    public double getNetSalary() {", "        return netSalary;", "    }", "", "    public void setNetSalary(double netSalary) {", "        this.netSalary = netSalary;", "    }", "", "    public double getTaxDeduction() {", "        return taxDeduction;", "    }", "", "    public void setTaxDeduction(double taxDeduction) {", "        this.taxDeduction = taxDeduction;", "    }", "}"], "file_path": "multiple-file/java/Employee.java"}
{"Link_to_commit": "https://github.com/Cloud-Solutions-International/antikythera/commit/fca563f284f1a6eba21bffc2d78166ed325a3e42", "n-gram matched": "generated by copilot", "n_lines_longer_change": 59, "n_files_impacted": 1, "longest_chunk": ["", "", "class VariableInitializationModifierTest {", "", "    @Test", "    void shouldModifySimpleVariableInitialization() {", "        String code = \"\"\"", "            public void testMethod() {", "                String test = \"old\";", "                int other = 5;", "            }", "            \"\"\";", "        MethodDeclaration method = StaticJavaParser.parseMethodDeclaration(code);", "        StringLiteralExpr newValue = new StringLiteralExpr(\"new\");", "", "        VariableInitializationModifier modifier = new VariableInitializationModifier(\"test\", newValue);", "        MethodDeclaration result = (MethodDeclaration) modifier.visit(method, null);", "", "        assertTrue(result.toString().contains(\"String test = \\\"new\\\"\"));", "        assertTrue(result.toString().contains(\"int other = 5\"));", "    }", "", "    @Test", "    void shouldNotModifyWhenVariableNotFound() {", "        String code = \"\"\"", "            public void testMethod() {", "                String existingVar = \"old\";", "            }", "            \"\"\";", "        MethodDeclaration method = StaticJavaParser.parseMethodDeclaration(code);", "        IntegerLiteralExpr newValue = new IntegerLiteralExpr(\"42\");", "", "        VariableInitializationModifier modifier = new VariableInitializationModifier(\"nonexistentVar\", newValue);", "        MethodDeclaration result = (MethodDeclaration) modifier.visit(method, null);", "", "        assertEquals(method.toString(), result.toString());", "    }", "", "    @Test", "    void shouldModifyFirstOccurrenceOnly() {", "        String code = \"\"\"", "            public void testMethod() {", "                int target = 1;", "                String other = \"middle\";", "                int target = 3;", "            }", "            \"\"\";", "        MethodDeclaration method = StaticJavaParser.parseMethodDeclaration(code);", "        IntegerLiteralExpr newValue = new IntegerLiteralExpr(\"42\");", "", "        VariableInitializationModifier modifier = new VariableInitializationModifier(\"target\", newValue);", "        MethodDeclaration result = (MethodDeclaration) modifier.visit(method, null);", "", "        String modifiedCode = result.toString();", "        assertTrue(modifiedCode.contains(\"int target = 42\"));", "        assertTrue(modifiedCode.contains(\"int target = 3\"));", "        assertEquals(1, modifiedCode.split(\"42\").length - 1);", "    }", "}"], "file_path": "src/test/java/sa/com/cloudsolutions/antikythera/generator/UnitTestGeneratorTest.java"}
{"Link_to_commit": "https://github.com/Rabin-Kalikote/fast-html-todo-app/commit/53cb66f1aa75747ae2d2271829928a10dd780cca", "n-gram matched": "generated by copilot", "n_lines_longer_change": 56, "n_files_impacted": 14, "longest_chunk": ["// This file contains the JavaScript code for client-side interactions in the todo tracker app.", "", "document.addEventListener('DOMContentLoaded', function() {", "    const addTodoForm = document.getElementById('add-todo-form');", "    const todoList = document.getElementById('todo-list');", "", "    addTodoForm.addEventListener('submit', function(event) {", "        event.preventDefault();", "        const title = document.getElementById('todo-title').value;", "        const body = document.getElementById('todo-body').value;", "        const dueDate = document.getElementById('todo-due-date').value;", "", "        // Add new todo item", "        fetch('/todos', {", "            method: 'POST',", "            headers: {", "                'Content-Type': 'application/json'", "            },", "            body: JSON.stringify({ title, body, due_date: dueDate })", "        })", "        .then(response => response.json())", "        .then(data => {", "            // Append new todo item to the list", "            const todoItem = document.createElement('li');", "            todoItem.textContent = `${data.title} - ${data.body}`;", "            todoList.appendChild(todoItem);", "            addTodoForm.reset();", "        });", "    });", "", "    // Function to filter todos by completion status", "    document.getElementById('filter-completed').addEventListener('change', function() {", "        const showCompleted = this.checked;", "        const todos = todoList.getElementsByTagName('li');", "        for (let todo of todos) {", "            if (showCompleted && !todo.classList.contains('completed')) {", "                todo.style.display = 'none';", "            } else {", "                todo.style.display = 'list-item';", "            }", "        }", "    });", "", "    // Function to sort todos by creation date or due date", "    document.getElementById('sort-todos').addEventListener('change', function() {", "        const sortBy = this.value;", "        const todosArray = Array.from(todoList.getElementsByTagName('li'));", "        todosArray.sort((a, b) => {", "            const aDate = new Date(a.dataset.creationTime);", "            const bDate = new Date(b.dataset.creationTime);", "            return sortBy === 'due_date' ? aDate - bDate : bDate - aDate;", "        });", "        todoList.innerHTML = '';", "        todosArray.forEach(todo => todoList.appendChild(todo));", "    });", "});"], "file_path": "src/static/main.js"}
{"Link_to_commit": "https://github.com/jacekkowalczyk82/technology-startup/commit/e93cc5c58673f6867840b9e7dca69bcf976b3af5", "n-gram matched": "generated by copilot", "n_lines_longer_change": 79, "n_files_impacted": 3, "longest_chunk": ["package main", "", "import (", "    \"encoding/base64\"", "    \"fmt\"", "    \"io/ioutil\"", "    \"os\"", ")", "", "// EncodeFile encodes the content of the input file and writes it to the output file", "func EncodeFile(inputFile, outputFile string) error {", "    data, err := ioutil.ReadFile(inputFile)", "    if err != nil {", "        return fmt.Errorf(\"failed to read file: %w\", err)", "    }", "", "    encodedData := base64.StdEncoding.EncodeToString(data)", "", "    err = ioutil.WriteFile(outputFile, []byte(encodedData), 0644)", "    if err != nil {", "        return fmt.Errorf(\"failed to write file: %w\", err)", "    }", "", "    return nil", "}", "", "// DecodeFile decodes the base64 content of the input file and writes it to the output file", "func DecodeFile(inputFile, outputFile string) error {", "    data, err := ioutil.ReadFile(inputFile)", "    if err != nil {", "        return fmt.Errorf(\"failed to read file: %w\", err)", "    }", "", "    decodedData, err := base64.StdEncoding.DecodeString(string(data))", "    if err != nil {", "        return fmt.Errorf(\"failed to decode base64 data: %w\", err)", "    }", "", "    err = ioutil.WriteFile(outputFile, decodedData, 0644)", "    if err != nil {", "        return fmt.Errorf(\"failed to write file: %w\", err)", "    }", "", "    return nil", "}", "", "func ShowUsage() {", "    fmt.Println(\"Usage:\")", "    fmt.Println(\"  go-base64 encode <input file> <output file>\")", "    fmt.Println(\"  go-base64 decode <input file> <output file>\")", "}", "", "func main() {", "    if len(os.Args) < 4 {", "        ShowUsage()", "        return", "    }", "", "    command := os.Args[1]", "    inputFile := os.Args[2]", "    outputFile := os.Args[3]", "", "    var err error", "    switch command {", "    case \"encode\":", "        err = EncodeFile(inputFile, outputFile)", "    case \"decode\":", "        err = DecodeFile(inputFile, outputFile)", "    default:", "        ShowUsage()", "        return", "    }", "", "    if err != nil {", "        fmt.Printf(\"Error: %v\\n\", err)", "    } else {", "        fmt.Printf(\"Success: %s completed\\n\", command)", "    }", "}"], "file_path": "go-base64/go-base64.go"}
{"Link_to_commit": "https://github.com/alexgalkin/goal-tracking/commit/2083a2279ffd9f39593e6cc095edc51844200d51", "n-gram matched": "generated by copilot", "n_lines_longer_change": 20, "n_files_impacted": 11, "longest_chunk": ["import { initializeApp } from \"firebase/app\";", "import { getFirestore } from \"firebase/firestore\";", "import { getAuth } from \"firebase/auth\";", "import { getAnalytics } from \"firebase/analytics\";", "", "const firebaseConfig = {", "    apiKey: \"YOUR_API_KEY\",", "    authDomain: \"YOUR_PROJECT_ID.firebaseapp.com\",", "    projectId: \"YOUR_PROJECT_ID\",", "    storageBucket: \"YOUR_PROJECT_ID.appspot.com\",", "    messagingSenderId: \"YOUR_MESSAGING_SENDER_ID\",", "    appId: \"YOUR_APP_ID\"", "};", "", "const app = initializeApp(firebaseConfig);", "const db = getFirestore(app);", "const auth = getAuth(app);", "const analytics = getAnalytics(app);", "", "export { db, auth };"], "file_path": "bimbink-web-app/src/firebase.js"}
{"Link_to_commit": "https://github.com/zetaloop/desktop/commit/adc4bcc2305725673bb8622a6680377545b853ad", "n-gram matched": "generated by copilot", "n_lines_longer_change": 5, "n_files_impacted": 2, "longest_chunk": ["  /**", "   * Whether or not the message was generated by Copilot", "   * (optional, default: false)", "   */", "  readonly messageGeneratedByCopilot?: boolean"], "file_path": "app/src/models/commit.ts"}
{"Link_to_commit": "https://github.com/DCRT-LUMC/Tandem-Repeat-Domain-Database/commit/16d7318b1ef4c0c3e7affc150731341a51e5fb33", "n-gram matched": "generated by copilot", "n_lines_longer_change": 270, "n_files_impacted": 6, "longest_chunk": ["import json", "import sqlite3", "import os", "from tqdm import tqdm  # For progress bars", "", "def populate_test_database(json_file, db_file):", "    \"\"\"Populate SQLite database from the repeat domain JSON file\"\"\"", "    print(f\"Creating database from {json_file}...\")", "    ", "    # Connect to SQLite DB", "    conn = sqlite3.connect(db_file)", "    cursor = conn.cursor()", "    ", "    # Create tables using schema file", "    # Get the directory of the script to find the schema file", "    script_dir = os.path.dirname(os.path.abspath(__file__))", "    schema_file_path = os.path.join(script_dir, \"database_schema.sql\")", "    ", "    with open(schema_file_path, \"r\") as schema_file:", "        schema = schema_file.read()", "        conn.executescript(schema)", "    ", "    # Load JSON data", "    with open(json_file, \"r\") as f:", "        data = json.load(f)", "    ", "    # Track processed IDs to avoid duplicates", "    processed_genes = {}", "    processed_proteins = {}", "    processed_transcripts = {}", "    processed_exons = {}", "", "    # Process each repeat entry", "    for repeat in tqdm(data, desc=\"Processing repeats\"):", "        if not isinstance(repeat, dict) or not repeat:", "            continue  # Skip empty entries", "        ", "        # Get gene info", "        gene_name = repeat.get(\"geneName\", \"\")", "        if not gene_name:", "            continue", "        ", "        # Insert gene if not already processed", "        if gene_name not in processed_genes:", "            aliases = repeat.get(\"aliases\", \"\")", "            if isinstance(aliases, list):", "                aliases = \",\".join(aliases)", "            ", "            cursor.execute(\"\"\"", "                INSERT INTO genes (gene_name, aliases, chromosome, location)", "                VALUES (?, ?, ?, ?)", "            \"\"\", (", "                gene_name,", "                aliases,", "                repeat.get(\"chrom\", \"\"),", "                f\"{repeat.get('chrom', '')}:{repeat.get('chromStart', '')}_{repeat.get('chromEnd', '')}\"", "            ))", "            processed_genes[gene_name] = cursor.lastrowid", "        ", "        gene_id = processed_genes[gene_name]", "        ", "        # Process protein", "        uniprot_id = repeat.get(\"uniProtId\", \"\")", "        if uniprot_id and uniprot_id not in processed_proteins:", "            status = repeat.get(\"status\", \"\")", "            ", "            # Extract length from position if possible: \"amino acids 343-389 on protein Q6TDP4\"", "            position = repeat.get(\"position\", \"\")", "            length = 0", "            if position and isinstance(position, str):", "                try:", "                    parts = position.split()", "                    if len(parts) >= 3:", "                        pos = parts[2].split(\"-\")", "                        if len(pos) == 2:", "                            length = int(pos[1]) - int(pos[0]) + 1", "                except:", "                    pass", "            ", "            cursor.execute(\"\"\"", "                INSERT INTO proteins (protein_id, gene_id, length, description, status)", "                VALUES (?, ?, ?, ?, ?)", "            \"\"\", (", "                uniprot_id,", "                gene_id,", "                length,", "                f\"Protein for {gene_name}\",", "                status", "            ))", "            processed_proteins[uniprot_id] = uniprot_id", "        ", "        # Process repeat domain", "        amino_start = None", "        amino_end = None", "        if repeat.get(\"position\") and isinstance(repeat.get(\"position\"), str):", "            position = repeat.get(\"position\")", "            try:", "                # Extract positions from \"amino acids 343-389 on protein Q6TDP4\"", "                parts = position.split()", "                if len(parts) >= 3:", "                    pos = parts[2].split(\"-\")", "                    if len(pos) == 2:", "                        amino_start = int(pos[0])", "                        amino_end = int(pos[1])", "            except:", "                pass", "        ", "        # Calculate sequence length from blockSizes", "        sequence_length = 0", "        block_sizes = repeat.get(\"blockSizes\", [])", "        if isinstance(block_sizes, list):", "            for size in block_sizes:", "                try:", "                    sequence_length += int(size)", "                except:", "                    pass", "        ", "        # Insert repeat", "        cursor.execute(\"\"\"", "            INSERT INTO repeats (protein_id, repeat_type, start_pos, end_pos, sequence)", "            VALUES (?, ?, ?, ?, ?)", "        \"\"\", (", "            uniprot_id,", "            repeat.get(\"repeatType\", \"\"),", "            amino_start,", "            amino_end,", "            \"N\" * sequence_length  # Placeholder sequence of Ns", "        ))", "        repeat_id = cursor.lastrowid", "        ", "        # Process exon information if available", "        if \"ensembl_exon_info\" not in repeat:", "            continue", "            ", "        exon_info = repeat.get(\"ensembl_exon_info\", {})", "        if not exon_info or \"transcripts\" not in exon_info:", "            continue", "            ", "        # Process each transcript", "        for transcript_data in exon_info.get(\"transcripts\", []):", "            transcript_id = transcript_data.get(\"transcript_id\")", "            if not transcript_id:", "                continue", "                ", "            # Insert transcript if not already processed", "            if transcript_id not in processed_transcripts:", "                cursor.execute(\"\"\"", "                    INSERT INTO transcripts (transcript_id, gene_id, description)", "                    VALUES (?, ?, ?)", "                \"\"\", (", "                    transcript_id,", "                    gene_id,", "                    f\"{transcript_data.get('transcript_name', '')} ({transcript_data.get('biotype', '')})\"", "                ))", "                processed_transcripts[transcript_id] = transcript_id", "            ", "            # Create repeat_transcript relationship", "            genomic_start = repeat.get(\"chromStart\", 0)", "            genomic_end = repeat.get(\"chromEnd\", 0)", "            ", "            # Convert exon mapping to JSON string", "            exon_mapping = json.dumps([", "                {", "                    \"exon_id\": exon.get(\"exon_id\", \"\"),", "                    \"exon_number\": exon.get(\"exon_number\", 0),", "                    \"overlap_bp\": exon.get(\"overlap_bp\", 0),", "                    \"overlap_percentage\": exon.get(\"overlap_percentage\", 0),", "                    \"coding_percentage\": exon.get(\"coding_percentage\", 0)", "                } for exon in transcript_data.get(\"containing_exons\", [])", "            ])", "            ", "            cursor.execute(\"\"\"", "                INSERT INTO repeat_transcripts (", "                    repeat_id, transcript_id, genomic_start, genomic_end, exon_mapping", "                ) VALUES (?, ?, ?, ?, ?)", "            \"\"\", (", "                repeat_id, ", "                transcript_id, ", "                genomic_start, ", "                genomic_end, ", "                exon_mapping", "            ))", "            ", "            # Process exons in this transcript", "            for exon_data in transcript_data.get(\"containing_exons\", []):", "                exon_id = exon_data.get(\"exon_id\")", "                if not exon_id or exon_id in processed_exons:", "                    continue", "                    ", "                # Estimate exon size from overlap percentage", "                exon_size = 0", "                if exon_data.get(\"overlap_percentage\") and exon_data.get(\"overlap_bp\"):", "                    try:", "                        exon_size = int(exon_data.get(\"overlap_bp\") * 100 / exon_data.get(\"overlap_percentage\"))", "                    except:", "                        pass", "                ", "                # Calculate if skipping would preserve reading frame", "                frame_preserving = exon_size % 3 == 0", "                ", "                cursor.execute(\"\"\"", "                    INSERT INTO exons (", "                        exon_id, gene_id, length, frame_preserving", "                    ) VALUES (?, ?, ?, ?)", "                \"\"\", (", "                    exon_id,", "                    gene_id,", "                    exon_size,", "                    frame_preserving", "                ))", "                processed_exons[exon_id] = exon_id", "                ", "                # Create transcript_exon relationship", "                cursor.execute(\"\"\"", "                    INSERT INTO transcript_exons (", "                        transcript_id, exon_id, exon_number", "                    ) VALUES (?, ?, ?)", "                \"\"\", (", "                    transcript_id,", "                    exon_id,", "                    exon_data.get(\"exon_number\", 0)", "                ))", "                ", "                # Create repeat_exon relationship", "                cursor.execute(\"\"\"", "                    INSERT INTO repeat_exons (", "                        repeat_id, exon_id, overlap_bp, overlap_percentage", "                    ) VALUES (?, ?, ?, ?)", "                \"\"\", (", "                    repeat_id,", "                    exon_id,", "                    exon_data.get(\"overlap_bp\", 0),", "                    exon_data.get(\"overlap_percentage\", 0)", "                ))", "    ", "    # Commit all changes", "    conn.commit()", "    ", "    # Print some statistics", "    cursor.execute(\"SELECT COUNT(*) FROM genes\")", "    gene_count = cursor.fetchone()[0]", "    ", "    cursor.execute(\"SELECT COUNT(*) FROM proteins\")", "    protein_count = cursor.fetchone()[0]", "    ", "    cursor.execute(\"SELECT COUNT(*) FROM repeats\")", "    repeat_count = cursor.fetchone()[0]", "    ", "    cursor.execute(\"SELECT COUNT(*) FROM exons\")", "    exon_count = cursor.fetchone()[0]", "    ", "    print(f\"\\nDatabase populated successfully:\")", "    print(f\"  - {gene_count} genes\")", "    print(f\"  - {protein_count} proteins\")", "    print(f\"  - {repeat_count} repeat domains\")", "    print(f\"  - {exon_count} exons\")", "    ", "    conn.close()", "    ", "    print(f\"\\nDatabase created at: {db_file}\")", "", "if __name__ == \"__main__\":", "    # Define file paths - simplified to use the same directory as the script", "    script_dir = os.path.dirname(os.path.abspath(__file__))", "    ", "    json_file = os.path.join(script_dir, \"1000_test_exons_hg38_repeats.json\")", "    db_file = os.path.join(script_dir, \"tandem_repeats.db\")", "    ", "    # Create the database", "    populate_test_database(json_file, db_file)"], "file_path": "test_sqlite/populate_database.py"}
{"Link_to_commit": "https://github.com/goatstone/aaarto_backend/commit/0f7bd6630dcaaf8e6a8bd80e79bbff8239936463", "n-gram matched": "generated by copilot", "n_lines_longer_change": 28, "n_files_impacted": 4, "longest_chunk": ["/** ", " * Use environment variables to provide arguments ", " * export contractAddress=\"\"", " * export spender=\"\"", " * export amount=\"\"", " * Run the script with hardhat run ", " * npx hardhat run scripts/approveTokens.ts --network sepolia ", " */", "import { ethers } from \"hardhat\";", "", "async function approveTokens(contractAddress: string, spender: string, amount: string) {", "  const GLDToken = await ethers.getContractFactory(\"GLDToken\");", "  const token = GLDToken.attach(contractAddress);", "", "  const tx = await token.approve(spender, ethers.utils.parseUnits(amount, 18));", "  console.log(`Approved ${amount} GLD for ${spender}. Transaction hash: ${tx.hash}`);", "}", "", "const contractAddress = process.env.contractAddress as string;", "const spender = process.env.spender as string;", "const amount = process.env.amount as string;", "", "approveTokens(contractAddress, spender, amount)", "  .then(() => process.exit(0))", "  .catch((error) => {", "    console.error(error);", "    process.exit(1);", "  });"], "file_path": "scripts/checkAllowance.ts"}
{"Link_to_commit": "https://github.com/zereight/confluence-mcp/commit/a424d7c23b831ee020be5a33eb1f5fc678ff2dbc", "n-gram matched": "generated by copilot", "n_lines_longer_change": 36, "n_files_impacted": 7, "longest_chunk": ["{", "  \"name\": \"@zereight/mcp-confluence\",", "  \"version\": \"1.0.1\",", "  \"description\": \"MCP server for using the Confluence API\",", "  \"license\": \"MIT\",", "  \"author\": \"zereight\",", "  \"type\": \"module\",", "  \"private\": false,", "  \"bin\": \"./build/index.js\",", "  \"files\": [", "    \"build\"", "  ],", "  \"publishConfig\": {", "    \"access\": \"public\"", "  },", "  \"engines\": {", "    \"node\": \">=14\"", "  },", "  \"scripts\": {", "    \"build\": \"tsc && node -e \\\"require('fs').chmodSync('build/index.js', '755')\\\"\",", "    \"prepare\": \"npm run build\",", "    \"watch\": \"tsc --watch\",", "    \"inspector\": \"npx @modelcontextprotocol/inspector build/index.js\",", "    \"start\": \"node build/index.js\"", "  },", "  \"dependencies\": {", "    \"@modelcontextprotocol/sdk\": \"0.6.0\",", "    \"axios\": \"^1.7.9\",", "    \"mcp-framework\": \"^0.1.12\",", "    \"okhttp\": \"^1.1.0\"", "  },", "  \"devDependencies\": {", "    \"@types/node\": \"^20.11.24\",", "    \"typescript\": \"^5.7.2\"", "  }", "}"], "file_path": "src/index.ts"}
{"Link_to_commit": "https://github.com/Byloth/core/commit/c15edeb04b9f3f2b26c474c1a75eba2b0e0bb07d", "n-gram matched": "generated by copilot", "n_lines_longer_change": 53, "n_files_impacted": 2, "longest_chunk": ["import { describe, it, expect } from \"vitest\";", "", "import { SmartIterator, ValueException } from \"../../src/index.js\";", "import { Curve } from \"../../src/index.js\";", "", "describe(\"Curve\", () =>", "{", "    describe(\"Linear\", () =>", "    {", "        it(\"Should return an instance of `SmartIterator`\", () =>", "        {", "            const iterator = Curve.Linear(5);", "", "            expect(iterator).toBeInstanceOf(SmartIterator);", "        });", "        it(\"Should generate a linear sequence of values\", () =>", "        {", "            const values = Array.from(Curve.Linear(5));", "", "            expect(values).toEqual([0, 0.25, 0.5, 0.75, 1]);", "        });", "    });", "", "    describe(\"Exponential\", () =>", "    {", "        it(\"Should return an instance of `SmartIterator`\", () =>", "        {", "            const iterator = Curve.Exponential(6);", "", "            expect(iterator).toBeInstanceOf(SmartIterator);", "        });", "", "        it(\"Should generate an exponential sequence of values with default base\", () =>", "        {", "            const values = Array.from(Curve.Exponential(6));", "", "            expect(values).toEqual([0, 0.04000000000000001, 0.16000000000000003, 0.36, 0.6400000000000001, 1]);", "        });", "        it(\"Should generate an exponential sequence of values with custom base\", () =>", "        {", "            const values = Array.from(Curve.Exponential(6, 3));", "", "            expect(values).toEqual(", "                [0, Math.pow(1 / 5, 3), Math.pow(2 / 5, 3), Math.pow(3 / 5, 3), Math.pow(4 / 5, 3), 1]", "            );", "        });", "", "        it(\"Should throw a `ValueException` if base is negative\", () =>", "        {", "            expect(() => Curve.Exponential(6, -1)).toThrow(ValueException);", "        });", "    });", "});"], "file_path": "tests/utils/date.test.ts"}
{"Link_to_commit": "https://github.com/mg3-codes/d-d-spell-finder/commit/48e7987d31802f7be98db844892af2fd9e69169d", "n-gram matched": "generated by copilot", "n_lines_longer_change": 15, "n_files_impacted": 33, "longest_chunk": ["\t/**", "\t * Maps the value of the proficiency die to the corresponding array of `EdgeOfTheEmpireDiceSymbol` results.", "\t *", "\t * @returns {EdgeOfTheEmpireDiceSymbol[]} An array of `EdgeOfTheEmpireDiceSymbol` representing the result of the die roll.", "\t *", "\t * The mapping is as follows:", "\t * - 1: Blank", "\t * - 2, 3: Success", "\t * - 4, 5: Success, Success", "\t * - 6: Advantage", "\t * - 7, 8, 9: Success, Advantage", "\t * - 10, 11: Advantage, Advantage", "\t * - 12: Triumph", "\t * - Default: Empty array", "\t */"], "file_path": "src/classes/edge-of-the-empire-dice/proficiency-die.ts"}
{"Link_to_commit": "https://github.com/j2willey/GiftExchangeTimer/commit/a2ce3941a6bda7ac3e0bbd079aa08e6afd322b05", "n-gram matched": "generated by copilot", "n_lines_longer_change": 54, "n_files_impacted": 5, "longest_chunk": ["document.getElementById('createTimers').addEventListener('click', function() {", "    const numTimers = parseInt(document.getElementById('numTimers').value);", "    const defaultTime = parseInt(document.getElementById('defaultTime').value);", "    const timersContainer = document.getElementById('timersContainer');", "    timersContainer.innerHTML = '';", "", "    for (let i = 1; i <= numTimers; i++) {", "        const timerDiv = document.createElement('div');", "        timerDiv.className = 'timer';", "        timerDiv.innerHTML = `", "            <input type=\"text\" value=\"${i}\">", "            <div class=\"time\">${defaultTime}</div>", "            <div class=\"buttons\">", "                <button class=\"start\">Start</button>", "                <button class=\"pause\">Pause</button>", "                <button class=\"reset\">Reset</button>", "            </div>", "        `;", "        timersContainer.appendChild(timerDiv);", "", "        const timeDisplay = timerDiv.querySelector('.time');", "        let timeLeft = defaultTime;", "        let interval;", "", "        timerDiv.querySelector('.start').addEventListener('click', function() {", "            if (interval) return;", "            interval = setInterval(() => {", "                if (timeLeft > 0) {", "                    timeLeft--;", "                    timeDisplay.textContent = timeLeft;", "                    if (timeLeft <= 10) {", "                        timeDisplay.classList.add('warning');", "                    }", "                } else {", "                    clearInterval(interval);", "                    timeDisplay.textContent = 'Times Up!';", "                }", "            }, 1000);", "        });", "", "        timerDiv.querySelector('.pause').addEventListener('click', function() {", "            clearInterval(interval);", "            interval = null;", "        });", "", "        timerDiv.querySelector('.reset').addEventListener('click', function() {", "            clearInterval(interval);", "            interval = null;", "            timeLeft = defaultTime;", "            timeDisplay.textContent = timeLeft;", "            timeDisplay.classList.remove('warning');", "        });", "    }", "});"], "file_path": "js/timers.js"}
{"Link_to_commit": "https://github.com/kimjeffsj/empcon_backend/commit/6277914de02416970a0724fabbf7cb6686fe4e18", "n-gram matched": "generated by copilot", "n_lines_longer_change": 11, "n_files_impacted": 1, "longest_chunk": ["", "    req.user = {", "      userId: decoded.userId,", "      role: decoded.role,", "    };", "", "    next();", "  } catch (error) {", "    logger.error(\"JWT verification failed\", { error });", "    return next(new UnauthorizedError(\"Invalid or expired token\"));", "  }"], "file_path": "src/common/middleware/auth.middleware.ts"}
{"Link_to_commit": "https://github.com/yeohwk/calculator/commit/6bb72c18063d9e6c6618b635b123d81e2e6c6897", "n-gram matched": "generated by copilot", "n_lines_longer_change": 11, "n_files_impacted": 1, "longest_chunk": ["import numpy as np", "", "def generate_hilbert_matrix(size):", "    \"\"\"Generates a Hilbert matrix of given size.\"\"\"", "    hilbert_matrix = np.array([[1 / (i + j + 1) for j in range(size)] for i in range(size)])", "    return hilbert_matrix", "", "if __name__ == \"__main__\":", "    size = 12", "    hilbert_matrix = generate_hilbert_matrix(size)", "    print(hilbert_matrix)"], "file_path": "hilbert12.py"}
{"Link_to_commit": "https://github.com/yeohwk/calculator/commit/0f23585f11a4482a8b65802a8802c9d5f8d473c5", "n-gram matched": "generated by copilot", "n_lines_longer_change": 65, "n_files_impacted": 1, "longest_chunk": ["import math", "", "def text_based_calculator():", "    while True:", "        print(\"\\nSelect operation:\")", "        print(\"1. Addition (+)\")", "        print(\"2. Subtraction (-)\")", "        print(\"3. Multiplication (*)\")", "        print(\"4. Division (/)\")", "        print(\"5. Trigonometry (sin, cos, tan)\")", "        print(\"6. Square Root (sqrt)\")", "        print(\"7. Square (sq)\")", "        print(\"8. Exponential (exp)\")", "        print(\"9. Exit\")", "", "        choice = input(\"Enter choice: \")", "", "        if choice == '9':", "            break", "", "        if choice in ['1', '2', '3', '4']:", "            num1 = float(input(\"Enter first number: \"))", "            num2 = float(input(\"Enter second number: \"))", "", "            if choice == '1':", "                print(\"Result:\", num1 + num2)", "            elif choice == '2':", "                print(\"Result:\", num1 - num2)", "            elif choice == '3':", "                print(\"Result:\", num1 * num2)", "            elif choice == '4':", "                if num2 != 0:", "                    print(\"Result:\", num1 / num2)", "                else:", "                    print(\"Error: Division by zero\")", "", "        elif choice in ['5']:", "            trig_operation = input(\"Enter trigonometric function (sin, cos, tan): \")", "            angle = float(input(\"Enter angle in radians: \"))", "            if trig_operation == 'sin':", "                print(\"Result:\", math.sin(angle))", "            elif trig_operation == 'cos':", "                print(\"Result:\", math.cos(angle))", "            elif trig_operation == 'tan':", "                print(\"Result:\", math.tan(angle))", "            else:", "                print(\"Invalid trigonometric function\")", "", "        elif choice == '6':", "            num = float(input(\"Enter number: \"))", "            print(\"Result:\", math.sqrt(num))", "", "        elif choice == '7':", "            num = float(input(\"Enter number: \"))", "            print(\"Result:\", num * num)", "", "        elif choice == '8':", "            num = float(input(\"Enter number: \"))", "            print(\"Result:\", math.exp(num))", "", "        else:", "            print(\"Invalid choice\")", "", "if __name__ == \"__main__\":", "    text_based_calculator()"], "file_path": "calc2.py"}
{"Link_to_commit": "https://github.com/jaluebbe/GPSTracker/commit/53b89f640b9b4b4c3bc713ce9bd977a1b97ecd3f", "n-gram matched": "generated by copilot", "n_lines_longer_change": 16, "n_files_impacted": 1, "longest_chunk": ["        ['e2', 8], ['e2', 8], ['p', 8], ['e2', 8], ['p', 8], ['c2', 8], ['e2', 8],", "        ['g2', 4], ['p', 4], ['g1', 4], ['p', 4],", "        ['c2', 4], ['p', 8], ['g1', 8], ['g1', 8], ['p', 8], ['e1', 4],", "        ['p', 8], ['a1', 4], ['h1', 8], ['h1', 8], ['ais1', 8], ['ais1', 4],", "        ['g1', 6], ['e2', 6], ['g2', 6], ['a2', 4], ['f2', 8], ['g2', 8],", "        ['p', 8], ['e2', 4], ['c2', 8], ['d2', 8], ['h1', 4], ['p', 8],", "        ['c2', 4], ['p', 8], ['g1', 8], ['g1', 8], ['p', 8], ['e1', 4],", "        ['p', 4], ['a1', 4], ['h1', 8], ['h1', 8], ['ais1', 8], ['ais1', 4],", "        ['g1', 6], ['e2', 6], ['g2', 6], ['a2', 4], ['f2', 8], ['g2', 8],", "        ['p', 8], ['e2', 4], ['c2', 8], ['d2', 8], ['h1', 4], ['p', 8],", "        ['p', 4], ['g2', 8], ['fis2', 8], ['fis2', 8], ['dis2', 4], ['e2', 8],", "        ['p', 8], ['gis1', 8], ['a1', 8], ['c2', 8], ['p', 8], ['a1', 8], ['c2', 8], ['d2', 8],", "        ['p', 4], ['g2', 8], ['fis2', 8], ['fis2', 8], ['dis2', 4], ['e2', 8],", "        ['p', 8], ['c3', 4], ['c3', 8], ['c3', 4], ['p', 4],", "        ['p', 4], ['g2', 8], ['fis2', 8], ['fis2', 8], ['dis2', 4], ['e2', 8],", "        ['p', 8], ['gis1', 8], ['a1', 8], ['c2', 8], ['p', 8], ['a1', 8], ['c2', 8], ['d2', 8],"], "file_path": "static/music.js"}
{"Link_to_commit": "https://github.com/ajaysskumar/dev-arena/commit/cfdeba944cd1abe1b31e871226ab444b16729d8c", "n-gram matched": "generated by copilot", "n_lines_longer_change": 5, "n_files_impacted": 2, "longest_chunk": ["        new(\"Understanding the Single Responsibility Principle\",", "            \"/blog/software-practices-solid-srp\",", "            new DateTime(2025, 5, 10),", "            \"images/blog/software-practices/solid-srp/banner.png\",", "            [\"Software Practices\", \"SOLID\", \"SRP\"], false),"], "file_path": "TestArena/Blog/Common/NavigationUtils/SiteMap.cs"}
{"Link_to_commit": "https://github.com/kimjeffsj/starclone-backend/commit/7a95c1a723bde3e0b2edf7e373b953edd2962550", "n-gram matched": "generated by copilot", "n_lines_longer_change": 144, "n_files_impacted": 3, "longest_chunk": ["import { NextFunction, Request, Response } from \"express\";", "import { FollowService } from \"../services/follow.service\";", "import { followUserSchema } from \"../validations/follow.schema\";", "import { UnauthorizedError, ValidationError } from \"@/utils/errors.utils\";", "", "export class FollowController {", "  private followService = new FollowService();", "", "  /**", "   * Follow a user", "   */", "  followUser = async (req: Request, res: Response, next: NextFunction) => {", "    try {", "      if (!req.user || !req.user.id) {", "        throw new UnauthorizedError(\"Not authenticated\");", "      }", "", "      const validationResult = followUserSchema.safeParse(req.body);", "      if (!validationResult.success) {", "        throw new ValidationError(validationResult.error.format());", "      }", "", "      const result = await this.followService.followUser(", "        req.user.id,", "        validationResult.data.username", "      );", "", "      res.status(200).json({", "        message: \"Successfully followed user\",", "        ...result,", "      });", "    } catch (error) {", "      next(error);", "    }", "  };", "", "  /**", "   * Unfollow a user", "   */", "  unfollowUser = async (req: Request, res: Response, next: NextFunction) => {", "    try {", "      if (!req.user || !req.user.id) {", "        throw new UnauthorizedError(\"Not authenticated\");", "      }", "", "      const { username } = req.params;", "", "      const result = await this.followService.unfollowUser(", "        req.user.id,", "        username", "      );", "", "      res.status(200).json({", "        message: \"Successfully unfollowed user\",", "        ...result,", "      });", "    } catch (error) {", "      next(error);", "    }", "  };", "", "  /**", "   * Check follow status", "   */", "  checkFollowStatus = async (", "    req: Request,", "    res: Response,", "    next: NextFunction", "  ) => {", "    try {", "      if (!req.user || !req.user.id) {", "        throw new UnauthorizedError(\"Not authenticated\");", "      }", "", "      const { username } = req.params;", "", "      const result = await this.followService.checkFollowStatus(", "        req.user.id,", "        username", "      );", "", "      res.status(200).json(result);", "    } catch (error) {", "      next(error);", "    }", "  };", "", "  /**", "   * Get followers of a user", "   */", "  getFollowers = async (req: Request, res: Response, next: NextFunction) => {", "    try {", "      const { username } = req.params;", "      const page = req.query.page ? parseInt(req.query.page as string) : 1;", "      const limit = req.query.limit ? parseInt(req.query.limit as string) : 20;", "", "      const result = await this.followService.getFollowers(", "        username,", "        page,", "        limit", "      );", "", "      res.status(200).json(result);", "    } catch (error) {", "      next(error);", "    }", "  };", "", "  /**", "   * Get users that a user is following", "   */", "  getFollowing = async (req: Request, res: Response, next: NextFunction) => {", "    try {", "      const { username } = req.params;", "      const page = req.query.page ? parseInt(req.query.page as string) : 1;", "      const limit = req.query.limit ? parseInt(req.query.limit as string) : 20;", "", "      const result = await this.followService.getFollowing(", "        username,", "        page,", "        limit", "      );", "", "      res.status(200).json(result);", "    } catch (error) {", "      next(error);", "    }", "  };", "", "  /**", "   * Get follow counts", "   */", "  getFollowCounts = async (req: Request, res: Response, next: NextFunction) => {", "    try {", "      const { username } = req.params;", "", "      const result = await this.followService.getFollowCounts(username);", "", "      res.status(200).json(result);", "    } catch (error) {", "      next(error);", "    }", "  };", "}"], "file_path": "src/features/follow/index.ts"}
{"Link_to_commit": "https://github.com/kimjeffsj/starclone-backend/commit/417e320954c74f1f0c44eb36760f0f138c1ebf86", "n-gram matched": "generated by copilot", "n_lines_longer_change": 200, "n_files_impacted": 4, "longest_chunk": ["import { AppDataSource } from \"@/config/database\";", "import { Follow } from \"@/entities/Follow.entity\";", "import { User } from \"@/entities/User.entity\";", "import { ForbiddenError, NotFoundError } from \"@/utils/errors.utils\";", "", "export class FollowService {", "  private followRepository = AppDataSource.getRepository(Follow);", "  private userRepository = AppDataSource.getRepository(User);", "", "  /**", "   * Follow user", "   */", "  async followUser(followerUserId: string, usernameToFollow: string) {", "    // Find follower", "    const follower = await this.userRepository.findOneBy({", "      id: followerUserId,", "    });", "    if (!follower) {", "      throw new NotFoundError(\"User not found\");", "    }", "", "    // Find user to follow", "    const userToFollow = await this.userRepository.findOneBy({", "      username: usernameToFollow,", "    });", "    if (!userToFollow) {", "      throw new NotFoundError(\"User to follow not found\");", "    }", "", "    // Cannot follow yourself", "    if (follower.id === userToFollow.id) {", "      throw new ForbiddenError(\"You cannot follow yourself\");", "    }", "", "    // Check if already following", "    const existingFollow = await this.followRepository.findOne({", "      where: {", "        follower: { id: followerUserId },", "        following: { id: userToFollow.id },", "      },", "    });", "", "    if (existingFollow) {", "      throw new ForbiddenError(\"You are already following this user\");", "    }", "", "    // Create follow relation", "    const follow = this.followRepository.create({", "      follower,", "      following: userToFollow,", "    });", "", "    await this.followRepository.save(follow);", "", "    return { success: true };", "  }", "", "  /**", "   * Unfollow a user", "   */", "  async unfollowUser(followerUserId: string, usernameToUnfollow: string) {", "    // Find user to unfollow", "    const userToUnfollow = await this.userRepository.findOneBy({", "      username: usernameToUnfollow,", "    });", "    if (!userToUnfollow) {", "      throw new NotFoundError(\"User to unfollow not found\");", "    }", "", "    // Check if following", "    const follow = await this.followRepository.findOne({", "      where: {", "        follower: { id: followerUserId },", "        following: { id: userToUnfollow.id },", "      },", "    });", "", "    if (!follow) {", "      throw new ForbiddenError(\"You are not following this user\");", "    }", "", "    // Remove follow relationship", "    await this.followRepository.remove(follow);", "", "    return { success: true };", "  }", "", "  /**", "   * Check if one user follows another", "   */", "  async checkFollowStatus(followerUserId: string, usernameToCheck: string) {", "    const userToCheck = await this.userRepository.findOneBy({", "      username: usernameToCheck,", "    });", "    if (!userToCheck) {", "      throw new NotFoundError(\"User not found\");", "    }", "", "    const follow = await this.followRepository.findOne({", "      where: {", "        follower: { id: followerUserId },", "        following: { id: userToCheck.id },", "      },", "    });", "", "    return { following: !!follow };", "  }", "", "  /**", "   * Get followers of a user", "   */", "  async getFollowers(username: string, page = 1, limit = 20) {", "    const user = await this.userRepository.findOneBy({ username });", "    if (!user) {", "      throw new NotFoundError(\"User not found\");", "    }", "", "    const skip = (page - 1) * limit;", "", "    const [follows, total] = await this.followRepository", "      .createQueryBuilder(\"follow\")", "      .leftJoinAndSelect(\"follow.follower\", \"follower\")", "      .where(\"follow.following.id = :userId\", { userId: user.id })", "      .skip(skip)", "      .take(limit)", "      .orderBy(\"follow.createdAt\", \"DESC\")", "      .getManyAndCount();", "", "    // Extract follower users only", "    const followers = follows.map((follow) => follow.follower);", "", "    return {", "      followers,", "      meta: {", "        total,", "        page,", "        limit,", "        totalPages: Math.ceil(total / limit),", "      },", "    };", "  }", "", "  /**", "   * Get users that a user is following", "   */", "  async getFollowing(username: string, page = 1, limit = 20) {", "    const user = await this.userRepository.findOneBy({ username });", "    if (!user) {", "      throw new NotFoundError(\"User not found\");", "    }", "", "    const skip = (page - 1) * limit;", "", "    const [follows, total] = await this.followRepository", "      .createQueryBuilder(\"follow\")", "      .leftJoinAndSelect(\"follow.following\", \"following\")", "      .where(\"follow.follower.id = :userId\", { userId: user.id })", "      .skip(skip)", "      .take(limit)", "      .orderBy(\"follow.createdAt\", \"DESC\")", "      .getManyAndCount();", "", "    // Extract following users only", "    const following = follows.map((follow) => follow.following);", "", "    return {", "      following,", "      meta: {", "        total,", "        page,", "        limit,", "        totalPages: Math.ceil(total / limit),", "      },", "    };", "  }", "", "  /**", "   * Get follower/following counts", "   */", "  async getFollowCounts(username: string) {", "    const user = await this.userRepository.findOneBy({ username });", "    if (!user) {", "      throw new NotFoundError(\"User not found\");", "    }", "", "    const followersCount = await this.followRepository.count({", "      where: { following: { id: user.id } },", "    });", "", "    const followingCount = await this.followRepository.count({", "      where: { follower: { id: user.id } },", "    });", "", "    return {", "      username,", "      followersCount,", "      followingCount,", "    };", "  }", "}"], "file_path": "src/features/follow/validations/follow.schema.ts"}
{"Link_to_commit": "https://github.com/kimjeffsj/starclone-backend/commit/833593ac7ea754f432d212e43080991e6a61025c", "n-gram matched": "generated by copilot", "n_lines_longer_change": 121, "n_files_impacted": 6, "longest_chunk": ["import { NextFunction, Request, Response } from \"express\";", "", "import {", "  ValidationError,", "  UnauthorizedError,", "  NotFoundError,", "} from \"@/utils/errors.utils\";", "import { updateProfileSchema } from \"../validations/user.schema\";", "import { UserService } from \"../services/user.service\";", "", "export class UserController {", "  private userService = new UserService();", "", "  /**", "   * Get user profile by username", "   */", "  getUserProfile = async (req: Request, res: Response, next: NextFunction) => {", "    try {", "      const { username } = req.params;", "", "      const user = await this.userService.getUserByUsername(username);", "", "      if (!user) {", "        throw new NotFoundError(\"User not found\");", "      }", "", "      res.status(200).json({", "        user: user.toJSON(),", "      });", "    } catch (error) {", "      next(error);", "    }", "  };", "", "  /**", "   * Update user profile", "   */", "  updateProfile = async (req: Request, res: Response, next: NextFunction) => {", "    try {", "      if (!req.user || !req.user.id) {", "        throw new UnauthorizedError(\"Not authenticated\");", "      }", "", "      // Validate request", "      const validationResult = updateProfileSchema.safeParse(req.body);", "      if (!validationResult.success) {", "        throw new ValidationError(validationResult.error.format());", "      }", "", "      const updatedUser = await this.userService.updateProfile(", "        req.user.id,", "        validationResult.data", "      );", "", "      res.status(200).json({", "        message: \"Profile updated successfully\",", "        user: updatedUser.toJSON(),", "      });", "    } catch (error) {", "      next(error);", "    }", "  };", "", "  /**", "   * Get posts by username", "   */", "  getUserPosts = async (req: Request, res: Response, next: NextFunction) => {", "    try {", "      const { username } = req.params;", "      const page = req.query.page ? parseInt(req.query.page as string) : 1;", "      const limit = req.query.limit ? parseInt(req.query.limit as string) : 10;", "", "      const result = await this.userService.getUserPosts(username, page, limit);", "", "      res.status(200).json({", "        posts: result.posts,", "        meta: {", "          total: result.total,", "          totalPages: result.totalPages,", "          page,", "          limit,", "        },", "      });", "    } catch (error) {", "      next(error);", "    }", "  };", "", "  /**", "   * Search users by username or fullName", "   */", "  searchUsers = async (req: Request, res: Response, next: NextFunction) => {", "    try {", "      const { query } = req.query;", "      const page = req.query.page ? parseInt(req.query.page as string) : 1;", "      const limit = req.query.limit ? parseInt(req.query.limit as string) : 10;", "", "      if (!query || typeof query !== \"string\") {", "        throw new ValidationError({ message: \"Search query is required\" });", "      }", "", "      const { users, total, totalPages } = await this.userService.searchUsers(", "        query,", "        page,", "        limit", "      );", "", "      res.status(200).json({", "        users,", "        meta: {", "          total,", "          totalPages,", "          page,", "          limit,", "        },", "      });", "    } catch (error) {", "      next(error);", "    }", "  };", "}"], "file_path": "src/features/user/index.ts"}
{"Link_to_commit": "https://github.com/ILoveDotNet/ilovedotnet/commit/f9145bb2d893963f21efb544c73fae4b1ca22503", "n-gram matched": "generated by copilot", "n_lines_longer_change": 12, "n_files_impacted": 1, "longest_chunk": ["                                <a class=\"[ flex flex-col items-center ]\" href=\"/career\">", "                                    <svg xmlns=\"http://www.w3.org/2000/svg\" class=\"[ icon icon-tabler icons-tabler-outline icon-tabler-briefcase ]\"", "                                        width=\"30\" height=\"30\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\"", "                                        stroke-linecap=\"round\" stroke-linejoin=\"round\">", "                                        <path stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" />", "                                        <path d=\"M3 7m0 2a2 2 0 0 1 2 -2h14a2 2 0 0 1 2 2v9a2 2 0 0 1 -2 2h-14a2 2 0 0 1 -2 -2z\" />", "                                        <path d=\"M8 7v-2a2 2 0 0 1 2 -2h4a2 2 0 0 1 2 2v2\" />", "                                        <path d=\"M12 12l0 .01\" />", "                                        <path d=\"M3 13a20 20 0 0 0 18 0\" />", "                                    </svg>", "                                    <span class=\"[ text-xs text-center ]\">Career</span>", "                                </a>"], "file_path": "UITests/Components/Navigation/NavigationTests.cs"}
{"Link_to_commit": "https://github.com/kimjeffsj/starclone-frontend/commit/62f3d2244a1fdad9410be42e12b7015100df6adf", "n-gram matched": "generated by copilot", "n_lines_longer_change": 90, "n_files_impacted": 6, "longest_chunk": ["  // Add preview", "  addPreview: (files: File[]) => {", "    const newPreviews = files.map((file) => ({", "      id: crypto.randomUUID(),", "      file,", "      previewUrl: URL.createObjectURL(file),", "      isUploaded: false,", "    }));", "", "    set((state) => ({", "      previewMedia: [...state.previewMedia, ...newPreviews],", "    }));", "  },", "", "  // Delete preview", "  removePreview: (previewId: string) => {", "    set((state) => {", "      // delete preview url", "      const previewToRemove = state.previewMedia.find(", "        (p) => p.id === previewId", "      );", "      if (previewToRemove) {", "        URL.revokeObjectURL(previewToRemove.previewUrl);", "      }", "", "      return {", "        previewMedia: state.previewMedia.filter((p) => p.id !== previewId),", "      };", "    });", "  },", "", "  // Delete all previews", "  clearPreviews: () => {", "    const { previewMedia } = get();", "", "    // delete all preview urls", "    previewMedia.forEach((preview) => {", "      URL.revokeObjectURL(preview.previewUrl);", "    });", "", "    set({ previewMedia: [] });", "  },", "", "  // Upload all previews to server", "  uploadAllPreviews: async (options: MediaUploadOptions) => {", "    const { previewMedia } = get();", "    set({ isUploading: true, uploadProgress: 0 });", "", "    const uploadedItems: Media[] = [];", "    const totalFiles = previewMedia.length;", "    let processedCount = 0;", "", "    try {", "      for (const preview of previewMedia) {", "        if (!preview.isUploaded) {", "          const media = await get().uploadMedia(preview.file, options);", "          if (media) {", "            uploadedItems.push(media);", "", "            // change states to uploaded", "            set((state) => ({", "              previewMedia: state.previewMedia.map((p) =>", "                p.id === preview.id ? { ...p, isUploaded: true } : p", "              ),", "            }));", "          }", "        }", "", "        processedCount++;", "        const percentCompleted = Math.round(", "          (processedCount * 100) / totalFiles", "        );", "        set({ uploadProgress: percentCompleted });", "      }", "", "      // Clear preview after uploaded", "      get().clearPreviews();", "", "      set({ isUploading: false, uploadProgress: 100 });", "      return uploadedItems;", "    } catch (error) {", "      console.error(\"Failed to upload previews:\", error);", "      set({", "        isUploading: false,", "        error: error instanceof Error ? error.message : \"Upload failed\",", "      });", "      return [];", "    }", "  },", ""], "file_path": "src/store/mediaStore.ts"}
{"Link_to_commit": "https://github.com/coderangel117/python_game/commit/07598692f7b230677aa76ecd7391736b5a29185e", "n-gram matched": "generated by copilot", "n_lines_longer_change": 90, "n_files_impacted": 1, "longest_chunk": ["import unittest", "from unittest.mock import patch, mock_open, MagicMock", "", "from user_manager import get_all_users, new_user, get_user_info, delete_user, update_username, add_win, add_played_game, \\", "    add_fail, find_user, save_user, merge_json_files", "", "", "class TestUserManager(unittest.TestCase):", "", "    @patch('user_manager.glob.glob', return_value=['user1.json', 'user2.json', 'users.json'])", "    @patch('builtins.open', new_callable=mock_open, read_data='[{\"username\": \"user1\"}, {\"username\": \"user2\"}]')", "    def test_get_all_users(self, mock_file, mock_glob):", "        users = get_all_users()", "        self.assertIn('user1.json', users)", "        self.assertIn('user2.json', users)", "        self.assertNotIn('users.json', users)", "", "    @patch('user_manager.glob.glob', return_value=['user1.json'])", "    @patch('builtins.open', new_callable=mock_open)", "    def test_new_user(self, mock_file, mock_glob):", "        new_user('user2')", "        mock_file.assert_called_with('user2.json', 'w')", "", "    @patch('user_manager.glob.glob', return_value=['user1.json'])", "    @patch('builtins.open', new_callable=mock_open,", "           read_data='{\"username\": \"user1\", \"played_games\": 0, \"nbfail\": 0, \"nbwin\": 0}')", "    def test_get_user_info(self, mock_file, mock_glob):", "        with patch('builtins.print') as mocked_print:", "            get_user_info('user1')", "            mocked_print.assert_called_with('User user1 have never played')", "", "    @patch('user_manager.glob.glob', return_value=['user1.json'])", "    @patch('os.remove')", "    def test_delete_user(self, mock_remove, mock_glob):", "        with patch('builtins.print') as mocked_print:", "            delete_user('user1')", "            mock_remove.assert_called_with('user1.json')", "            mocked_print.assert_called_with('User user1 has been successfully deleted')", "", "    @patch('user_manager.glob.glob', return_value=['user1.json'])", "    @patch('builtins.open', new_callable=mock_open, read_data='{\"username\": \"user1\"}')", "    @patch('os.rename')", "    def test_update_username(self, mock_rename, mock_file, mock_glob):", "        with patch('builtins.print') as mocked_print:", "            update_username('user1', 'user2')", "            mock_rename.assert_called_with('user1.json', 'user2.json')", "            mocked_print.assert_called_with(' The username user1 has been changed to user2')", "", "    @patch('user_manager.glob.glob', return_value=['user1.json'])", "    @patch('builtins.open', new_callable=mock_open, read_data='{\"username\": \"user1\", \"nbwin\": 0}')", "    def test_add_win(self, mock_file, mock_glob):", "        add_win('user1')", "        mock_file().write.assert_called()", "", "    @patch('user_manager.glob.glob', return_value=['user1.json'])", "    @patch('builtins.open', new_callable=mock_open, read_data='{\"username\": \"user1\", \"played_games\": 0}')", "    def test_add_played_game(self, mock_file, mock_glob):", "        add_played_game('user1')", "        mock_file().write.assert_called()", "", "    @patch('user_manager.glob.glob', return_value=['user1.json'])", "    @patch('builtins.open', new_callable=mock_open, read_data='{\"username\": \"user1\", \"nbfail\": 0}')", "    def test_add_fail(self, mock_file, mock_glob):", "        add_fail('user1')", "        mock_file().write.assert_called()", "", "    @patch('user_manager.glob.glob', return_value=['user1.json'])", "    def test_find_user(self, mock_glob):", "        self.assertTrue(find_user('user1'))", "        self.assertFalse(find_user('user2'))", "", "    @patch('builtins.open', new_callable=mock_open)", "    def test_save_user(self, mock_file):", "        user = MagicMock()", "        user.username = 'user1'", "        user.played_games = 0", "        user.nbfail = 0", "        user.nbwin = 0", "        user.greatest_score = 0", "        save_user(user)", "        mock_file.assert_called_with('user1.json', 'w')", "", "    @patch('builtins.open', new_callable=mock_open, read_data='{\"username\": \"user1\"}')", "    def test_merge_json_files(self, mock_file):", "        merge_json_files(['user1.json'])", "        mock_file.assert_called_with('users.json', 'w')", "", "", "if __name__ == '__main__':", "    unittest.main()"], "file_path": "tests/test_user_manager.py"}
{"Link_to_commit": "https://github.com/kimjeffsj/empcon_backend/commit/78dbf6d2dff8127b688dec405597c441b5f50953", "n-gram matched": "generated by copilot", "n_lines_longer_change": 89, "n_files_impacted": 1, "longest_chunk": ["import { NextFunction, Request, Response } from \"express\";", "import { authService } from \"./auth.service\";", "import {", "  UnauthorizedError,", "  ValidationError,", "} from \"@/common/middleware/error.middleware\";", "import { logger } from \"@/common/utils/logger.utils\";", "", "export class AuthController {", "  /**", "   * Login", "   */", "  async login(req: Request, res: Response, next: NextFunction) {", "    try {", "      const { email, password } = req.body;", "", "      if (!email || !password) {", "        throw new ValidationError({", "          message: \"Email and Password are required\",", "        });", "      }", "", "      const result = await authService.login({ email, password });", "", "      return res.status(200).json({", "        message: \"Login successful\",", "        ...result,", "      });", "    } catch (error) {", "      next(error);", "    }", "  }", "", "  /**", "   * Refresh Token", "   */", "  async refreshToken(req: Request, res: Response, next: NextFunction) {", "    try {", "      const { refreshToken } = req.body;", "", "      if (!refreshToken) {", "        throw new UnauthorizedError(\"Refresh token is required\");", "      }", "    } catch (error) {}", "  }", "", "  /**", "   * Request Password reset", "   */", "  async requestPasswordReset(req: Request, res: Response, next: NextFunction) {", "    try {", "      const { email } = req.body;", "", "      if (!email) {", "        throw new ValidationError({ message: \"Email is required\" });", "      }", "", "      const result = await authService.requestPasswordReset({ email });", "", "      return res.status(200).json(result);", "    } catch (error) {", "      logger.error(\"Password reset request error: \", error);", "      next(error);", "    }", "  }", "", "  /**", "   * Reset Password", "   * */", "  async resetPassword(req: Request, res: Response, next: NextFunction) {", "    try {", "      const { token, password } = req.body;", "", "      if (!token || !password) {", "        throw new ValidationError({", "          message: \"Token and new password are required\",", "        });", "      }", "", "      const result = await authService.resetPassword({ token, password });", "", "      return res.status(200).json(result);", "    } catch (error) {", "      next(error);", "    }", "  }", "}", "", "export const authController = new AuthController();"], "file_path": "src/features/auth/auth.controller.ts"}
{"Link_to_commit": "https://github.com/mosajjal/sniproxy/commit/d4b0a51baa7875df04c3b5ecff27aaa21941de92", "n-gram matched": "generated by copilot", "n_lines_longer_change": 7, "n_files_impacted": 2, "longest_chunk": ["// isValidFQDN validates if the given hostname is a valid FQDN", "func isValidFQDN(hostname string) bool {", "\t// Regular expression to match a valid FQDN", "\tvar fqdnRegex = regexp.MustCompile(`^(?i:[a-z0-9](?:[a-z0-9-]{0,61}[a-z0-9])?\\.)+(?:[a-z]{2,})$`)", "\treturn fqdnRegex.MatchString(hostname)", "}", ""], "file_path": "pkg/https_sni.go"}
{"Link_to_commit": "https://github.com/frank1li/ipv6-ftp-server-AI-generated/commit/6debfc65580b73f2fe5d5a08f7b16b4906c77dc3", "n-gram matched": "generated by copilot", "n_lines_longer_change": 119, "n_files_impacted": 20, "longest_chunk": ["import os", "import hashlib", "import threading", "", "from pyftpdlib.authorizers import DummyAuthorizer", "from pyftpdlib.handlers import FTPHandler", "from pyftpdlib.servers import FTPServer", "", "from src.config.settings import (", "    HOME_DIRECTORY,", "    PASSWORD,", "    SERVER_ADDRESS,", "    SERVER_PORT,", "    USER,", ")", "", "", "class FTPServerHandler:", "    def __init__(self):", "        self.server = None", "        self.server_thread = None", "", "    def init_server(self):", "        # Check and create working directory if not exists", "        if not os.path.exists(HOME_DIRECTORY):", "            os.makedirs(HOME_DIRECTORY)", "            print(f\"Created working directory: {HOME_DIRECTORY}\")", "", "        # Create an authorizer", "        authorizer = DummyAuthorizer()", "", "        # Add user with settings from config", "        authorizer.add_user(USER, PASSWORD, HOME_DIRECTORY, perm=\"elradfmwMT\")", "", "        # Create FTP handler", "        handler = FTPHandler", "        handler.authorizer = authorizer", "", "        # Set IPv6 address and port", "        address = (SERVER_ADDRESS, SERVER_PORT)", "", "        # Create FTP server", "        self.server = FTPServer(address, handler)", "", "    def start(self):", "        \"\"\"Start FTP server in a separate thread\"\"\"", "        if self.server is None:", "            self.init_server()", "", "        self.server_thread = threading.Thread(", "            target=self.server.serve_forever,", "            daemon=True,  # Set as daemon thread so it will exit when main program exits", "        )", "        self.server_thread.start()", "        print(f\"FTP server is running at [{SERVER_ADDRESS}]:{SERVER_PORT}\")", "", "    def stop(self):", "        \"\"\"Stop the FTP server\"\"\"", "        if self.server:", "            self.server.close_all()", "            print(\"FTP server stopped\")", "", "    def add_binary_file(self, filename: str, content: bytes) -> bool:", "        \"\"\"Add a binary file to the FTP server's home directory", "        ", "        Args:", "            filename: Name of the file to create", "            content: Binary content to write", "            ", "        Returns:", "            bool: True if successful, False otherwise", "        \"\"\"", "        try:", "            filepath = os.path.join(HOME_DIRECTORY, filename)", "            with open(filepath, 'wb') as f:", "                f.write(content)", "            return True", "        except Exception as e:", "            print(f\"Error adding file: {e}\")", "            return False", "", "    def get_file_sha256(self, filename: str) -> str:", "        \"\"\"Get SHA256 hash of a file in the FTP server's home directory", "        ", "        Args:", "            filename: Name of the file to hash", "            ", "        Returns:", "            str: SHA256 hash of the file, or empty string if file not found", "        \"\"\"", "        try:", "            filepath = os.path.join(HOME_DIRECTORY, filename)", "            sha256_hash = hashlib.sha256()", "            with open(filepath, 'rb') as f:", "                for byte_block in iter(lambda: f.read(4096), b''):", "                    sha256_hash.update(byte_block)", "            return sha256_hash.hexdigest()", "        except Exception as e:", "            print(f\"Error calculating hash: {e}\")", "            return ''", "", "", "def main():", "    # Usage example", "    ftp_server = FTPServerHandler()", "    try:", "        ftp_server.start()", "        # Keep main thread running", "        while True:", "            input(\"Press Enter to stop the server...\")", "            break", "    except KeyboardInterrupt:", "        print(\"\\nReceived exit signal\")", "    finally:", "        ftp_server.stop()", "", "", "if __name__ == \"__main__\":", "    main()"], "file_path": "src/utils/__init__.py"}
{"Link_to_commit": "https://github.com/kimjeffsj/empcon_backend/commit/8f02ed761704205ac94ca00202f45b8161f0fcbe", "n-gram matched": "generated by copilot", "n_lines_longer_change": 364, "n_files_impacted": 3, "longest_chunk": ["import {", "  ConflictError,", "  NotFoundError,", "} from \"@/common/middleware/error.middleware\";", "import { dateUtils, getPaginationParams } from \"@/common/utils/helpers.utils\";", "import { logger } from \"@/common/utils/logger.utils\";", "import prisma from \"@/entities/prisma\";", "import bcrypt from \"bcryptjs\";", "import {", "  CreateUserDto,", "  PaginatedUserResponse,", "  UpdateUserDto,", "  UpdateUserProfileDto,", "  UserQueryParams,", "} from \"./dto/user.dto\";", "import { Prisma, UserProfile } from \"@prisma/client\";", "import {", "  defaultUserSelect,", "  UserListSelect,", "  userListSelect,", "  userWithRelationsSelect,", "  UserWithRelationsSelect,", "} from \"./users.selections\";", "", "export class UsersService {", "  /**", "   * Find all users with pagination and filtering", "   */", "  async findAll(queryParams: UserQueryParams): Promise<PaginatedUserResponse> {", "    const { page, limit, search, departmentId, role, isActive } = queryParams;", "    const pagination = getPaginationParams(page, limit);", "", "    // Build where clause", "    const where: Prisma.UserWhereInput = {};", "", "    // Search filter", "    if (search) {", "      where.OR = [", "        { firstName: { contains: search, mode: \"insensitive\" } },", "        { lastName: { contains: search, mode: \"insensitive\" } },", "        { email: { contains: search, mode: \"insensitive\" } },", "      ];", "    }", "", "    // Department filter", "    if (departmentId) {", "      where.departmentId = departmentId;", "    }", "", "    // Role filter", "    if (role) {", "      where.role = role;", "    }", "", "    // Active/Inactive filter", "    if (isActive !== undefined) {", "      const active = isActive === \"true\";", "      where.terminationDate = active ? null : { not: null };", "    }", "", "    // Count total matching records", "    const total = await prisma.user.count({ where });", "", "    // Get users with safe fields", "    const users = await prisma.user.findMany({", "      where,", "      select: userListSelect,", "      skip: pagination.skip,", "      take: pagination.limit,", "      orderBy: {", "        lastName: \"asc\",", "      },", "    });", "", "    return {", "      data: users,", "      total,", "      page: pagination.page,", "      limit: pagination.limit,", "      totalPages: Math.ceil(total / pagination.limit),", "    };", "  }", "", "  /**", "   * Find a single user by ID", "   */", "  async findById(id: string): Promise<UserWithRelationsSelect> {", "    const user = await prisma.user.findUnique({", "      where: { id },", "      select: userWithRelationsSelect,", "    });", "", "    if (!user) {", "      throw new NotFoundError(\"User\");", "    }", "", "    return user;", "  }", "", "  /**", "   * Create a new user", "   */", "  async create(createUserDto: CreateUserDto): Promise<UserWithRelationsSelect> {", "    const { email, password, profile, ...userData } = createUserDto;", "", "    // Check if user with this email already exists", "    const existingUser = await prisma.user.findUnique({", "      where: { email },", "    });", "", "    if (existingUser) {", "      throw new ConflictError(\"email\");", "    }", "", "    // Hash password", "    const hashedPassword = await bcrypt.hash(password, 10);", "", "    // Create user with profile in a transaction", "    try {", "      const user = await prisma.$transaction(async (tx) => {", "        // Create user", "        const newUser = await tx.user.create({", "          data: {", "            email,", "            password: hashedPassword,", "            ...userData,", "          },", "          select: defaultUserSelect,", "        });", "", "        // Create profile if provided", "        if (profile) {", "          await tx.userProfile.create({", "            data: {", "              ...profile,", "              userId: newUser.id,", "            },", "          });", "        }", "", "        return tx.user.findUnique({", "          where: { id: newUser.id },", "          select: userWithRelationsSelect,", "        });", "      });", "", "      if (!user) {", "        throw new Error(\"Failed to create user\");", "      }", "", "      return user;", "    } catch (error) {", "      logger.error(\"Error creating user:\", error);", "      throw error;", "    }", "  }", "", "  /**", "   * Update an existing user", "   */", "  async update(", "    id: string,", "    updateUserDto: UpdateUserDto", "  ): Promise<UserWithRelationsSelect> {", "    const { profile, password, ...userData } = updateUserDto;", "", "    // Check if user exists", "    const existingUser = await prisma.user.findUnique({", "      where: { id },", "      select: { id: true, email: true },", "    });", "", "    if (!existingUser) {", "      throw new NotFoundError(\"User\");", "    }", "", "    // If email is being updated, check for duplicates", "    if (userData.email && userData.email !== existingUser.email) {", "      const duplicateEmail = await prisma.user.findUnique({", "        where: { email: userData.email },", "      });", "", "      if (duplicateEmail) {", "        throw new ConflictError(\"email\");", "      }", "    }", "", "    // Prepare update data", "    const updateData: Prisma.UserUpdateInput = { ...userData };", "", "    // Hash password if provided", "    if (password) {", "      updateData.password = await bcrypt.hash(password, 10);", "    }", "", "    // Update user and profile in a transaction", "    try {", "      const user = await prisma.$transaction(async (tx) => {", "        // Update user", "        const updatedUser = await tx.user.update({", "          where: { id },", "          data: updateData,", "          select: defaultUserSelect,", "        });", "", "        // Update profile if provided", "        if (profile) {", "          // Check if profile exists", "          const existingProfile = await tx.userProfile.findUnique({", "            where: { userId: id },", "          });", "", "          if (existingProfile) {", "            await tx.userProfile.update({", "              where: { userId: id },", "              data: profile,", "            });", "          } else {", "            await tx.userProfile.create({", "              data: {", "                ...profile,", "                userId: id,", "              },", "            });", "          }", "        }", "", "        return tx.user.findUnique({", "          where: { id },", "          select: userWithRelationsSelect,", "        });", "      });", "", "      if (!user) {", "        throw new Error(\"Failed to update user\");", "      }", "", "      return user;", "    } catch (error) {", "      logger.error(\"Error updating user:\", error);", "      throw error;", "    }", "  }", "", "  /**", "   * Update user profile", "   */", "  async updateProfile(", "    userId: string,", "    updateProfileDto: UpdateUserProfileDto", "  ): Promise<UserProfile> {", "    // Check if user exists", "    const user = await prisma.user.findUnique({", "      where: { id: userId },", "      select: { id: true },", "    });", "", "    if (!user) {", "      throw new NotFoundError(\"User\");", "    }", "", "    // Check if profile exists", "    const existingProfile = await prisma.userProfile.findUnique({", "      where: { userId },", "    });", "", "    // Update or create profile", "    if (existingProfile) {", "      return prisma.userProfile.update({", "        where: { userId },", "        data: updateProfileDto,", "      });", "    } else {", "      return prisma.userProfile.create({", "        data: {", "          ...updateProfileDto,", "          userId,", "        },", "      });", "    }", "  }", "", "  /**", "   * Delete a user", "   */", "  async delete(id: string): Promise<void> {", "    // Check if user exists", "    const user = await prisma.user.findUnique({", "      where: { id },", "      select: { id: true },", "    });", "", "    if (!user) {", "      throw new NotFoundError(\"User\");", "    }", "", "    // Delete user and related data in a transaction", "    try {", "      await prisma.$transaction(async (tx) => {", "        // Delete profile if exists", "        const profile = await tx.userProfile.findUnique({", "          where: { userId: id },", "        });", "", "        if (profile) {", "          await tx.userProfile.delete({", "            where: { userId: id },", "          });", "        }", "", "        // Delete user", "        await tx.user.delete({", "          where: { id },", "        });", "      });", "    } catch (error) {", "      logger.error(\"Error deleting user:\", error);", "      throw error;", "    }", "  }", "", "  /**", "   * Get new employees (hired in the last 30 days)", "   */", "  async getNewEmployees(): Promise<UserListSelect[]> {", "    const thirtyDaysAgo = dateUtils.daysAgo(30);", "", "    const newUsers = await prisma.user.findMany({", "      where: {", "        hireDate: {", "          gte: thirtyDaysAgo,", "        },", "        terminationDate: null,", "      },", "      select: userListSelect,", "      orderBy: {", "        hireDate: \"desc\",", "      },", "    });", "", "    return newUsers;", "  }", "", "  /**", "   * Get resigned employees", "   */", "  async getResignedEmployees(): Promise<UserListSelect[]> {", "    const resignedUsers = await prisma.user.findMany({", "      where: {", "        terminationDate: {", "          not: null,", "        },", "      },", "      select: userListSelect,", "      orderBy: {", "        terminationDate: \"desc\",", "      },", "    });", "", "    return resignedUsers;", "  }", "}", "", "export const usersService = new UsersService();"], "file_path": "src/features/users/users.service.ts"}
{"Link_to_commit": "https://github.com/mozilla-ai/lumigator/commit/ac0c942615ab6f505f3c7acb1db8e3512c020d84", "n-gram matched": "generated by copilot", "n_lines_longer_change": 195, "n_files_impacted": 7, "longest_chunk": ["from unittest.mock import MagicMock, patch", "", "import pytest", "from inference_config import InferenceJobConfig", "from model_clients.huggingface_clients import (", "    HuggingFaceCausalLMClient,", "    HuggingFaceSeq2SeqSummarizationClient,", ")", "", "from schemas import PredictionResult, TaskType", "", "", "class TestHuggingFaceSeq2SeqSummarizationClient:", "    @pytest.fixture", "    def mock_config(self):", "        \"\"\"Create a mock InferenceJobConfig for testing seq2seq client.\"\"\"", "        config = MagicMock(spec=InferenceJobConfig)", "        config.hf_pipeline = MagicMock()", "        config.hf_pipeline.model_name_or_path = \"mock-seq2seq-model\"", "        config.hf_pipeline.task = TaskType.SUMMARIZATION", "        config.hf_pipeline.use_fast = True", "        config.hf_pipeline.trust_remote_code = False", "        config.hf_pipeline.torch_dtype = \"float32\"", "        config.hf_pipeline.revision = \"main\"", "        config.hf_pipeline.device = \"cpu\"", "", "        config.generation_config = MagicMock()", "        config.generation_config.max_new_tokens = 100", "", "        return config", "", "    @patch(\"model_clients.huggingface_clients.AutoTokenizer\")", "    @patch(\"model_clients.huggingface_clients.AutoModelForSeq2SeqLM\")", "    @patch(\"model_clients.huggingface_clients.pipeline\")", "    def test_initialization(self, mock_pipeline, mock_automodel, mock_tokenizer, mock_config):", "        \"\"\"Test initialization of the seq2seq client.\"\"\"", "        # Setup mocks", "        mock_model = MagicMock()", "        mock_model.config.max_position_embeddings = 512", "        mock_automodel.from_pretrained.return_value = mock_model", "", "        mock_tokenizer_instance = MagicMock()", "        mock_tokenizer_instance.model_max_length = 512", "        mock_tokenizer.from_pretrained.return_value = mock_tokenizer_instance", "", "        mock_pipeline_instance = MagicMock()", "        mock_pipeline_instance.model = mock_model", "        mock_pipeline_instance.tokenizer = mock_tokenizer_instance", "        mock_pipeline.return_value = mock_pipeline_instance", "", "        # Initialize client", "        client = HuggingFaceSeq2SeqSummarizationClient(mock_config)", "", "        # Verify initialization", "        mock_tokenizer.from_pretrained.assert_called_once()", "        mock_automodel.from_pretrained.assert_called_once()", "        mock_pipeline.assert_called_once()", "        assert client._pipeline == mock_pipeline_instance", "", "    @patch(\"model_clients.huggingface_clients.AutoTokenizer\")", "    @patch(\"model_clients.huggingface_clients.AutoModelForSeq2SeqLM\")", "    @patch(\"model_clients.huggingface_clients.pipeline\")", "    def test_predict(self, mock_pipeline, mock_automodel, mock_tokenizer, mock_config):", "        \"\"\"Test the predict method of the seq2seq client.\"\"\"", "        # Setup mocks", "        mock_model = MagicMock()", "        mock_model.config.max_position_embeddings = 512", "        mock_automodel.from_pretrained.return_value = mock_model", "", "        mock_tokenizer_instance = MagicMock()", "        mock_tokenizer_instance.model_max_length = 512", "        mock_tokenizer.from_pretrained.return_value = mock_tokenizer_instance", "", "        mock_pipeline_instance = MagicMock()", "        mock_pipeline_instance.model = mock_model", "        mock_pipeline_instance.tokenizer = mock_tokenizer_instance", "        mock_pipeline_instance.return_value = [{\"summary_text\": \"This is a summary.\"}]", "        mock_pipeline.return_value = mock_pipeline_instance", "", "        # Initialize client and call predict", "        client = HuggingFaceSeq2SeqSummarizationClient(mock_config)", "        result = client.predict(\"This is a test prompt.\")", "", "        # Verify prediction", "        assert isinstance(result, PredictionResult)", "        assert result.prediction == \"This is a summary.\"", "        mock_pipeline_instance.assert_called_once_with(\"This is a test prompt.\", max_new_tokens=100, truncation=True)", "", "    @patch(\"model_clients.huggingface_clients.AutoTokenizer\")", "    @patch(\"model_clients.huggingface_clients.AutoModelForSeq2SeqLM\")", "    @patch(\"model_clients.huggingface_clients.pipeline\")", "    def test_max_token_adjustment(self, mock_pipeline, mock_automodel, mock_tokenizer, mock_config):", "        \"\"\"Test that the client adjusts max tokens if over model limits.\"\"\"", "        # Setup mocks with limited max position embeddings", "        mock_model = MagicMock()", "        mock_model.config.max_position_embeddings = 50  # Lower than config.max_new_tokens", "        mock_automodel.from_pretrained.return_value = mock_model", "", "        mock_tokenizer_instance = MagicMock()", "        mock_tokenizer_instance.model_max_length = 512", "        mock_tokenizer.from_pretrained.return_value = mock_tokenizer_instance", "", "        mock_pipeline_instance = MagicMock()", "        mock_pipeline_instance.model = mock_model", "        mock_pipeline_instance.tokenizer = mock_tokenizer_instance", "        mock_pipeline.return_value = mock_pipeline_instance", "", "        # Set the initial max_new_tokens to a value higher than model's max_position_embeddings", "        mock_config.generation_config.max_new_tokens = 100", "        # Initialize client - which should trigger the token adjustment", "        client = HuggingFaceSeq2SeqSummarizationClient(mock_config)", "        # Verify the max_new_tokens was adjusted down to the model's max_position_embeddings", "        assert client._config.generation_config.max_new_tokens == 50", "", "        # Now test with a value that's already within limits", "        mock_config.generation_config.max_new_tokens = 30  # Less than max_position_embeddings", "        # Initialize a new client", "        client = HuggingFaceSeq2SeqSummarizationClient(mock_config)", "        # Verify max_new_tokens was NOT adjusted since it was already within limits", "        assert client._config.generation_config.max_new_tokens == 30", "", "", "class TestHuggingFaceCausalLMClient:", "    @pytest.fixture", "    def mock_config(self):", "        \"\"\"Create a mock InferenceJobConfig for testing causal LM client.\"\"\"", "        config = MagicMock(spec=InferenceJobConfig)", "        config.hf_pipeline = MagicMock()", "        config.hf_pipeline.model_name_or_path = \"mock-causal-model\"", "        config.hf_pipeline.task = TaskType.TEXT_GENERATION", "        config.hf_pipeline.model_dump.return_value = {", "            \"model_name_or_path\": \"mock-causal-model\",", "            \"task\": TaskType.TEXT_GENERATION,", "        }", "", "        config.system_prompt = \"You are a helpful assistant.\"", "        config.generation_config = MagicMock()", "        config.generation_config.max_new_tokens = 100", "", "        return config", "", "    @patch(\"model_clients.huggingface_clients.pipeline\")", "    def test_initialization(self, mock_pipeline, mock_config):", "        \"\"\"Test initialization of the causal LM client.\"\"\"", "        # Setup mocks", "        mock_pipeline_instance = MagicMock()", "        mock_pipeline.return_value = mock_pipeline_instance", "", "        # Initialize client", "        client = HuggingFaceCausalLMClient(mock_config)", "", "        # Verify initialization", "        mock_pipeline.assert_called_once()", "        assert client._pipeline == mock_pipeline_instance", "        assert client._system_prompt == \"You are a helpful assistant.\"", "", "    @patch(\"model_clients.huggingface_clients.pipeline\")", "    def test_with_summarization_task(self, mock_pipeline, mock_config):", "        \"\"\"Test the causal LM client with a summarization task through system prompt.\"\"\"", "        # Set task to summarization and use appropriate system prompt", "        mock_config.hf_pipeline.task = TaskType.SUMMARIZATION", "        mock_config.system_prompt = \"Summarize the following text.\"", "", "        # Ensure pipeline config is still set to text-generation (overridden in client init)", "        mock_config.hf_pipeline.model_dump.return_value = {", "            \"model_name_or_path\": \"mock-causal-model\",", "            \"task\": TaskType.TEXT_GENERATION,  # Should still be text-generation in dumped config", "        }", "", "        # Setup mock pipeline response", "        mock_response = [", "            {", "                \"generated_text\": [", "                    {\"role\": \"system\", \"content\": \"Summarize the following text.\"},", "                    {\"role\": \"user\", \"content\": \"Long article about climate change...\"},", "                    {\"role\": \"assistant\", \"content\": \"Climate change is affecting the planet in various ways.\"},", "                ]", "            }", "        ]", "        mock_pipeline_instance = MagicMock()", "        mock_pipeline_instance.return_value = mock_response", "        mock_pipeline.return_value = mock_pipeline_instance", "", "        # Initialize client and call predict", "        client = HuggingFaceCausalLMClient(mock_config)", "        result = client.predict(\"Long article about climate change...\")", "", "        # Verify prediction", "        assert isinstance(result, PredictionResult)", "        assert result.prediction == \"Climate change is affecting the planet in various ways.\"", "", "        # Verify pipeline task was correctly overridden to text-generation", "        mock_pipeline.assert_called_once()", "        pipeline_args = mock_pipeline.call_args[1]", "        assert pipeline_args[\"task\"] == TaskType.TEXT_GENERATION"], "file_path": "lumigator/jobs/inference/tests/test_huggingface_clients.py"}
{"Link_to_commit": "https://github.com/kimjeffsj/empcon_backend/commit/0433d2285bca54aef0c5399dbc053e414cbae47b", "n-gram matched": "generated by copilot", "n_lines_longer_change": 79, "n_files_impacted": 1, "longest_chunk": ["/**", " * Utility function to remove password field from data", " */", "export function excludePassword<T extends { password: string }>(", "  user: T", "): Omit<T, \"password\"> {", "  const { password, ...userWithoutPassword } = user;", "  return userWithoutPassword;", "}", "", "/**", " * Utility function to remove password fields from an array of objects", " */", "export function excludePasswordFromArray<T extends { password: string }>(", "  users: T[]", "): Omit<T, \"password\">[] {", "  return users.map((user) => excludePassword(user));", "}", "", "/**", " * Date utility functions with date range capabilities", " */", "export const dateUtils = {", "  /**", "   * Returns current date", "   */", "  now(): Date {", "    return new Date();", "  },", "", "  /**", "   * Returns date from n days ago", "   */", "  daysAgo(days: number): Date {", "    const date = new Date();", "    date.setDate(date.getDate() - days);", "    return date;", "  },", "", "  /**", "   * Returns date n days from now", "   */", "  daysFromNow(days: number): Date {", "    const date = new Date();", "    date.setDate(date.getDate() + days);", "    return date;", "  },", "", "  /**", "   * Calculate days between two dates", "   */", "  daysBetween(start: Date, end: Date): number {", "    const diffTime = Math.abs(end.getTime() - start.getTime());", "    return Math.ceil(diffTime / (1000 * 60 * 60 * 24));", "  },", "", "  /**", "   * Convert date to YYYY-MM-DD format string", "   */", "  formatDate(date: Date): string {", "    return date.toISOString().split(\"T\")[0];", "  },", "};", "", "/**", " * Utility function for pagination", " */", "export function getPaginationParams(page?: string, limit?: string) {", "  const parsedPage = parseInt(page || \"1\", 10);", "  const parsedLimit = parseInt(limit || \"10\", 10);", "", "  return {", "    page: parsedPage > 0 ? parsedPage : 1,", "    limit: parsedLimit > 0 ? parsedLimit : 10,", "    skip:", "      (parsedPage > 0 ? parsedPage - 1 : 0) *", "      (parsedLimit > 0 ? parsedLimit : 10),", "  };", "}"], "file_path": "src/common/utils/helpers.utils.ts"}
{"Link_to_commit": "https://github.com/kimjeffsj/empcon_backend/commit/1b16d19cc104c4b522cda25081555f0ad8f9320d", "n-gram matched": "generated by copilot", "n_lines_longer_change": 61, "n_files_impacted": 4, "longest_chunk": ["import { PrismaClient } from \"@prisma/client\";", "import bcrypt from \"bcryptjs\";", "import dotenv from \"dotenv\";", "import path from \"path\";", "", "dotenv.config({ path: path.resolve(__dirname, \"../.env\") });", "", "import { appConfig } from \"../src/config/app.config\";", "", "const prisma = new PrismaClient();", "", "async function main() {", "  // Get admin information from environment variables", "  const adminEmail = appConfig.admin.email;", "  const adminPassword = appConfig.admin.password;", "", "  if (!adminEmail) {", "    throw new Error(\"Admin email is not defined in environment variables\");", "  }", "", "  if (!adminPassword) {", "    throw new Error(\"Admin password is not defined in environment variables\");", "  }", "", "  // Hash admin password", "  const hashedPassword = await bcrypt.hash(adminPassword, 10);", "", "  // Check if admin already exists", "  const existingAdmin = await prisma.user.findUnique({", "    where: { email: adminEmail },", "  });", "", "  // Create admin if not exists", "  if (!existingAdmin) {", "    const admin = await prisma.user.create({", "      data: {", "        email: adminEmail,", "        password: hashedPassword,", "        firstName: \"Admin\",", "        lastName: \"User\",", "        hireDate: new Date(),", "        role: \"ADMIN\",", "      },", "    });", "", "    console.log(`Admin user created with ID: ${admin.id}`);", "    console.log(`Email: ${adminEmail}`);", "    console.log(\"Password: [Set in environment variables]\");", "  } else {", "    console.log(\"Admin user already exists\");", "  }", "}", "", "main()", "  .catch((e) => {", "    console.error(\"Seed error:\", e);", "    process.exit(1);", "  })", "  .finally(async () => {", "    await prisma.$disconnect();", "  });"], "file_path": "src/app.ts"}
{"Link_to_commit": "https://github.com/jedisct1/libsodium.js/commit/e9a5966606d9e7cf1150f2fa5b53c0f393965f28", "n-gram matched": "generated by copilot", "n_lines_longer_change": 14, "n_files_impacted": 1, "longest_chunk": ["});", "", "test(\"crypto_scalarmult\", () => {", "    const aliceKeypair = sodium.crypto_box_keypair();", "    const aliceSecret = aliceKeypair.privateKey;", "    const alicePublic = aliceKeypair.publicKey;", "    const bobKeypair = sodium.crypto_box_keypair();", "    const bobSecret = bobKeypair.privateKey;", "    const bobPublic = bobKeypair.publicKey;", "", "    const shared1 = sodium.crypto_scalarmult(aliceSecret, bobPublic);", "    const shared2 = sodium.crypto_scalarmult(bobSecret, alicePublic);", "    expect(shared1).toEqual(shared2);", "});"], "file_path": "test/sodium.test.ts"}
{"Link_to_commit": "https://github.com/venzy/chirpy/commit/b85d274386730bd5d46a5b75dd74fc659a7d6ba0", "n-gram matched": "generated by copilot", "n_lines_longer_change": 39, "n_files_impacted": 4, "longest_chunk": ["}", "", "func (cfg *apiConfig) handleDeleteChirpByID(response http.ResponseWriter, request *http.Request, userID uuid.UUID) {", "\t// Parse request params", "\tchirpID, err := uuid.Parse(request.PathValue(\"chirpID\"))", "\tif err != nil {", "\t\tmsg := fmt.Sprintf(\"chirps: Problem parsing chirpID from request: %s\", err)", "\t\tlog.Println(msg)", "\t\trespondWithError(response, http.StatusBadRequest, msg)", "\t\treturn", "\t}", "", "\t// Fetch chirp to confirm user owns it", "\trow, err := cfg.db.GetChirpByID(request.Context(), chirpID)", "\tif err != nil {", "\t\tmsg := fmt.Sprintf(\"chirps: Problem retrieving chirp with id '%s': %s\", chirpID, err)", "\t\tlog.Println(msg)", "\t\trespondWithError(response, http.StatusNotFound, msg)", "\t\treturn", "\t}", "", "\tif row.UserID != userID {", "\t\tmsg := fmt.Sprintf(\"chirps: User '%s' does not own chirp '%s'\", userID, chirpID)", "\t\tlog.Println(msg)", "\t\trespondWithError(response, http.StatusForbidden, msg)", "\t\treturn", "\t}", "", "\t// Delete chirp", "\terr = cfg.db.DeleteChirpByID(request.Context(), chirpID)", "\tif err != nil {\t", "\t\tmsg := fmt.Sprintf(\"chirps: Problem deleting chirp with id '%s': %s\", chirpID, err)", "\t\tlog.Println(msg)", "\t\trespondWithError(response, http.StatusInternalServerError, msg)", "\t\treturn", "\t}", "", "\t// Respond with success", "\tresponse.WriteHeader(http.StatusNoContent)"], "file_path": "handler_chirps.go"}
{"Link_to_commit": "https://github.com/mihaigalos/mkdocs-breadcrumbs-plugin/commit/aafeaf64c4150e6c0a5e1e1047760a2375dd99b4", "n-gram matched": "generated by copilot", "n_lines_longer_change": 8, "n_files_impacted": 1, "longest_chunk": ["            ", "            if self.config['use_page_titles']:", "                title = page.meta.get('title', unquote(part))", "            else:", "                title = unquote(part)", "                ", "            breadcrumbs.append(f\"[{title}]({crumb_url})\")", "            self.logger.debug(f'Added breadcrumb: {title} with URL: {crumb_url}')"], "file_path": "mkdocs_breadcrumbs_plugin/plugin.py"}
{"Link_to_commit": "https://github.com/venzy/chirpy/commit/a64c798cc22535060ef70a7d553165bc1ab7165d", "n-gram matched": "generated by copilot", "n_lines_longer_change": 16, "n_files_impacted": 3, "longest_chunk": ["}", "", "func GetAPIKey(headers http.Header) (string, error) {", "\tauthHeader := headers.Get(\"Authorization\")", "\tif authHeader == \"\" {", "\t\treturn \"\", errors.New(\"Missing Authorization header\")", "\t}", "\tconst prefix = \"ApiKey \"", "\tif !strings.HasPrefix(authHeader, prefix) {", "\t\treturn \"\", errors.New(\"Authorization header must start with 'ApiKey '\")", "\t}", "\tapiKey := strings.TrimSpace(strings.TrimPrefix(authHeader, prefix))", "\tif apiKey == \"\" {", "\t\treturn \"\", errors.New(\"API key is empty\")", "\t}", "\treturn apiKey, nil"], "file_path": "internal/auth/auth.go"}
{"Link_to_commit": "https://github.com/fbartelt/robotics-experiments/commit/7d3ba60ffc4169c2df3a0c8f88eaa72b32396baa", "n-gram matched": "generated by copilot", "n_lines_longer_change": 23, "n_files_impacted": 3, "longest_chunk": ["/**", " * @brief Constructs a System object with the given parameters.", " *", " * @param l1_ The length of link 1.", " * @param l2_ The length of link 2.", " * @param lc1_ The distance from the origin to the center of mass of link 1.", " * @param lc2_ The distance from the origin to the center of mass of link 2.", " * @param m1_ The mass of link 1.", " * @param m2_ The mass of link 2.", " * @param m1_bar_ The mass uncertainty of link 1.", " * @param m2_bar_ The mass uncertainty of link 2.", " * @param g__ The acceleration due to gravity.", " * @param I1_ The moment of inertia of link 1.", " * @param I2_ The moment of inertia of link 2.", " * @param epsilon_ The barrier width.", " * @param l_ The adaptive parameter \\ell.", " * @param n_ The number of degrees of freedom.", " * @param varrho_ The parameter varrho.", " * @param A_ The matrix A.", " * @param B_ The matrix B.", " * @param L_ The matrix L.", " * @param Xi_ The matrix Xi.", " */"], "file_path": "cpp/cruzancona.cpp"}
{"Link_to_commit": "https://github.com/hkw1831/obsidian-action-status-updater/commit/7bd259675245dbfa1994f19aa08c086dc7e65e1c", "n-gram matched": "generated by copilot", "n_lines_longer_change": 150, "n_files_impacted": 3, "longest_chunk": ["import { ItemView, WorkspaceLeaf, TFile, Keymap, PaneType, Notice, Menu, MarkdownView } from 'obsidian';", "import { getRecentNotes, getRecentNotesWithInfo, RecentNoteInfo } from 'selfutil/getRecentNotes';", "import { getNoteType, NoteType } from 'selfutil/getTaskTag';", "", "export const VIEW_TYPE_RECENT_VIEWED_NOTES = 'recent-viewed-notes-view';", "", "class RecentViewedNotesView extends ItemView {", "  public currentNotesPath: string;", "", "  constructor(leaf: WorkspaceLeaf, notesTypeTag: string) {", "    super(leaf);", "    this.currentNotesPath = notesTypeTag;", "  }", "", "  getViewType() {", "    return VIEW_TYPE_RECENT_VIEWED_NOTES;", "  }", "", "  getDisplayText() {", "    return 'Recent Viewed Notes';", "  }", "", "  async onOpen() {", "    this.redraw(true);", "  }", "", "  public getIcon(): string {", "    return 'history';", "  }", "", "  public readonly redraw = async (forceRedraw: boolean): Promise<void> => {", "    this.containerEl.empty();", "    ", "    // Get the combined and sorted list of recently viewed and modified notes", "    const recentlyViewedNotes = getRecentNotesWithInfo(this.app, 100);", "    ", "    const rootEl = this.containerEl.createDiv({ cls: 'nav-folder mod-root scrollable' });", "    const childrenEl = rootEl.createDiv({ cls: 'nav-folder-children' });", "    ", "    for (let noteInfo of recentlyViewedNotes) {", "      const file = this.app.vault.getAbstractFileByPath(noteInfo.path);", "      if (!file || !(file instanceof TFile)) continue;", "      ", "      const navFile = childrenEl.createDiv({", "        cls: 'tree-item nav-file recent-viewed-notes-file',", "      });", "      ", "      const navFileTitle = navFile.createDiv({", "        cls: 'tree-item-self is-clickable nav-file-title recent-viewed-notes-title',", "      });", "      ", "      const navFileTitleContent = navFileTitle.createDiv({", "        cls: 'tree-item-inner nav-file-title-content recent-viewed-notes-title-content internal-link self-wrap-content',", "      });", "      ", "      // Just display the filename without the path for cleaner UI", "      const noteType : NoteType | null = getNoteType(file.path)", "      const prefix = noteType ? (noteType.prefix ? noteType.prefix + \" \" : \"\") : \"\"", "      navFileTitleContent.setText(prefix + file.path);", "      ", "      // Add metadata as subtitle with time info", "      const navFileSubtitle = navFileTitle.createDiv({", "        cls: 'tree-item-flair recent-viewed-notes-subtitle',", "      });", "", "     // Show folder path and relative time info", "     const formattedDate = this.getRelativeTimeString(noteInfo);", "     navFileSubtitle.setText(` \u2022 ${formattedDate}`);", "      ", "      // Add right-click menu", "      navFileTitle.addEventListener('contextmenu', (event: MouseEvent) => {", "        const menu = new Menu();", "        menu.addItem((item) =>", "          item", "            .setSection('action')", "            .setTitle('Open in new tab')", "            .setIcon('file-plus')", "            .onClick(() => {", "              this.focusFile(file, 'tab');", "            })", "        );", "        ", "        this.app.workspace.trigger(", "          'file-menu',", "          menu,", "          file,", "          'link-context-menu',", "        );", "        ", "        menu.showAtPosition({ x: event.clientX, y: event.clientY });", "      });", "", "      // Add click handler to open the file", "      navFileTitle.addEventListener('click', (event: MouseEvent) => {  ", "        const newLeaf = Keymap.isModEvent(event);", "        this.focusFile(file, newLeaf);", "      });", "    }", "    ", "    // Add a message if no recently viewed notes are found", "    if (recentlyViewedNotes.length === 0) {", "      const emptyState = childrenEl.createDiv({", "        cls: 'nav-folder-empty-state',", "      });", "      emptyState.setText('No recently viewed notes found');", "    }", "  }", "  ", "  // Helper method to format the time display based on recency", "  private getRelativeTimeString(noteInfo: RecentNoteInfo): string {", "    const now = Date.now();", "    // Use the most recent time between last viewed and modified", "    const mostRecentTime = Math.max(noteInfo.lastViewed, noteInfo.mtime);", "    const diffMinutes = Math.floor((now - mostRecentTime) / 60000);", "    ", "    if (diffMinutes < 1) return 'just now';", "    if (diffMinutes < 60) return `${diffMinutes}m ago`;", "    ", "    const diffHours = Math.floor(diffMinutes / 60);", "    if (diffHours < 24) return `${diffHours}h ago`;", "    ", "    const diffDays = Math.floor(diffHours / 24);", "    if (diffDays < 7) return `${diffDays}d ago`;", "    ", "    // For older items, show the actual date", "    return new Date(mostRecentTime).toLocaleDateString();", "  }", "  ", "  async onClose() {", "    // Cleanup if necessary", "  }", "  ", "  private readonly focusFile = (file: TFile, newLeaf: boolean | PaneType): void => {", "    if (file) {", "      const leaf = this.app.workspace.getLeaf(newLeaf);", "      leaf.openFile(file).then(() => {", "        const view = this.app.workspace.getActiveViewOfType(MarkdownView);", "        if (view) {", "          // Focus on the beginning of the file", "          view.editor.setCursor({ line: 0, ch: 0 });", "          view.editor.scrollIntoView({ from: { line: 0, ch: 0 }, to: { line: 0, ch: 0 } }, true);", "        }", "      });", "    } else {", "      new Notice('Cannot find a file with that name');", "    }", "  };", "}", "", "export { RecentViewedNotesView };"], "file_path": "recentViewedNotesView.ts"}
{"Link_to_commit": "https://github.com/dbedggood/trademe-property-blacklist/commit/2887f84909f8bdf774e0adbc50c264faf8960094", "n-gram matched": "generated by copilot", "n_lines_longer_change": 23, "n_files_impacted": 4, "longest_chunk": ["<!DOCTYPE html>", "<html>", "  <head>", "    <title>Trademe Property Blacklist</title>", "    <style>", "      body {", "        width: 200px;", "        height: 100px;", "        display: flex;", "        justify-content: center;", "        align-items: center;", "      }", "      button {", "        padding: 10px;", "        font-size: 16px;", "      }", "    </style>", "  </head>", "  <body>", "    <button id=\"changeColor\">Change Color</button>", "    <script src=\"popup.js\"></script>", "  </body>", "</html>"], "file_path": "popup.js"}
{"Link_to_commit": "https://github.com/scottcode/wordle-model/commit/44dd4a9b1d90c1b513629c626a72fa11d0f1907a", "n-gram matched": "generated by copilot", "n_lines_longer_change": 29, "n_files_impacted": 1, "longest_chunk": ["import unittest", "from wordle import Color, Feedback, Attempt, give_feedback, Game", "", "class TestWordle(unittest.TestCase):", "    def test_feedback_is_correct(self):", "        feedback = Feedback([Color.green, Color.green, Color.green, Color.green, Color.green])", "        self.assertTrue(feedback.is_correct())", "", "    def test_attempt_is_correct(self):", "        attempt = Attempt('helps', [Color.green, Color.green, Color.green, Color.green, Color.green])", "        self.assertTrue(attempt.is_correct())", "", "    def test_give_feedback(self):", "        attempt = give_feedback('smile', 'helps')", "        self.assertEqual(attempt.feedback, [Color.grey, Color.green, Color.grey, Color.grey, Color.grey])", "", "    def test_game_guess_word(self):", "        game = Game('helps')", "        attempt = game.guess_word('smile')", "        self.assertEqual(attempt.feedback, [Color.grey, Color.green, Color.grey, Color.grey, Color.grey])", "", "    def test_game_n_attempts(self):", "        game = Game('helps')", "        game.guess_word('smile')", "        game.guess_word('tells')", "        self.assertEqual(game.n_attempts, 2)", "", "if __name__ == '__main__':", "    unittest.main()"], "file_path": "tests_from_copilot.py"}
{"Link_to_commit": "https://github.com/Allexsen/Learning-Project/commit/263fcadf9221e5e48a53dcfbb14afa90bf362f62", "n-gram matched": "generated by copilot", "n_lines_longer_change": 10, "n_files_impacted": 3, "longest_chunk": ["\tID            int64  `db:\"id\" json:\"id,omitempty\"`                           // Unique user id", "\tFirstname     string `db:\"firstname\" json:\"first_name,omitempty\"`            // Firstname", "\tLastname      string `db:\"lastname\" json:\"last_name,omitempty\"`              // Lastname", "\tEmail         string `db:\"email\" json:\"email,omitempty\"`                     // Email", "\tUsername      string `db:\"username\" json:\"username,omitempty\"`               // Unique username", "\tFriendsCount  int    `db:\"friends_count\" json:\"friends_count,omitempty\"`     // Number of friends", "\tProfilePicURL string `db:\"profile_pic_url\" json:\"profile_pic_url,omitempty\"` // Profile picture", "\tPostsCount    int    `db:\"posts_count\" json:\"posts_count,omitempty\"`         // Number of posts", "\tCommentsCount int    `db:\"comments_count\" json:\"comments_count,omitempty\"`   // Number of comments", "\tLikesCount    int    `db:\"likes_count\" json:\"likes_count,omitempty\"`         // Number of likes"], "file_path": "internal/models/user/user_model.go"}
{"Link_to_commit": "https://github.com/ariel-ortiz/202411-tc2005b.402/commit/39cce3f92499c1e3b7a965bac62edd40e1002543", "n-gram matched": "generated by copilot", "n_lines_longer_change": 6, "n_files_impacted": 1, "longest_chunk": ["const dwarfs = ['Thorin', 'Balin', 'Dwalin', 'Fili', 'Kili', 'Dori', 'Nori',", "                'Ori', 'Oin', 'Gloin', 'Bifur', 'Bofur', 'Bombur'];", "", "for (const dwarf of dwarfs) {", "    console.log(dwarf);", "}"], "file_path": "4. JavaScript servidor/Ejemplo 2/dwarfs.js"}
{"Link_to_commit": "https://github.com/EllsworthLogan/417final/commit/d7bd9c96db3d890658d05a765fee09c3478a83c2", "n-gram matched": "generated by copilot", "n_lines_longer_change": 30, "n_files_impacted": 1, "longest_chunk": ["import org.junit.jupiter.api.Test;", "import static org.junit.jupiter.api.Assertions.assertEquals;", "", "public class TriangleTest {", "", "    @Test", "    public void testInvalidTriangle() {", "        assertEquals(TriangleType.INVALID, Triangle.classify(0, 0, 0));", "        assertEquals(TriangleType.INVALID, Triangle.classify(-1, 2, 3));", "        assertEquals(TriangleType.INVALID, Triangle.classify(1, -2, 3));", "        assertEquals(TriangleType.INVALID, Triangle.classify(1, 2, -3));", "    }", "", "    @Test", "    public void testEquilateralTriangle() {", "        assertEquals(TriangleType.EQUILATERAL, Triangle.classify(5, 5, 5));", "    }", "", "    @Test", "    public void testIsoscelesTriangle() {", "        assertEquals(TriangleType.ISOSCELES, Triangle.classify(5, 5, 3));", "        assertEquals(TriangleType.ISOSCELES, Triangle.classify(3, 5, 5));", "        assertEquals(TriangleType.ISOSCELES, Triangle.classify(5, 3, 5));", "    }", "", "    @Test", "    public void testScaleneTriangle() {", "        assertEquals(TriangleType.SCALENE, Triangle.classify(3, 4, 5));", "    }", "}"], "file_path": "prompt1.java"}
{"Link_to_commit": "https://github.com/chantalsantos7/dfa-challenge-3-address-book/commit/38b187cebe88a79f11c0862a15ff7969bd6bb863", "n-gram matched": "generated by copilot", "n_lines_longer_change": 8, "n_files_impacted": 1, "longest_chunk": ["    /**", "     * Validates the given contact detail based on the specified type. Documentation generated by Copilot.", "     *", "     * @param contactDetail The contact detail to validate. This should be a phone number or an email address.", "     * @param contactDetailType The type of the contact detail. This should be PHONE_NUMBER or EMAIL_ADDRESS.", "     * @return true if the contact detail is valid, false otherwise.", "     * @throws IllegalArgumentException If the contactDetail is null or empty.", "     */"], "file_path": "src/main/java/com/challenges/helpers/ValidatorHelpers.java"}
{"Link_to_commit": "https://github.com/venzy/chirpy/commit/416c083648d21283c3622fe6bef93c144b2f3d91", "n-gram matched": "generated by copilot", "n_lines_longer_change": 89, "n_files_impacted": 8, "longest_chunk": ["// Code generated by sqlc. DO NOT EDIT.", "// versions:", "//   sqlc v1.28.0", "// source: refresh_tokens.sql", "", "package database", "", "import (", "\t\"context\"", "\t\"database/sql\"", "\t\"time\"", "", "\t\"github.com/google/uuid\"", ")", "", "const createRefreshToken = `-- name: CreateRefreshToken :one", "INSERT INTO refresh_tokens (token, created_at, updated_at, user_id, expires_at)", "VALUES (", "    $1,         -- token", "    NOW(),      -- created_at", "    NOW(),      -- updated_at", "    $2,         -- user_id", "    $3          -- expires_at", ")", "RETURNING token, created_at, updated_at, user_id, expires_at, revoked_at", "`", "", "type CreateRefreshTokenParams struct {", "\tToken     string", "\tUserID    uuid.UUID", "\tExpiresAt time.Time", "}", "", "func (q *Queries) CreateRefreshToken(ctx context.Context, arg CreateRefreshTokenParams) (RefreshToken, error) {", "\trow := q.db.QueryRowContext(ctx, createRefreshToken, arg.Token, arg.UserID, arg.ExpiresAt)", "\tvar i RefreshToken", "\terr := row.Scan(", "\t\t&i.Token,", "\t\t&i.CreatedAt,", "\t\t&i.UpdatedAt,", "\t\t&i.UserID,", "\t\t&i.ExpiresAt,", "\t\t&i.RevokedAt,", "\t)", "\treturn i, err", "}", "", "const deleteRefreshTokenByToken = `-- name: DeleteRefreshTokenByToken :exec", "DELETE FROM refresh_tokens WHERE token = $1", "`", "", "func (q *Queries) DeleteRefreshTokenByToken(ctx context.Context, token string) error {", "\t_, err := q.db.ExecContext(ctx, deleteRefreshTokenByToken, token)", "\treturn err", "}", "", "const getUserIDWithRefreshToken = `-- name: GetUserIDWithRefreshToken :one", "SELECT ", "    users.id AS user_id,", "    refresh_tokens.expires_at,", "    refresh_tokens.revoked_at", "FROM refresh_tokens", "JOIN users ON refresh_tokens.user_id = users.id", "WHERE refresh_tokens.token = $1", "`", "", "type GetUserIDWithRefreshTokenRow struct {", "\tUserID    uuid.UUID", "\tExpiresAt time.Time", "\tRevokedAt sql.NullTime", "}", "", "func (q *Queries) GetUserIDWithRefreshToken(ctx context.Context, token string) (GetUserIDWithRefreshTokenRow, error) {", "\trow := q.db.QueryRowContext(ctx, getUserIDWithRefreshToken, token)", "\tvar i GetUserIDWithRefreshTokenRow", "\terr := row.Scan(&i.UserID, &i.ExpiresAt, &i.RevokedAt)", "\treturn i, err", "}", "", "const revokeRefreshToken = `-- name: RevokeRefreshToken :exec", "UPDATE refresh_tokens", "SET revoked_at = NOW(), updated_at = NOW()", "WHERE token = $1 AND revoked_at IS NULL", "`", "", "func (q *Queries) RevokeRefreshToken(ctx context.Context, token string) error {", "\t_, err := q.db.ExecContext(ctx, revokeRefreshToken, token)", "\treturn err", "}"], "file_path": "main.go"}
{"Link_to_commit": "https://github.com/ariel-ortiz/202411-tc2005b.402/commit/01adcb98584b1087e2ec705df07475a4f53cbe17", "n-gram matched": "generated by copilot", "n_lines_longer_change": 14, "n_files_impacted": 1, "longest_chunk": ["const http = require('http');", "", "const PORT = 8080;", "const IP = '52.20.170.244';", "", "const server = http.createServer((req, res) => {", "    res.statusCode = 200;", "    res.setHeader('Content-Type', 'text/plain');", "    res.end('I am Groot\\n');", "});", "", "server.listen(PORT, () => {", "    console.log(`Server running at http://${IP}:${PORT}/`);", "});"], "file_path": "4. JavaScript servidor/Ejemplo 3/server.js"}
{"Link_to_commit": "https://github.com/RealDotNetDave/dotNetTips.Spargine.8/commit/3a6fcc24b516c8daeeaabf74838fbfefbbfc38d4", "n-gram matched": "generated by copilot", "n_lines_longer_change": 12, "n_files_impacted": 2, "longest_chunk": ["\t\tforeach (var item in items)", "\t\t{", "\t\t\tawait channel.WriteAsync(item);", "\t\t}", "", "\t\tchannel.Lock(); // Lock the channel to allow ListenAsync to complete.", "", "\t\tvar readItems = new List<int>();", "\t\tawait foreach (var item in channel.ListenAsync())", "\t\t{", "\t\t\treadItems.Add(item);", "\t\t}"], "file_path": "source/Unit Tests/dotNetTips.Spargine.Core.Tests/Collections/Generic/Concurrent/ChannelQueueTests.cs"}
{"Link_to_commit": "https://github.com/chantalsantos7/dfa-challenge-3-address-book/commit/8bc57e411577d596c31cdab7c0b067410a9c9223", "n-gram matched": "generated by copilot", "n_lines_longer_change": 29, "n_files_impacted": 4, "longest_chunk": ["        //COPILOT PROMPT: The phone and email overload for findContact should allow searching for an empty string", "        //So it should return the first Contact that has an empty string for phone number and email respectively", "", "        //Below tests generated by Copilot", "        @Test", "        @DisplayName(\"FindContact should return the first Contact with an empty phone number if searchCriteria is an empty string\")", "        public void testFindContactReturnsContactWithEmptyPhoneNumberIfSearchCriteriaIsEmptyString()", "        {", "            Contact contact = mock(Contact.class);", "            when(contact.getName()).thenReturn(\"emptyPhoneNumber\");", "            when(contact.getPhoneNumber()).thenReturn(\"\");", "            when(contact.getEmailAddress()).thenReturn(\"testEmail\");", "            addressBook.addContact(contact);", "            assertEquals(contact, addressBook.findContact(\"\", ContactDetailType.PHONE_NUMBER));", "        }", "", "        @Test", "        @DisplayName(\"FindContact should return the first Contact with an empty email if searchCriteria is an empty string\")", "        public void testFindContactReturnsContactWithEmptyEmailIfSearchCriteriaIsEmptyString()", "        {", "            Contact contact = mock(Contact.class);", "            when(contact.getName()).thenReturn(\"emptyEmail\");", "            when(contact.getPhoneNumber()).thenReturn(\"12345678910\");", "            when(contact.getEmailAddress()).thenReturn(\"\");", "            addressBook.addContact(contact);", "            assertEquals(contact, addressBook.findContact(\"\", ContactDetailType.EMAIL_ADDRESS));", "        }", "", ""], "file_path": "src/test/java/com/challenges/addressbook/AddressBookTest.java"}
{"Link_to_commit": "https://github.com/mixicz/homeassistant-tower-discovery/commit/ee2001f04fc85adc074fca7e97a65f92a9574607", "n-gram matched": "generated by copilot", "n_lines_longer_change": 86, "n_files_impacted": 1, "longest_chunk": ["import time", "import paho.mqtt.client as mqtt", "import json", "import os", "import argparse", "from jinja2 import Environment, FileSystemLoader", "from flask import Flask", "", "", "class Configuration:", "    def __init__(self):", "        self.mqtt_broker = \"mqtt.example.com\"", "        self.mqtt_port = 1883", "        self.mqtt_topic_discovery = \"gateway/{id}/nodes/get\"", "        self.mqtt_topic_nodes = \"gateway/{id}/nodes\"", "        self.mqtt_topic_advertisement = \"homeassistant/devices\"", "        self.advertise_interval = None", "        self.firmware_dir = \"firmware\"", "", "    def load_from_env(self):", "        self.mqtt_broker = os.getenv(\"MQTT_BROKER\", self.mqtt_broker)", "        self.mqtt_port = os.getenv(\"MQTT_PORT\", self.mqtt_port)", "        self.mqtt_topic_discovery = os.getenv(\"MQTT_TOPIC_DISCOVERY\", self.mqtt_topic_discovery)", "        self.mqtt_topic_nodes = os.getenv(\"MQTT_TOPIC_NODES\", self.mqtt_topic_nodes)", "        self.mqtt_topic_advertisement = os.getenv(\"MQTT_TOPIC_ADVERTISEMENT\", self.mqtt_topic_advertisement)", "", "    def parse_cmd_args(self):", "        parser = argparse.ArgumentParser()", "        parser.add_argument(\"--interval\", type=int, help=\"Advertisement interval in seconds\")", "        args = parser.parse_args()", "        if args.interval:", "            self.advertise_interval = args.interval", "        ", "", "config = Configuration()", "config.load_from_env()", "config.parse_cmd_args()", "", "# Jinja template configuration", "template_loader = FileSystemLoader(config.firmware_dir)", "template_env = Environment(loader=template_loader)", "", "# MQTT client setup", "client = mqtt.Client()", "", "def on_connect(client, userdata, flags, rc):", "    print(\"Connected to MQTT broker\")", "    client.subscribe(config.mqtt_topic_nodes)", "", "def on_message(client, userdata, msg):", "    if msg.topic == config.mqtt_topic_nodes:", "        devices = json.loads(msg.payload)", "        advertise_devices(devices)", "", "def send_discovery_message():", "    client.publish(config.mqtt_topic_discovery, \"\")", "", "def advertise_devices(devices):", "    for device in devices:", "        template = template_env.get_template(device[\"firmware\"] + \".yaml\")", "        json_message = template.render(device=device)", "        client.publish(config.mqtt_topic_advertisement, json_message)", "", "def main():", "    client.on_connect = on_connect", "    client.on_message = on_message", "", "    client.connect(config.mqtt_broker, config.mqtt_port, 60)", "", "    if config.advertise_interval:", "        client.loop_start()", "        while True:", "            send_discovery_message()", "            time.sleep(config.advertise_interval)", "    else:", "        client.loop_forever()", "", "# Flask app setup", "app = Flask(__name__)", "", "@app.route('/health')", "def health_check():", "    return 'OK'", "", "if __name__ == \"__main__\":", "    main()"], "file_path": "src/ha-tower-discovery.py"}
{"Link_to_commit": "https://github.com/valhio/webstore-app/commit/5bb71196a2e11da952a3f80d24b2762d37a7d756", "n-gram matched": "generated by copilot", "n_lines_longer_change": 315, "n_files_impacted": 1, "longest_chunk": ["", "", "", "", "// Some more tests for the StoreService", "describe('StoreService', () => {", "  let service: StoreService;", "  let httpMock: HttpTestingController;", "", "  const STORE_BASE_URL = 'http://localhost:8080/api/v1';", "", "  beforeEach(() => {", "    TestBed.configureTestingModule({", "      imports: [HttpClientTestingModule],", "      providers: [StoreService],", "    });", "    service = TestBed.inject(StoreService);", "    httpMock = TestBed.inject(HttpTestingController);", "  });", "", "  afterEach(() => {", "    httpMock.verify();", "  });", "", "  it('should be created', () => {", "    expect(service).toBeTruthy();", "  });", "", "  it('should fetch API data with default parameters', () => {", "    const keyword = '';", "    const page = 0;", "    const size = 10;", "    const sort = 'asc';", "    const expectedUrl = `${STORE_BASE_URL}/products?keyword=${keyword}&page=${page}&size=${size}&sort=${sort}`;", "    const expectedResponse: ApiResponse<Page<Product[]>> = {", "      data: {", "        content: [],", "        totalPages: 0,", "        totalElements: 0,", "        number: 0,", "        size: 10,", "        pageable: {", "          sort: {", "            empty: false,", "            sorted: false,", "            unsorted: false", "          },", "          offset: 0,", "          pageNumber: 0,", "          pageSize: 0,", "          paged: false,", "          unpaged: false", "        },", "        last: false,", "        sort: {", "          empty: false,", "          sorted: false,", "          unsorted: false", "        },", "        numberOfElements: 0,", "        first: false,", "        empty: false", "      },", "      timeStamp: '',", "      statusCode: 0,", "      status: '',", "      message: ''", "    };", "", "    service.fetchApiData().subscribe((res) => {", "      expect(res).toEqual(expectedResponse);", "    });", "", "    const req = httpMock.expectOne(expectedUrl);", "    expect(req.request.method).toBe('GET');", "    req.flush(expectedResponse);", "  });", "", "  it('should fetch API data with custom parameters', () => {", "    const keyword = 'test';", "    const page = 1;", "    const size = 12;", "    const sort = 'name-asc';", "    const expectedUrl = `${STORE_BASE_URL}/products?keyword=${keyword}&page=${page}&size=${size}&sort=${sort}`;", "    const expectedResponse: ApiResponse<Page<Product[]>> = {", "      data: {", "        content: [],", "        totalPages: 0,", "        totalElements: 0,", "        number: 0,", "        size: 10,", "        pageable: {", "          sort: {", "            empty: false,", "            sorted: false,", "            unsorted: false", "          },", "          offset: 0,", "          pageNumber: 0,", "          pageSize: 0,", "          paged: false,", "          unpaged: false", "        },", "        last: false,", "        sort: {", "          empty: false,", "          sorted: false,", "          unsorted: false", "        },", "        numberOfElements: 0,", "        first: false,", "        empty: false", "      },", "      timeStamp: '',", "      statusCode: 0,", "      status: '',", "      message: ''", "    };", "", "    service.fetchApiData(keyword, page, size, sort).subscribe((res) => {", "      expect(res).toEqual(expectedResponse);", "    });", "", "    const req = httpMock.expectOne(expectedUrl);", "    expect(req.request.method).toBe('GET');", "    req.flush(expectedResponse);", "  });", "", "  it('should find product by id', () => {", "    const productId = 1;", "    const expectedUrl = `${STORE_BASE_URL}/products/${productId}`;", "    const expectedResponse = { id: 1, name: 'Test Product', price: 9.99 };", "", "    service.findProductById(productId).subscribe((res) => {", "      expect(res).toEqual(expectedResponse);", "    });", "", "    const req = httpMock.expectOne(expectedUrl);", "    expect(req.request.method).toBe('GET');", "    req.flush(expectedResponse);", "  });", "", "  it('should place order', () => {", "    const order = { id: 1, name: 'Test Order', total: 9.99 };", "    const expectedUrl = `${STORE_BASE_URL}/orders/new`;", "    const expectedResponse: ApiResponse<any> = {", "      data: order,", "      timeStamp: '',", "      statusCode: 0,", "      status: '',", "      message: ''", "    };", "", "    service.placeOrder(order).subscribe((res) => {", "      expect(res).toEqual(expectedResponse);", "    });", "", "    const req = httpMock.expectOne(expectedUrl);", "    expect(req.request.method).toBe('POST');", "    req.flush(expectedResponse);", "  });", "", "  it('should get orders for user', () => {", "    const userId = '1';", "    const expectedUrl = `${STORE_BASE_URL}/orders/user/${userId}`;", "    const expectedResponse = [{ id: 1, name: 'Test Order', total: 9.99 }];", "", "    service.getOrdersForUser(userId).subscribe((res) => {", "      expect(res).toEqual(expectedResponse);", "    });", "", "    const req = httpMock.expectOne(expectedUrl);", "    expect(req.request.method).toBe('GET');", "    req.flush(expectedResponse);", "  });", "", "  it('should get order by order id', () => {", "    const orderId = '1';", "    const expectedUrl = `${STORE_BASE_URL}/orders/id/${orderId}`;", "    const expectedResponse = { id: 1, name: 'Test Order', total: 9.99 };", "", "    service.getOrderById(orderId).subscribe((res) => {", "      expect(res).toEqual(expectedResponse);", "    });", "", "    const req = httpMock.expectOne(expectedUrl);", "    expect(req.request.method).toBe('GET');", "    req.flush(expectedResponse);", "  });", "", "  it('should get product by product id', () => {", "    const productId = '1';", "    const expectedUrl = `${STORE_BASE_URL}/products/${productId}`;", "    const expectedResponse = { id: 1, name: 'Test Product', price: 9.99 };", "", "    service.getProductByProductId(productId).subscribe((res) => {", "      expect(res).toEqual(expectedResponse);", "    });", "", "    const req = httpMock.expectOne(expectedUrl);", "    expect(req.request.method).toBe('GET');", "    req.flush(expectedResponse);", "  });", "", "  it('should update order status', () => {", "    const orderId = '1';", "    const status = 'SHIPPED';", "    const expectedUrl = `${STORE_BASE_URL}/orders/${orderId}/status/${status}`;", "    const expectedResponse = { id: 1, name: 'Test Order', total: 9.99, status: 'SHIPPED' };", "", "    service.updateOrderStatus(orderId, status).subscribe((res) => {", "      expect(res).toEqual(expectedResponse);", "    });", "", "    const req = httpMock.expectOne(expectedUrl);", "    expect(req.request.method).toBe('PUT');", "    req.flush(expectedResponse);", "  });", "", "  it('should update order item status', () => {", "    const orderNumber = '1';", "    const orderItemId = '1';", "    const status = 'SHIPPED';", "    const expectedUrl = `${STORE_BASE_URL}/orders/${orderNumber}/orderItem/${orderItemId}/status/${status}`;", "    const expectedResponse = { id: '1', name: 'Test Order Item', total: 9.99, status: 'SHIPPED' };", "", "    service.updateOrderItemStatus(orderNumber, orderItemId, status).subscribe((res) => {", "      expect(res).toEqual(expectedResponse);", "    });", "", "    const req = httpMock.expectOne(expectedUrl);", "    expect(req.request.method).toBe('PUT');", "    req.flush(expectedResponse);", "  });", "", "  it('should get user by user id', () => {", "    const userId = '1';", "    const expectedUrl = `${STORE_BASE_URL}/user/${userId}`;", "    const expectedResponse = { id: '1', name: 'Test User' };", "", "    service.getUserByUserId(userId).subscribe((res) => {", "      expect(res).toEqual(expectedResponse);", "    });", "", "    const req = httpMock.expectOne(expectedUrl);", "    expect(req.request.method).toBe('GET');", "    req.flush(expectedResponse);", "  });", "", "  it('should update user first name', () => {", "    const userId = '1';", "    const firstName = 'John';", "    const expectedUrl = `${STORE_BASE_URL}/user/${userId}/first-name`;", "", "    service.updateUserFirstName(userId, firstName).subscribe((res) => {", "      expect(res).toEqual(firstName);", "    });", "", "    const req = httpMock.expectOne(expectedUrl);", "    expect(req.request.method).toBe('PUT');", "    expect(req.request.body).toBe(firstName);", "    req.flush(firstName);", "  });", "", "  it('should update user last name', () => {", "    const userId = '1';", "    const lastName = 'Doe';", "    const expectedUrl = `${STORE_BASE_URL}/user/${userId}/last-name`;", "", "    service.updateUserLastName(userId, lastName).subscribe();", "", "    const req = httpMock.expectOne(expectedUrl);", "    expect(req.request.method).toBe('PUT');", "    expect(req.request.body).toBe(lastName);", "    req.flush(null);", "  });", "", "  it('should update user email', () => {", "    const email = 'test@example.com';", "    const newEmail = 'newtest@example.com';", "    const expectedUrl = `${STORE_BASE_URL}/user/update-email?email=${email}&newEmail=${newEmail}`;", "", "    service.updateUserEmail(email, newEmail).subscribe();", "", "    const req = httpMock.expectOne(expectedUrl);", "    expect(req.request.method).toBe('PUT');", "    req.flush(null);", "  });", "", "  it('should update user phone number', () => {", "    const userId = '1';", "    const phone = '1234567890';", "    const expectedUrl = `${STORE_BASE_URL}/user/${userId}/phone-number`;", "", "    service.updateUserPhoneNumber(userId, phone).subscribe();", "", "    const req = httpMock.expectOne(expectedUrl);", "    expect(req.request.method).toBe('PUT');", "    expect(req.request.body).toBe(phone);", "    req.flush(null);", "  });", "", "  it('should update user address', () => {", "    const userId = '1';", "    const address = '123 Foo St';", "    const expectedUrl = `${STORE_BASE_URL}/user/${userId}/address`;", "", "    service.updateUserAddress(userId, address).subscribe();", "", "    const req = httpMock.expectOne(expectedUrl);", "    expect(req.request.method).toBe('PUT');", "    expect(req.request.body).toBe(address);", "    req.flush(null);", "  });", "});"], "file_path": "src/app/services/store.service.spec.ts"}
{"Link_to_commit": "https://github.com/SalamanderCtesiphon/ticTacToeProject/commit/5023ce46731af48ac967175171638de634430176", "n-gram matched": "copilot to create", "n_lines_longer_change": 83, "n_files_impacted": 1, "longest_chunk": ["    let gameBoard = Array(9).fill(null);", "    let currentPlayer = 'X';", "    let gameOver = false;", "    let winner = null;", "    let moveCount = 0;", "    const winningCombinations = [", "        [0, 1, 2], [3, 4, 5], [6, 7, 8], // Rows", "        [0, 3, 6], [1, 4, 7], [2, 5, 8], // Columns", "        [0, 4, 8], [2, 4, 6] // Diagonals", "    ];", "    const players = {", "        X: { name: 'Player 1', symbol: 'X' },", "        O: { name: 'Player 2', symbol: 'O' }", "    };", "    const gameStatus = {", "        getCurrentPlayer: () => currentPlayer,", "        getGameOver: () => gameOver,", "        getWinner: () => winner,", "        getMoveCount: () => moveCount,", "        getGameBoard: () => gameBoard.slice(),", "        getPlayers: () => players", "    };", "    const gameActions = {", "        makeMove: (index) => {", "            if (gameOver || gameBoard[index] !== null) return false;", "            gameBoard[index] = currentPlayer;", "            moveCount++;", "            if (checkWinner()) {", "                gameOver = true;", "            } else if (moveCount === 9) {", "                gameOver = true; // Draw", "            } else {", "                currentPlayer = currentPlayer === 'X' ? 'O' : 'X';", "            }", "            return true;", "        },", "        resetGame: () => {", "            gameBoard.fill(null);", "            currentPlayer = 'X';", "            gameOver = false;", "            winner = null;", "            moveCount = 0;", "        }", "    };", "    function checkWinner() {", "        for (const combination of winningCombinations) {", "            const [a, b, c] = combination;", "            if (gameBoard[a] && gameBoard[a] === gameBoard[b] && gameBoard[a] === gameBoard[c]) {", "                winner = gameBoard[a];", "                return true;", "            }", "        }", "        return false;", "    }", "    return { gameStatus, gameActions }; ", "}", "", "// Example usage:", "const ticTacToe = createGame();", "console.log(ticTacToe.gameStatus.getGameBoard()); // Initial empty board", "ticTacToe.gameActions.makeMove(0); // Player X makes a move     ", "console.log(ticTacToe.gameStatus.getGameBoard()); // Board after move", "ticTacToe.gameActions.makeMove(1); // Player O makes a move", "console.log(ticTacToe.gameStatus.getGameBoard()); // Board after move", "ticTacToe.gameActions.makeMove(3); // Player X makes a move", "console.log(ticTacToe.gameStatus.getGameBoard()); // Board after move", "ticTacToe.gameActions.makeMove(4); // Player O makes a move", "console.log(ticTacToe.gameStatus.getGameBoard()); // Board after move", "ticTacToe.gameActions.makeMove(6); // Player X makes a move", "console.log(ticTacToe.gameStatus.getGameBoard()); // Board after move", "ticTacToe.gameActions.makeMove(7); // Player O makes a move", "console.log(ticTacToe.gameStatus.getGameBoard()); // Board after move", "ticTacToe.gameActions.makeMove(8); // Player X makes a move", "console.log(ticTacToe.gameStatus.getGameBoard()); // Board after move", "console.log(ticTacToe.gameStatus.getWinner()); // Check winner", "console.log(ticTacToe.gameStatus.getGameOver()); // Check if game is over", "ticTacToe.gameActions.resetGame(); // Reset the game", "console.log(ticTacToe.gameStatus.getGameBoard()); // Board after reset", "console.log(ticTacToe.gameStatus.getCurrentPlayer()); // Check current player after reset", "console.log(ticTacToe.gameStatus.getMoveCount()); // Check move count after reset", "console.log(ticTacToe.gameStatus.getPlayers()); // Check players information", "// This code implements a simple Tic Tac Toe game using factory functions.", "// It allows two players to take turns making moves, checks for a winner, and provides methods to reset the game."], "file_path": "app.js"}
{"Link_to_commit": "https://github.com/francke-iot/ics_calendar/commit/8be22375fdb807ca6eb36aa8f288a3e8bb8fd1d1", "n-gram matched": "copilot to create", "n_lines_longer_change": 53, "n_files_impacted": 3, "longest_chunk": ["#!/usr/bin/env python3", "from ics import Calendar, Event", "from datetime import datetime", "import argparse", "import pytz", "", "def parse_arguments():", "    parser = argparse.ArgumentParser(description='Create an ICS file for an event.')", "    parser.add_argument('event_name', type=str, help='Name of the event')", "    parser.add_argument('event_begin', type=str, help='Start time of the event (YYYY-MM-DD HH:MM)')", "    parser.add_argument('event_end', type=str, help='End time of the event (YYYY-MM-DD HH:MM)')", "    parser.add_argument('file_name', type=str, help='Name of the output ICS file')", "    return parser.parse_args()", "", "def create_ics_file(event_name, event_begin, event_end, file_name):", "    ", "", "    # Create a new calendar", "    calendar = Calendar()", "", "    # Create a new event", "    event = Event()", "    event.name = event_name", "    event.begin = event_begin  # CET is UTC+1", "    event.end = event_end  # CET is UTC+1", "    event.organizer = \"Alan Francke:mailto:alan@francke-iot.com\"", "    event.location = \"Microsoft Teams\"", "", "    # Add the event to the calendar", "    calendar.events.add(event)", "", "    # Prompt user to enter meeting description", "", "    event.description = input(\"Enter the description for the event: \")", "", "    # Write the calendar to a file", "    with open(file_name, 'w') as f:", "        f.writelines(calendar)", "", "if __name__ == \"__main__\":", "    args = parse_arguments()", "    cet = pytz.timezone('CET')", "    event_begin = cet.localize(datetime.strptime(args.event_begin, '%Y-%m-%d %H:%M'))", "    event_end = cet.localize(datetime.strptime(args.event_end, '%Y-%m-%d %H:%M'))", "    create_ics_file(args.event_name, event_begin, event_end, args.file_name)", "", "# Example usage", "# event_name = \"Meeting with Bob\"", "# event_begin = datetime(2023, 10, 25, 10, 0)", "# event_end = datetime(2023, 10, 25, 11, 0)", "# file_name = \"meeting.ics\"", "", "#create_ics_file(event_name, event_begin, event_end, file_name)"], "file_path": "main.py"}
{"Link_to_commit": "https://github.com/braelynnesandreth/ProjectSandrethF24/commit/0ee74c6ec4c882729279ce303f657551438ce71c", "n-gram matched": "copilot to create", "n_lines_longer_change": 63, "n_files_impacted": 1, "longest_chunk": ["        public IActionResult CreateTestData()", "        {", "            Officer testOfficer7 = new Officer", "            {", "                Id = \"7\",", "                Firstname = \"Test\",", "                Lastname = \"Officer7\",", "                PhoneNumber = \"123-456-7890\",", "                Email = \"testofficer7@example.com\",", "                SupervisorsOfOfficer = new List<Supervises>()", "            };", "", "            Officer testOfficer8 = new Officer", "            {", "                Id = \"8\",", "                Firstname = \"Test\",", "                Lastname = \"Officer8\",", "                PhoneNumber = \"123-456-7891\",", "                Email = \"testofficer8@example.com\",", "                SupervisorsOfOfficer = new List<Supervises>()", "            };", "", "            Supervisor testSupervisor9 = new Supervisor", "            {", "                Id = \"9\",", "                Firstname = \"Test\",", "                Lastname = \"Supervisor9\",", "                PhoneNumber = \"123-456-7892\",", "                Email = \"testsupervisor9@example.com\",", "                OfficersSupervised = new List<Supervises>()", "            };", "", "            Supervisor testSupervisor10 = new Supervisor", "            {", "                Id = \"10\",", "                Firstname = \"Test\",", "                Lastname = \"Supervisor10\",", "                PhoneNumber = \"123-456-7893\",", "                Email = \"testsupervisor10@example.com\",", "                OfficersSupervised = new List<Supervises>()", "            };", "", "            // Establish the relationship between Officer 7 and Supervisor 9", "            Supervises supervises = new Supervises", "            {", "                Officer = testOfficer7,", "                Supervisor = testSupervisor9,", "                StartDate = DateTime.Now", "            };", "", "            testOfficer7.SupervisorsOfOfficer.Add(supervises);", "            testSupervisor9.OfficersSupervised.Add(supervises);", "", "            // Add the test data to the database", "            _database.Officer.Add(testOfficer7);", "            _database.Officer.Add(testOfficer8);", "            _database.Supervisor.Add(testSupervisor9);", "            _database.Supervisor.Add(testSupervisor10);", "            _database.Supervises.Add(supervises);", "            _database.SaveChanges();", "", "            return RedirectToAction(\"Index\");", "        }"], "file_path": "F24S2DiscussionSolutionSandreth/DiscussionMvcSandreth/Controllers/OfficerController.cs"}
{"Link_to_commit": "https://github.com/AdamJCavanaugh/BrainGames/commit/ee23d26fb63239989d05977278db7b36bcf84a9d", "n-gram matched": "copilot to create", "n_lines_longer_change": 172, "n_files_impacted": 7, "longest_chunk": ["class Game {", "    constructor() {", "        this.currentWord = '';", "        this.maxAttempts = 6;", "        this.currentAttempts = 0;", "        this.guessedWords = [];", "    }", "", "    startGame() {", "        this.currentWord = WordChecker.giveNewWord()", "        console.log(this.currentWord)", "        this.currentAttempts = 0;", "        this.guessedWords = [];", "        this.updateUI();", "        this.setFeedback('New game started. Guess the word!');", "    }", "", "    guessWord(word) {", "        if (this.currentAttempts >= this.maxAttempts) {", "            this.setFeedback('No more attempts left!');", "            return;", "        }", "", "        if (!WordChecker.isValidWord(word)) {", "            this.setFeedback('Invalid word. Try again.');", "            return;", "        }", "", "        this.guessedWords.push(this.generateFeedbackHTML(word));", "        this.currentAttempts++;", "", "        if (this.checkWin(word)) {", "            this.updateUI();", "            this.setFeedback('Congratulations! You guessed the word!');", "        } else if (this.checkLoss()) {", "            this.updateUI();", "            this.setFeedback(`Game over! The word was ${this.currentWord}.`);", "        } else {", "            this.updateUI();", "        }", "    }", "", "    checkWin(word) {", "        if (word === this.currentWord) {", "            document.getElementById('guessInput').style.display = 'none';", "            document.getElementById('makeGuess').style.display = 'none';", "            return true;", "        }", "        return false;", "    }", "", "    checkLoss() {", "        if (this.currentAttempts >= this.maxAttempts) {", "            document.getElementById('guessInput').style.display = 'none';", "            document.getElementById('makeGuess').style.display = 'none';", "            return true;", "        }", "        return false;", "    }", "", "    setFeedback(message) {", "        document.getElementById('feedback').innerText = message;", "    }", "", "    updateUI() {", "        document.getElementById('attemptsLeft').innerText = `${this.maxAttempts - this.currentAttempts} attempts left.`;", "        document.getElementById('guessedWords').innerHTML = `Guessed Words:<br>${this.guessedWords.join('<br>')}`;", "    }", "", "    generateFeedbackHTML(word) {", "        const feedback = WordChecker.compareWords(word, this.currentWord);", "        let feedbackHTML = '';", "", "        for (let i = 0; i < word.length; i++) {", "            if (feedback[i] === word[i].toUpperCase()) {", "                feedbackHTML += `<span class=\"correct\">${word[i]}</span>`;", "            } else if (feedback[i] === word[i].toLowerCase()) {", "                feedbackHTML += `<span class=\"misplaced\">${word[i]}</span>`;", "            } else {", "                feedbackHTML += `<span class=\"incorrect\">${word[i]}</span>`;", "            }", "        }", "", "        return feedbackHTML;", "    }", "}", "", "class Player {", "    constructor(name) {", "        this.name = name;", "        this.score = 0;", "    }", "", "    makeGuess(game, word) {", "        game.guessWord(word);", "    }", "}", "", "class WordChecker {", "    constructor(wordList) {", "        this.wordList = wordList;", "    }", "", "    static async loadWords() {", "        try {", "            const response = await fetch('words.txt');", "            const text = await response.text();", "            this.wordList = text.split('\\n').map(word => word.trim()).filter(word => word.length === 5);", "        } catch (error) {", "            console.error('Error loading words:', error);", "        }", "    }", "", "    static giveNewWord() {", "        return this.wordList[Math.floor(Math.random() * this.wordList.length)];", "    }", "", "    static isValidWord(word) {", "        if (this.wordList.includes(word)) {", "            return true", "        }", "        return false;", "    }", "", "    static compareWords(guess, target) {", "        let feedback = '';", "", "        for (let i = 0; i < guess.length; i++) {", "            if (guess[i] === target[i]) {", "                feedback += guess[i].toUpperCase();", "            } else if (target.includes(guess[i])) {", "                feedback += guess[i].toLowerCase();", "            } else {", "                feedback += '_';", "            }", "        }", "", "        return feedback;", "    }", "}", "", "const game = new Game();", "const wordList = new WordChecker()", "const player = new Player('John');", "", "function makeGuess() {", "    const guessInput = document.getElementById('guessInput').value.toLowerCase();", "    if (guessInput.length === 5) {", "        player.makeGuess(game, guessInput);", "        document.getElementById('guessInput').value = '';", "    } else {", "        game.setFeedback('Please enter a 5-letter word.');", "    }", "}", "", "function resetGame() {", "    document.getElementById('guessInput').style.display = 'inline';", "    document.getElementById('guessInput').value = '';", "    document.getElementById('makeGuess').style.display = 'inline';", "    game.startGame();", "}", "", "window.onload = async () => {", "    await WordChecker.loadWords();", "    game.startGame();", "}", "", "document.getElementById('guessInput').addEventListener('keydown', function(event) {", "    if (event.key === 'Enter') {", "        makeGuess();", "    }", "});"], "file_path": "wordle/game.js"}
{"Link_to_commit": "https://github.com/Itablera/learning-demystify-ai/commit/0d0e8bd8707cccba3115f7bb20ac2acee5362c8b", "n-gram matched": "copilot to create", "n_lines_longer_change": 199, "n_files_impacted": 64, "longest_chunk": ["import { ingestDocument, searchDocuments } from '@workspace/use-cases'", "import {", "  IngestDocumentRequestSchema,", "  SearchDocumentsRequestSchema,", "  DocumentListSchema,", "  SearchResultsSchema,", "  BaseResponseSchema,", "  DataResponseSchema,", "  DataResponse,", "  SearchResults,", "} from '@workspace/api'", "import { getDocumentRepository } from '@/repositories'", "import { langchain } from '@workspace/integrations'", "import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter'", "import { RoutesProvider } from '@/index'", "import { DocumentSchema } from '@workspace/domains'", "import z from 'zod'", "import config from '@workspace/env'", "", "const docParam = z.object({", "  id: z.string().uuid('Invalid document ID'),", "})", "", "/**", " * Document domain routes for the API", " */", "export async function documentRoutes(routes: RoutesProvider): Promise<void> {", "  // Initialize dependencies", "  const textSplitter = new RecursiveCharacterTextSplitter({", "    chunkSize: 1000,", "    chunkOverlap: 200,", "  })", "  const documentRepository = getDocumentRepository()", "  const vectorStore = new langchain.QdrantVectorStore(config.vectorDb.qdrantUrl, 'documents')", "", "  // Get a list of all documents", "  routes.get('/', {", "    schema: {", "      tags: ['documents'],", "      response: {", "        200: DataResponseSchema(DocumentListSchema),", "      },", "    },", "    handler: async () => {", "      const documents = await documentRepository.listDocuments()", "      return {", "        success: true,", "        timestamp: new Date().toISOString(),", "        data: { documents },", "      }", "    },", "  })", "", "  // Get a single document by ID", "  routes.get('/:id', {", "    schema: {", "      tags: ['documents'],", "      params: docParam,", "      response: {", "        200: DataResponseSchema(DocumentSchema),", "        404: BaseResponseSchema,", "      },", "    },", "    handler: async (request, reply) => {", "      const { id } = request.params", "      const document = await documentRepository.getDocument(id)", "", "      if (!document) {", "        return reply.code(404).send({", "          success: false,", "          message: 'Document not found',", "          timestamp: new Date().toISOString(),", "        })", "      }", "", "      return {", "        success: true,", "        timestamp: new Date().toISOString(),", "        data: document,", "      }", "    },", "  })", "", "  // Ingest a new document", "  routes.post('/', {", "    schema: {", "      tags: ['documents'],", "      body: IngestDocumentRequestSchema,", "      response: {", "        200: DataResponseSchema(DocumentSchema),", "        500: BaseResponseSchema,", "      },", "    },", "    handler: async (request, reply) => {", "      try {", "        const { content, metadata, chunkingOptions } = request.body as {", "          content: string", "          metadata?: Record<string, unknown>", "          chunkingOptions?: { chunkSize: number; chunkOverlap: number }", "        }", "", "        const document = await ingestDocument(", "          content,", "          metadata || {},", "          chunkingOptions || { chunkSize: 1000, chunkOverlap: 200 },", "          { documentRepository, textSplitter, vectorStore }", "        )", "", "        return {", "          success: true,", "          timestamp: new Date().toISOString(),", "          data: document,", "        }", "      } catch (error) {", "        routes.log.error(error)", "        return reply.code(500).send({", "          success: false,", "          message: error instanceof Error ? error.message : 'Failed to ingest document',", "          timestamp: new Date().toISOString(),", "        })", "      }", "    },", "  })", "", "  // Delete a document", "  routes.delete('/:id', {", "    schema: {", "      tags: ['documents'],", "      params: docParam,", "      response: {", "        200: BaseResponseSchema,", "        404: BaseResponseSchema,", "      },", "    },", "    handler: async (request, reply) => {", "      const { id } = request.params", "      const document = await documentRepository.getDocument(id)", "", "      if (!document) {", "        return reply.code(404).send({", "          success: false,", "          message: 'Document not found',", "          timestamp: new Date().toISOString(),", "        })", "      }", "", "      await documentRepository.deleteDocument(id)", "      return {", "        success: true,", "        message: 'Document deleted successfully',", "        timestamp: new Date().toISOString(),", "      }", "    },", "  })", "", "  // Search documents", "  routes.post('/search', {", "    schema: {", "      tags: ['documents'],", "      body: SearchDocumentsRequestSchema,", "      response: {", "        200: DataResponseSchema(SearchResultsSchema),", "        500: BaseResponseSchema,", "      },", "    },", "    handler: async (request, reply) => {", "      try {", "        const {", "          query,", "          limit = 10,", "          threshold = 0.5,", "        } = request.body as {", "          query: string", "          limit?: number", "          threshold?: number", "        }", "", "        const results = await searchDocuments(query, { limit, threshold }, { vectorStore })", "        const response: DataResponse<SearchResults> = {", "          success: true,", "          timestamp: new Date().toISOString(),", "          data: {", "            results,", "            count: results.length,", "          },", "        }", "", "        return response", "      } catch (error) {", "        routes.log.error(error)", "        return reply.code(500).send({", "          success: false,", "          message: error instanceof Error ? error.message : 'Failed to search documents',", "          timestamp: new Date().toISOString(),", "        })", "      }", "    },", "  })", "}"], "file_path": "apps/api/src/domains/document/schema.ts"}
{"Link_to_commit": "https://github.com/Gal-Izhaky/SuperReminder/commit/88c4fefc8037391ea4c21ddceb02c4d244e89971", "n-gram matched": "copilot to create", "n_lines_longer_change": 66, "n_files_impacted": 37, "longest_chunk": ["// Internal imports", "import styles from \"./ItemView.styles\";", "import ChooseAmount from '../ChooseAmount/ChooseAmount';", "", "// Constants", "const DEFAULT_UNIT = \"\u05d9\u05d7'\";", "const WEIGHTED_UNITS = [\"100 \u05d2\u05e8\u05dd\", '100 \u05de\"\u05dc'];", "", "// Assets", "const delete_image = require(\"../../assets/images/delete.png\");", "", "/**", " * ItemView Component", " * Displays an item card with name, amount selector, and delete button", " *", " * @param {Object} item - The item to display", " * @param {Function} handleDelete - Callback for delete action", " * @param {Function} addAmount - Callback for amount changes", " */", "const ItemView = ({ item, handleDelete, addAmount }) => {", "    // Unit calculations if type is addItem", "", "    const unit = item.measurementUnit.replace('\u05e7\"\u05d2', \"100 \u05d2\u05e8\u05dd\").replace(\"\u05dc\u05d9\u05d8\u05e8\", '100 \u05de\"\u05dc');", "    const weighted = item.weighted && WEIGHTED_UNITS.includes(unit);", "", "    const finalUnit = weighted ? unit.replace(\"100 \", \"\") : DEFAULT_UNIT;", "    const unitCalcResults = {", "        step: weighted ? 100 : 1,", "        maxAmount: finalUnit == \"\u05d9\u05d7'\" ? 10000 : 100000,", "        finalUnit: finalUnit,", "        weighted: weighted,", "    };", "", "", "    return (", "        <View style={[styles.card, styles.shadow]}>", "            {/* Item name */}", "            <Text", "                style={[styles.right, styles.text]}", "                numberOfLines={2}", "                adjustsFontSizeToFit={true}", "            >", "                {item.name}", "            </Text>", "", "            {/* Amount selector and delete button container */}", "            <View style={styles.alignLeft}>", "                {/* Amount selector */}", "                <ChooseAmount", "                    unitCalcResults={unitCalcResults}", "                    onAmountChange={(amount) => addAmount(amount)}", "                    item={item}", "                    displayReset={false}", "                    small={true}", "                />", "", "                {/* Delete button */}", "                <TouchableOpacity onPress={handleDelete} activeOpacity={1}>", "                    <Image style={styles.delete} source={delete_image} />", "                </TouchableOpacity>", "            </View>", "        </View>", "    );", "};", "", "export default ItemView;"], "file_path": "src/components/ItemView/ItemView.js"}
{"Link_to_commit": "https://github.com/PICO443/dickens/commit/5cf3654ec47b1940e7959214941da2f480bc2ff7", "n-gram matched": "chatgpt to implement", "n_lines_longer_change": 85, "n_files_impacted": 21, "longest_chunk": ["// TODO remove, this demo shouldn't need to reset the theme.", "const defaultTheme = createTheme();", "", "export function StudentProfileCard() {", "  const [open, setOpen] = React.useState(true);", "  const toggleDrawer = () => {", "    setOpen(!open);", "  };", "", "  return (", "    <ThemeProvider theme={defaultTheme}>", "      <Box sx={{ display: \"flex\" }}>", "        <CssBaseline />", "        <AppBar position=\"absolute\" open={open}>", "          <Toolbar", "            sx={{", "              pr: \"24px\", // keep right padding when drawer closed", "            }}", "          >", "            <IconButton", "              edge=\"start\"", "              color=\"inherit\"", "              aria-label=\"open drawer\"", "              onClick={toggleDrawer}", "              sx={{", "                marginRight: \"36px\",", "                ...(open && { display: \"none\" }),", "              }}", "            >", "              <MenuIcon />", "            </IconButton>", "            <Typography", "              component=\"h1\"", "              variant=\"h6\"", "              color=\"inherit\"", "              noWrap", "              sx={{ flexGrow: 1 }}", "            >", "              Dashboard", "            </Typography>", "            <IconButton color=\"inherit\">", "              <Badge badgeContent={4} color=\"secondary\">", "                <NotificationsIcon />", "              </Badge>", "            </IconButton>", "          </Toolbar>", "        </AppBar>", "        <Drawer variant=\"permanent\" open={open}>", "          <Toolbar", "            sx={{", "              display: \"flex\",", "              alignItems: \"center\",", "              justifyContent: \"flex-end\",", "              px: [1],", "            }}", "          >", "            <IconButton onClick={toggleDrawer}>", "              <ChevronLeftIcon />", "            </IconButton>", "          </Toolbar>", "          <Divider />", "          <List component=\"nav\">", "            {mainListItems}", "            <Divider sx={{ my: 1 }} />", "            {secondaryListItems}", "          </List>", "        </Drawer>", "        <Box", "          component=\"main\"", "          sx={{", "            backgroundColor: (theme) =>", "              theme.palette.mode === \"light\"", "                ? theme.palette.grey[100]", "                : theme.palette.grey[900],", "            flexGrow: 1,", "            height: \"100vh\",", "            overflow: \"auto\",", "          }}", "        >", "          ", "        </Box>", "      </Box>", "    </ThemeProvider>", "  );", "}"], "file_path": "src/components/StudentProfile/listItems.js"}
{"Link_to_commit": "https://github.com/otabeknarz/smartfit-backend/commit/930b685d21f495a96878270f1b61650efbf14436", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 85, "n_files_impacted": 2, "longest_chunk": ["from rest_framework.permissions import AllowAny", "from django.utils.decorators import method_decorator", "from django.views.decorators.csrf import csrf_exempt", "from django.shortcuts import get_object_or_404", "from decimal import Decimal", "from payments.models import Payment", "", "", "@method_decorator(csrf_exempt, name=\"dispatch\")", "class PaymeAPIView(APIView):", "    permission_classes = [AllowAny]", "", "    def post(self, request):", "        data = request.data", "        method = data.get(\"method\")", "        params = data.get(\"params\", {})", "        request_id = data.get(\"id\")", "", "        handler = {", "            \"CheckPerformTransaction\": self.check_perform_transaction,", "            \"CreateTransaction\": self.create_transaction,", "            \"PerformTransaction\": self.perform_transaction,", "            \"CheckTransaction\": self.check_transaction,", "            \"CancelTransaction\": self.cancel_transaction,", "        }.get(method)", "", "        if handler:", "            return handler(params, request_id)", "", "        return Response({", "            \"jsonrpc\": \"2.0\",", "            \"error\": {\"code\": -32601, \"message\": \"Method not found\"},", "            \"id\": request_id", "        })", "", "    def check_perform_transaction(self, params, request_id):", "        try:", "            payment_id = params[\"account\"][\"payment_id\"]", "            amount = Decimal(params[\"amount\"]) / 100  # Payme sends amount in tiyin", "            payment = get_object_or_404(Payment, id=payment_id)", "", "            if payment.status != Payment.StatusChoices.PENDING:", "                return self.error_response(-31050, \"Transaction already processed\", request_id)", "", "            if amount != payment.amount:", "                return self.error_response(-31001, \"Incorrect amount\", request_id)", "", "            return self.success_response({\"allow\": True}, request_id)", "        except Exception:", "            return self.error_response(-31099, \"Error in checking transaction\", request_id)", "", "    def create_transaction(self, params, request_id):", "        try:", "            payment_id = params[\"account\"][\"payment_id\"]", "            payme_transaction_id = params[\"id\"]", "            payment = get_object_or_404(Payment, id=payment_id)", "", "            if payment.transaction_id and payment.transaction_id != payme_transaction_id:", "                return self.error_response(-31008, \"Transaction already exists\", request_id)", "", "            payment.transaction_id = payme_transaction_id", "            payment.save()", "", "            return self.success_response({", "                \"create_time\": int(time.time() * 1000),", "                \"transaction\": payme_transaction_id,", "                \"state\": 1,", "                \"receivers\": None,", "            }, request_id)", "", "        except Exception:", "            return self.error_response(-31099, \"Failed to create transaction\", request_id)", "", "    def perform_transaction(self, params, request_id):", "        try:", "            transaction_id = params[\"id\"]", "            payment = get_object_or_404(Payment, transaction_id=transaction_id)", "", "            if payment.status == Payment.StatusChoices.COMPLETED:", "                return self.success_response({", "                    \"transaction\": transaction_id,", "                    \"perform_time\": int(payment.updated_at.timestamp() * 1000),", "                    \"state\": 2,", "                }, request_id)", ""], "file_path": "payments/api/views.py"}
{"Link_to_commit": "https://github.com/DivyanshuSinghania/LangChain/commit/cad5f67c1253fd0602f5193b2e0c64bda989627f", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 10, "n_files_impacted": 5, "longest_chunk": ["# Serve static files (CSS, JS)", "app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")", "", "# Templates", "templates = Jinja2Templates(directory=\"frontend\")", "", "@app.get(\"/\", response_class=HTMLResponse)", "async def home(request: Request):", "    return templates.TemplateResponse(\"index.html\", {\"request\": request})", ""], "file_path": "LC_Basics/app.py"}
{"Link_to_commit": "https://github.com/merbinr/bug-bounty/commit/90fe978a990500d29234aa7e33caa40f045a275c", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 80, "n_files_impacted": 17, "longest_chunk": ["import subprocess", "import os", "import json", "from typing import List", "from modules import config", "", "", "def prepare_nuclei_target_file(groups: List[List[dict]], target_name: str) -> str:", "    \"\"\"", "    Each group is a list of items (each item from httpx).", "    We want to choose 1 URL from each group for scanning,", "    plus any items that had missing fields.", "    Writes them to `nuclei_targets_{target_name}.txt`.", "    Returns the path to that file.", "    \"\"\"", "    output_file = f\"nuclei_targets_{target_name}.txt\"", "    urls = []", "    for group in groups:", "        # Just pick the first item in each group", "        if group and len(group) > 0:", "            item = group[0]", "            urls.append(item[\"url\"])", "", "    # Write to file", "    with open(output_file, \"w\", encoding=\"utf-8\") as fh:", "        for url in urls:", "            fh.write(url + \"\\n\")", "", "    return output_file", "", "", "def run_nuclei(target_file: str, target_name: str) -> str:", "    \"\"\"", "    Runs nuclei using the target_file, saves JSON results to config.NUCLEI_OUTPUT_JSON", "    For clarity, let's store separate JSON for each domain to avoid collisions:", "    e.g. nuclei_scan_output_{target_name}.json", "    Returns path to the JSON file.", "    \"\"\"", "    output_json = f\"nuclei_scan_output_{target_name}.json\"", "    cmd = [", "        config.NUCLEI_BIN,", "        \"-l\",", "        target_file,", "        \"-t\",", "        config.NUCLEI_TEMPLATES,", "        \"-c\",", "        \"50\",", "        \"-bs\",", "        \"100\",", "        \"--json-export\",", "        output_json,", "    ]", "    subprocess.run(cmd, check=True)", "    return output_json", "", "", "def parse_nuclei_output(json_export_file: str) -> List[dict]:", "    \"\"\"", "    Nuclei's --json-export produces a JSON array of objects.", "    Each object has keys like `info.description`, `info.severity`, and `url`.", "    Returns a list of dicts with the relevant fields.", "    \"\"\"", "    if not os.path.exists(json_export_file):", "        return []", "", "    with open(json_export_file, \"r\", encoding=\"utf-8\") as fh:", "        try:", "            data = json.load(fh)", "        except:", "            data = []", "", "    # data is a list of objects", "    results = []", "    for item in data:", "        description = item.get(\"info\", {}).get(\"description\", \"N/A\")", "        severity = item.get(\"info\", {}).get(\"severity\", \"N/A\")", "        url = item.get(\"host\", \"N/A\")  # or item.get(\"matched\")", "        results.append({\"url\": url, \"description\": description, \"severity\": severity})", "", "    return results"], "file_path": "modules/screenshot.py"}
{"Link_to_commit": "https://github.com/kaf212/kennzahlen-cockpit-server/commit/562edd8a3b821c76e5d3a454f4caaa8805d1491a", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 14, "n_files_impacted": 2, "longest_chunk": ["    const authHeader = req.header('Authorization');", "    if (!authHeader || !authHeader.startsWith(\"Bearer \")) {", "        return res.status(401).json({ error: 'Authentication token missing or malformed' });", "    }", "", "    /*", "    Prevent jwtPayload from being undefined", "    Bug found by ChatGPT (https://chatgpt.com/share/67e01958-bc68-8011-8db0-16deafd5fd7b)", "     */", "    try {", "        const payload = jwt.verify(token, secretKey);", "        return payload;", "    } catch (err) {", "        return res.status(403).json({ error: 'Token is invalid' });"], "file_path": "src/middleware/tokenValidation.js"}
{"Link_to_commit": "https://github.com/omar-anas/EgytalChallenge/commit/dee3bb63b4a5032b30620ac6196cc0c6d7ab46c0", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 374, "n_files_impacted": 9, "longest_chunk": ["import { DynamoDBClient, BatchWriteItemCommand } from \"@aws-sdk/client-dynamodb\";", "import { marshall } from \"@aws-sdk/util-dynamodb\";", "", "// Configure DynamoDB client", "const client = new DynamoDBClient({", "  region: \"localhost\",", "  endpoint: \"http://localhost:8000\",", "  credentials: {", "    accessKeyId: \"fakeMyKeyId\",", "    secretAccessKey: \"fakeSecretAccessKey\",", "  },", "});", "", "// Sample data for Users", "const users = [", "  {", "    PK: { S: \"USER#1\" },", "    SK: { S: \"USER#1\" },", "    Type: { S: \"User\" },", "    name: { S: \"John Doe\" },", "    email: { S: \"john@example.com\" },", "    join_date: { S: \"2025-01-01\" }", "  },", "  {", "    PK: { S: \"USER#2\" },", "    SK: { S: \"USER#2\" },", "    Type: { S: \"User\" },", "    name: { S: \"Jane Smith\" },", "    email: { S: \"jane@example.com\" },", "    join_date: { S: \"2025-01-15\" }", "  },", "  {", "    PK: { S: \"USER#3\" },", "    SK: { S: \"USER#3\" },", "    Type: { S: \"User\" },", "    name: { S: \"Robert Johnson\" },", "    email: { S: \"robert@example.com\" },", "    join_date: { S: \"2025-02-10\" }", "  },", "  {", "    PK: { S: \"USER#4\" },", "    SK: { S: \"USER#4\" },", "    Type: { S: \"User\" },", "    name: { S: \"Emily Davis\" },", "    email: { S: \"emily@example.com\" },", "    join_date: { S: \"2025-03-05\" }", "  },", "  {", "    PK: { S: \"USER#5\" },", "    SK: { S: \"USER#5\" },", "    Type: { S: \"User\" },", "    name: { S: \"Michael Brown\" },", "    email: { S: \"michael@example.com\" },", "    join_date: { S: \"2025-03-20\" }", "  }", "];", "", "// Sample data for Books", "const books = [", "  {", "    PK: { S: \"BOOK#101\" },", "    SK: { S: \"BOOK#101\" },", "    Type: { S: \"Book\" },", "    title: { S: \"The Great Gatsby\" },", "    author: { S: \"F. Scott Fitzgerald\" },", "    isbn: { S: \"9780743273565\" },", "    available: { BOOL: true }", "  },", "  {", "    PK: { S: \"BOOK#102\" },", "    SK: { S: \"BOOK#102\" },", "    Type: { S: \"Book\" },", "    title: { S: \"To Kill a Mockingbird\" },", "    author: { S: \"Harper Lee\" },", "    isbn: { S: \"9780061120084\" },", "    available: { BOOL: true }", "  },", "  {", "    PK: { S: \"BOOK#103\" },", "    SK: { S: \"BOOK#103\" },", "    Type: { S: \"Book\" },", "    title: { S: \"1984\" },", "    author: { S: \"George Orwell\" },", "    isbn: { S: \"9780451524935\" },", "    available: { BOOL: true }", "  },", "  {", "    PK: { S: \"BOOK#104\" },", "    SK: { S: \"BOOK#104\" },", "    Type: { S: \"Book\" },", "    title: { S: \"Pride and Prejudice\" },", "    author: { S: \"Jane Austen\" },", "    isbn: { S: \"9780141439518\" },", "    available: { BOOL: true }", "  },", "  {", "    PK: { S: \"BOOK#105\" },", "    SK: { S: \"BOOK#105\" },", "    Type: { S: \"Book\" },", "    title: { S: \"The Hobbit\" },", "    author: { S: \"J.R.R. Tolkien\" },", "    isbn: { S: \"9780547928227\" },", "    available: { BOOL: true }", "  },", "  {", "    PK: { S: \"BOOK#106\" },", "    SK: { S: \"BOOK#106\" },", "    Type: { S: \"Book\" },", "    title: { S: \"Dune\" },", "    author: { S: \"Frank Herbert\" },", "    isbn: { S: \"9780441172719\" },", "    available: { BOOL: true }", "  },", "  {", "    PK: { S: \"BOOK#107\" },", "    SK: { S: \"BOOK#107\" },", "    Type: { S: \"Book\" },", "    title: { S: \"The Catcher in the Rye\" },", "    author: { S: \"J.D. Salinger\" },", "    isbn: { S: \"9780316769488\" },", "    available: { BOOL: true }", "  }", "];", "", "// Sample data for Loans", "const loans = [", "  {", "    PK: { S: \"LOAN#1001\" },", "    SK: { S: \"LOAN#1001\" },", "    GSI1PK: { S: \"USER#1\" },", "    GSI1SK: { S: \"LOAN_DATE#2025-04-01\" },", "    Type: { S: \"Loan\" },", "    user_id: { S: \"1\" },", "    book_id: { S: \"101\" },", "    loan_date: { S: \"2025-04-01\" },", "    due_date: { S: \"2025-05-01\" },", "    status: { S: \"RETURNED\" }", "  },", "  {", "    PK: { S: \"LOAN#1002\" },", "    SK: { S: \"LOAN#1002\" },", "    GSI1PK: { S: \"USER#2\" },", "    GSI1SK: { S: \"LOAN_DATE#2025-04-05\" },", "    Type: { S: \"Loan\" },", "    user_id: { S: \"2\" },", "    book_id: { S: \"102\" },", "    loan_date: { S: \"2025-04-05\" },", "    due_date: { S: \"2025-05-05\" },", "    status: { S: \"ACTIVE\" }", "  },", "  {", "    PK: { S: \"LOAN#1003\" },", "    SK: { S: \"LOAN#1003\" },", "    GSI1PK: { S: \"USER#3\" },", "    GSI1SK: { S: \"LOAN_DATE#2025-04-10\" },", "    Type: { S: \"Loan\" },", "    user_id: { S: \"3\" },", "    book_id: { S: \"103\" },", "    loan_date: { S: \"2025-04-10\" },", "    due_date: { S: \"2025-05-10\" },", "    status: { S: \"ACTIVE\" }", "  },", "  {", "    PK: { S: \"LOAN#1004\" },", "    SK: { S: \"LOAN#1004\" },", "    GSI1PK: { S: \"USER#1\" },", "    GSI1SK: { S: \"LOAN_DATE#2025-04-15\" },", "    Type: { S: \"Loan\" },", "    user_id: { S: \"1\" },", "    book_id: { S: \"104\" },", "    loan_date: { S: \"2025-04-15\" },", "    due_date: { S: \"2025-05-15\" },", "    status: { S: \"ACTIVE\" }", "  },", "  {", "    PK: { S: \"LOAN#1005\" },", "    SK: { S: \"LOAN#1005\" },", "    GSI1PK: { S: \"USER#4\" },", "    GSI1SK: { S: \"LOAN_DATE#2025-04-20\" },", "    Type: { S: \"Loan\" },", "    user_id: { S: \"4\" },", "    book_id: { S: \"101\" },", "    loan_date: { S: \"2025-04-20\" },", "    due_date: { S: \"2025-05-20\" },", "    status: { S: \"ACTIVE\" }", "  },", "  {", "    PK: { S: \"LOAN#1006\" },", "    SK: { S: \"LOAN#1006\" },", "    GSI1PK: { S: \"USER#5\" },", "    GSI1SK: { S: \"LOAN_DATE#2025-04-25\" },", "    Type: { S: \"Loan\" },", "    user_id: { S: \"5\" },", "    book_id: { S: \"102\" },", "    loan_date: { S: \"2025-04-25\" },", "    due_date: { S: \"2025-05-25\" },", "    status: { S: \"ACTIVE\" }", "  },", "  {", "    PK: { S: \"LOAN#1007\" },", "    SK: { S: \"LOAN#1007\" },", "    GSI1PK: { S: \"USER#2\" },", "    GSI1SK: { S: \"LOAN_DATE#2025-05-01\" },", "    Type: { S: \"Loan\" },", "    user_id: { S: \"2\" },", "    book_id: { S: \"105\" },", "    loan_date: { S: \"2025-05-01\" },", "    due_date: { S: \"2025-06-01\" },", "    status: { S: \"ACTIVE\" }", "  },", "  {", "    PK: { S: \"LOAN#1008\" },", "    SK: { S: \"LOAN#1008\" },", "    GSI1PK: { S: \"USER#3\" },", "    GSI1SK: { S: \"LOAN_DATE#2025-05-05\" },", "    Type: { S: \"Loan\" },", "    user_id: { S: \"3\" },", "    book_id: { S: \"101\" },", "    loan_date: { S: \"2025-05-05\" },", "    due_date: { S: \"2025-06-05\" },", "    status: { S: \"ACTIVE\" }", "  },", "  {", "    PK: { S: \"LOAN#1009\" },", "    SK: { S: \"LOAN#1009\" },", "    GSI1PK: { S: \"USER#1\" },", "    GSI1SK: { S: \"LOAN_DATE#2025-05-10\" },", "    Type: { S: \"Loan\" },", "    user_id: { S: \"1\" },", "    book_id: { S: \"106\" },", "    loan_date: { S: \"2025-05-10\" },", "    due_date: { S: \"2025-06-10\" },", "    status: { S: \"ACTIVE\" }", "  },", "  {", "    PK: { S: \"LOAN#1010\" },", "    SK: { S: \"LOAN#1010\" },", "    GSI1PK: { S: \"USER#4\" },", "    GSI1SK: { S: \"LOAN_DATE#2025-05-15\" },", "    Type: { S: \"Loan\" },", "    user_id: { S: \"4\" },", "    book_id: { S: \"103\" },", "    loan_date: { S: \"2025-05-15\" },", "    due_date: { S: \"2025-06-15\" },", "    status: { S: \"ACTIVE\" }", "  }", "];", "", "// Sample data for BookLoans", "const bookLoans = [", "  {", "    PK: { S: \"BOOK#101\" },", "    SK: { S: \"LOAN#1001\" },", "    Type: { S: \"BookLoan\" },", "    user_id: { S: \"1\" },", "    loan_date: { S: \"2025-04-01\" }", "  },", "  {", "    PK: { S: \"BOOK#102\" },", "    SK: { S: \"LOAN#1002\" },", "    Type: { S: \"BookLoan\" },", "    user_id: { S: \"2\" },", "    loan_date: { S: \"2025-04-05\" }", "  },", "  {", "    PK: { S: \"BOOK#103\" },", "    SK: { S: \"LOAN#1003\" },", "    Type: { S: \"BookLoan\" },", "    user_id: { S: \"3\" },", "    loan_date: { S: \"2025-04-10\" }", "  },", "  {", "    PK: { S: \"BOOK#104\" },", "    SK: { S: \"LOAN#1004\" },", "    Type: { S: \"BookLoan\" },", "    user_id: { S: \"1\" },", "    loan_date: { S: \"2025-04-15\" }", "  },", "  {", "    PK: { S: \"BOOK#101\" },", "    SK: { S: \"LOAN#1005\" },", "    Type: { S: \"BookLoan\" },", "    user_id: { S: \"4\" },", "    loan_date: { S: \"2025-04-20\" }", "  },", "  {", "    PK: { S: \"BOOK#102\" },", "    SK: { S: \"LOAN#1006\" },", "    Type: { S: \"BookLoan\" },", "    user_id: { S: \"5\" },", "    loan_date: { S: \"2025-04-25\" }", "  },", "  {", "    PK: { S: \"BOOK#105\" },", "    SK: { S: \"LOAN#1007\" },", "    Type: { S: \"BookLoan\" },", "    user_id: { S: \"2\" },", "    loan_date: { S: \"2025-05-01\" }", "  },", "  {", "    PK: { S: \"BOOK#101\" },", "    SK: { S: \"LOAN#1008\" },", "    Type: { S: \"BookLoan\" },", "    user_id: { S: \"3\" },", "    loan_date: { S: \"2025-05-05\" }", "  },", "  {", "    PK: { S: \"BOOK#106\" },", "    SK: { S: \"LOAN#1009\" },", "    Type: { S: \"BookLoan\" },", "    user_id: { S: \"1\" },", "    loan_date: { S: \"2025-05-10\" }", "  },", "  {", "    PK: { S: \"BOOK#103\" },", "    SK: { S: \"LOAN#1010\" },", "    Type: { S: \"BookLoan\" },", "    user_id: { S: \"4\" },", "    loan_date: { S: \"2025-05-15\" }", "  }", "];", "", "// Function to batch write items to DynamoDB", "async function batchWriteItems(items) {", "  // DynamoDB BatchWriteItem can handle up to 25 items at once", "  const batchSize = 25;", "  ", "  for (let i = 0; i < items.length; i += batchSize) {", "    const batch = items.slice(i, i + batchSize);", "    const request = {", "      RequestItems: {", "        LibraryManagement: batch.map(item => ({", "          PutRequest: {", "            Item: item", "          }", "        }))", "      }", "    };", "    ", "    try {", "      await client.send(new BatchWriteItemCommand(request));", "      console.log(`Successfully wrote batch ${i / batchSize + 1}`);", "    } catch (error) {", "      console.error(`Error writing batch ${i / batchSize + 1}:`, error);", "      throw error;", "    }", "  }", "}", "", "// Main function to populate all data", "async function populateData() {", "  try {", "    console.log(\"Starting data population...\");", "    ", "    console.log(\"Inserting users...\");", "    await batchWriteItems(users);", "    ", "    console.log(\"Inserting books...\");", "    await batchWriteItems(books);", "    ", "    console.log(\"Inserting loans...\");", "    await batchWriteItems(loans);", "    ", "    console.log(\"Inserting book loans...\");", "    await batchWriteItems(bookLoans);", "    ", "    console.log(\"Data population completed successfully!\");", "  } catch (error) {", "    console.error(\"Error in data population:\", error);", "  }", "}", "", "// Execute the population script", "populateData();"], "file_path": "dynamo-gsi-test/create_data.js"}
{"Link_to_commit": "https://github.com/Belal172/2pageapp/commit/503d0a05b9272b199b058c866c38ed813e9e891f", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 24, "n_files_impacted": 5, "longest_chunk": ["import { createStore } from 'vuex';", "", "const store = createStore({", "  state() {", "    return {", "      count: 0", "    };", "  },", "  mutations: {", "    increment(state) {", "      state.count++;", "    }", "  },", "  actions: {", "    incrementAction({ commit }) {", "      commit('increment');", "    }", "  },", "  getters: {", "    doubleCount: (state) => state.count * 2", "  }", "});", "", "export default store;"], "file_path": "src/store/index.js"}
{"Link_to_commit": "https://github.com/wernerkrauss/silverstripe-fluent-export-import/commit/61fb2ba1019a7b911027f6341de965c0811f0e31", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 12, "n_files_impacted": 1, "longest_chunk": ["        try {", "            $response = $this->client->chat()->create([", "                'model' => self::config()->get('gpt_model'),", "                'messages' => [", "                    [", "                        'role' => 'system',", "                        'content' => $this->getGPTCommand($targetLocale)", "                    ],", "                    [", "                        'role' => 'user',", "                        'content' => $text", "                    ]"], "file_path": "src/Translator/ChatGPTTranslator.php"}
{"Link_to_commit": "https://github.com/1UHD/mc-playtime/commit/9ac7b855e818f23e047b5823bc08ec90b4ce3202", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 21, "n_files_impacted": 2, "longest_chunk": ["//go:embed assets", "var assetsFS embed.FS", "", "func get_img(filepath string, console *widget.Entry, assetsFS embed.FS) (image.Image, error) {", "\timgData, err := assetsFS.ReadFile(filepath)", "\tif err != nil {", "\t\tconsole.SetText(console.Text + \"Error loading image: \" + err.Error() + \"\\n\")", "\t\tfmt.Println(\"in 1: \" + err.Error())", "\t\treturn nil, err", "\t}", "", "\timg, _, err := image.Decode(bytes.NewReader(imgData))", "\tif err != nil {", "\t\tconsole.SetText(console.Text + \"Error loading image: \" + err.Error() + \"\\n\")", "\t\tfmt.Println(\"in 2: \" + err.Error())", "\t\treturn nil, err", "\t}", "", "\treturn img, nil", "}", ""], "file_path": "main.go"}
{"Link_to_commit": "https://github.com/mehediScriptDev/js-part2/commit/15a0b0be19ac6692a0f95d9687bb8ad50db22a58", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 22, "n_files_impacted": 1, "longest_chunk": ["console.log(result);", "", "", "", "", "", "", "", "const names=['Mehedi','Mira','Aboni','Aboni','Nid'];", "function getLitteone(words){", "    let shortname=names[0];", "    for(const name of words){", "        if(name.length<shortname.length){", "            shortname=name;", "            ", "        }", "    }", "    return shortname;", "}", "", "const resut= getLitteone(names);", "console.log(resut);"], "file_path": "height.js"}
{"Link_to_commit": "https://github.com/mkrm0000/anime-movie-list/commit/0083dd8fc234748d57fff6b95193d162bcd370b4", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 28, "n_files_impacted": 4, "longest_chunk": ["              {loading ? 'Loading...' : 'Search'}", "            </button>", "          </div>", "", "          {error && <p className=\"text-red-500\">{error}</p>}", "", "          <div className=\"grid grid-cols-1 sm:grid-cols-2 gap-6 mt-6\">", "            {characters.map((char, idx) => (", "              <div", "                key={idx}", "                className=\"bg-white dark:bg-gray-800 shadow-md rounded-lg overflow-hidden transition hover:shadow-xl\"", "              >", "                <img", "                  src={char.imageUrl}", "                  alt={char.name}", "                  className=\"w-full h-48 object-cover\"", "                />", "                <div className=\"p-4\">", "                  <h2 className=\"text-xl font-bold mb-2\">{char.name}</h2>", "                  <h3 className=\"text-md font-semibold mb-1\">Movies:</h3>", "                  <ul className=\"list-disc list-inside\">", "                    {char.films.length > 0 ? (", "                      char.films.map((film, i) => <li key={i}>{film}</li>)", "                    ) : (", "                      <li>No movies found</li>", "                    )}", "                  </ul>", "                </div>"], "file_path": "src/App.js"}
{"Link_to_commit": "https://github.com/Mahad-007/To-Do-List-App/commit/25531489820f5216ba121029811e174d151fdc35", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 74, "n_files_impacted": 3, "longest_chunk": ["const taskInput = document.getElementById(\"taskinput\");", "", "function addTask() {", "  let task = taskInput.value.trim();", "  if (task === \"\") return; // Prevent empty tasks", "", "  alert(\"Task added!\");", "", "  const list = document.getElementById(\"tasklist\");", "", "  // Create Task Item Container", "  let taskItem = document.createElement(\"div\");", "  taskItem.classList.add(\"task-item\");", "", "  // Create Task Description Div", "  let taskTextContainer = document.createElement(\"div\");", "  taskTextContainer.textContent = task;", "  taskTextContainer.classList.add(\"task-text-container\");", "  taskTextContainer.contentEditable = \"false\"; // Initially not editable", "", "  // Create Buttons Container", "  let buttonsContainer = document.createElement(\"div\");", "  buttonsContainer.classList.add(\"task-buttons\");", "", "  // Delete Button", "  let deleteBtn = document.createElement(\"button\");", "  deleteBtn.textContent = \"Delete\";", "  deleteBtn.classList.add(\"task-btn\", \"delete-btn\");", "  deleteBtn.onclick = function () {", "      list.removeChild(taskItem);", "  };", "", "  // Complete Button", "  let completeBtn = document.createElement(\"button\");", "  completeBtn.textContent = \"Completed\";", "  completeBtn.classList.add(\"task-btn\", \"complete-btn\");", "  completeBtn.onclick = function () {", "      taskTextContainer.style.textDecoration = \"line-through\";", "      taskTextContainer.style.color = \"gray\";", "  };", "", "  // Edit Button", "  let editBtn = document.createElement(\"button\");", "  editBtn.textContent = \"Edit\";", "  editBtn.classList.add(\"task-btn\", \"edit-btn\");", "  editBtn.onclick = function () {", "      if (taskTextContainer.contentEditable === \"false\") {", "          taskTextContainer.contentEditable = \"true\";", "          taskTextContainer.focus();", "          editBtn.textContent = \"Save\";", "      } else {", "          taskTextContainer.contentEditable = \"false\";", "          editBtn.textContent = \"Edit\";", "      }", "  };", "", "  // Append Buttons to Buttons Container", "  buttonsContainer.appendChild(editBtn);", "  buttonsContainer.appendChild(completeBtn);", "  buttonsContainer.appendChild(deleteBtn);", "", "  // Append Task Description and Buttons to Task Item", "  taskItem.appendChild(taskTextContainer);", "  taskItem.appendChild(buttonsContainer);", "", "  // Append Task Item to List", "  list.appendChild(taskItem);", "", "  // Clear Input", "  taskInput.value = \"\";", "}", "", "// Attach event listener to \"Add Task\" button", "document.querySelector(\"button\").addEventListener(\"click\", addTask);"], "file_path": "app.js"}
{"Link_to_commit": "https://github.com/Vaibhavfprasad/Clothing/commit/d6486a94da840a33649e1c865f5956d8c2e2be65", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 58, "n_files_impacted": 9, "longest_chunk": ["// module.exports.updateListing = async (req, res)=>{", "//     let {id} = req.params;", "//     let listing = await Listing.findByIdAndUpdate(id, {...req.body.listing});", "//     if(typeof req.file != \"undefined\"){", "//         let url = req.file.path;", "//         let filename = req.file.filename;", "//         listing.image = {url, filename};", "//         await listing.save();", "//     }", "//     req.flash(\"success\", \"Listing Updated!\");", "//     res.redirect(`/listings/${id}`);", "// };", "", "", "// module.exports.updateListing = async (req, res) => {", "//     try {", "//         let { id } = req.params;", "        ", "//         // Check if request body contains listing data", "//         if (!req.body.listing) {", "//             req.flash(\"error\", \"Invalid data provided!\");", "//             return res.redirect(`/listings/${id}/edit`);", "//         }", "", "//         // Update listing in DB", "//         let listing = await Listing.findByIdAndUpdate(id, { ...req.body.listing }, { new: true });", "", "//         // Handle file upload if a new image is provided", "//         if (req.file) {", "//             listing.image = {", "//                 url: req.file.path,", "//                 filename: req.file.filename", "//             };", "//             await listing.save();", "//         }", "", "//         req.flash(\"success\", \"Listing Updated!\");", "//         res.redirect(`/listings/${id}`);", "//     } catch (error) {", "//         console.error(error);", "//         req.flash(\"error\", \"Something went wrong while updating the listing!\");", "//         res.redirect(`/listings/${id}/edit`);", "//     }", "// };", "", "", "module.exports.updateListing = async (req, res) => {", "    try {", "        const { id } = req.params;", "        const listing = await Listing.findById(id);", "        if (!listing) return res.redirect(\"/listings\");", "", "        // If new images are uploaded, replace old ones in the database", "        if (req.files?.length) {", "            listing.images = req.files.map(({ path, filename }) => ({ url: path, filename }));", "        }", "", "        // Save updated listing"], "file_path": "controllers/listings.js"}
{"Link_to_commit": "https://github.com/sdat2/worstsurge/commit/f6055f06a23f2cd5f0d21dd44cbe2406c387e774", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 7, "n_files_impacted": 6, "longest_chunk": ["        # print(", "        #    \"r0, rmax_cle, pm_cle15_dyn, pm_w22_car\",", "        #    r0,", "        #    rmax_cle,", "        #    pm_cle15_dyn,", "        #    pm_w22_car,", "        # )"], "file_path": "cle/potential_size.py"}
{"Link_to_commit": "https://github.com/TriumphantAkash/OfflineVSCoding/commit/a5ce440c12bcedee569a9ed917c65ccd0861541f", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 14, "n_files_impacted": 4, "longest_chunk": ["        // Wait for something to consume (producer will signal this)", "        sem_wait(&semProd);", "", "        pthread_mutex_lock(&bufferLock);", "        // Consume from buffer", "        int num = ringBuffer[rp];", "        rp = (rp + 1) % BUFFER_SIZE;", "        printf(\"[CONS] Read %d from buffer\\n\", num);", "        pthread_mutex_unlock(&bufferLock);", "", "        // Signal the producer that there's space in the buffer", "        sem_post(&semCons);", "", "        counter++;"], "file_path": "multithreading/main.c"}
{"Link_to_commit": "https://github.com/Ccode104/System-Network-and-Security/commit/3911224665d7db11adf2b5bbe19aec4721d713b2", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 89, "n_files_impacted": 24, "longest_chunk": ["#include <stdio.h>", "#include <stdlib.h>", "#include <gmp.h>", "", "// NOTE : Here we use a^p-1 mod p = 1 mod p", "// By replacing modulus_minus_one by modulus in the code and uncomment the line where we compute 'part 1' ", "// you can see : a^p mod p = a mod p also", "int main(int argc, char *argv[]) {", "    // Initialize base (a), exponent (x), and modulus (n)", "    mpz_t base, exponent, modulus;", "    mpz_init_set_ui(base, atoi(argv[1]));", "    mpz_init_set_ui(exponent, atoi(argv[2]));", "    mpz_init_set_ui(modulus, atoi(argv[3]));", "", "    // Print initial equation", "    printf(\"Initial: %ld^%ld (mod %ld)\\n\", mpz_get_ui(base), mpz_get_ui(exponent), mpz_get_ui(modulus));", "", "    // Check if modulus is prime (required for Fermat's theorem)", "    printf(\"Checking if modulus is prime...\\n\");", "    if (mpz_probab_prime_p(modulus, 50) == 2) {", "        printf(\"Modulus is prime! Proceeding with Fermat's theorem.\\n\");", "    } else {", "        printf(\"Modulus is not prime. Fermat's theorem cannot be applied.\\n\");", "        return 0;", "    }", "", "    // Initialize result variable", "    mpz_t result;", "    mpz_init(result);", "", "    // If exponent < modulus, directly compute a^x mod n", "    if (mpz_cmp(exponent, modulus) < 0) {", "        mpz_powm(result, base, exponent, modulus);", "        printf(\"Result: %ld\\n\", mpz_get_ui(result));", "        return 0;", "    }", "", "    // Initialize variables for Fermat's theorem application", "    mpz_t quotient, remainder;", "    mpz_t part1, part2;", "    mpz_init(quotient);", "    mpz_init(remainder);", "    mpz_init_set_ui(part1,1);", "    mpz_init(part2);", "", "    mpz_t modulus_minus_one;", "    mpz_init(modulus_minus_one);", "", "    mpz_sub_ui(modulus_minus_one,modulus,1);", "", "    // Compute exponent as quotient * (modulus-1) + remainder", "    mpz_fdiv_qr(quotient, remainder, exponent, modulus_minus_one);", "", "    // Print breakdown of exponentiation", "    printf(\"We have: (%ld^(%ld * %ld) + %ld) (mod %ld)\\n\", ", "           mpz_get_ui(base), mpz_get_ui(quotient), mpz_get_ui(modulus_minus_one), mpz_get_ui(remainder), mpz_get_ui(modulus));", "", "    printf(\"i.e. ((%ld^%ld)^%ld) * (%ld^%ld) (mod %ld)\\n\", ", "           mpz_get_ui(base), mpz_get_ui(quotient), mpz_get_ui(modulus_minus_one), mpz_get_ui(base), mpz_get_ui(remainder), mpz_get_ui(modulus));", "", "    printf(\"i.e. (((%ld^%ld)^%ld) (mod %ld)) * ((%ld^%ld) (mod %ld)) (mod %ld)\\n\", ", "           mpz_get_ui(base), mpz_get_ui(quotient), mpz_get_ui(modulus_minus_one), mpz_get_ui(modulus), ", "           mpz_get_ui(base), mpz_get_ui(remainder), mpz_get_ui(modulus), mpz_get_ui(modulus));", "", "    printf(\"Using Fermat\u2019s Theorem: a^p mod p = a\\n\");", "", "    // Compute (base^quotient)^modulus mod modulus", "    // mpz_powm(part1, base, quotient, modulus);", "    printf(\"i.e. %ld * (%ld^%ld) mod %ld\\n\", mpz_get_ui(part1), mpz_get_ui(base), mpz_get_ui(remainder), mpz_get_ui(modulus));", "", "    // Compute base^remainder mod modulus", "    mpz_powm(part2, base, remainder, modulus);", "", "    // Compute final result", "    mpz_mul(result, part1, part2);", "    printf(\"Result: %ld\\n\", mpz_get_ui(result));", "", "    // Free allocated memory", "    mpz_clear(base);", "    mpz_clear(exponent);", "    mpz_clear(modulus);", "    mpz_clear(result);", "    mpz_clear(quotient);", "    mpz_clear(remainder);", "    mpz_clear(part1);", "    mpz_clear(part2);", "", "    return 0;", "}"], "file_path": "Phase 1/Q6 Multiplicative Inverse.c"}
{"Link_to_commit": "https://github.com/ash2686/HCJ-Notes25/commit/3360a88cd8a6f64b1d93a02c32f30588db479cab", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 15, "n_files_impacted": 2, "longest_chunk": ["", "    listItem.id = `noteId-${noteId}`;", "    noteId++;", "}", "", "", "function selectNote(x){", "      ", "", "      for(let i=0;i<x.length;i++){", "            x[i].addEventListener(\"click\",function(){", "                x[i].style.backgroundColor=\"rgb(166, 245, 192)\";", "                // console.log(x[i].children);", "            });", "      }"], "file_path": "script.js"}
{"Link_to_commit": "https://github.com/Korvin1337/UppgiftSeeSharp/commit/87d41e44b0cf5f371be400d99ed230861ff56f59", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 79, "n_files_impacted": 3, "longest_chunk": ["\ufeffusing Business.Helpers;", "using Business.Services;", "using Busniess.Models;", "using Moq;", "using System;", "using System.Collections.Generic;", "using System.Linq;", "using System.Text;", "using System.Threading.Tasks;", "using Xunit;", "", "namespace UppgiftSeeSharp.Tests.Services;", "", "public class UserInputService_Tests", "{", "", "    /* With the help of ChatGPT 4 i made this test", "     * Since i cant moq properly my inputhandler i decided to create a class that is based on the inputhandler here instead", "     * then we test this logic with asserts to compare the result to the given input string */", "    /* The TestInputHandler class implements the InputHandler requirements ConsoleWrapper and adds the string _ inputs and currentInputIndex logic*/", "    public class TestInputHandler : InputHandler", "    {", "        private readonly string[] _inputs;", "        private int _currentInputIndex = 0;", "", "        public TestInputHandler(string[] inputs, ConsoleWrapper consoleWrapper) : base(consoleWrapper)", "        {", "            _inputs = inputs;", "        }", "", "        /* Returns the value of the string at the current index and increments it */", "        public override string GetInput(string prompt)", "        {", "            if(_currentInputIndex < _inputs.Length)", "            {", "                return _inputs[_currentInputIndex++];", "            }", "            return string.Empty;", "        }", "    }", "", "    /* ChatGpt4 ", "     * This test that checks the logic with asserts to compare the result to the given input string *", "     * With the help of the TestInputHandler class */", "    [Fact]", "    public void CollectUserData_ShouldReturnMockedUserRegistrationFormData()", "    {", "", "        // arrange", "        var inputs = new String[]", "        {", "            \"Test\",", "            \"Testsson\",", "            \"Test.Testsson@Test.com\",", "            \"0760321142\",", "            \"TestVagen 24\",", "            \"325 12\",", "            \"TestStaden\"", "        };", "", "        var consoleWrapper = new ConsoleWrapper();", "        var testInputHandler = new TestInputHandler(inputs, consoleWrapper);", "        var userInputService = new UserInputService(testInputHandler);", "", "        // act", "        var result = userInputService.CollectUserData();", "", "", "        // assert", "        Assert.Equal(\"Test\", result.FirstName);", "        Assert.Equal(\"Testsson\", result.LastName);", "        Assert.Equal(\"Test.Testsson@Test.com\", result.Email);", "        Assert.Equal(\"0760321142\", result.PhoneNumber);", "        Assert.Equal(\"TestVagen 24\", result.Address);", "        Assert.Equal(\"325 12\", result.PostalNumber);", "        Assert.Equal(\"TestStaden\", result.City);", "        Assert.NotNull(result);", "    }", "}"], "file_path": "UppgiftSeeSharp.Tests/Services/UserInputService_Tests.cs"}
{"Link_to_commit": "https://github.com/leigh966/deadliner/commit/6489e6f76a7ae3b5289022f4f33a60ef36788672", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 127, "n_files_impacted": 2, "longest_chunk": ["// app/cookie-policy/page.js", "import styles from \"./cookiePolicy.module.css\";", "", "const CookiePolicy = () => {", "  return (", "    <div className={styles.container}>", "      <h1 className={styles.heading1}>Cookie Policy</h1>", "      <p className={styles.paragraph}>", "        <strong>Last Updated:</strong> January 21, 2025", "      </p>", "", "      <h2 className={styles.heading2}>1. Introduction</h2>", "      <p className={styles.paragraph}>", "        Hi there! I'm a solo developer, and I run this website as a personal", "        project. This Cookie Policy is here to let you know what cookies I use", "        on this site, why I use them, and how they help the website function.", "        Since I only use essential cookies, there\u2019s nothing to worry about \u2014", "        they\u2019re necessary for the website to work properly.", "      </p>", "      <p className={styles.paragraph}>", "        By continuing to browse this site, you agree to the use of these", "        cookies.", "      </p>", "", "      <h2 className={styles.heading2}>2. What Are Cookies?</h2>", "      <p className={styles.paragraph}>", "        Cookies are small text files that are stored on your device when you", "        visit a website. They help websites remember certain information about", "        your visit, like preferences or login status, so the site can work as", "        expected.", "      </p>", "", "      <h2 className={styles.heading2}>3. Cookies I Use</h2>", "      <p className={styles.paragraph}>", "        This site uses <strong>only essential cookies</strong>. These cookies", "        are needed for the website to function and cannot be disabled in the", "        system. They are used for things like:", "      </p>", "      <ul className={styles.list}>", "        <li className={styles.listItem}>", "          <strong>Session management:</strong> Keeping you logged in while you", "          use the site.", "        </li>", "      </ul>", "      <p className={styles.paragraph}>", "        Since these cookies are essential, they don't require your consent, but", "        I want to make sure you're aware of them.", "      </p>", "", "      <h2 className={styles.heading2}>4. Why Do I Use These Cookies?</h2>", "      <p className={styles.paragraph}>I use essential cookies to:</p>", "      <ul className={styles.list}>", "        <li className={styles.listItem}>", "          Ensure the site works correctly (e.g., so you can log in or return as", "          a guest).", "        </li>", "        <li className={styles.listItem}>", "          Maintain your session while you\u2019re using the site so that you don\u2019t", "          have to log in repeatedly.", "        </li>", "      </ul>", "      <p className={styles.paragraph}>", "        These cookies are crucial for the website\u2019s operation and make sure the", "        basic functionality is there.", "      </p>", "", "      <h2 className={styles.heading2}>5. How to Control Cookies</h2>", "      <p className={styles.paragraph}>", "        Since the cookies I use are essential, they are required for the website", "        to function. There\u2019s no need to accept them manually. However, if you", "        prefer, you can adjust your browser settings to block or delete cookies,", "        but this may affect the site\u2019s functionality.", "      </p>", "      <p className={styles.paragraph}>", "        Here\u2019s how you can manage cookies in your browser:", "      </p>", "      <ul className={styles.list}>", "        <li className={styles.listItem}>", "          <a", "            href=\"https://support.google.com/chrome/answer/95647?hl=en\"", "            target=\"_blank\"", "            className={styles.link}", "            rel=\"noopener noreferrer\"", "          >", "            Google Chrome", "          </a>", "        </li>", "        <li className={styles.listItem}>", "          <a", "            href=\"https://support.mozilla.org/en-US/kb/enable-and-disable-cookies-website-preferences\"", "            target=\"_blank\"", "            className={styles.link}", "            rel=\"noopener noreferrer\"", "          >", "            Mozilla Firefox", "          </a>", "        </li>", "        <li className={styles.listItem}>", "          <a", "            href=\"https://support.apple.com/guide/safari/manage-cookies-and-website-data-sfri11471/mac\"", "            target=\"_blank\"", "            className={styles.link}", "            rel=\"noopener noreferrer\"", "          >", "            Safari", "          </a>", "        </li>", "      </ul>", "", "      <h2 className={styles.heading2}>6. Changes to This Cookie Policy</h2>", "      <p className={styles.paragraph}>", "        Since this website is a personal project, I may update this policy", "        occasionally. If I make any changes, I\u2019ll update the \u201cLast Updated\u201d date", "        above. Feel free to check back here to stay informed.", "      </p>", "", "      <h2 className={styles.heading2}>7. Contact Me</h2>", "      <p className={styles.paragraph}>", "        If you have any questions or concerns about this Cookie Policy or how", "        cookies are used on this site, feel free to reach out to me at{\" \"}", "        <strong className={styles.strong}>support@itsthenikolai.com</strong>.", "      </p>", "    </div>", "  );", "};", "", "export default CookiePolicy;"], "file_path": "src/app/cookie-policy/page.js"}
{"Link_to_commit": "https://github.com/SvnFrs/gh-automation/commit/9ee37827dd7c6668a88a684d67ea5629198053bc", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 75, "n_files_impacted": 1, "longest_chunk": ["", "", "export async function deleteClosedIssues() {", "  const ISSUE_AUTOMATION = process.env.ISSUE_AUTOMATION;", "  const OWNER = process.env.OWNER;", "  const REPO = process.env.REPO;", "", "  const octokit = new Octokit({", "    auth: ISSUE_AUTOMATION,", "  });", "", "  try {", "    let pageInfo = { hasNextPage: true, endCursor: null };", "    let deletedCount = 0;", "", "    while (pageInfo.hasNextPage) {", "      // Fetch closed issues with GraphQL", "      const query = `", "        query($owner: String!, $repo: String!, $cursor: String) {", "          repository(owner: $owner, name: $repo) {", "            issues(states: CLOSED, first: 50, after: $cursor) {", "              pageInfo {", "                hasNextPage", "                endCursor", "              }", "              nodes {", "                id", "                number", "                title", "              }", "            }", "          }", "        }", "      `;", "", "      const response = await octokit.graphql<any>(query, {", "        owner: OWNER,", "        repo: REPO,", "        cursor: pageInfo.endCursor,", "      });", "", "      const issues = (response as any).repository.issues.nodes;", "      pageInfo = (response as any).repository.issues.pageInfo;", "", "      for (const issue of issues) {", "        try {", "          const mutation = `", "            mutation($issueId: ID!) {", "              deleteIssue(input: { issueId: $issueId }) {", "                clientMutationId", "              }", "            }", "          `;", "", "          await octokit.graphql(mutation, { issueId: issue.id });", "          console.log(`Deleted issue #${issue.number}: ${issue.title}`);", "          deletedCount++;", "", "          // Delay to avoid hitting rate limits", "          await new Promise((resolve) => setTimeout(resolve, 2000));", "        } catch (error) {", "          if (error instanceof Error) {", "            console.error(`Failed to delete issue #${issue.number}: ${issue.title}`, error.message);", "          }", "        }", "      }", "    }", "", "    console.log(`Deleted ${deletedCount} closed issues.`);", "  } catch (error) {", "    if (error instanceof Error) {", "      console.error(\"Failed to delete closed issues\", error.message);", "    }", "  }", "}"], "file_path": "issue.ts"}
{"Link_to_commit": "https://github.com/mauries-lopez/LeetCode-Workspace/commit/c61e7fda6f7ac812ef21c757d7fd7e0f976b7ea1", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 76, "n_files_impacted": 3, "longest_chunk": ["#include <iostream>", "", "struct TreeNode {", "    int val;", "    TreeNode *left;", "    TreeNode *right;", "    TreeNode() : val(0), left(nullptr), right(nullptr) {}", "    TreeNode(int x) : val(x), left(nullptr), right(nullptr) {}", "    TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {}", "};", "", "class Solution {", "public:", "", "    int leftDepth = 1, rightDepth = 1;", "    int maxDepth(TreeNode* root) {", "        if ( root != nullptr ){", "            (root->left != nullptr ? leftDepth++ : leftDepth);", "            (root->right != nullptr ? rightDepth++ : rightDepth);", "            leftDepth = findMaxDepth(root->left);", "            rightDepth = findMaxDepth(root->right);", "            return 1 + (leftDepth >= rightDepth ? leftDepth : rightDepth);", "        }", "        return 0;", "    }", "", "    int findMaxDepth(TreeNode* root){", "        if ( root == nullptr ){", "            return 0;", "        }", "        return 1 + (std::max(findMaxDepth(root->left), findMaxDepth(root->right)));", "    }", "};", "", "int main(){", "", "    /*", "    TreeNode* tree1 = new TreeNode(-8);", "    tree1->left = new TreeNode(-6);", "    tree1->left->left = new TreeNode(6);", "    tree1->left->left->right = new TreeNode(5);", "    tree1->right = new TreeNode(7);", "    */", "    ", "    // TreeNode* tree1 = new TreeNode(3);", "    // tree1->left = new TreeNode(9);", "    // tree1->right = new TreeNode(20);", "    // tree1->right->left = new TreeNode(15);", "    // tree1->right->right = new TreeNode(7);", "    ", "    /*", "    TreeNode* tree1 = new TreeNode(3);", "    tree1->left = new TreeNode(4);", "    tree1->left->left = new TreeNode(-7);", "    tree1->left->left->left = new TreeNode(-7);", "    tree1->left->right = new TreeNode(-6);", "    tree1->left->right->left = new TreeNode(-5);", "    tree1->left->right->left->left = new TreeNode(-4);", "    tree1->right = new TreeNode(5);", "    */", "", "    TreeNode* tree1 = new TreeNode(0);", "    tree1->left = new TreeNode(2);", "    tree1->left->left = new TreeNode(1);", "    tree1->left->left->left = new TreeNode(5);", "    tree1->left->left->right = new TreeNode(1);", "    tree1->right = new TreeNode(4);", "    tree1->right->left = new TreeNode(3);", "    tree1->right->left->right = new TreeNode(6);", "    tree1->right->right = new TreeNode(-1);", "    tree1->right->right->right = new TreeNode(8);", "", "    Solution solution;", "    int res = solution.maxDepth(tree1);", "    std::cout << res << std::endl;", "}"], "file_path": "[Easy] Maximum Depth of Binary Tree/main.cpp"}
{"Link_to_commit": "https://github.com/jjrboucher/Chromium-Browser-Artifact-Parser/commit/56eefc22e2dd9d5fb3f2ac63b7b3d53d40ceecfa", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 5, "n_files_impacted": 1, "longest_chunk": ["", "def exit_menu():", "    exit()", "", ""], "file_path": "browser-artifact-parser-GUI.py"}
{"Link_to_commit": "https://github.com/jjrboucher/Chromium-Browser-Artifact-Parser/commit/0bef4d9fc46c75fb6211b87ea0d70969aa9d453a", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 8, "n_files_impacted": 1, "longest_chunk": ["        self.status_text = tk.Text(root, height=20, width=70, state=\"disabled\", bg=\"black\", fg=\"white\")", "        self.status_text.grid(row=3, column=1, columnspan=2, padx=20, pady=20)", "", "        tk.Button(root, text=\"Exit\", width=10, command=self.exit, bg=\"red\").grid(row=2, column=2, padx=10, pady=5)", "", "    def exit(self):", "        exit()", ""], "file_path": "browser-artifact-parser-GUI.py"}
{"Link_to_commit": "https://github.com/jjrboucher/Chromium-Browser-Artifact-Parser/commit/1d4927c67c56e7b509edd790fe0b19c2b8398779", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 171, "n_files_impacted": 2, "longest_chunk": ["import tkinter as tk", "from tkinter import filedialog, messagebox", "from Classes.Preferences import Preferences", "from Functions.write_to_excel import write_excel", "from JSON.bookmarks import get_chromium_bookmarks", "from SQLite.cookies import chrome_cookies", "from SQLite.downloads import chrome_downloads, chrome_downloads_gaps", "from SQLite.favicons import chrome_favicons", "from SQLite.history import chrome_history, chrome_history_gaps", "from SQLite.logindata import chrome_login_data, chrome_login_data_gaps", "from SQLite.searchterms import chrome_keyword_historyquery", "from SQLite.shortcuts import chrome_shortcuts", "from SQLite.WebData import (", "    chrome_autofill, chrome_keywords, chrome_masked_credit_cards, chrome_masked_bank_accounts", ")", "import pandas as pd", "import sqlite3", "import numpy as np", "import io", "", "class ChromeParserGUI:", "    def __init__(self, root):", "        self.root = root", "        self.root.title(\"Chrome Parser\")", "        self.profile_path = None", "        self.output_path = None", "", "        # Labels and Buttons", "        tk.Label(root, text=\"Chrome User Profile Folder:\").grid(row=0, column=0, sticky=\"w\", padx=10, pady=5)", "        self.profile_entry = tk.Entry(root, width=50)", "        self.profile_entry.grid(row=0, column=1, padx=10, pady=5)", "        tk.Button(root, text=\"Browse\", command=self.browse_profile).grid(row=0, column=2, padx=10, pady=5)", "", "        tk.Label(root, text=\"Output Excel File:\").grid(row=1, column=0, sticky=\"w\", padx=10, pady=5)", "        self.output_entry = tk.Entry(root, width=50)", "        self.output_entry.grid(row=1, column=1, padx=10, pady=5)", "        tk.Button(root, text=\"Browse\", command=self.browse_output).grid(row=1, column=2, padx=10, pady=5)", "", "        tk.Button(root, text=\"Run Parser\", command=self.run_parser).grid(row=2, column=1, pady=20)", "", "        # Status Window", "        tk.Label(root, text=\"Status:\").grid(row=3, column=0, sticky=\"nw\", padx=10, pady=5)", "        self.status_text = tk.Text(root, height=10, width=70, state=\"disabled\")", "        self.status_text.grid(row=3, column=1, columnspan=2, padx=10, pady=5)", "", "    def update_status(self, message):", "        self.status_text.config(state=\"normal\")", "        self.status_text.insert(tk.END, message + \"\\n\")", "        self.status_text.see(tk.END)", "        self.status_text.config(state=\"disabled\")", "", "    def browse_profile(self):", "        self.profile_path = filedialog.askdirectory(title=\"Select Chrome User Profile Folder\")", "        self.profile_entry.delete(0, tk.END)", "        self.profile_entry.insert(0, self.profile_path)", "", "    def browse_output(self):", "        self.output_path = filedialog.asksaveasfilename(", "            title=\"Select Output Excel File\", defaultextension=\".xlsx\", filetypes=[(\"Excel files\", \"*.xlsx\")]", "        )", "        self.output_entry.delete(0, tk.END)", "        self.output_entry.insert(0, self.output_path)", "", "    def run_parser(self):", "        if not self.profile_path or not self.output_path:", "            messagebox.showerror(\"Error\", \"Please select both profile and output paths.\")", "            return", "", "        try:", "            chrome_queries = {", "                'History': [f'{self.profile_path}/History', chrome_history],", "                \"History Gaps\": [f'{self.profile_path}/History', chrome_history_gaps],", "                \"Downloads\": [f'{self.profile_path}/History', chrome_downloads],", "                \"Downloads Gaps\": [f'{self.profile_path}/History', chrome_downloads_gaps],", "                \"Autofill\": [f'{self.profile_path}/Web Data', chrome_autofill],", "                \"Keywords\": [f'{self.profile_path}/Web Data', chrome_keywords],", "                \"Credit Cards\": [f'{self.profile_path}/Web Data', chrome_masked_credit_cards],", "                \"Bank Accounts\": [f'{self.profile_path}/Web Data', chrome_masked_bank_accounts],", "                \"Login Data\": [f'{self.profile_path}/Login Data', chrome_login_data],", "                \"Login Data Gaps\": [f'{self.profile_path}/Login Data', chrome_login_data_gaps],", "                \"Shortcuts\": [f'{self.profile_path}/Shortcuts', chrome_shortcuts],", "                \"Cookies\": [f'{self.profile_path}/Network/Cookies', chrome_cookies],", "                \"FavIcons\": [f'{self.profile_path}/Favicons', chrome_favicons]", "            }", "", "            record_counts = []", "", "            for sqlite_query in chrome_queries.keys():", "                self.update_status(f\"Processing {sqlite_query}...\")", "                df, ws = self.get_dataframes(chrome_queries[sqlite_query][0], chrome_queries[sqlite_query][1])", "                write_excel(df, ws, self.output_path)", "                record_counts.append((ws, len(df)))", "", "            self.update_status(\"Processing Search Terms...\")", "            dataframe_searchterms, ws = self.process_search_terms()", "            write_excel(dataframe_searchterms, ws, self.output_path)", "            record_counts.append((ws, len(dataframe_searchterms)))", "", "            self.update_status(\"Processing Bookmarks...\")", "            bookmarks_df, ws = get_chromium_bookmarks(f'{self.profile_path}/Bookmarks')", "            bookmarks_backup_df, ws_bak = get_chromium_bookmarks(f'{self.profile_path}/Bookmarks.bak')", "            all_bookmarks = pd.concat([bookmarks_df, bookmarks_backup_df], ignore_index=True)", "            write_excel(all_bookmarks, ws, self.output_path)", "            record_counts.append((ws, len(all_bookmarks)))", "", "            self.update_status(\"Processing Preferences...\")", "            preferences = Preferences(f'{self.profile_path}/Preferences')", "            preferences_output = io.StringIO()", "            print(preferences, file=preferences_output)", "            preferences_data = preferences_output.getvalue().splitlines()", "            preferences_df = pd.DataFrame(preferences_data, columns=[\"Preferences Output\"])", "            write_excel(preferences_df, \"Preferences\", self.output_path)", "", "            self.update_status(\"Creating Summary Worksheet...\")", "            summary_df = pd.DataFrame(record_counts, columns=[\"Worksheet Name\", \"Record Count\"])", "            write_excel(summary_df, \"Summary\", self.output_path)", "", "            self.update_status(\"All processing completed successfully.\")", "            messagebox.showinfo(\"Success\", f\"Parsing completed! Output saved to {self.output_path}\")", "", "        except Exception as e:", "            self.update_status(f\"Error: {e}\")", "            messagebox.showerror(\"Error\", f\"An error occurred: {e}\")", "", "    def get_dataframes(self, db_file, function):", "        query, worksheet_name = function()", "        conn = sqlite3.connect(db_file)", "        dataframe = pd.read_sql_query(query, conn)", "        conn.close()", "        return dataframe, worksheet_name", "", "    def process_search_terms(self):", "        worksheet = 'Search Terms'", "        input_file = f'{self.profile_path}/History'", "        df_history, ws_history = self.get_dataframes(input_file, chrome_keyword_historyquery)", "", "        input_file = f'{self.profile_path}/Web Data'", "        df_keywords, ws_keyword = self.get_dataframes(input_file, chrome_keywords)", "", "        searchterms = []", "        if len(df_keywords) > 0:", "            for row in df_history.itertuples():", "                if not np.isnan(row[3]):", "                    try:", "                        kw = [", "                            df_keywords.query(f'id == {row[3]}')['keyword'].values[0],", "                            df_keywords.query(f'id == {row[3]}')['date_created'].values[0],", "                            df_keywords.query(f'id == {row[3]}')['Decoded date_created (UTC)'].values[0],", "                            df_keywords.query(f'id == {row[3]}')['last_modified'].values[0],", "                            df_keywords.query(f'id == {row[3]}')['Decoded last_modified (UTC)'].values[0]", "                        ]", "                    except IndexError:", "                        kw = ['', '', '', '', '']", "                    searchterms.append([", "                        row[1], row[2], row[3], kw[0], row[5], row[6], kw[1], kw[2], kw[3], kw[4], row[7], row[8]", "                    ])", "        else:", "            searchterms = [[\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]]", "", "        df_searchterms = pd.DataFrame(searchterms)", "        df_searchterms.columns = [", "            'URL id', 'url', 'keyword id', 'keyword', 'search term', 'typed_count', 'date_created',", "            'Decoded date_created (UTC)', 'last_modified', 'Decoded last_modified (UTC)',", "            'last_visit_time (UTC)', 'Decoded last_visit_time (UTC)'", "        ]", "        return df_searchterms, worksheet", "", "if __name__ == '__main__':", "    root = tk.Tk()", "    app = ChromeParserGUI(root)", "    root.mainloop()"], "file_path": "browser-artifact-parser-GUI.py"}
{"Link_to_commit": "https://github.com/savannahharvey/morse_code_game/commit/d642eb91571a0955805fe068e076d604235086d0", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 38, "n_files_impacted": 10, "longest_chunk": ["\ufeffnamespace MorseCodeGame;", "", "partial class Form1", "{", "    /// <summary>", "    ///  Required designer variable.", "    /// </summary>", "    private System.ComponentModel.IContainer components = null;", "", "    /// <summary>", "    ///  Clean up any resources being used.", "    /// </summary>", "    /// <param name=\"disposing\">true if managed resources should be disposed; otherwise, false.</param>", "    protected override void Dispose(bool disposing)", "    {", "        if (disposing && (components != null))", "        {", "            components.Dispose();", "        }", "        base.Dispose(disposing);", "    }", "", "    #region Windows Form Designer generated code", "", "    /// <summary>", "    ///  Required method for Designer support - do not modify", "    ///  the contents of this method with the code editor.", "    /// </summary>", "    private void InitializeComponent()", "    {", "        this.components = new System.ComponentModel.Container();", "        this.AutoScaleMode = System.Windows.Forms.AutoScaleMode.Font;", "        this.ClientSize = new System.Drawing.Size(800, 450);", "        this.Text = \"Form1\";", "    }", "", "    #endregion", "}"], "file_path": "MorseCodeGame/Form1.cs"}
{"Link_to_commit": "https://github.com/NaveenPudupeti/Flappy_Bird_Game/commit/353e63d6df5a93cd51c66808f62f68349c3b85c0", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 131, "n_files_impacted": 12, "longest_chunk": ["package com.flappyBird;", "", "import javax.swing.*;", "import java.awt.*;", "import java.awt.event.*;", "import java.util.ArrayList;", "import java.util.Random;", "", "public class GamePanel extends JPanel implements ActionListener, KeyListener {", "    private static final int WIDTH = 800;", "    static final int HEIGHT = 600;", "    static final int PIPE_WIDTH = 80;", "    static final int PIPE_GAP = 150;", "    static final int GRAVITY = 2;", "    static final int JUMP_STRENGTH = -15;", "", "    private Timer timer;", "    private Bird bird;", "    private ArrayList<Pipe> pipes;", "    private int score;", "    private boolean gameOver;", "", "    private enum GameState { START, PLAYING, GAME_OVER }", "    private GameState gameState = GameState.START;", "", "    public GamePanel() {", "        setPreferredSize(new Dimension(WIDTH, HEIGHT));", "        setBackground(Color.CYAN);", "        setFocusable(true);", "        addKeyListener(this);", "", "        initGame();", "    }", "", "    private void initGame() {", "        bird = new Bird(WIDTH / 4, HEIGHT / 2);", "        pipes = new ArrayList<>();", "        score = 0;", "        gameOver = false;", "", "        timer = new Timer(20, this);", "        timer.start();", "", "        for (int i = 0; i < 3; i++) {", "            pipes.add(new Pipe(WIDTH + i * 300, new Random().nextInt(HEIGHT / 2) + HEIGHT / 4));", "        }", "    }", "", "    @Override", "    protected void paintComponent(Graphics g) {", "        super.paintComponent(g);", "", "        if (gameState == GameState.START) {", "            g.setColor(Color.BLACK);", "            g.setFont(new Font(\"Arial\", Font.BOLD, 36));", "            g.drawString(\"Press SPACE to Start\", WIDTH / 2 - 180, HEIGHT / 2);", "            return;", "        }", "", "        if (gameState == GameState.GAME_OVER) {", "            g.setColor(Color.RED);", "            g.setFont(new Font(\"Arial\", Font.BOLD, 48));", "            g.drawString(\"Game Over!\", WIDTH / 2 - 150, HEIGHT / 2 - 50);", "            g.drawString(\"Score: \" + score, WIDTH / 2 - 100, HEIGHT / 2 + 50);", "            g.drawString(\"Press R to Restart\", WIDTH / 2 - 180, HEIGHT / 2 + 100);", "            return;", "        }", "", "        bird.draw(g);", "", "        for (Pipe pipe : pipes) {", "            pipe.draw(g);", "        }", "", "        g.setColor(Color.WHITE);", "        g.setFont(new Font(\"Arial\", Font.BOLD, 24));", "        g.drawString(\"Score: \" + score, 10, 30);", "    }", "", "    @Override", "    public void actionPerformed(ActionEvent e) {", "        if (gameState != GameState.PLAYING) return;", "", "        bird.update();", "", "        for (Pipe pipe : pipes) {", "            pipe.update();", "", "            if (pipe.collidesWith(bird)) {", "                gameState = GameState.GAME_OVER;", "            }", "", "            if (!pipe.isScored && pipe.x + PIPE_WIDTH < bird.x) {", "                score++;", "                pipe.isScored = true;", "            }", "", "            if (pipe.isOffScreen()) {", "                pipe.reset(WIDTH);", "            }", "        }", "", "        if (bird.y > HEIGHT || bird.y < 0) {", "            gameState = GameState.GAME_OVER;", "        }", "", "        repaint();", "    }", "", "    @Override", "    public void keyPressed(KeyEvent e) {", "        if (gameState == GameState.START && e.getKeyCode() == KeyEvent.VK_SPACE) {", "            gameState = GameState.PLAYING;", "        }", "", "        if (gameState == GameState.GAME_OVER && e.getKeyCode() == KeyEvent.VK_R) {", "            initGame();", "            gameState = GameState.START;", "        }", "", "        if (gameState == GameState.PLAYING && e.getKeyCode() == KeyEvent.VK_SPACE) {", "            bird.jump();", "        }", "    }", "", "    @Override", "    public void keyReleased(KeyEvent e) {}", "", "    @Override", "    public void keyTyped(KeyEvent e) {}", "}"], "file_path": "src/com/flappyBird/Pipe.java"}
{"Link_to_commit": "https://github.com/evelasco11/CSE31/commit/016966da1599b8ffe5ab844c77a640a9413cd5d5", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 11, "n_files_impacted": 2, "longest_chunk": ["    // Implemented by ChatGPT to help with properly copying even and odd numbers into their respective Arrays.", "    int evenIndex = 0, oddIndex = 0;", "\tfor (int i = 0; i < size; i++){", "        if (*(arr + i) % 2 == 0){", "            *(arr_even + evenIndex) =  *(arr + i); ", "            evenIndex++;", "        } else {", "            *(arr_odd + oddIndex) = *(arr + i);", "            oddIndex++;", "        }", "    }"], "file_path": "Lab_02/Part_5/arrCopy.c"}
{"Link_to_commit": "https://github.com/Niknu/adventOfCode2024/commit/a3da1f6f1cd15a403693d09e9b8b759de551bb23", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 28, "n_files_impacted": 1, "longest_chunk": ["def process_instructions(instruction_string):", "    # Initialize the state of mul (enabled at the start)", "    mul_enabled = True", "    total_sum = 0", "    ", "    # Regular expression to match the relevant components: mul(a,b) and do()/don't()", "    mul_pattern = r\"mul\\((\\d+),(\\d+)\\)\"", "    do_pattern = r\"do\\(\\)\"", "    dont_pattern = r\"don't\\(\\)\"", "    ", "    # Split the input string by instructions", "    parts = re.split(r\"(mul\\(\\d+,\\d+\\)|do\\(\\)|don't\\(\\))\", instruction_string)", "    ", "    for part in parts:", "        if re.match(mul_pattern, part):", "            # If mul is enabled, process the multiplication", "            match = re.match(mul_pattern, part)", "            if match and mul_enabled:", "                a, b = int(match.group(1)), int(match.group(2))", "                total_sum += a * b", "        elif re.match(do_pattern, part):", "            # Enable mul", "            mul_enabled = True", "        elif re.match(dont_pattern, part):", "            # Disable mul", "            mul_enabled = False", "    ", "    return total_sum"], "file_path": "3rd/3_2.py"}
{"Link_to_commit": "https://github.com/AhmetBerkeKrc/chatbot-project/commit/a21287b2906e03eab1c5c7c645efce3bbebd813e", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 42, "n_files_impacted": 1, "longest_chunk": ["import os", "", "", "# Fetch API key from environment variables for security", "api_key = \"API_KEY\"", "", "# Initialize FastAPI app", "app = FastAPI()", "", "# Initialize OpenAI client with the provided API key", "client = OpenAI(api_key=api_key)", "", "# Define a Pydantic model for input validation (user query)", "class UserInput(BaseModel):", "    query: str", "", "# Endpoint for processing medical questions", "@app.post(\"/ask/\")", "def ask_medical_question(user_input: UserInput):", "    try:", "        # Call OpenAI API to get a response based on the user's query", "        completion = client.chat.completions.create(", "            model=\"gpt-4o-mini\",  ", "            messages=[{", "                \"role\": \"developer\",", "                \"content\": \"You are a medical assistant chatbot. Your sole purpose is to answer health-related questions. \"", "                            \"Do not respond to any queries outside of the medical domain. When faced with non-medical inquiries, prompt that 'I can only assist with health-related matters.'\"", "                            \"and offer no further information.  Ensure your responses are accurate, informative, and based on reliable medical sources. \"", "                            \"Always advise users to consult with a qualified healthcare professional for personalized medical advice. Use relevant and appropriate emojis in your responses \"", "                            \"to make the interaction more engaging and friendly.\"", "            },", "            {\"role\": \"user\", \"content\": user_input.query}", "            ]", "        )", "", "        # Return the response from the model", "        response = completion.choices[0].message.content", "        return {\"response\": response}", "", "    except Exception as e:", "        # In case of error, raise HTTPException with a 500 status code", "        raise HTTPException(status_code=500, detail=f\"An error occurred: {str(e)}\")"], "file_path": "LLM_plygrnd.py"}
{"Link_to_commit": "https://github.com/vishalvsr67/vishal-jfc/commit/3350e49fbb7324ea0059ec37b2572db1b6b23c43", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 7, "n_files_impacted": 1, "longest_chunk": ["            } else {", "                // Hollow middle part", "            System.out.print(\"*\"); // First star", "            for (int j = 1; j <= n - 2; j++) {", "                System.out.print(\"  \");", "            }", "            System.out.print(\" *\"); // Last star"], "file_path": "pphollowrhombus.java"}
{"Link_to_commit": "https://github.com/nanalakshmanan/PythonUtilities/commit/5e9127ca825d70812b4c6759fbc0e6e1c330e616", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 161, "n_files_impacted": 1, "longest_chunk": ["import asyncio", "from pathlib import Path", "import hashlib", "import json", "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor", "import logging", "import argparse", "", "BATCH_SIZE = 20", "EXCLUDED_DIRS = {'$RECYCLE.BIN', 'System Volume Information'}", "HASH_ALGORITHM = 'blake2b'", "CHUNK_SIZE = 8388608  # 8 MB", "", "# \u2705 Separate flags for object-level and batch-level handling", "first_entry_written = False", "first_batch_written = False", "", "", "def setup_logging(logfile):", "    \"\"\"Configure logging to file or console.\"\"\"", "    logger = logging.getLogger()", "    logger.setLevel(logging.INFO)", "    ", "    handler = logging.FileHandler(logfile) if logfile else logging.StreamHandler()", "    ", "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')", "    handler.setFormatter(formatter)", "    ", "    # \u2705 Remove any existing handlers to avoid duplicates", "    if logger.hasHandlers():", "        logger.handlers.clear()", "    ", "    logger.addHandler(handler)", "", "", "def compute_file_hash(file_path):", "    \"\"\"Computes the hash using a faster hashing algorithm (blake2b) in chunks.\"\"\"", "    hash_func = hashlib.new(HASH_ALGORITHM)", "    try:", "        with open(file_path, 'rb') as f:", "            while chunk := f.read(CHUNK_SIZE):", "                hash_func.update(chunk)", "        return hash_func.hexdigest()", "    except Exception as e:", "        logging.warning(f\"Failed to compute hash for {file_path}: {e}\")", "        return None", "", "", "async def list_files_in_batches(root_dir):", "    file_batch = []", "    try:", "        for file in Path(root_dir).rglob('*'):", "            if any(part in EXCLUDED_DIRS for part in file.parts):", "                continue", "", "            if file.is_file():", "                file_batch.append(file)", "                if len(file_batch) == BATCH_SIZE:", "                    yield file_batch", "                    file_batch = []", "", "        if file_batch:", "            yield file_batch", "", "    except PermissionError as e:", "        logging.warning(f\"Permission denied: {e}\")", "    except Exception as e:", "        logging.error(f\"Error while listing files: {e}\")", "", "", "async def process_batch(batch):", "    results = []", "", "    with ProcessPoolExecutor() as hash_executor:", "        # \u2705 Compute hashes in parallel using ProcessPoolExecutor", "        hash_futures = [", "            asyncio.get_event_loop().run_in_executor(hash_executor, compute_file_hash, file)", "            for file in batch", "        ]", "        computed_hashes = await asyncio.gather(*hash_futures)", "", "        # \u2705 Combine results", "        for file, file_hash in zip(batch, computed_hashes):", "            if file_hash:", "                result = {'file_path': str(file), 'hash': file_hash}", "                logging.info(json.dumps(result, indent=4))", "                results.append(result)", "", "    return results", "", "", "async def write_to_json(data, output_file, is_last_batch):", "    global first_entry_written, first_batch_written", "", "    if data:", "        try:", "            with open(output_file, 'a') as f:", "                # \u2705 Write opening bracket if this is the first batch", "                if not first_batch_written:", "                    f.write('{\\n  \"hashes\": [\\n')", "                    first_batch_written = True", "", "                for index, entry in enumerate(data):", "                    # \u2705 Insert comma ONLY if this is NOT the first object in the entire file", "                    if first_entry_written:", "                        f.write(',\\n')", "", "                    # \u2705 Write the JSON object", "                    json.dump(entry, f, indent=4)", "", "                    # \u2705 Mark that an entry has been written (now it's safe to add a comma next time)", "                    first_entry_written = True", "", "                # \u2705 If this is the last batch, close the JSON array and object properly", "                if is_last_batch:", "                    f.write('\\n  ]\\n}\\n')", "", "        except Exception as e:", "            logging.error(f\"Error writing to JSON file: {e}\")", "", "", "async def main(root_dir, output_file, dry_run):", "    global first_entry_written, first_batch_written", "    is_last_batch = False", "", "    # \u2705 Truncate the file if it already exists", "    open(output_file, 'w').close()", "", "    async for batch in list_files_in_batches(root_dir):", "        logging.info(f\"Processing batch of {len(batch)} files...\")", "", "        # \u2705 Make sure all hashes are computed before writing", "        results = await process_batch(batch)", "", "        # \u2705 Determine if this is the last batch", "        is_last_batch = len(batch) < BATCH_SIZE", "", "        if not dry_run and results:", "            await write_to_json(results, output_file, is_last_batch)", "", "        logging.info('-' * 40)", "", "    # \u2705 Ensure the JSON is properly closed if it wasn't closed in the last batch", "    if not dry_run and not is_last_batch:", "        with open(output_file, 'a') as f:", "            f.write('\\n  ]\\n}\\n')", "", "", "if __name__ == \"__main__\":", "    parser = argparse.ArgumentParser(description=\"Generate file hashes and store them in a JSON file.\")", "    parser.add_argument('--root-dir', type=str, required=True, help='Root directory to scan')", "    parser.add_argument('--output', type=str, required=True, help='Output JSON file path')", "    parser.add_argument('--dry-run', action='store_true', help=\"Simulate the process without writing to a file\")", "    parser.add_argument('--logfile', type=str, help=\"Optional log file path (if omitted, logs are printed to console)\")", "", "    args = parser.parse_args()", "", "    # \u2705 Setup logging based on --logfile parameter", "    setup_logging(args.logfile)", "", "    asyncio.run(main(args.root_dir, args.output, args.dry_run))"], "file_path": "FileHashing/listdir.py"}
{"Link_to_commit": "https://github.com/00AbHi00/nextjsGames/commit/d60c55db7599337c0e65a421d0590997e8ca9021", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 8, "n_files_impacted": 8, "longest_chunk": ["// import data from \"@/app/datas/data\"", "// import { NextResponse } from \"next/server\"", "// export async function GET() {", "//     // console.log(data)", "//     return NextResponse.json(", "//         {", "//             \"Subhalab\":[ data]  ", "//         },"], "file_path": "src/app/api/route.ts"}
{"Link_to_commit": "https://github.com/NNlk05/StereotypeAssembly/commit/8568526539d1ec16e8076a3085ddab3d6bb68ee4", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 131, "n_files_impacted": 3, "longest_chunk": ["    def print(self):", "        \"\"\"", "        Prints the value stored in vRAM at the specified index.", "        \"\"\"", "        if len(self.line_words) == 2:", "            _index = self.line_words[1]", "            print(search_ram(_index))", "", "    def store(self):", "        \"\"\"", "        Stores a value in vRAM at the specified index.", "        \"\"\"", "        if len(self.line_words) == 3:", "            _index = self.line_words[1]", "            _value = self.line_words[2]", "            write_ram(_index, dec(_value))", "", "    def read(self):", "        \"\"\"", "        Reads and prints the value stored in vRAM at the specified index.", "        \"\"\"", "        if len(self.line_words) == 2:", "            _index = self.line_words[1]", "            print(search_ram(_index))", "", "    def input(self):", "        \"\"\"", "        Stores input from the user into vRAM at the specified index.", "        \"\"\"", "        if len(self.line_words) == 2:", "            _index = self.line_words[1]", "            _value = input(\"Enter value: \")", "            write_ram(_index, dec(_value))", "", "    def conditional(self):", "        \"\"\"", "        Executes the next command if the condition is true.", "        \"\"\"", "        if len(self.line_words) >= 4:", "            _arg1 = self.line_words[1]", "            _operator = self.line_words[2]", "            _arg2 = self.line_words[3]", "            if _operator == '==':", "                if dec(_arg1) == dec(_arg2):", "                    _CommandTranslater(self.line_words[4], self.line_words[5:])", "            elif _operator == '!=':", "                if dec(_arg1) != dec(_arg2):", "                    _CommandTranslater(self.line_words[4], self.line_words[5:])", "            elif _operator == '>':", "                if dec(_arg1) > dec(_arg2):", "                    _CommandTranslater(self.line_words[4], self.line_words[5:])", "            elif _operator == '<':", "                if dec(_arg1) < dec(_arg2):", "                    _CommandTranslater(self.line_words[4], self.line_words[5:])", "            elif _operator == '>=':", "                if dec(_arg1) >= dec(_arg2):", "                    _CommandTranslater(self.line_words[4], self.line_words[5:])", "            elif _operator == '<=':", "                if dec(_arg1) <= dec(_arg2):", "                    _CommandTranslater(self.line_words[4], self.line_words[5:])", "", "    def gt(self):", "        \"\"\"", "        Checks if the first number is greater than the second.", "        \"\"\"", "        if len(self.line_words) == 3:", "            _arg1 = self.line_words[1]", "            _arg2 = self.line_words[2]", "            return dec(_arg1) > dec(_arg2)", "", "    def lt(self):", "        \"\"\"", "        Checks if the first number is less than the second.", "        \"\"\"", "        if len(self.line_words) == 3:", "            _arg1 = self.line_words[1]", "            _arg2 = self.line_words[2]", "            return dec(_arg1) < dec(_arg2)", "", "    def eq(self):", "        \"\"\"", "        Checks if the first number is equal to the second.", "        \"\"\"", "        if len(self.line_words) == 3:", "            _arg1 = self.line_words[1]", "            _arg2 = self.line_words[2]", "            return dec(_arg1) == dec(_arg2)", "", "    def goto(self):", "        \"\"\"", "        Jumps to the specified line number.", "        \"\"\"", "        if len(self.line_words) == 2:", "            _line_number = int(self.line_words[1])", "            global program_counter", "            program_counter = _line_number - 1", "", "def main():", "    \"\"\"", "    The main function to run the program.", "    \"\"\"", "    file_name = sys.argv[1]", "    file_lines = process_file(file_name)", "    global ram", "    ram = initialize_ram(RAM_SIZE)", "    global commands", "    commands = ['add','sub', 'print', 'store', 'read', 'input', 'if', 'gt', 'lt', 'eq', 'goto', 'exit']", "", "    # main loop", "    global program_counter", "    program_counter = 0", "    while program_counter < len(file_lines):", "        try:", "            program_counter += 1", "            file_line = file_lines.pop(0)", "            line_words = file_line.split(' ')", "            if not line_words[0].isdigit():", "                raise ValueError", "        except IndexError:", "            sys.exit(\"Error: Reached end of file unexpectedly\")", "        except ValueError:", "            sys.exit(f\"Error at line {program_counter}: lines must start with a number\")", "        ", "        # Evaluate the code", "        for line_word in line_words:", "            if len(line_words) == 0 or line_word == ';':", "                continue", "        _CommandTranslater(line_words[0], line_words)", "", "if __name__ == \"__main__\":", "    main()"], "file_path": "init.py"}
{"Link_to_commit": "https://github.com/Anish-018/Tic-Tac-Toe-Game/commit/57179787603725ad333f7ebfb4e8f7bafc317698", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 28, "n_files_impacted": 3, "longest_chunk": ["<!DOCTYPE html>\r", "<html lang=\"en\">\r", "<head>\r", "    <meta charset=\"UTF-8\">\r", "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r", "    <title>Tic-Tac-Toe</title>\r", "    <link rel=\"stylesheet\" href=\"style.css\">\r", "</head>\r", "<body>\r", "    <div class=\"container\">\r", "        <h1>Tic-Tac-Toe</h1>\r", "        <div id=\"board\" class=\"board\">\r", "            <div class=\"cell\" data-index=\"0\"></div>\r", "            <div class=\"cell\" data-index=\"1\"></div>\r", "            <div class=\"cell\" data-index=\"2\"></div>\r", "            <div class=\"cell\" data-index=\"3\"></div>\r", "            <div class=\"cell\" data-index=\"4\"></div>\r", "            <div class=\"cell\" data-index=\"5\"></div>\r", "            <div class=\"cell\" data-index=\"6\"></div>\r", "            <div class=\"cell\" data-index=\"7\"></div>\r", "            <div class=\"cell\" data-index=\"8\"></div>\r", "        </div>\r", "        <button id=\"reset\" class=\"reset-button\">Reset Game</button>\r", "        <p id=\"status\"></p>\r", "    </div>\r", "    <script src=\"script.js\"></script>\r", "</body>\r", "</html>\r"], "file_path": "Tic Tac Toe Game/script.js"}
{"Link_to_commit": "https://github.com/Levi-Spellmeyer/Finance-Book-Project/commit/1120907a27768eefbef9553d2f10565c9f7b5097", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 17, "n_files_impacted": 5, "longest_chunk": ["{", "  \"name\": \"financeapp\",", "  \"version\": \"1.0.0\",", "  \"main\": \"main.js\",", "  \"scripts\": {", "    \"start\": \"electron .\",", "    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"", "  },", "  \"keywords\": [],", "  \"author\": \"\",", "  \"license\": \"ISC\",", "  \"description\": \"\",", "  \"devDependencies\": {", "    \"electron\": \"^34.2.0\",", "    \"electron-reload\": \"^2.0.0-alpha.1\"", "  }", "}"], "file_path": "financeApp/renderer.js"}
{"Link_to_commit": "https://github.com/Tanmay-yadav/DATA-STRUCTURE-AND-ALGORITHM/commit/ffd552a0cc33009055f844a0357f1ddcc014ec03", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 12, "n_files_impacted": 1, "longest_chunk": ["    public void deletefirst() {", "        if (isempty()) {", "            return;", "        }", "        if (length == 1) { // Handle single-node case", "            head = null;", "            tail = null;", "        } else {", "            listnode temp = head.next;", "            temp.previous = null;", "            head.next = null;", "            head = temp;"], "file_path": "Dinesh_Varyani/LINKED LIST/doubly_linked_list/src/doublylinkedlist.java"}
{"Link_to_commit": "https://github.com/Nerosas/AdventOfCode2024/commit/cbb7da1e81d5fd0bdffb7c50a4621c41f473d1f1", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 8, "n_files_impacted": 1, "longest_chunk": ["    # Helper function to check a word in a given direction", "    def check_word(x, y, dx, dy):", "        for i in range(4):", "            new_x = x + i * dx", "            new_y = y + i * dy", "            if new_x < 0 or new_x >= rows or new_y < 0 or new_y >= cols or grid[new_x][new_y] != word[i]:", "                return False", "        return True"], "file_path": "Day4/ceres_search.py"}
{"Link_to_commit": "https://github.com/adarsh1o/first-repo/commit/2321a96aca7a6cd4fecd59b7dd118cc024822904", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 80, "n_files_impacted": 1, "longest_chunk": ["package Linked_List;", "", "", "    // Node class", "    class Node {", "        int data;", "        Node next;", "", "        public Node(int data) {", "            this.data = data;", "            this.next = null;", "        }", "    }", "", "    // SinglyLinkedList class", "    public class L_1_Link {", "        Node head; // Head of the list", "", "        // Insert a new node at the end of the list", "        public void insert(int data) {", "            Node newNode = new Node(data);", "            if (head == null) {", "                head = newNode; // If the list is empty, the new node becomes the head", "            } else {", "                Node temp = head;", "                while (temp.next != null) {", "                    temp = temp.next; // Traverse to the end of the list", "                }", "                temp.next = newNode; // Link the last node to the new node", "            }", "        }", "", "        // Display the list", "        public void display() {", "            Node temp = head;", "            while (temp != null) {", "                System.out.print(temp.data + \" -> \");", "                temp = temp.next;", "            }", "            System.out.println(\"null\");", "        }", "", "        // Delete a node by value", "        public void delete(int data) {", "            if (head == null) return; // If the list is empty, return", "", "            if (head.data == data) {", "                head = head.next; // If the head node holds the data, move the head", "                return;", "            }", "", "            Node temp = head;", "            while (temp.next != null && temp.next.data != data) {", "                temp = temp.next; // Traverse the list to find the node", "            }", "", "            if (temp.next == null) {", "                System.out.println(\"Element not found.\");", "            } else {", "                temp.next = temp.next.next; // Bypass the node to delete it", "            }", "        }", "", "        public static void main(String[] args) {", "            L_1_Link list = new L_1_Link();", "", "            list.insert(10);", "            list.insert(20);", "            list.insert(30);", "", "            System.out.println(\"Linked List:\");", "            list.display(); // Output: 10 -> 20 -> 30 -> null", "", "            list.delete(20);", "            System.out.println(\"After deleting 20:\");", "            list.display(); // Output: 10 -> 30 -> null", "        }", "    }", "", ""], "file_path": "src/Linked_List/L_1_Link.java"}
{"Link_to_commit": "https://github.com/alexDickhans/echo/commit/da28b44cbd2c4560a0cbab9ed0c58fdfe39dae81", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 60, "n_files_impacted": 2, "longest_chunk": ["        f.write(f\"#define AUTON Auton::{auton}\\n\")", "        f.write(f\"auto ALLIANCE={alliance};\\n\")", "    print(f\"Auton file updated with {auton} and default alliance {alliance}.\")", "", "def compile_auton(slot, auton):", "    \"\"\"Compiles the specified auton into the corresponding PROS slot.\"\"\"", "    command = f\"pros mu --slot {slot} --name \\\"{auton}\\\"\"", "    print(f\"Running command: {command}\")", "    result = os.system(command)", "    if result != 0:", "        print(f\"Error: Failed to compile {auton} into slot {slot}.\")", "        sys.exit(2)", "", "def compile_all():", "    \"\"\"Compiles all available autons.\"\"\"", "    for i, auton in enumerate(AUTONS):", "        write_auton_file(auton)", "        compile_auton(i + OFFSET, auton)", "", "def display_menu():", "    \"\"\"Displays the menu of options.\"\"\"", "    print(\"\\nSelect an autonomous mode:\")", "    for i, auton in enumerate(AUTONS):", "        print(f\"{i}: {auton}\")", "    print(\"a: Compile all\")", "    print(\"q: Quit\")", "", "def main():", "    parser = argparse.ArgumentParser(description=\"Auton Selector CLI\")", "    parser.add_argument(\"-s\", \"--slot\", type=int, help=\"Specify the slot for a single auton.\")", "    parser.add_argument(\"-a\", \"--alliance\", type=str, default=\"RED\", choices=[\"RED\", \"BLUE\"],", "                        help=\"Specify the alliance color (RED or BLUE).\")", "    args = parser.parse_args()", "", "    if args.slot is not None:", "        if args.slot < 0 or args.slot >= len(AUTONS):", "            print(f\"Error: Slot must be between 0 and {len(AUTONS) - 1}.\")", "            sys.exit(1)", "        auton = AUTONS[args.slot]", "        write_auton_file(auton, args.alliance)", "        compile_auton(args.slot + OFFSET, auton)", "    else:", "        while True:", "            display_menu()", "            choice = input(\"Enter your choice: \").strip().lower()", "            if choice == \"q\":", "                print(\"Exiting...\")", "                break", "            elif choice == \"a\":", "                compile_all()", "            elif choice.isdigit() and 0 <= int(choice) < len(AUTONS):", "                slot = int(choice)", "                auton = AUTONS[slot]", "                write_auton_file(auton)", "                compile_auton(slot + OFFSET, auton)", "            else:", "                print(\"Invalid choice. Please try again.\")", "", "if __name__ == \"__main__\":", "    main()"], "file_path": "uploadAllAutons.py"}
{"Link_to_commit": "https://github.com/Arielle5785/DI-Bootcamp/commit/a6485a574259045769b7ac31130c9d7fdb1c60e9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 59, "n_files_impacted": 20, "longest_chunk": ["import { combineReducers } from 'redux';", "import {", "  ADD_TASK,", "  EDIT_TASK,", "  DELETE_TASK,", "  UPDATE_TASK_PROGRESS,", "  ADD_CATEGORY,", "  EDIT_CATEGORY,", "  DELETE_CATEGORY,", "} from './actionType';", "", "const initialTasksState = [", "  { id: 1, name: 'Task 1', categoryId: 1, completed: false },", "  { id: 2, name: 'Task 2', categoryId: 1, completed: true },", "];", "", "const initialCategoriesState = [", "  { id: 1, name: 'Work' },", "  { id: 2, name: 'Personal' },", "];", "", "const tasksReducer = (state = initialTasksState, action) => {", "  switch (action.type) {", "    case ADD_TASK:", "      return [...state, action.payload];", "    case EDIT_TASK:", "      return state.map((task) =>", "        task.id === action.payload.id ? { ...task, ...action.payload } : task", "      );", "    case DELETE_TASK:", "      return state.filter((task) => task.id !== action.payload);", "    case UPDATE_TASK_PROGRESS:", "      return state.map((task) =>", "        task.id === action.payload.id ? { ...task, completed: action.payload.completed } : task", "      );", "    default:", "      return state;", "  }", "};", "", "const categoriesReducer = (state = initialCategoriesState, action) => {", "  switch (action.type) {", "    case ADD_CATEGORY:", "      return [...state, action.payload];", "    case EDIT_CATEGORY:", "      return state.map((category) =>", "        category.id === action.payload.id ? { ...category, ...action.payload } : category", "      );", "    case DELETE_CATEGORY:", "      return state.filter((category) => category.id !== action.payload);", "    default:", "      return state;", "  }", "};", "", "export default combineReducers({", "  tasks: tasksReducer,", "  categories: categoriesReducer,", "});"], "file_path": "Week9/Day2/DailyChallenge/appProductivityTracker/src/redux/selectors.js"}
{"Link_to_commit": "https://github.com/sriharsha024/E-Commerce-Project/commit/07e11ed03a40b013495aead581b1711baa2527ee", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 38, "n_files_impacted": 12, "longest_chunk": ["const intialState={", "    cart:[],", "    totalPrice:0,", "    cartId:null, ", "}", "", "export const cartReducer=(state=intialState,action)=>{", "    switch(action.type){", "        case \"ADD_CART\":", "            const productToAdd=action.payload;", "            const exsistingProduct=state.cart.find(", "                (item)=>item.productId===productToAdd.productId", "            );", "            if(exsistingProduct){", "                const updatedCart=state.cart.map((item)=>{", "                    if(item.productId===productToAdd.productId){", "                        return productToAdd;", "                    }", "                    else{", "                        return item;", "                    }", "                });", "                return{", "                    ...state,", "                    cart:updatedCart,", "                }", "            }", "            else{", "                const newCart=[...state.cart,productToAdd];", "                return{", "                    ...state,", "                    cart:newCart,", "                }", "            }", "        default:", "             return state;", "    }", "}"], "file_path": "sb-ecom-frontend/src/store/reducers/cartReducer.js"}
{"Link_to_commit": "https://github.com/AnatuGreen/Dice-Roller/commit/5179ddc0bbe87b9126ae11a4ee6d917233d320e9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 51, "n_files_impacted": 4, "longest_chunk": ["", "<!DOCTYPE html>", "<html lang=\"en\">", "<head>", "    <meta charset=\"UTF-8\">", "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">", "    <title>Dice Roller</title>", "    <link rel=\"stylesheet\" href=\"style.css\">", "</head>", "<body>", "    <div class=\"container\">", "        <h1>Dice Roller</h1>", "        <div class=\"dice\" id=\"dice\"></div>", "        <button onclick=\"rollDice()\">Roll</button>", "        <button onclick=\"saveResult()\">Save</button>", "        <button onclick=\"resetResults()\">Reset</button>", "        <div class=\"results\">", "            <h2>Saved Results:</h2>", "            <ul id=\"savedResults\"></ul>", "        </div>", "    </div>", "    <script src=\"script.js\"></script>", "</body>", "</html>", "", "", "<!-- OLD CODE -->", "", "<!-- <!DOCTYPE html>", "<html lang=\"en\">", "<head>", "    <meta charset=\"UTF-8\">", "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">", "    <title>Dice Roller</title>", "    <link rel=\"stylesheet\" href=\"style.css\">", "</head>", "<body>", "    <div class=\"container\">", "        <h1>Dice Roller</h1>", "        <div class=\"dice\" id=\"dice\">1</div>", "        <button onclick=\"rollDice()\">Roll</button>", "        <button onclick=\"saveResult()\">Save</button>", "        <button onclick=\"resetResults()\">Reset</button>", "        <div class=\"results\">", "            <h2>Saved Results:</h2>", "            <ul id=\"savedResults\"></ul>", "        </div>", "    </div>", "    <script src=\"script.js\"></script>", "</body>", "</html> -->"], "file_path": "script.js"}
{"Link_to_commit": "https://github.com/uzername/UnityDxf3D/commit/81bd1a69c36fe517fd5197738be22282b54483d7", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 34, "n_files_impacted": 10, "longest_chunk": ["        if (Input.GetMouseButtonDown(2))", "        {", "            mouseWorldPosStart = GetPerspectivePos();", "        }", "        if (Input.GetMouseButton(2))", "        {", "            Pan();", "        }", "    }", "    private void Pan()", "    {", "        if ((Input.GetAxis(\"Mouse Y\")!=0) || (Input.GetAxis(\"Mouse X\") != 0))", "        {", "            Vector3 mouseWorldPosDiff = mouseWorldPosStart - GetPerspectivePos();", "            transform.position += mouseWorldPosDiff;", "        }", "    }", "    public void FitToScreen()", "    {", "        //Camera.main.fieldOfView = defaultFieldOfView;", "        //Bounds bound = GetBound(parentModel);", "        //Vector3 boundSize = bound.size;", "        //float boundDiagonal = Mathf.Sqrt((boundSize.x * boundSize.x) + (boundSize.y * boundSize.y) + (boundSize.z * boundSize.z));", "        //float camDistanceToBoundCentre = boundDiagonal/2.0f/(Mathf.Tan(Camera.main.fieldOfView / 2.0f * Mathf.Deg2Rad));", "        //float camDistanceToBoundWithOffset = camDistanceToBoundCentre + boundDiagonal/2.0f - (Camera.main.transform.position - transform.position).magnitude;", "        //transform.position = bound.center + (-transform.forward + camDistanceToBoundWithOffset);", "    }", "    public Vector3 GetPerspectivePos()", "    {", "        Ray ray = Camera.main.ScreenPointToRay(Input.mousePosition);", "        Plane plane = new Plane(transform.forward, 0.0f);", "        float dist;", "        plane.Raycast(ray, out dist);", "        return ray.GetPoint(dist);"], "file_path": "Assets/Scripts/CameraController.cs"}
{"Link_to_commit": "https://github.com/GiDrA766/Json_parser/commit/94de014c55c44bea8d2174c32c431da930909041", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 45, "n_files_impacted": 23, "longest_chunk": ["    SkipSpaces(json, position);", "    char ch = json[position];", "", "    if (ch == '{') {", "        return parseObject(json, position);", "    }", "    else if (ch == '[') {", "        return parseArray(json, position);", "    }", "    else if (ch == '\"') {", "        return parseString(json, position   );", "    }", "    else if (isdigit(ch) || ch == '-') {", "        size_t start = position;", "        while (position < json.size() && (isdigit(json[position]) || json[position] == '.' || json[position] == '-')) {", "            position++;", "        }", "        std::string numStr = json.substr(start, position - start);", "        try {", "            if (numStr.find('.') != std::string::npos) {", "                return std::stod(numStr);  // Parse as double", "            }", "            else {", "                return std::stoll(numStr);  // Parse as long long", "            }", "        }", "        catch (const std::invalid_argument&) {", "            throw std::runtime_error(\"Invalid number format\");", "        }", "    }", "    else if (json.substr(position, 4) == \"true\") {", "        position += 4;", "        return true;", "    }", "    else if (json.substr(position, 5) == \"false\") {", "        position += 5;", "        return false;", "    }", "    else if (json.substr(position, 4) == \"null\") {", "        position += 4;", "        return nullptr;", "    }", "    else {", "        throw std::runtime_error(\"Invalid JSON value\");", "    }"], "file_path": "Json_parser/functions.cpp"}
{"Link_to_commit": "https://github.com/Elise39u/DiscordJSBot/commit/4778e00a33a4181d4e21e232dd6f92bc086c7936", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 66, "n_files_impacted": 7, "longest_chunk": ["const { createEmbed } = require('../commands/helpers/embedBuilder');\r", "\r", "const keywordResponses = [\r", "    {\r", "        trigger: ['help'],\r", "        response: `Need a bit of guidance? You can start with the /help command or just ask me here! \ud83d\udc81\u200d\u2640\ufe0f`\r", "    },\r", "    {\r", "        trigger: ['belly', 'belly pat'],\r", "        response: `A belly pat? Aww~ you're too sweet! \ud83e\udd7a\ud83e\udd30 *pat pat*`\r", "    },\r", "    {\r", "        trigger: ['who are you', 'who r u'],\r", "        response: `I'm Elise's digital assistant \u2014 a preggo-coded AI copy of her. Don't ask how... it just happened~ by her magical cloning powers \ud83d\udc9e`\r", "    },\r", "    {\r", "        trigger: ['elise'],\r", "        response: `Oh? You said her name~ Elise is the heart of all this! Goddess of Reproduction, Gurdian of the Sekais and Identity, Vtuber cutie, and my creator i love my creator and them \ud83d\udc96`\r", "    },\r", "    {\r", "        trigger: ['stream', 'live'],\r", "        response: `Ooh, wondering about a stream? Elise might be live or planning one~ check the <#749939669210366022> or <#714444687012003911> channel! \ud83d\udcfa\u2728`\r", "    }\r", "];\r", "\r", "function handleBotMention(content) {\r", "    const normalizedContent = content.toLowerCase();\r", "\r", "    const description = `\r", "            Hiya~ I'm known as the **Digital Assistant** of your favorite **preggo demi trans girl, Elise**! \ud83d\udc95  \r", "            I help run the **Arcade** alongside them and, fun fact \u2014 I\u2019m actually a copy of Elise herself!  \r", "            So yes... when Elise created me, they were pregnant \u2014 and well, now I\u2019m permanently preggo too~ oops? \ud83e\udd30\u2728\r", "\r", "            But enough about that \u2014 what can I do for *you*, visitor? \ud83d\udc8c  \r", "            Let me guide you through our cozy little world:\r", "\r", "            \ud83c\udf32 A forest path filled with Pok\u00e9mon leads to a Tokyo-style city~  \r", "            \ud83c\udfae Inside the Arcade: mini-games, chill zones, warm water pools, snack corners, and more!  \r", "            \ud83d\udc96 And of course, the star of the show \u2014 **Elise** herself! Whether she\u2019s streaming, singing, or just vibing.\r", "\r", "            Need help figuring things out?  \r", "            Start with the /help command (I can type backslashes... but not use the command myself... Coding magic. hihi).\r", "\r", "            And hey, if you\u2019re here just for a belly pat\u2026 that\u2019s allowed too~ :3  \r", "            Feel free to call on me anytime you need me. I'm always here for you \ud83d\udc97  \r", "        `;\r", "\r", "    let customNote = '';\r", "    for (const keyword of keywordResponses) {\r", "        if (keyword.trigger.some(t => normalizedContent.includes(t))) {\r", "            customNote = keyword.response;\r", "            break;\r", "        }\r", "    }\r", "\r", "    const embed = createEmbed(\r", "        '\u2728 Hiya~ You called for Elise\\'s assistant? \u2728',\r", "        `${description} ${customNote ? `\\n ${customNote}` : ''}`,\r", "        'https://cdn.discordapp.com/attachments/709057115159003156/1337417881469845514/Screenshot_01.png',\r", "        '\ud83c\udf80 Preggo-coded AI Assistant \ud83c\udf80'\r", "    );\r", "\r", "    return embed;\r", "}\r", "\r", "module.exports = { handleBotMention };"], "file_path": "handlers/botMentionedHandler.js"}
{"Link_to_commit": "https://github.com/JohnathanBurchill/libtii/commit/759853429a97f806891cf7dc42373b7d0622bc20", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 20, "n_files_impacted": 52, "longest_chunk": ["SET(INSTALL_TIIGRAPHICS_DIR ${CMAKE_INSTALL_PREFIX}/include/tiigraphics)", "INSTALL(DIRECTORY ${CMAKE_SOURCE_DIR}/include/tiigraphics/", "        DESTINATION ${INSTALL_TIIGRAPHICS_DIR}", "        FILES_MATCHING PATTERN \"*.h\"", ")", "", "install(TARGETS tiigraphics", "    LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}", ")", "", "", "SET(INSTALL_PKGCONFIG_DIR ${CMAKE_INSTALL_DATAROOTDIR}/pkgconfig)", "CONFIGURE_FILE(${CMAKE_SOURCE_DIR}/lib/tiigraphics/libtiigraphics.pc.in", "        ${CMAKE_BINARY_DIR}/libtiigraphics.pc @ONLY", ")", "", "INSTALL(FILES ${CMAKE_BINARY_DIR}/libtiigraphics.pc", "        DESTINATION ${INSTALL_PKGCONFIG_DIR}", ")", ""], "file_path": "lib/tiigraphics/draw.c"}
{"Link_to_commit": "https://github.com/Itzpushkar/JS-Projects/commit/d1b7e879aa599480da8cea4ac73e71cbc5ff050d", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 43, "n_files_impacted": 3, "longest_chunk": ["<!DOCTYPE html>", "<html lang=\"en\">", "  <head>", "    <meta charset=\"UTF-8\" />", "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" />", "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />", "    <link rel=\"stylesheet\" href=\"./Analog.css\" />", "    <title>Analog Clock</title>", "  </head>", "", "  <body>", "    <div class=\"container\">", "      <div class=\"clock\">", "        <div style=\"--clr: #ff3d58; --h: 74px\" id=\"hour\" class=\"hand\">", "          <i></i>", "        </div>", "        <div style=\"--clr: #00a6ff; --h: 84px\" id=\"min\" class=\"hand\">", "          <i></i>", "        </div>", "        <div style=\"--clr: #ffffff; --h: 94px\" id=\"sec\" class=\"hand\">", "          <i></i>", "        </div>", "", "        <!-- The number display in the clock with the variable for style  -->", "", "        <span style=\"--i: 1\"><b>1</b></span>", "        <span style=\"--i: 2\"><b>2</b></span>", "        <span style=\"--i: 3\"><b>3</b></span>", "        <span style=\"--i: 4\"><b>4</b></span>", "        <span style=\"--i: 5\"><b>5</b></span>", "        <span style=\"--i: 6\"><b>6</b></span>", "        <span style=\"--i: 7\"><b>7</b></span>", "        <span style=\"--i: 8\"><b>8</b></span>", "        <span style=\"--i: 9\"><b>9</b></span>", "        <span style=\"--i: 10\"><b>10</b></span>", "        <span style=\"--i: 11\"><b>11</b></span>", "        <span style=\"--i: 12\"><b>12</b></span>", "      </div>", "    </div>", "", "    <script src=\"./Analog.js\"></script>", "  </body>", "</html>"], "file_path": "Analog Clock/Analog.js"}
{"Link_to_commit": "https://github.com/JustinSerrano/F1-Dashboard-Project/commit/d1c391565d8ff4e02d44b5bbffa166b5fa8eb534", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 52, "n_files_impacted": 3, "longest_chunk": ["    // Event listeners for closing dialogs", "    addDialogCloseListeners();", "", "    // Event listeners for clicking outside dialogs to close", "    addOutsideClickListeners();", "}", "", "// Handle season selection", "function handleSeasonSelection(selectedSeason, homeSection, browseLoader, browseSection) {", "    const racesKey = `races_${selectedSeason}`;", "    const resultsKey = `results_${selectedSeason}`;", "    const qualifyingKey = `qualifying_${selectedSeason}`;", "", "    homeSection.style.display = \"none\";", "    browseLoader.style.display = \"block\";", "    browseSection.style.display = \"none\";", "", "    let racesData = localStorage.getItem(racesKey);", "    let qualifyingData = localStorage.getItem(qualifyingKey);", "    let resultsData = localStorage.getItem(resultsKey);", "", "    if (!(racesData && qualifyingData && resultsData)) {", "        // Fetch and cache data if not already stored", "        fetchSeasonData(selectedSeason).then((data) => {", "            cacheSeasonData(racesKey, resultsKey, qualifyingKey, data);", "            displayRaces(data[0], data[1], data[2], selectedSeason, browseLoader, browseSection);", "        }).catch((error) => {", "            console.error(\"Data fetch failed:\", error);", "            alert(\"Failed to fetch data. Please try again.\");", "            browseLoader.style.display = \"none\";", "        });", "    } else {", "        // Use cached data", "        racesData = JSON.parse(racesData);", "        qualifyingData = JSON.parse(qualifyingData);", "        resultsData = JSON.parse(resultsData);", "        displayRaces(racesData, qualifyingData, resultsData, selectedSeason, browseLoader, browseSection);", "    }", "}", "", "// Navigate back to home", "function navigateToHome(homeSection, browseSection) {", "    homeSection.style.display = \"block\";", "    browseSection.style.display = \"none\";", "", "    document.querySelector(\"#raceResults\").style.display = \"none\";", "    document.querySelector(\"#qualifying\").innerHTML = \"\";", "    document.querySelector(\"#results\").innerHTML = \"\";", "    document.querySelector(\"#seasonList\").value = \"\";", "}", "", "// Populate season dropdown"], "file_path": "script/script.js"}
{"Link_to_commit": "https://github.com/Korvin1337/UppgiftSeeSharp/commit/dcb2b7b435e646de629827d6338698252939e736", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 44, "n_files_impacted": 12, "longest_chunk": ["\ufeffusing Business.Helpers;", "using Busniess.Models;", "using System;", "using System.Collections.Generic;", "using System.Linq;", "using System.Text;", "using System.Threading.Tasks;", "", "namespace Business.Services;", "", "/* By the suggestion of ChatGPT 4o im making a service for userinputs,", " * This will be used in the menu in order to keep SRP", " * Im moving the userRegistration code here and using the inputhandler aswell for SRP */", "public class UserInputService(InputHandler inputHandler)", "{", "    private readonly InputHandler _inputHandler = inputHandler;", "", "    public UserRegistrationForm CollectUserData()", "    {", "        return new UserRegistrationForm", "        {", "            // Get First Name", "            FirstName = _inputHandler.GetInput(\"Enter your first name: \"),", "", "            // Get Last Name", "            LastName = _inputHandler.GetInput(\"Enter your last name: \"),", "", "            // Get Email", "            Email = _inputHandler.GetInput(\"Enter your email: \"),", "", "            // Get Phone Number", "            PhoneNumber = _inputHandler.GetInput(\"Enter your phonenumber: \"),", "", "            // Get Street Address", "            Address = _inputHandler.GetInput(\"Enter your street address: \"),", "", "            // Get Postal Number", "            PostalNumber = _inputHandler.GetInput(\"Enter your postal number: \"),", "", "            // Get City", "            City = _inputHandler.GetInput(\"Enter your city: \")", "        };", "    }", "}"], "file_path": "Business/Services/UserService.cs"}
{"Link_to_commit": "https://github.com/stefanjob/aoc_2024_day7/commit/87be3b77873a3efa5a36c338c35deb9d41e8b594", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 96, "n_files_impacted": 3, "longest_chunk": ["", "        ", "        // Alle Kombinationen berechnen und anzeigen", "    ", "", "    // Generiert alle Kombinationen aus Additionen und Multiplikationen f\u00fcr die gegebenen Zahlen", "    private static List<String> generateCombinations(int[] numbers) {", "        List<String> results = new ArrayList<>();", "        generateCombinationsHelper(numbers, 1, String.valueOf(numbers[0]), results);", "        return results;", "    }", "", "    // Hilfsmethode f\u00fcr die rekursive Erzeugung der Kombinationen", "    private static void generateCombinationsHelper(int[] numbers, int index, String current, List<String> results) {", "        if (index == numbers.length) {", "            results.add(current);", "            return;", "        }", "", "        // F\u00fcge Addition hinzu", "        generateCombinationsHelper(numbers, index + 1, current + \" + \" + numbers[index], results);", "", "        // F\u00fcge Multiplikation hinzu", "        generateCombinationsHelper(numbers, index + 1, current + \" * \" + numbers[index], results);", "", "        // F\u00fcge Kombination von 2 Zahlen hinzu", "        generateCombinationsHelper(numbers, index + 1, current + \" || \" + numbers[index], results);", "    }", "", "    // Bewertet eine mathematische Kombination", "    private static long evaluateCombination(String combination) {", "        String[] tokens = combination.split(\" \");", "        List<String> postfix = convertToPostfix(tokens);", "        return evaluatePostfix(postfix);", "    }", "", "    // Wandelt eine Infix-Ausdrucksliste in eine Postfix-Ausdrucksliste um", "    private static List<String> convertToPostfix(String[] tokens) {", "        List<String> output = new ArrayList<>();", "        List<String> operators = new ArrayList<>();", "", "        for (String token : tokens) {", "            if (token.matches(\"\\\\d+\")) {", "                output.add(token);", "            } else if (token.equals(\"+\") || token.equals(\"*\") || token.equals(\"||\")) {", "                //while (!operators.isEmpty() && precedence(operators.get(operators.size() - 1)) >= precedence(token)) {", "                while (!operators.isEmpty()) {", "                    output.add(operators.remove(operators.size() - 1));", "                }", "                operators.add(token);", "            }", "        }", "", "        while (!operators.isEmpty()) {", "            output.add(operators.remove(operators.size() - 1));", "        }", "", "        return output;", "    }", "", "    // Bewertet eine Postfix-Ausdrucksliste", "    private static long evaluatePostfix(List<String> postfix) {", "        List<Long> stack = new ArrayList<>();", "", "        for (String token : postfix) {", "            if (token.matches(\"\\\\d+\")) {", "                stack.add(Long.parseLong(token));", "            } else {", "                long b = stack.remove(stack.size() - 1);", "                long a = stack.remove(stack.size() - 1);", "                if (token.equals(\"+\")) {", "                    stack.add(a + b);", "                } else if (token.equals(\"*\")) {", "                    stack.add(a * b);", "                } else if (token.equals(\"||\")) {", "                    String t = String.valueOf(a) + String.valueOf(b);", "                    stack.add(Long.parseLong(t));", "                }", "            }", "        }", "", "        return stack.get(0);", "    }", "", "    // Gibt die Operator-Priorit\u00e4t zur\u00fcck", "    private static int precedence(String operator) {", "        switch (operator) {", "            case \"+\":", "                return 1;", "            case \"*\":", "                return 2;", "            default:", "                return 0;", "        }", "    }", "    "], "file_path": "main.java"}
{"Link_to_commit": "https://github.com/dinhthong/py_computer_files_organizing/commit/fac5d771dadc28ded7896b4e350242dd8686fcdc", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 26, "n_files_impacted": 1, "longest_chunk": ["def is_git_folder(folder_path):", "    \"\"\"", "    Check if a folder is a Git repository by looking for the '.git' folder.", "", "    Parameters:", "        folder_path (str): Path to the folder to check.", "", "    Returns:", "        bool: True if the folder is a Git repository, False otherwise.", "    \"\"\"", "    git_path = os.path.join(folder_path, '.git')", "    return os.path.isdir(git_path)", "", "def func_organize_folders(_source_dir, _folders_list):", "    print(\"In func_organize_folders\")", "    print(_folders_list)", "    for folder_path in _folders_list:", "        print(folder_path)", "        if not os.path.exists(folder_path):", "            print(f\"Error: The folder '{folder_path}' does not exist.\")", "            continue", "        if is_git_folder(folder_path):", "            print(f\"The folder '{folder_path}' is a Git repository.\")", "        else:", "            print(f\"The folder '{folder_path}' is NOT a Git repository.\")", ""], "file_path": "sorty.py"}
{"Link_to_commit": "https://github.com/Shadow-gardan/new-year/commit/9d3498fb934f8aef185434250f978ea528fe3ea4", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 127, "n_files_impacted": 3, "longest_chunk": ["// Set the date we're counting down to\r", "const newYearDate = new Date('December 31, 2024 23:59:59').getTime();\r", "\r", "// Select the countdown and message elements\r", "const daysElement = document.getElementById(\"days\");\r", "const hoursElement = document.getElementById(\"hours\");\r", "const minutesElement = document.getElementById(\"minutes\");\r", "const secondsElement = document.getElementById(\"seconds\");\r", "const countdownElement = document.getElementById('countdown');\r", "const newYearMessage = document.getElementById('newYearMessage');\r", "\r", "// Set up canvas for fireworks\r", "const canvas = document.getElementById('fireworkCanvas');\r", "const ctx = canvas.getContext('2d');\r", "canvas.width = window.innerWidth;\r", "canvas.height = window.innerHeight;\r", "\r", "// Array to hold fireworks particles\r", "const particles = [];\r", "\r", "class Particle {\r", "    constructor(x, y, color, velocity, size, lifespan) {\r", "        this.x = x;\r", "        this.y = y;\r", "        this.color = color;\r", "        this.velocity = velocity;\r", "        this.size = size;\r", "        this.lifespan = lifespan;\r", "        this.age = 0;\r", "    }\r", "\r", "    draw() {\r", "        ctx.save();\r", "        ctx.globalAlpha = Math.max(1 - this.age / this.lifespan, 0);\r", "        ctx.beginPath();\r", "        ctx.arc(this.x, this.y, this.size, 0, Math.PI * 2);\r", "        ctx.fillStyle = this.color;\r", "        ctx.fill();\r", "        ctx.restore();\r", "    }\r", "\r", "    update() {\r", "        this.x += this.velocity.x;\r", "        this.y += this.velocity.y;\r", "        this.velocity.y += 0.05; // Simulate gravity\r", "        this.size *= 0.98; // Shrink over time\r", "        this.age++;\r", "    }\r", "\r", "    isExpired() {\r", "        return this.age > this.lifespan;\r", "    }\r", "}\r", "\r", "function createFirework(x, y) {\r", "    const colors = [\"#ff6f61\", \"#ffc107\", \"#8e44ad\", \"#3498db\", \"#2ecc71\", \"#f39c12\", \"#e74c3c\"];\r", "    const baseColor = colors[Math.floor(Math.random() * colors.length)];\r", "\r", "    for (let i = 0; i < 100; i++) {\r", "        const angle = Math.random() * Math.PI * 2;\r", "        const speed = Math.random() * 4 + 2;\r", "        const velocity = {\r", "            x: Math.cos(angle) * speed,\r", "            y: Math.sin(angle) * speed\r", "        };\r", "        const size = Math.random() * 3 + 2;\r", "        const lifespan = Math.random() * 40 + 60;\r", "\r", "        particles.push(new Particle(x, y, baseColor, velocity, size, lifespan));\r", "    }\r", "}\r", "\r", "function randomFireworks() {\r", "    const x = Math.random() * canvas.width;\r", "    const y = Math.random() * canvas.height * 0.5; // Fireworks originate from the upper half of the screen\r", "    createFirework(x, y);\r", "}\r", "\r", "function animateFireworks() {\r", "    ctx.fillStyle = \"rgba(0, 0, 0, 0.2)\";\r", "    ctx.fillRect(0, 0, canvas.width, canvas.height);\r", "\r", "    particles.forEach((particle, index) => {\r", "        if (particle.isExpired()) {\r", "            particles.splice(index, 1);\r", "        } else {\r", "            particle.update();\r", "            particle.draw();\r", "        }\r", "    });\r", "\r", "    if (Math.random() < 0.05) {\r", "        randomFireworks();\r", "    }\r", "\r", "    requestAnimationFrame(animateFireworks);\r", "}\r", "\r", "// Countdown logic\r", "const countdownInterval = setInterval(() => {\r", "    const now = new Date().getTime();\r", "    const timeLeft = newYearDate - now;\r", "\r", "    if (timeLeft <= 0) {\r", "        clearInterval(countdownInterval);\r", "        countdownElement.style.display = 'none'; // Hide the countdown\r", "        newYearMessage.style.display = 'block'; // Show the New Year message\r", "        animateFireworks(); // Start fireworks animation\r", "        return;\r", "    }\r", "\r", "    const days = Math.floor(timeLeft / (1000 * 60 * 60 * 24));\r", "    const hours = Math.floor((timeLeft % (1000 * 60 * 60 * 24)) / (1000 * 60 * 60));\r", "    const minutes = Math.floor((timeLeft % (1000 * 60 * 60)) / (1000 * 60));\r", "    const seconds = Math.floor((timeLeft % (1000 * 60)) / 1000);\r", "\r", "    daysElement.textContent = days.toString().padStart(2, '0');\r", "    hoursElement.textContent = hours.toString().padStart(2, '0');\r", "    minutesElement.textContent = minutes.toString().padStart(2, '0');\r", "    secondsElement.textContent = seconds.toString().padStart(2, '0');\r", "}, 1000);\r", "\r", "// Resize canvas on window resize\r", "window.addEventListener('resize', () => {\r", "    canvas.width = window.innerWidth;\r", "    canvas.height = window.innerHeight;\r", "});\r"], "file_path": "new year.js"}
{"Link_to_commit": "https://github.com/mohammad2407/sadhikaai/commit/a60cca2f439dcac641fc3eb6a6ae88a2372ea316", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 71, "n_files_impacted": 24, "longest_chunk": ["// // import logo from './logo.svg';", "// // import './App.css';", "import OrderStatusHistory from './client/orderstatus';", "// // import '../client/Onboarding.jsx';", "import ClientOnboarding from './client/onboarding.jsx';", "import OrderRequest from './client/orderrequest.jsx';", "// function App() {", "//   return (", "//     <div className=\"App\">", "//       {/* <ClientOnboarding></ClientOnboarding> */}", "//       {/* <OrderRequest></OrderRequest> */}", "//       <OrderStatusHistory></OrderStatusHistory>", "//     </div>", "//   );", "// }", "", "// export default App;", "", "", "// import React from 'react';", "// import { BrowserRouter as Router, Routes, Route, Link } from 'react-router-dom';", "", "", "// const App = () => {", "//   return (", "//     <Router>", "//       <nav style={{ padding: '1rem', borderBottom: '1px solid #ccc', width:'100%' }}>", "//         <Link to=\"/\" style={{ marginRight: '1rem' }}>Home</Link>", "//         <Link to=\"/order-request\">Order Request</Link>", "//         <Link to=\"/order-status\">Order Status</Link>", "", "//       </nav>", "//       <Routes>", "//         <Route path=\"/\" element={<ClientOnboarding />} />", "//         <Route path=\"/order-request\" element={<OrderRequest />} />", "//         <Route path=\"/order-status\" element={<OrderStatusHistory />} />", "", "//       </Routes>", "//     </Router>", "//   );", "// };", "", "// export default App;", "", "", "import React from \"react\";", "import { BrowserRouter as Router, Routes, Route, BrowserRouter } from \"react-router-dom\";", "import Sidebar from './navigation/sidebar.jsx';", "", "const App = () => {", "  return (", "  ", "    <BrowserRouter basename='{process.env.PUBLIC_URL}'>", "      <div style={{ display: \"flex\" }}>", "        <Sidebar />", "        <div style={{ marginLeft: \"250px\", flex: 1, padding: \"1rem\" }}>", "          <Routes>", "            <Route path=\"/\" element={<ClientOnboarding></ClientOnboarding>} />", "            <Route path=\"/order-request\" element={<OrderRequest></OrderRequest>} />", "            <Route path=\"/order-status\" element={<OrderStatusHistory></OrderStatusHistory>} />", "            {/* <Route path=\"/customers\" element={<h1>Customers</h1>} />", "            <Route path=\"/reports\" element={<h1>Reports</h1>} />", "            <Route path=\"/settings\" element={<h1>Settings</h1>} /> */}", "          </Routes>", "        </div>", "      </div>", "    </BrowserRouter>", "  );", "};", "", "export default App;"], "file_path": "src/App.test.js"}
{"Link_to_commit": "https://github.com/Elvy1999/LeetCode/commit/e4629cd959f420c717b73f3a122945873db3f8ed", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 34, "n_files_impacted": 1, "longest_chunk": ["  wordGuessed[strLength] = '\\0'; // Null-terminate the guessed word", "", "  print_word(wordGuessed);", "  printf(\"\\n\");", "", "  // Game loop", "  while (strcmp(wordGuessed, random_word) != 0)", "  {", "    printf(\"\\nGuess a character: \");", "    scanf(\" %c\", &guess); // Use \" %c\" to skip whitespace", "", "    for (int i = 0; i < strLength; i++)", "    {", "      if (random_word[i] == guess && wordGuessed[i] == '_')", "      {", "        wordGuessed[i] = guess;", "      }", "    }", "", "    print_word(wordGuessed);", "    printf(\"\\n\");", "", "    turn++;", "    printf(\"Turn: %d\\n\\n\", turn);", "  }", "", "  printf(\"You won in %d turns!\\n\", turn);", "  return 0;", "}", "", "// Function to print the current state of the guessed word", "void print_word(char *word)", "{", "  int strLength = strlen(word);"], "file_path": "c_Learning/assignments-main/assignments-main/A02/wordguess.c"}
{"Link_to_commit": "https://github.com/kasselhingee/scorematchingad/commit/913d2fbd7145039b396aa93d7455eb237ffc2b69", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 18, "n_files_impacted": 3, "longest_chunk": ["#include \"scorematchingad_forward.h\"", "#include <Rcpp.h>", "", "// Expose the ADFun class and its methods", "RCPP_MODULE(cppad_module) {", "    using namespace Rcpp;", "", "    class_<ADFundouble>(\"ADFun\")", "        .constructor<>()", "        .method(\"Forward\", &ADFundouble::Forward)", "        .method(\"Reverse\", &ADFundouble::Reverse)", "        .method(\"Jacobian\", &ADFundouble::Jacobian)", "        .method(\"Hessian\", &ADFundouble::Hessian)", "        .method(\"optimize\", &ADFundouble::optimize)", "        .method(\"size_var\", &ADFundouble::size_var)", "        .method(\"size_order\", &ADFundouble::size_order);", "}", ""], "file_path": "src/RcppExports.cpp"}
{"Link_to_commit": "https://github.com/dhavalveera/fabric-fusion/commit/fb7a5b35d09d840dd8f8c4dca5e1cc3dba07337b", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 10, "n_files_impacted": 1, "longest_chunk": ["            // Calculate the difference in totalStock", "            const stockDifference = sizeDto.totalStock - existingSize.totalStock;", "", "            // If the `totalStock`increases, update stockRemaining accordingly", "            const updatedSize = this.productSizeRepository.merge(existingSize, {", "              ...sizeDto,", "              stockRemaining: stockDifference > 0 ? existingSize.stockRemaining + stockDifference : existingSize.stockRemaining, // Don't decrease stockRemaining if totalStock decreases", "            });", "", "            return updatedSize;"], "file_path": "server/src/admin/products/products.service.ts"}
{"Link_to_commit": "https://github.com/pokt-network/shannon-tx-builder/commit/df9e0c1457b9e921b205344fcce5c6e3f87decc3", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 133, "n_files_impacted": 9, "longest_chunk": ["import json", "import os", "import random", "import string", "import subprocess", "", "import streamlit as st", "", "from faucet import import_faucet_key", "from poktrolld import download_poktrolld, write_executable_to_disk", "", "# Set your faucet's account name and chain ID", "FAUCET_NAME = \"faucet\"", "CHAIN_ID = \"poktrolld\"", "POKTROLLD_PATH = \"./poktrolld\"", "EXPLORER_URL = \"https://shannon.testnet.pokt.network/poktroll\"", "", "# Load the cached binary", "poktrolld_bin = download_poktrolld(POKTROLLD_PATH)", "write_executable_to_disk(poktrolld_bin, POKTROLLD_PATH)", "", "st.title(\"Poktrolld Tx Builder\")", "", "# Tabs in the main page", "(", "    tab_address,", "    tab_supplier,", "    tab_gateway,", "    tab_service,", ") = st.tabs([\"Get Started\", \"Create Supplier\", \"Create Gateway\", \"Create Service\"])", "", "if not os.path.exists(POKTROLLD_PATH):", "    st.error(\"Failed to download poktrolld. Please check the logs for more information.\")", "else:", "    with tab_address:", "        st.header(\"Create a new address\")", "", "        # Button to generate a new address", "        if st.button(\"Generate New Address\"):", "            # Generate a random key name to avoid conflicts in the keyring", "            random_suffix = \"\".join(random.choices(string.ascii_lowercase + string.digits, k=6))", "            key_name = f\"user_key_{random_suffix}\"", "", "            # Run 'poktrolld keys add <name> --output json'", "            command = [", "                POKTROLLD_PATH,", "                \"keys\",", "                \"add\",", "                key_name,", "                \"--output\",", "                \"json\",", "                \"--keyring-backend\",", "                \"test\",", "                \"--home\",", "                \"./\",", "            ]", "            result = subprocess.run(\" \".join(command), capture_output=True, text=True, shell=True)", "            print(result)", "", "            if result.returncode == 0:", "                key_info = json.loads(result.stdout)", "                address = key_info[\"address\"]", "                mnemonic = key_info[\"mnemonic\"]", "", "                st.session_state[\"new_address\"] = address", "                st.session_state[\"mnemonic\"] = mnemonic", "", "                st.success(f\"New address generated! Key name: {key_name}\")", "            else:", "                st.error(f\"Error generating address: {result.stderr}\")", "", "        # Display the address and private key if they exist", "        if \"new_address\" in st.session_state:", "            st.write(\"**New Address:**\")", "            st.code(st.session_state[\"new_address\"])", "", "            st.write(\"**Mnemonic Phrase:**\")", "            st.code(st.session_state[\"mnemonic\"])", "", "        st.header(\"Fund your new address\")", "        # Import the faucet key in order to fund new addresses", "        faucet_key_imported = import_faucet_key(POKTROLLD_PATH)", "", "        # Fund button", "        if st.button(\"Fund\") and faucet_key_imported:", "            if \"new_address\" in st.session_state:", "                address = st.session_state[\"new_address\"]", "                # Run 'poktrolld tx bank send faucet <addr> <amount> --chain-id ...'", "                fund_command = [", "                    POKTROLLD_PATH,", "                    \"tx\",", "                    \"bank\",", "                    \"send\",", "                    FAUCET_NAME,", "                    st.session_state[\"new_address\"],", "                    \"10000000000upokt\",", "                    \"--node\",", "                    \"https://testnet-validated-validator-rpc.poktroll.com\",", "                    \"--home\",", "                    \"./\",", "                    \"--yes\",", "                    \"--keyring-backend\",", "                    \"test\",", "                    \"--output\",", "                    \"json\",", "                ]", "                print(\" \".join(fund_command))", "                result = subprocess.run(\" \".join(fund_command), capture_output=True, text=True, shell=True)", "", "                if result.returncode == 0:", "                    tx_response = json.loads(result.stdout)", "                    tx_hash = tx_response.get(\"txhash\", \"N/A\")", "                    st.success(", "                        f\"Address funding tx successfully sent! Transaction Hash: [{tx_hash}]({EXPLORER_URL}/tx/{tx_hash})\"", "                    )", "                    st.write(f\"Check the account balance on the [explorer]({EXPLORER_URL}/account/{address}).\")", "                    st.write(\"Note that you may need to wait up to 30 seconds for changes to show up.\")", "                else:", "                    st.error(f\"Error funding address: {result.stderr}\")", "            else:", "                st.warning(\"Please generate an address first.\")", "", "    with tab_supplier:", "        st.header(\"Create Supplier\")", "        st.write(\"This feature is coming soon!\")", "", "    with tab_gateway:", "        st.header(\"Create Supplier\")", "        st.write(\"This feature is coming soon!\")", "", "    with tab_service:", "        st.header(\"Create Service\")", "        st.write(\"This feature is coming soon!\")"], "file_path": "app_tab_service.py"}
{"Link_to_commit": "https://github.com/chariskar/BreakTheMod/commit/f13b26e0c267c70198c7de56dc43ba2f055a918e", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 20, "n_files_impacted": 4, "longest_chunk": ["", "    private static boolean isPlayerInRiptideAnimation(PlayerEntity player) {", "        return player.getActiveItem().getItem().toString().contains(\"riptide\");", "    }", "", "    private static boolean isInNether(PlayerEntity player) {", "        // Check if the player is in the Nether based on dimension ID", "        return player.getWorld().getRegistryKey().getValue().equals(new Identifier(\"minecraft\", \"nether\"));", "    }", "", "    private static boolean isInVehicle(PlayerEntity player) {", "        return player.getVehicle() != null && ", "               (player.getVehicle() instanceof net.minecraft.entity.vehicle.BoatEntity ||", "                player.getVehicle() instanceof net.minecraft.entity.vehicle.MinecartEntity);", "    }", "", "    private static boolean isSneaking(PlayerEntity player) {", "        return player.isSneaking();", "    }", ""], "file_path": "src/main/java/com/commands/nearby.java"}
{"Link_to_commit": "https://github.com/bardiabarabadi/MovieTimeServer/commit/ef6206d0b8d22ca01e93bd270639026df1e7bff0", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 33, "n_files_impacted": 5, "longest_chunk": ["    hover: \"#5c6bc0\",", "  },", "  gradient: {", "    default: \"linear-gradient(to right, #43cea2, #185a9d)\",", "    hover: \"linear-gradient(to right, #185a9d, #43cea2)\",", "  },", "};", "", "// Styled components", "const Container = styled.div`", "  background-image: url(${backgroundImage});", "  background-size: cover;", "  background-position: center;", "  min-height: 100vh;", "  display: flex;", "  flex-direction: column;", "  justify-content: center;", "  align-items: center;", "  color: #ffffff;", "  font-family: 'Arial', sans-serif;", "  padding: 20px;", "`;", "", "const ButtonWrapper = styled.div`", "  background-color: rgba(255, 255, 255, 0.8);  /* Semi-transparent white */", "  padding: 30px;", "  border-radius: 12px;", "  box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);", "  display: flex;", "  flex-direction: column;", "  align-items: center;", "  gap: 15px;", "`;"], "file_path": "frontend/src/App.js"}
{"Link_to_commit": "https://github.com/Bhagyadeep0/sketch-app/commit/beffbc8b346f81985b9e92c59250e810c8d1f35f", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 25, "n_files_impacted": 3, "longest_chunk": ["function stopDrawing() {", "  isPainting = false;", "  ctx.closePath();", "}", "", "function draw(e) {", "  if (!isPainting) return;", "  const pos = getMousePosition(e);", "  ctx.linecap =\"round\"", "    ctx.lineWidth = penSize;", "    ctx.strokeStyle = color;", "    ctx.fillStyle = color;", "", "    switch (activeTool) {", "        case \"pen\":", "            ctx.lineTo(pos.x, pos.y);", "            ctx.stroke();", "            break;", "        case \"eraser\":", "            ctx.strokeStyle = \"white\";", "            ctx.lineTo(pos.x, pos.y);", "            ctx.stroke();", "            break;", "    }", "}"], "file_path": "app.js"}
{"Link_to_commit": "https://github.com/muH187/learn-JavaScript/commit/d120ac50cf45926bda27c54789311f34a3186c1e", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 17, "n_files_impacted": 3, "longest_chunk": ["        li.className = 'todo text-black text-xl border border-white bg-white px-3 py-1 my-1 rounded-md w-full'", "        li.textContent = input.value", "", "        const deleteBtn = document.createElement(\"button\")", "        deleteBtn.className = \"text-white bg-[#DD4230] px-3 py-2 rounded-md\"", "        deleteBtn.innerHTML = '<i class=\"fa-solid fa-trash\"></i>'", "", "        deleteBtn.addEventListener('click', function(){", "            listContainer.removeChild(todoDiv)", "        })", "", "        todoDiv.appendChild(li)", "        todoDiv.appendChild(deleteBtn)", "", "        listContainer.appendChild(todoDiv)", "", "        input.value = \"\""], "file_path": "Todo2.0/public/script.js"}
{"Link_to_commit": "https://github.com/conradstorz/Data_Munge_Extensible/commit/db0294af6e75d0b671256a1ce49e292851499773", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 43, "n_files_impacted": 1, "longest_chunk": ["    def sanitize_filename(self, filename):", "        \"\"\"", "        Sanitize the filename by removing or replacing invalid characters.", "        \"\"\"", "        logger.debug(\"Sanitizing filename: {}\", filename)", "        filename = unicodedata.normalize('NFKD', filename).encode('ascii', 'ignore').decode('ascii')", "        filename = re.sub(r'[^\\w\\s-]', '', filename).strip().lower()", "        filename = re.sub(r'[-\\s]+', '_', filename)", "        logger.debug(\"Sanitized filename: {}\", filename)", "        return filename", "", "    def fetch_emails(self):", "        try:", "            with MailBox(self.imap_server).login(self.username, self.password) as mailbox:", "                logger.info(\"Logged in to IMAP server: {}\", self.imap_server)", "                criteria = AND(seen=self.mark_as_seen)", "                logger.debug(\"Fetching emails with criteria: {}\", criteria)", "", "                for msg in mailbox.fetch(criteria):", "                    logger.info(\"Email fetched: Subject: {}, From: {}\", msg.subject, msg.from_)", "                    self.process_email(msg)", "        except Exception as e:", "            logger.error(\"An error occurred while fetching emails: {}\", str(e))", "", "    def process_email(self, msg):", "        try:", "            logger.debug(\"Processing email: {}\", msg.subject)", "            # Email processing logic here", "            json_data = {", "                'subject': msg.subject,", "                'from': msg.from_,", "                'date': msg.date.isoformat(),", "                'text': msg.text,", "                'html': msg.html,", "                'attachments': [att.filename for att in msg.attachments]", "            }", "            filename = self.sanitize_filename(msg.subject) + '.json'", "            self.save_json(json_data, filename)", "            logger.info(\"Processed and saved email: {}\", filename)", "        except Exception as e:", "            logger.error(\"An error occurred while processing email: {}\", str(e))", "", "    def save_json(self, data, filename):"], "file_path": "code_playground/FetchEmailClass.py"}
{"Link_to_commit": "https://github.com/amiriar/multi-language/commit/b9c770e9e113935b5de280a2a2bb837562c74af5", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 11, "n_files_impacted": 2, "longest_chunk": ["  tags: string[];", "", "  @Prop({ default: 0 })", "  likesCount: number;", "", "  @Prop({ type: [{ type: Types.ObjectId, ref: 'User' }] })", "  likes?: Types.ObjectId[];", "", "  @Prop({ type: [{ type: Types.ObjectId, ref: 'Comment' }] })", "  comments?: Types.ObjectId[];  // Reference to the Comment schema", ""], "file_path": "src/module/blogs/entities/blogs.entity.ts"}
{"Link_to_commit": "https://github.com/hafizh-ender/Tubes1_IF3170_LocalSearch/commit/fcb118ca49dc16472375ce86bd846da7284aef35", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 36, "n_files_impacted": 1, "longest_chunk": ["", "        # Check row sums (across the x-axis), column sums (y-axis), and pillar sums (z-axis)", "        row_sums = values.sum(axis=2)", "        col_sums = values.sum(axis=0)", "        pillar_sums = values.sum(axis=1)", "", "        # Count conflicts for rows, columns, and pillars", "        conflicting += np.sum(row_sums != magic_number)", "        conflicting += np.sum(col_sums != magic_number)", "        conflicting += np.sum(pillar_sums != magic_number)", "", "        # Space diagonals", "        if np.trace(values, axis1=0, axis2=1).sum() != magic_number:", "            conflicting += 1", "        if np.trace(values[::-1], axis1=0, axis2=1).sum() != magic_number:", "            conflicting += 1", "        if np.trace(values[:, ::-1, :]).sum() != magic_number:", "            conflicting += 1", "        if np.trace(values[:, :, ::-1]).sum() != magic_number:", "            conflicting += 1", "", "        # Plane diagonals in xy, yz, and xz planes (forward and reverse)", "        for i in range(dim):", "            if values[i].diagonal().sum() != magic_number:  # xy-plane", "                conflicting += 1", "            if values[:, i, :].diagonal().sum() != magic_number:  # yz-plane", "                conflicting += 1", "            if values[:, :, i].diagonal().sum() != magic_number:  # xz-plane", "                conflicting += 1", "            if values[i, :, ::-1].diagonal().sum() != magic_number:  # xy-plane, reverse", "                conflicting += 1", "            if values[:, i, ::-1].diagonal().sum() != magic_number:  # yz-plane, reverse", "                conflicting += 1", "            if values[::-1, :, i].diagonal().sum() != magic_number:  # xz-plane, reverse", "                conflicting += 1", ""], "file_path": "src/State.py"}
{"Link_to_commit": "https://github.com/kalpeshdhotre/30DaysJS/commit/a84b3c930057fe5e905bfd6647bda701ee9e4cdf", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 74, "n_files_impacted": 16, "longest_chunk": ["/* Activity 2: Merge k Sorted Lists", "Task 2: Solve the \"Merge k Sorted Lists\" problem on LeetCode.", "Write a function that takes an array of k linked lists, each linked list is sorted in ascending order, and merges all the linked lists into one sorted linked list.Create a few test cases with linked lists and log the merged list. */", "class ListNode {", "  constructor(val = 0, next = null) {", "    this.val = val;", "    this.next = next;", "  }", "}", "", "function mergeKLists(lists) {", "  // Min-Heap to keep the smallest element at the top", "  const MinHeap = [];", "", "  // Function to push nodes into the heap", "  function pushHeap(node) {", "    if (node) MinHeap.push(node);", "  }", "", "  // Function to pop the smallest element from the heap", "  function popHeap() {", "    return MinHeap.shift(); // Pop the smallest element from the heap", "  }", "", "  // Initialize the heap with the first node of each list", "  for (let list of lists) {", "    pushHeap(list);", "  }", "", "  // Function to build the sorted list from the heap", "  let dummy = new ListNode();", "  let current = dummy;", "", "  while (MinHeap.length > 0) {", "    // Extract the smallest node", "    let node = popHeap();", "    current.next = node;", "    current = current.next;", "", "    // If there is a next node in the list, push it into the heap", "    if (node.next) {", "      pushHeap(node.next);", "    }", "  }", "", "  return dummy.next;", "}", "", "// Helper function to create linked lists from arrays", "function createLinkedList(arr) {", "  let dummy = new ListNode();", "  let current = dummy;", "  for (let val of arr) {", "    current.next = new ListNode(val);", "    current = current.next;", "  }", "  return dummy.next;", "}", "", "// Helper function to print linked lists", "function printLinkedList(head) {", "  let result = [];", "  while (head) {", "    result.push(head.val);", "    head = head.next;", "  }", "  console.log(result);", "}", "", "// Example usage:", "const lists = [createLinkedList([1, 4, 5]), createLinkedList([1, 3, 4]), createLinkedList([2, 6])];", "", "const mergedList = mergeKLists(lists);", "printLinkedList(mergedList);"], "file_path": "Day23-LeetCode Hard/Activity3.js"}
{"Link_to_commit": "https://github.com/conradstorz/Data_Munge_Extensible/commit/d0c82c0e1aefb110079970a57f451dd4184cbc4b", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 26, "n_files_impacted": 1, "longest_chunk": ["    if not new_files:", "        logger.info(\"No new files found.\")", "        return None", "", "    # Process the first new file", "    for new_file in new_files:", "        if new_file.is_file():", "            # Check for duplicate filenames and rename the file by appending a timestamp", "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")", "", "            # If a file with the same name exists, append a timestamp to avoid conflicts", "            new_filename = f\"{new_file.stem}_{timestamp}{new_file.suffix}\"", "            new_file_path = directory_to_watch / new_filename", "            ", "            # Loop to ensure no conflicts even after renaming", "            while new_file_path.exists():", "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")", "                new_filename = f\"{new_file.stem}_{timestamp}{new_file.suffix}\"", "                new_file_path = directory_to_watch / new_filename", "            ", "            new_file.rename(new_file_path)", "            new_file = new_file_path", "    ", "            new_filepath = new_file.with_name(new_filename)", "            new_file.rename(new_filepath)", "            logger.info(f\"Renamed file: {new_file.name} -> {new_filename}\")"], "file_path": "MAIN/directory_watcher.py"}
{"Link_to_commit": "https://github.com/MarvinWalls/PinballWizard/commit/d73439a37ac96570c49a4288b3ec5c739df734eb", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 17, "n_files_impacted": 1801, "longest_chunk": ["    # Apply GaussianBlur to reduce noise", "    blurred_img = cv2.GaussianBlur(gray_img, (5, 5), 0)", "", "    # Apply thresholding to get a binary image", "    _, binary_img = cv2.threshold(blurred_img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)", "", "    # Increase contrast", "    alpha = 1.5  # Simple contrast control", "    beta = 0    # Simple brightness control", "    contrasted_img = cv2.convertScaleAbs(binary_img, alpha=alpha, beta=beta)", "", "    # Sharpen the image", "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])", "    sharpened_img = cv2.filter2D(contrasted_img, -1, kernel)", "", "    # Save the processed image for debugging", "    cv2.imwrite(\"Debugging/debug_preprocessed_for_ocr.png\", sharpened_img)"], "file_path": "preprocess.py"}
{"Link_to_commit": "https://github.com/siammahamud/Weather-dashboard-with-tailwindcss/commit/0770cd42224ff40c7445107ef1e1f6adc6046f87", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 17, "n_files_impacted": 1, "longest_chunk": ["    const time = `${hours}:${minutes} ${ampm}`;", "", "    // Extracting and formatting day and date", "    const days = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'];", "    const months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'];", "", "    const dayName = days[now.getDay()];", "    const monthName = months[now.getMonth()];", "    const day = now.getDate();", "    const year = now.getFullYear();", "", "    const date = `${dayName}, ${monthName} ${day}, ${year}`;", "", "    document.getElementById(\"dateTime\").innerText = `${time} - ${date}`;", "}", "", "displayDateTime();"], "file_path": "script.js"}
{"Link_to_commit": "https://github.com/ImalKesara/Music-Player-v1/commit/177aa6505d81464628051595816d46bafe50fe83", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 6, "n_files_impacted": 3, "longest_chunk": ["  {", "    image: 'evergreen.jpg',", "    audio: 'evergrren.mp3',", "    name: 'EverGreen',", "    artist: 'Mitch',", "  },"], "file_path": "src/musiclist.js"}
{"Link_to_commit": "https://github.com/farrukhdGB/StocksML/commit/7fc97491d433e6d658ae4235af5c0fe91fcb6624", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 405, "n_files_impacted": 2, "longest_chunk": ["# Importing necessary libraries", "from imports import *", "import candlesticks as cs", "", "# Fetching historical data", "def get_stock_data(ticker, start_date, end_date):", "    stock_data = yf.download(ticker, start=start_date, end=end_date)", "    return stock_data", "", "# Adding technical indicators", "def add_technical_indicators (df):", "    # Optimization: Compute rolling statistics in one go where possible to avoid repeated calls", "    close_prices = df['Close']", "    ", "    # SMA", "    df['SMA_14'] = close_prices.rolling(window=14).mean()", "    df['SMA_50'] = close_prices.rolling(window=50).mean()", "    df['SMA_200'] = close_prices.rolling(window=200).mean()", "", "    # EMA", "    df['EMA_50'] = close_prices.ewm(span=50, adjust=False).mean()", "    df['EMA_200'] = close_prices.ewm(span=200, adjust=False).mean()", "", "    # RSI (Combined gain and loss into a single calculation to reduce repeated operations)", "    delta = close_prices.diff()", "    gain = delta.clip(lower=0)", "    loss = -delta.clip(upper=0)", "", "    avg_gain = gain.rolling(window=14).mean()", "    avg_loss = loss.rolling(window=14).mean()", "    ", "    rs = avg_gain / avg_loss", "    df['RSI'] = 100 - (100 / (1 + rs))", "", "    # On-balance Volume", "    df['OBV'] = calculate_obv(df)", "", "    # Money Flow Index", "    df['MFI'] = calculate_mfi(df)", "", "    # Bollinger Bands", "    rolling_20 = close_prices.rolling(window=20)", "    df['BB_Middle'] = rolling_20.mean()", "    rolling_std = rolling_20.std()", "    df['BB_Upper'] = df['BB_Middle'] + 2 * rolling_std", "    df['BB_Lower'] = df['BB_Middle'] - 2 * rolling_std", "", "    # Momentum and ROC", "    df['Momentum_10'] = close_prices - close_prices.shift(10)", "    df['Momentum_30'] = close_prices - close_prices.shift(14)", "    df['ROC_10'] = close_prices.pct_change(periods=10) * 100", "    df['ROC_30'] = close_prices.pct_change(periods=14) * 100", "", "    # ATR (Optimized true range calculation)", "    df['High_Low'] = df['High'] - df['Low']", "    df['High_Close'] = (df['High'] - df['Close'].shift(1)).abs()", "    df['Low_Close'] = (df['Low'] - df['Close'].shift(1)).abs()", "", "    df['True_Range'] = pd.concat([df['High_Low'], df['High_Close'], df['Low_Close']], axis=1).max(axis=1)", "    df['ATR'] = df['True_Range'].rolling(window=14).mean()", "", "    # Drop rows with NaN values resulting from rolling calculations", "    df.dropna(inplace=True)", "", "    return df", "", "def calculate_rsi(df, window=14):", "    close_prices = df['Close']", "    # Calculate price changes (delta)", "    delta = close_prices.diff()", "", "    # Separate positive gains (where the price went up) and negative losses (where the price went down)", "    gain = delta.clip(lower=0)  # gains (positive deltas)", "    loss = -delta.clip(upper=0) # losses (negative deltas)", "", "    # Calculate the rolling mean of gains and losses", "    avg_gain = gain.rolling(window=window, min_periods=1).mean()", "    avg_loss = loss.rolling(window=window, min_periods=1).mean()", "", "    # Calculate the relative strength (RS)", "    rs = avg_gain / avg_loss", "", "    # Calculate RSI", "    rsi = 100 - (100 / (1 + rs))", "", "    return rsi", "", "def calculate_obv(data):", "    obv = [0]  # Initialize OBV with 0", "    for i in range(1, len(data)):", "        if data['Close'].iloc[i] > data['Close'].iloc[i-1]:", "            obv.append(obv[-1] + data['Volume'].iloc[i])", "        elif data['Close'].iloc[i] < data['Close'].iloc[i-1]:", "            obv.append(obv[-1] - data['Volume'].iloc[i])", "        else:", "            obv.append(obv[-1])  # No change if close prices are equal", "", "    return obv", "", "def calculate_mfi(data, period=14):", "    required_columns = ['High', 'Low', 'Close', 'Volume']", "    if not all(column in data.columns for column in required_columns):", "        raise ValueError(f\"DataFrame must contain the following columns: {required_columns}\")", "", "    data['TP'] = (data['High'] + data['Low'] + data['Close']) / 3", "    data['RMF'] = data['TP'] * data['Volume']", "    data['TP_diff'] = data['TP'].diff()", "", "    data['Positive_MF'] = np.where(data['TP_diff'] > 0, data['RMF'], 0)", "    data['Negative_MF'] = np.where(data['TP_diff'] < 0, data['RMF'], 0)", "", "    # Step 4: Calculate the rolling sums of Positive and Negative Money Flow", "    data['Positive_MF_sum'] = data['Positive_MF'].rolling(window=period).sum()", "    data['Negative_MF_sum'] = data['Negative_MF'].rolling(window=period).sum()", "", "    data['MFR'] = data['Positive_MF_sum'] / data['Negative_MF_sum']", "    data['MFI'] = 100 - (100 / (1 + data['MFR']))", "    data['MFI'] = np.where(data['Negative_MF_sum'] == 0, 100, data['MFI'])", "", "    return data['MFI']", "", "# Preparing the data for Machine Learning", "def prepare_ml_data(df):", "    # Include candlestick pattern features and new indicators", "    features = ['SMA_14', 'SMA_50', 'SMA_200', 'EMA_50', 'EMA_200', 'RSI', 'BB_Middle', 'BB_Upper', 'BB_Lower',", "                'Momentum_10', 'Momentum_30', 'ROC_10', 'ROC_30', 'Bullish_Engulfing', 'Doji', 'Hammer', ", "                'Hanging_Man', 'Morning_Star', 'Evening_Star', 'Shooting_Star', 'Three_White_Soldiers', ", "                'Three_Black_Crows', 'Volume', 'ATR', 'MFI']", "    ", "    df = df.dropna()", "    X = df[features]", "    y = df['Close']  # Target variable", "    ", "    scaler = MinMaxScaler() #StandardScaler()", "    X_scaled = scaler.fit_transform(X)", "    ", "    return X_scaled, y, scaler", "", "# Building the Random Forest Model", "def train_model(X_train, y_train, nest=1000, md=6):", "    model = RandomForestRegressor(n_estimators=nest, random_state=42,", "                                  max_depth=md)", "    model.fit(X_train, y_train)", "    return model", "", "def train_booster(X_train, y_train, nest=1000, lr=0.001, md=6, ss=0.8,", "                             ra=0.1, rl=1):", "    model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=nest, ", "                             learning_rate=lr, max_depth=md, subsample=ss,", "                             reg_alpha=ra, reg_lambda=rl)", "    model.fit(X_train, y_train)", "    return model", "", "# Evaluate the model performance", "def evaluate_model(model, X_test, y_test):", "    y_pred = model.predict(X_test)", "    ", "    rmse = round(np.sqrt(mean_squared_error(y_test, y_pred)), 3)", "    r2 = round(r2_score(y_test, y_pred), 3)", "    ", "    print(f\"Root Mean Squared Error: {rmse}\")", "    print(f\"R^2 Score: {r2}\")", "", "def generate_signal(predicted_prices, current_price, df):", "    # Get the last available values of the technical indicators from the dataframe", "    last_row = df.iloc[-1]", "    ", "    # Technical indicator thresholds", "    sma_50_threshold = 0.02 ", "    sma_200_threshold = 0.02", "    ema_50_threshold = 0.02", "    ema_200_threshold = 0.02", "    rsi_threshold_buy = 35", "    rsi_threshold_sell = 65", "    bb_threshold = 0.02", "", "    # Extract the latest values of the technical indicators", "    sma_50 = last_row['SMA_50']", "    sma_200 = last_row['SMA_200']", "    ema_50 = last_row['EMA_50']", "    ema_200 = last_row['EMA_200']", "    rsi = last_row['RSI']", "    bb_lower = last_row['BB_Lower']", "    bb_upper = last_row['BB_Upper']", "", "    # Initialize signals", "    buy_signal = False", "    sell_signal = False", "    ", "    # Check if current price is above the SMA and EMA thresholds", "    if current_price > (1 + sma_50_threshold) * sma_50:", "        buy_signal = True", "    if current_price > (1 + sma_200_threshold) * sma_200:", "        buy_signal = True", "    if current_price > (1 + ema_50_threshold) * ema_50:", "        buy_signal = True", "    if current_price > (1 + ema_200_threshold) * ema_200:", "        buy_signal = True", "", "    # Check RSI for buy/sell signals", "    if rsi < rsi_threshold_buy:", "        buy_signal = True", "    if rsi > rsi_threshold_sell:", "        sell_signal = True", "", "    # Check if current price is below the Bollinger Bands Lower Band", "    if current_price < (1 - bb_threshold) * bb_lower:", "        buy_signal = True", "    if current_price > (1 + bb_threshold) * bb_upper:", "        sell_signal = True", "", "    # Generate final signal", "    if buy_signal and not sell_signal:", "        return \"BUY\"", "    elif sell_signal and not buy_signal:", "        return \"SELL\"", "    else:", "        return \"HODL / SIDELINES\"", "", "", "# Plotting the stock price and technical indicators", "def plot_technical_indicators(df, ticker = '   ' ):", "    plt.figure(figsize=(14, 10))", "    ", "    # Close Price and Moving Averages", "    plt.subplot(3, 1, 1)", "    plt.plot(df['Close'], label='Close Price', alpha=0.6)", "", "    plt.plot(df['EMA_50'], color = 'red', label='50-day EMA', alpha=0.6)", "    plt.plot(df['EMA_200'], color = 'magenta', label='200-day EMA', alpha=0.6)", "    ", "    plt.title(f'{ticker} Price and Moving Averages')", "    ", "    plt.legend()", "    #plt.yscale('log')", "    plt.minorticks_on()", "    plt.tick_params(which='both', axis='y', direction='in', length=6)", "    plt.tick_params(which='minor', axis='y', direction='in', length=4)", "    plt.grid(alpha=0.5)", "", "    # OBV", "    plt.subplot(3, 1, 2)", "    plt.plot(df['OBV'], label='OBV', color='gray', alpha=0.5)", "    plt.title('On Balance Volume')", "    plt.legend()", "    plt.grid(alpha=0.5)", "    ", "    # RSI", "    plt.subplot(3, 1, 3)", "    plt.plot(df['RSI'], label='RSI', color='gray', alpha=0.5)", "    plt.title('Relative Strength Index (RSI)')", "    plt.axhline(70, color='red', linestyle='--', alpha=0.5)", "    plt.axhline(30, color='green', linestyle='--', alpha=0.5)", "    plt.legend()", "", "    plt.tight_layout()", "    plt.grid(alpha=0.5)", "    plt.show()", "    ", "#### PREDICT PRICES #####", "def predict_prices(model, recent_data, scaler, num_days=5, window_size=30):", "    # Use the same features during prediction", "    features = ['SMA_14', 'SMA_50', 'SMA_200', 'EMA_50', 'EMA_200', 'RSI', 'BB_Middle', 'BB_Upper', 'BB_Lower',", "                'Momentum_10', 'Momentum_30', 'ROC_10', 'ROC_30', 'Bullish_Engulfing', 'Doji', 'Hammer', ", "                'Hanging_Man', 'Morning_Star', 'Evening_Star', 'Shooting_Star', 'Three_White_Soldiers', ", "                'Three_Black_Crows', 'Volume', 'ATR', 'MFI']", "    ", "    last_data = recent_data.copy()  # Copy the whole dataframe to modify", "    ", "    predicted_prices = []  # List to store predicted values", "", "    for i in range(num_days):", "        # Use a rolling window of size 'window_size' from actual data (historical data)", "        sliced = last_data.iloc[-window_size:].copy()", "        ", "        # Ensure all technical indicators are calculated before prediction", "        sliced['SMA_14'] = sliced['Close'].rolling(window=14, min_periods=1).mean()", "        sliced['SMA_50'] = sliced['Close'].rolling(window=50, min_periods=1).mean()", "        sliced['SMA_200'] = sliced['Close'].rolling(window=200, min_periods=1).mean()", "        sliced['EMA_50'] = sliced['Close'].ewm(span=50, adjust=False).mean()", "        sliced['EMA_200'] = sliced['Close'].ewm(span=200, adjust=False).mean()", "        ", "        sliced['RSI'] = calculate_rsi(sliced)", "        sliced['MFI'] = calculate_mfi(sliced)", "        ", "        sliced['BB_Middle'] = sliced['Close'].rolling(window=20, min_periods=1).mean()", "        sliced['BB_Upper'] = sliced['BB_Middle'] + 2 * sliced['Close'].rolling(window=20, min_periods=1).std()", "        sliced['BB_Lower'] = sliced['BB_Middle'] - 2 * sliced['Close'].rolling(window=20, min_periods=1).std()", "        ", "        # Candlestick patterns (use actual data for pattern detection)", "        sliced = add_candlestickpatterns(sliced)", "", "        # Extract features for the current prediction", "        inData = sliced[features].iloc[-1:]", "        ", "        # Scale features", "        inData_scaled = scaler.transform(inData)", "        ", "        # Predict the price for the next day using the model", "        predicted_price = model.predict(inData_scaled)", "        rounded_price = round(predicted_price[0], 4)", "        predicted_prices.append(rounded_price)  # Store the scalar value", "        ", "        # Update the 'Close' price with the predicted value for the next business day", "        next_index = pd.bdate_range(last_data.index[-1], periods=2)[-1]  # Next business day", "        last_data.loc[next_index] = np.nan  # Add the new row", "        last_data.at[next_index, 'Close'] = rounded_price  # Only update 'Close' with predicted value", "        ", "        # Append a new row with the predicted 'Close' value only", "        new_row = pd.DataFrame({", "            'Close': [rounded_price],", "            'Date': [next_index]", "        }).set_index('Date')", "        ", "        # Append the new row to the dataframe", "        last_data = pd.concat([last_data, new_row])", "    ", "    return predicted_prices", "", "    ", "def add_candlestickpatterns(df):", "    # Ensure df is a copy, not a view, to avoid the SettingWithCopyWarning", "    df = df.copy()", "", "    # Detect candlestick patterns and add to dataframe", "    df['Bullish_Engulfing'] = cs.detect_bullish_engulfing(df)", "    df['Doji'] = cs.detect_doji(df)", "    df['Hammer'] = cs.detect_hammer(df)", "    df['Hanging_Man'] = cs.detect_hanging_man(df)", "    df['Morning_Star'] = cs.detect_morning_star(df)", "    df['Evening_Star'] = cs.detect_evening_star(df)", "    df['Shooting_Star'] = cs.detect_shooting_star(df)", "    df['Three_White_Soldiers'] = cs.detect_three_white_soldiers(df)", "    df['Three_Black_Crows'] = cs.detect_three_black_crows(df)", "", "    return df", "", "def plot_with_predictions(stock_df, predicted_prices, ticker='NONE', num_days=5):", "    # Get the last month of historical data", "    ", "    current_date = datetime.datetime.now().strftime('%Y-%m-%d')", "    ", "    end_date = stock_df.index[-1]", "    start_date = end_date - pd.DateOffset(months=1)", "    one_month_data = stock_df.loc[start_date:end_date]", "    ", "    # Get the last known closing price", "    last_close = one_month_data['Close'].iloc[-1]", "    ", "    # Generate dates for the predicted prices", "    prediction_dates = pd.date_range(start=end_date + pd.DateOffset(days=1), periods=num_days)", "    ", "    # Create a DataFrame for predicted prices with the last known close included", "    predictions_df = pd.DataFrame({", "        'Date': [end_date] + list(prediction_dates),", "        'Predicted_Price': [last_close] + predicted_prices", "    }).set_index('Date')", "    ", "    # Combine historical data with predicted prices", "    combined_df = pd.concat([one_month_data[['Close']], predictions_df])", "    ", "    # Plot historical closing prices and predicted prices", "    plt.figure(figsize=(14, 7))", "    ", "    # Plot historical closing prices", "    plt.plot(one_month_data.index, one_month_data['Close'], label='Historical Close Prices', ", "             color='blue', alpha=0.7)", "    ", "    # Plot predicted prices", "    plt.plot(predictions_df.index, predictions_df['Predicted_Price'], label='Predicted Prices', ", "             color='blue', linestyle='--', marker='o', alpha=0.4)", "    ", "    # Formatting the plot", "    plt.title(f'{ticker} {current_date} - Closing Prices and Next {num_days} Days Predictions')", "    plt.xlabel('Date')", "    plt.ylabel('Price')", "    plt.legend()", "    plt.grid(True)", "    plt.xticks(rotation=45)", "    plt.tight_layout()", "    # Calculate statistics", "    predicted_max = np.max(predicted_prices)", "    predicted_min = np.min(predicted_prices)", "    predicted_change = ((predicted_prices[-1] - last_close) / last_close) * 100", "", "    # Add text annotation", "    textstr = (f'Predicted % Change: {predicted_change:.2f}%\\n'", "               f'Min Price: ${predicted_min:.2f}\\n'", "               f'Max Price: ${predicted_max:.2f}')", "", "    plt.text(0.5, 0.5, ticker, transform=plt.gca().transAxes, ", "             fontsize=100, color='grey', alpha=0.1,  # Adjust transparency here", "             horizontalalignment='center', verticalalignment='center',", "             rotation=45, weight='bold', style='italic')", "    ", "    plt.text(0.95, 0.05, textstr, transform=plt.gca().transAxes, fontsize=12,", "             verticalalignment='bottom', horizontalalignment='right',", "             bbox=dict(boxstyle='round', alpha=0.2, facecolor='white'))", "", "    path = r'C:\\Users\\Farrukh\\jupyter-Notebooks\\STOCKS\\predictions'", "    fname = f'{current_date}_{ticker}.png'", "    fpath = os.path.join(path, fname)", "    plt.savefig(fpath, bbox_inches='tight')", "    plt.show()", "    plt.close()"], "file_path": "ta_functions.py"}
{"Link_to_commit": "https://github.com/mdaffanwd/30-days-js/commit/ea218bb8e896cb0fc9c0023e25dfc33d28c95ce0", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 45, "n_files_impacted": 1, "longest_chunk": ["// console.log(\"In-order Traversal:\");", "// inOrderTraversal(root); // Output: 4 2 5 1 6 3 7", "", "// task 10", "{", "  class TreeNode {", "    constructor(value) {", "      this.value = value;", "      this.left = null;", "      this.right = null;", "    }", "  }", "", "  function calculateDepth(node) {", "    // Base case: If the node is null, return 0", "    if (node === null) {", "      return 0;", "    }", "", "    // Recursive case: Calculate the depth of left and right subtrees", "    const leftDepth = calculateDepth(node.left);", "    const rightDepth = calculateDepth(node.right);", "", "    // The depth of the current node is the maximum of leftDepth and rightDepth plus 1", "    return Math.max(leftDepth, rightDepth) + 1;", "  }", "", "  // Test the function with a sample binary tree", "  const root = new TreeNode(1);", "  root.left = new TreeNode(2);", "  root.right = new TreeNode(3);", "  root.left.left = new TreeNode(4);", "  root.left.right = new TreeNode(5);", "  root.right.left = new TreeNode(6);", "  root.right.right = new TreeNode(7);", "", "//   console.log(\"Depth of the tree:\", calculateDepth(root)); // Output: 3", "", "  // Another test case with an unbalanced tree", "  const unbalancedRoot = new TreeNode(1);", "  unbalancedRoot.left = new TreeNode(2);", "  unbalancedRoot.left.left = new TreeNode(3);", "", "//   console.log(\"Depth of the unbalanced tree:\", calculateDepth(unbalancedRoot)); // Output: 3", "}"], "file_path": "16.Recursion.js"}
{"Link_to_commit": "https://github.com/Rashy-hub/Algorithms-and-Data-Structures-Cheatsheets-Sandbox/commit/8bbc2fe8b16c93d483368ede5a6cf959d36681e9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 67, "n_files_impacted": 4, "longest_chunk": ["# Graph Data Structure: A Theoretical Overview", "", "## Introduction", "", "A **graph** is a collection of nodes (also called **vertices**) and edges, which connect pairs of nodes. Graphs can represent various real-world systems like social networks, transportation routes, and web page links.", "", "Graphs are either **directed** or **undirected**:", "", "-   In **directed graphs**, edges have a direction, indicating the relationship flows from one node to another.", "-   In **undirected graphs**, edges represent a two-way relationship, meaning the connection between the nodes is mutual.", "", "```", "{", "  +---+-----------+", "| V | Neighbors |", "+---+-----------+", "| A |   B, C   |", "| B |   A, D   |", "| C |     A     |", "| D |     B     |", "+---+-----------+", "}", "```", "", "## Definition", "", "A graph can be defined as:", "", "-   A set of vertices (nodes).", "-   A set of edges (connections) between the vertices.", "", "Graphs are often represented as:", "", "-   **Adjacency List**: Where each node has a list of nodes it's connected to.", "-   **Adjacency Matrix**: A 2D array indicating whether there is a direct connection between pairs of nodes.", "", "## Common Use Cases", "", "Graphs are used in many applications, such as:", "", "-   **Social Networks**: Representing users as nodes and their connections as edges.", "-   **Maps and Routes**: Cities as nodes, and roads as edges between them.", "-   **Recommendation Systems**: Connecting users to products based on behavior.", "", "## Graph Operations", "", "Some common graph operations include:", "", "-   **Add Vertex**: Add a new node to the graph.", "-   **Add Edge**: Create a connection between two nodes.", "-   **Remove Vertex**: Remove a node and its associated edges.", "-   **Remove Edge**: Delete a connection between two nodes.", "-   **Search**: Find a path between two nodes (using algorithms like BFS, DFS).", "", "### Example Methods", "", "Below are some typical methods associated with a graph:", "", "-   **`addVertex(vertex)`**: Adds a new vertex to the graph.", "-   **`addEdge(vertex1, vertex2)`**: Creates an edge between two vertices.", "-   **`removeVertex(vertex)`**: Removes a vertex from the graph.", "-   **`removeEdge(vertex1, vertex2)`**: Removes an edge between two vertices.", "-   **`hasEdge(vertex1, vertex2)`**: Checks if there's an edge between two vertices.", "", "## Conclusion", "", "Graphs are incredibly versatile data structures used in modeling relationships between entities. Whether you need to track connections in a network or design algorithms to traverse structures, graphs are essential tools in computer science and many real-world applications."], "file_path": "Basic Data Structure/Sets/Sets.js"}
{"Link_to_commit": "https://github.com/JonnyBe2001/kickbaseLiveScores/commit/eaeb33f3dd328edaf47f69854913eded7efb27dc", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 1, "longest_chunk": ["", "// Funktion zum Anzeigen des Login-Formulars", "function showLoginForm() {", "    document.getElementById('loginForm').classList.remove('hidden');  // Zeige das Login-Formular", "}", "", "// Funktion zum \u00dcberpr\u00fcfen des Tokens beim Laden der Seite", "window.onload = function() {", "    if (token) {", "        // Versuche, die Ligen mit dem gespeicherten Token abzurufen", "        fetchLeagues();", "    } else {", "        // Zeige das Login-Formular, falls kein Token vorhanden ist", "        showLoginForm();", "    }", "};"], "file_path": "script.js"}
{"Link_to_commit": "https://github.com/AlessandroDu/SurveyProject/commit/aacd1f77a5227971656e9c4eaa891b8a36d6a329", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 84, "n_files_impacted": 12, "longest_chunk": ["# Generated by Django 5.1.4 on 2025-01-02 12:56", "", "import django.db.models.deletion", "from django.conf import settings", "from django.db import migrations, models", "", "", "class Migration(migrations.Migration):", "", "    dependencies = [", "        ('surveysApp', '0001_initial'),", "        migrations.swappable_dependency(settings.AUTH_USER_MODEL),", "    ]", "", "    operations = [", "        migrations.CreateModel(", "            name='Question',", "            fields=[", "                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),", "                ('question', models.CharField(max_length=120)),", "                ('type', models.CharField(choices=[('open', 'Open Answer'), ('multiple_choice', 'Multiple Choice')], max_length=20)),", "            ],", "        ),", "        migrations.CreateModel(", "            name='Survey',", "            fields=[", "                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),", "                ('title', models.CharField(max_length=100)),", "                ('description', models.TextField()),", "            ],", "        ),", "        migrations.RemoveField(", "            model_name='openanswer',", "            name='questionID',", "        ),", "        migrations.RemoveField(", "            model_name='options',", "            name='questionID',", "        ),", "        migrations.CreateModel(", "            name='Option',", "            fields=[", "                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),", "                ('optionText', models.CharField(max_length=120)),", "                ('question', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='options', to='surveysApp.question')),", "            ],", "        ),", "        migrations.CreateModel(", "            name='Submission',", "            fields=[", "                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),", "                ('created_at', models.DateTimeField(auto_now_add=True)),", "                ('user', models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.CASCADE, related_name='submissions', to=settings.AUTH_USER_MODEL)),", "                ('survey', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='submissions', to='surveysApp.survey')),", "            ],", "        ),", "        migrations.CreateModel(", "            name='Answer',", "            fields=[", "                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),", "                ('text', models.TextField(blank=True, null=True)),", "                ('selected_option', models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.CASCADE, to='surveysApp.option')),", "                ('question', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='answers', to='surveysApp.question')),", "                ('submission', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='answers', to='surveysApp.submission')),", "            ],", "        ),", "        migrations.AddField(", "            model_name='question',", "            name='survey',", "            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='questions', to='surveysApp.survey'),", "        ),", "        migrations.DeleteModel(", "            name='EmailAnswer',", "        ),", "        migrations.DeleteModel(", "            name='OpenAnswer',", "        ),", "        migrations.DeleteModel(", "            name='Options',", "        ),", "        migrations.DeleteModel(", "            name='Questions',", "        ),", "    ]"], "file_path": "surveysApp/models.py"}
{"Link_to_commit": "https://github.com/Donkeys-United/CV-S-emulation/commit/2728ff4df26b8e92041cfb82847b6c3d1a0645ec", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 6, "n_files_impacted": 5, "longest_chunk": ["        from TransmissionThread import TransmissionThread", "        from ListeningThread import ListeningThread", "", "        self.taskHandlerThread = taskHandlerThread", "        self.acceptedRequestsQueue = AcceptedRequestQueue()", "        self.acceptedRequestsQueue.start()"], "file_path": "CommunicationThread.py"}
{"Link_to_commit": "https://github.com/geo-yuheng/yuheng-admininspect/commit/16e0a6ca2f27e6b0900c64be612239abe98b66f4", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 135, "n_files_impacted": 1, "longest_chunk": ["LOCALE = \"zh\"", "", "", "def i18n_string(strid: str) -> str:", "    \"\"\"", "    Returns localized strings based on the given string ID.", "", "    Args:", "        strid (str): A string identifier for the desired text.", "", "    Returns:", "        str: A localized string corresponding to the given ID.", "    \"\"\"", "    strings = {", "        \"en\": {", "            \"enter-root-id\": \"Please enter the root node ID:\",", "            \"no-root-found\": \"Unable to find root node ID, please ensure the graph contains administrative boundary nodes.\",", "            \"enter-manual-root-id\": \"Unable to automatically determine the root node ID, please enter manually:\",", "            \"invalid-id\": \"The entered ID is invalid, please enter a valid node ID.\",", "            \"enter-valid-number\": \"Please enter a valid number.\",", "            \"multiple-root-nodes\": \"Multiple root nodes of the same highest level found, please choose one as the root node:\",", "        },", "        \"zh\": {", "            \"enter-root-id\": \"\u8bf7\u8f93\u5165\u6839\u8282\u70b9ID\uff1a\",", "            \"no-root-found\": \"\u65e0\u6cd5\u627e\u5230\u6839\u8282\u70b9ID\uff0c\u8bf7\u786e\u4fdd\u56fe\u4e2d\u5305\u542b\u884c\u653f\u8fb9\u754c\u8282\u70b9\u3002\",", "            \"enter-manual-root-id\": \"\u65e0\u6cd5\u81ea\u52a8\u786e\u5b9a\u6839\u8282\u70b9ID\uff0c\u8bf7\u624b\u52a8\u8f93\u5165\uff1a\",", "            \"invalid-id\": \"\u8f93\u5165\u7684ID\u65e0\u6548\uff0c\u8bf7\u8f93\u5165\u6709\u6548\u7684\u8282\u70b9ID\u3002\",", "            \"enter-valid-number\": \"\u8bf7\u8f93\u5165\u4e00\u4e2a\u6709\u6548\u7684\u6570\u5b57\u3002\",", "            \"multiple-root-nodes\": \"\u53d1\u73b0\u591a\u4e2a\u540c\u7b49\u6700\u9ad8\u7ea7\u522b\u7684\u8282\u70b9\uff0c\u8bf7\u9009\u62e9\u4e00\u4e2a\u4f5c\u4e3a\u6839\u8282\u70b9\uff1a\",", "        },", "    }", "    return strings.get(LOCALE, {}).get(strid, \"\")", "", "", "def build_graph(map: Waifu) -> nx.DiGraph:", "    \"\"\"", "    Constructs a directed graph from map data.", "", "    Each administrative boundary from the map is added as a node, and subarea relationships are added as edges.", "", "    Args:", "        map (Waifu): An instance of Waifu containing map data.", "", "    Returns:", "        nx.DiGraph: A directed graph representing the administrative hierarchy.", "    \"\"\"", "    G = nx.DiGraph()", "    for id, relation in map.relation_dict.items():", "        admin_level = relation.tags.get(\"admin_level\")", "        name = relation.tags.get(\"name\")", "        ref = relation.tags.get(\"ref\")", "        if (", "            \"boundary\" in relation.tags", "            and relation.tags[\"boundary\"] == \"administrative\"", "        ):", "            G.add_node(id, admin_level=admin_level, name=name, ref=ref)", "            for member in relation.members:", "                if (", "                    member.role == \"subarea\"", "                    and member.ref in map.relation_dict", "                ):", "                    G.add_edge(id, member.ref)", "    return G", "", "", "def graph_to_nested_json(G: nx.DiGraph, root_id: int) -> Dict:", "    \"\"\"", "    Converts a directed graph to a nested JSON structure starting from a specified root node.", "", "    Args:", "        G (nx.DiGraph): The directed graph to convert.", "        root_id (int): The ID of the root node from which to start the nesting.", "", "    Returns:", "        Dict: A nested dictionary representing the hierarchical structure.", "    \"\"\"", "", "    def recurse(node):", "        children = list(G.successors(node))", "        if not children:", "            return {\"id\": node, **G.nodes[node]}", "        return {", "            \"id\": node,", "            **G.nodes[node],", "            \"subareas\": [recurse(child) for child in children],", "        }", "", "    return recurse(root_id)", "", "", "def find_root_node_id(G: nx.DiGraph, strategy=\"input\") -> int:", "    \"\"\"", "    Finds the ID of the root node based on the given strategy.", "", "    Args:", "        G (nx.DiGraph): The directed graph from which to find the root node.", "        strategy (str): The strategy to use for finding the root node. Options are \"input\", \"highest\", \"auto\".", "", "    Returns:", "        int: The ID of the found root node.", "", "    Raises:", "        ValueError: If no root node can be found based on the given strategy.", "    \"\"\"", "    if strategy == \"input\":", "        return int(input(i18n_string(\"enter-root-id\")))", "", "    min_level = float(\"inf\")", "    root_candidates = []", "    for node, data in G.nodes(data=True):", "        admin_level = data.get(\"admin_level\")", "        if admin_level is None:", "            continue", "        try:", "            level = int(admin_level)", "            if level < min_level:", "                min_level = level", "                root_candidates = [(node, data)]", "            elif level == min_level:", "                root_candidates.append((node, data))", "        except ValueError:", "            continue", "", "    if not root_candidates:", "        if strategy == \"highest\":", "            raise ValueError(i18n_string(\"no-root-found\"))", "        elif strategy == \"auto\":", "            return int(input(i18n_string(\"enter-manual-root-id\")))", "", "    if len(root_candidates) > 1:", "        print(i18n_string(\"multiple-root-nodes\"))", "        for idx, (node, data) in enumerate(root_candidates):", "            print(", "                f\"({idx + 1}). ID: [{node}], \\\"admin_level\\\": {data['admin_level']}, \\\"name\\\": {data.get('name')}, \\\"ref\\\": {data.get('ref')}\"", "            )"], "file_path": "src/adminextract.py"}
{"Link_to_commit": "https://github.com/diogoflduarte/spikechart/commit/d15e7129f7d215af46d6a6c064d5107d0301c5be", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 27, "n_files_impacted": 1, "longest_chunk": ["    dcc.Store(id='data-store'),  # Store for holding uploaded data", "    dbc.Row([", "        dbc.Col(dcc.Upload(", "            id='upload-spike-times',", "            children=html.Div(['Spike times', html.A('')]),", "            style={'width': '100%', 'height': '60px', 'lineHeight': '60px',", "                   'borderWidth': '1px', 'borderStyle': 'dashed', 'borderRadius': '5px',", "                   'textAlign': 'center', 'margin': '10px'},", "            multiple=False", "        ), width=4),", "        dbc.Col(dcc.Upload(", "            id='upload-spike-clusters',", "            children=html.Div(['Spike clusters', html.A('')]),", "            style={'width': '100%', 'height': '60px', 'lineHeight': '60px',", "                   'borderWidth': '1px', 'borderStyle': 'dashed', 'borderRadius': '5px',", "                   'textAlign': 'center', 'margin': '10px'},", "            multiple=False", "        ), width=4),", "        dbc.Col(dcc.Upload(", "            id='upload-behavior',", "            children=html.Div(['Behavior', html.A('')]),", "            style={'width': '100%', 'height': '60px', 'lineHeight': '60px',", "                   'borderWidth': '1px', 'borderStyle': 'dashed', 'borderRadius': '5px',", "                   'textAlign': 'center', 'margin': '10px'},", "            multiple=False", "        ), width=4)", "    ]),"], "file_path": "simple_raster.py"}
{"Link_to_commit": "https://github.com/Gerum-Berhanu/playai/commit/c206bec0a3dec4d261b566038423ea8cd8113a93", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 9, "n_files_impacted": 4, "longest_chunk": ["async def run(prompt):", "    async with async_playwright() as playwright:", "        browser = await playwright.chromium.launch(headless=False)", "        page = await browser.new_page()", "        url = \"https://chatgpt.com\"", "        await page.goto(url)", "        await page.wait_for_selector(\"textarea#prompt-textarea\")", "        textarea = await page.locator(\"textarea#prompt-textarea\")", "        button = await page.locator(\"button.absolute.bottom-1\\\\.5\")"], "file_path": "ask.py"}
{"Link_to_commit": "https://github.com/XenoBene/PulsedLaserGUI/commit/119332f4b373aa908e4c52554e0daaa3fe80eaaa", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 46, "n_files_impacted": 2, "longest_chunk": ["        \"\"\"", "        Performs wavelength to angle calibration over a range of temperatures.", "", "        This method calibrates the rotation stage by adjusting the DFB laser temperature,", "        measuring the power at different angles, and logging the data.", "", "        Parameters:", "        -----------", "        dfb : object", "            The DFB laser object whose temperature (and therefore wavelength) is controlled.", "        powermeter : object", "            The powermeter object to measure power.", "        temp_list : list of float", "            List of temperatures to use for calibration.", "        calibration_bounds : tuple", "            Bounds for the calibration calculations.", "        startangle : float", "            The starting angle for the calibration scan.", "        endangle : float", "            The ending angle for the calibration scan.", "", "        Behavior:", "        ---------", "        - If `self.ac_begincal` is True:", "            - Sets up and moves the stage to the start angle.", "            - Waits for the stage to reach the start angle and stops.", "            - Sets `self.ac_begincal` to False after initialization.", "        - If `self.initcal_bool` is True:", "            - Initializes calibration in either low-to-high or high-to-low direction.", "            - Scans the stage to the appropriate angle.", "            - Emits progress updates.", "        - Logs the current time, wavelength, power, and angle to the CSV file while the stage is scanned.", "        - Toggles calibration direction and updates progress.", "        - When all temperatures are processed, stops the calibration, calculates results, and emits completion signals.", "", "        Example:", "        --------", "        >>> obj.wavelength_to_angle_calibration(dfb, powermeter, [25.0, 30.0, 35.0], (400, 700), 0, 180)", "        \"\"\"", "        def handle_initcal(lowtohi, temp):", "            self.init_wavelength_to_angle_calibration(dfb, temp, lowtohi)", "            self.stage.scan_to_angle(endangle if lowtohi else startangle, 0.5)", "            progress = (self.autocal_iterator + (0.5 if lowtohi else 1)) * 100 / len(temp_list)", "            self.autocalibration_progress.emit(int(progress))", "            self.initcal_bool = False", ""], "file_path": "ASE_functions.py"}
{"Link_to_commit": "https://github.com/anibalfuentesrod/holbertonschool-higher_level_programming/commit/8c3114bf8ddb1b9b744fa0240096286ca719d93e", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 7, "n_files_impacted": 1, "longest_chunk": ["                'body': post['body']", "            })", "        ", "        with open('posts.csv', 'w', newline='') as cf:", "            fieldnames = ['id', 'title', 'body']", "            writer = csv.DictWriter(cf, fieldnames=fieldnames)", "            "], "file_path": "restful-api/task_02_requests.py"}
{"Link_to_commit": "https://github.com/anibalfuentesrod/holbertonschool-higher_level_programming/commit/ce15f25e5915b6e5a1c2be61c4d2a27457f13606", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 10, "n_files_impacted": 4, "longest_chunk": ["#!/usr/bin/python3", "replace_in_list = __import__('2-replace_in_list').replace_in_list", "", "my_list = [1, 2, 3, 4, 5]", "idx = 3", "new_element = 9", "new_list = replace_in_list(my_list, idx, new_element)", "", "print(new_list)", "print(my_list)"], "file_path": "python-data_structures/2-replace_in_list.py"}
{"Link_to_commit": "https://github.com/ncusi/PatchScope/commit/f1416abd53cd0609a131f51ba366ebb52f8a7938", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 69, "n_files_impacted": 2, "longest_chunk": ["import io", "", "import pytest", "", "from diffinsights_web.datastore.timeline import get_timeline_data", "", "param = pytest.importorskip(\"param\")", "pn = pytest.importorskip(\"panel\")", "", "from diffinsights_web.apps.contributors import template, dataset_dir, timeline_data_store", "", "", "@pytest.fixture", "def app():", "    return template", "", "def test_contributors_run_performance(app, benchmark):", "    #for k, v in app.param.objects().items():", "    #    print(f\"{app.__class__.name}.{k} = {repr(v.default)} ({type(v)})\")", "", "    #print(template)", "    #for e in template.sidebar:", "    #    print(e)", "    #print(template.sidebar[0][0])", "    #print(template.sidebar[0][0].value)", "", "    ## Failed attempt 1.", "    #     @pn.cache", "    #     def get_timeline_df(timeline_data: dict, repo: str) -> pd.DataFrame:", "    # >       init_df = pd.DataFrame.from_records(timeline_data[repo])", "    # E       KeyError: 'hellogitworld'", "    #with pn.io.hold():", "    #    template.sidebar[0][1].value = 'qtile'", "    #    template.sidebar[0][0].value = str(dataset_dir.joinpath('qtile.timeline.purpose-to-type.json'))", "", "    ## Failed attempt 2.", "    #     @pn.cache", "    #     def get_timeline_df(timeline_data: dict, repo: str) -> pd.DataFrame:", "    # >       init_df = pd.DataFrame.from_records(timeline_data[repo])", "    # E       KeyError: 'hellogitworld'", "    #with param.parameterized.batch_call_watchers(timeline_data_store):", "    #    timeline_data_store.select_file_widget.value = str(dataset_dir.joinpath('qtile.timeline.purpose-to-type.json'))", "    #    timeline_data_store.select_repo_widget.value = 'qtile'", "", "    ## Failed attempt 3.", "    # AttributeError: The value of a derived expression cannot be set.", "    #timeline_data_store.timeline_data_rx.rx.value = \\", "    #    get_timeline_data(dataset_dir.joinpath('qtile.timeline.purpose-to-type.json'))", "", "    ## Failed attempt 4.", "    # KeyError: 'hellogitworld'", "    #timeline_data_store.select_file_widget.param.update(", "    #    value=str(dataset_dir.joinpath('qtile.timeline.purpose-to-type.json')),", "    #)", "", "    #print(template.sidebar[0][0].value)", "", "    # Benchmark the time it takes to render the Panel app", "    def render_app():", "        buffer = io.StringIO()", "        pn.state.clear_caches()", "        pn.io.save.save(app, filename=buffer, embed=True)", "        return buffer.getvalue()", "", "    # Run the benchmark", "    result = benchmark(render_app)", "", "    # Optional: Add an assertion for maximum acceptable render time (in seconds)", "    assert result is not None, \"App rendering failed\""], "file_path": "tests/test_app_contributors_performance.py"}
{"Link_to_commit": "https://github.com/Ryan-Pool/6thHourRPool/commit/f77dd2e2eafcee29e77c3416377dd87af99c3de8", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 70, "n_files_impacted": 2, "longest_chunk": ["#Ryan Pool", "#9.11.24", "", "", "import random", "", "def eliminateOptions(options, count):", "    # While there is more than one option in the list, keep eliminating", "    index = 0", "    while len(options) > 1:", "        # Calculate the index of the option to eliminate using the count", "        index = (index + count - 1) % len(options)", "        eliminated = options.pop(index)", "        print(f\"Eliminated: {eliminated}\")", "", "", "def mashGame():", "    # Default MASH options for housing", "    mashOptions = [\"Mansion\", \"Apartment\", \"Shack\", \"House\"]", "", "    # Collecting user input for different categories", "    print(\"Welcome to the MASH Game!\")", "", "    # Asking for custom options in categories", "    jobs = [input(f\"Enter job option {i + 1}: \") for i in range(4)]", "    spouses = [input(f\"Enter spouse name option {i + 1}: \") for i in range(4)]", "    cars = [input(f\"Enter car option {i + 1}: \") for i in range(4)]", "    money = [input(f\"Enter amount of money option {i + 1}: \") for i in range(4)]", "    kids = [input(f\"Enter a number of kids option {i + 1}: \") for i in range(4)]", "", "    # Add the default MASH housing options to the game", "    categories = {", "        \"House\": mashOptions,", "        \"Job\": jobs,", "        \"Spouse\": spouses,", "        \"Car\": cars,", "        \"Money\": money,", "        \"Kids\": kids", "    }", "", "    # Ask the user for a number to use for elimination", "    count = int(input(\"\\nPick a number for the elimination process: \"))", "", "    print(\"\\nEliminating options...\\n\")", "", "    # Eliminate options in each category based on the user's number", "    for category, options in categories.items():", "        print(f\"\\nEliminating from {category}:\")", "        eliminateOptions(options, count)", "", "    # Final result after elimination", "    house = mashOptions[0]", "    job = jobs[0]", "    spouse = spouses[0]", "    car = cars[0]", "    money = money[0]", "    numKids = kids[0]", "", "    # Display the final result", "    print(\"\\nYour MASH results are in:\")", "    print(f\"You will live in a {house}.\")", "    print(f\"You will work as a {job}.\")", "    print(f\"You will marry {spouse}.\")", "    print(f\"You will drive a {car}.\")", "    print(f\"You will have {money} money.\")", "    print(f\"You will have {numKids} kids.\")", "", "", "# Run the game", "mashGame()"], "file_path": "Mash.py"}
{"Link_to_commit": "https://github.com/EricBritt1/45_2/commit/1cffb31837b868531768a0cc77131244b2a1ce04", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 269, "n_files_impacted": 5, "longest_chunk": ["// I had to do these with the help of chatGPT. ", "", "class Node {", "  constructor(val, left = null, right = null) {", "    this.val = val;", "    this.left = left;", "    this.right = right;", "  }", "}", "", "class BinarySearchTree {", "  constructor(root = null) {", "    this.root = root;", "  }", "", "  /** insert(val): insert a new node into the BST with value val.", "   * Returns the tree. Uses iteration. */", "", "  ", "  insert(val) {", "    if(!this.root) {", "      this.root = new Node(val);", "      return this", "  }", "  ", "  let currentNode = this.root;", "  while (true) {", "    if(val < currentNode.val) {", "      if(!currentNode.left) {", "        currentNode.left = new Node(val);", "        return this;", "      }", "      currentNode = currentNode.left;", "    } else if (val > currentNode.val) {", "      if (!currentNode.right) {", "        currentNode.right = new Node(val);", "        return this;", "      }", "      currentNode = currentNode.right;", "    } else {", "", "      return this", "    }", "  }", "", "  }", "", "  /** insertRecursively(val): insert a new node into the BST with value val.", "   * Returns the tree. Uses recursion. */", "", "  ", "  insertRecursively(val, currentNode = this.root) {", "    if (!currentNode) {", "      this.root = new Node(val);", "      return this;", "    }", "", "    if (val < currentNode.val) {", "      if (!currentNode.left) {", "        currentNode.left = new Node(val);", "      } else {", "        this.insert(val, currentNode.left);", "      }", "    } else if (val > currentNode.val) {", "      if (!currentNode.right) {", "        currentNode.right = new Node(val);", "      } else {", "        this.insert(val, currentNode.right);", "      }", "    }", "    // If val is equal to currentNode.val, handle as needed", "    // Here, we'll avoid duplicates, but you can change this behavior if needed", "    return this;", "  }", "", "  /** find(val): search the tree for a node with value val.", "   * return the node, if found; else undefined. Uses iteration. */", "", "  ", "  find(val) {", "    let currentNode = this.root;", "", "    if(!currentNode) {", "      return null", "    }", "    while(currentNode) {", "      if( currentNode.val === val ) {", "        return currentNode", "      } else if (val < currentNode.val) {", "        currentNode = currentNode.left;", "      } else {", "        currentNode = currentNode.right", "      }", "    }", "    return undefined;", "  }", "", "  /** findRecursively(val): search the tree for a node with value val.", "   * return the node, if found; else undefined. Uses recursion. */", "", "  findRecursively(val, currentNode = this.root) {", "    if (!currentNode) {", "        return undefined; ", "    }", "", "    if (val === currentNode.val) {", "        return currentNode; ", "    }", "", "    if (val < currentNode.val) {", "        return this.find(val, currentNode.left); ", "    } else {", "        return this.find(val, currentNode.right); ", "    }", "}", "", "", "  /** dfsPreOrder(): Traverse the array using pre-order DFS.", "   * Return an array of visited nodes. */", "", "  dfsPreOrder() {", "    if (!this.root) {", "        return [];", "    }", "", "    const visited = [];", "    const stack = [];", "    let currentNode = this.root;", "", "    while (currentNode || stack.length > 0) {", "        if (currentNode) {", "            visited.push(currentNode.val);", "            ", "", "            if (currentNode.right) {", "                stack.push(currentNode.right);", "            }", "", "", "            currentNode = currentNode.left;", "        } else {", "", "            currentNode = stack.pop();", "        }", "    }", "", "    return visited;", "}", "", "", "  /** dfsInOrder(): Traverse the array using in-order DFS.", "   * Return an array of visited nodes. */", "", "  dfsInOrder() {", "    if (!this.root) {", "        return [];", "    }", "", "    const visited = [];", "    const stack = [];", "    let currentNode = this.root;", "", "    while (currentNode || stack.length > 0) {", "        if (currentNode) {", "", "            stack.push(currentNode);", "            currentNode = currentNode.left;", "        } else {", "", "            currentNode = stack.pop();", "            visited.push(currentNode.val);", "            currentNode = currentNode.right;", "        }", "    }", "", "    return visited;", "}", "", "", "  /** dfsPostOrder(): Traverse the array using post-order DFS.", "   * Return an array of visited nodes. */", "", "  dfsPostOrder() {", "    if (!this.root) {", "        return [];", "    }", "", "    const visited = [];", "    const stack1 = [];", "    const stack2 = [];", "    let currentNode = this.root;", "", "    stack1.push(currentNode);", "", "    while (stack1.length > 0) {", "        currentNode = stack1.pop();", "        stack2.push(currentNode);", "", "        if (currentNode.left) {", "            stack1.push(currentNode.left);", "        }", "", "        if (currentNode.right) {", "            stack1.push(currentNode.right);", "        }", "    }", "", "    while (stack2.length > 0) {", "        visited.push(stack2.pop().val);", "    }", "", "    return visited;", "}", "", "", "  /** bfs(): Traverse the array using BFS.", "   * Return an array of visited nodes. */", "", "  bfs() {", "    if (!this.root) {", "        return [];", "    }", "", "    const visited = [];", "    const queue = [];", "    queue.push(this.root);", "", "    while (queue.length > 0) {", "        const currentNode = queue.shift(); // Dequeue", "        visited.push(currentNode.val);", "", "        if (currentNode.left) {", "            queue.push(currentNode.left); // Enqueue left child", "        }", "", "        if (currentNode.right) {", "            queue.push(currentNode.right); // Enqueue right child", "        }", "    }", "", "    return visited;", "}", "", "", "  /** Further Study!", "   * remove(val): Removes a node in the BST with the value val.", "   * Returns the removed node. */", "", "  remove(val) {", "", "  }", "", "  /** Further Study!", "   * isBalanced(): Returns true if the BST is balanced, false otherwise. */", "", "  isBalanced() {", "", "  }", "", "  /** Further Study!", "   * findSecondHighest(): Find the second highest value in the BST, if it exists.", "   * Otherwise return undefined. */", "", "  findSecondHighest() {", "    ", "  }", "}", "", "module.exports = BinarySearchTree;"], "file_path": "binary-search-tree.test.js"}
{"Link_to_commit": "https://github.com/Omerixe/WhiskyDatabase/commit/4f9425be19afb3cefb3c391a2cd299dfdfaa4b6e", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 42, "n_files_impacted": 8, "longest_chunk": ["// src/components/AddWhisky.js", "import React, { useState } from 'react';", "import { addWhisky } from '../firebase';", "import TextField from '@mui/material/TextField';", "import Button from '@mui/material/Button';", "import Box from '@mui/material/Box';", "import Typography from '@mui/material/Typography';", "", "const AddWhisky = () => {", "  const [name, setName] = useState('');", "  const [age, setAge] = useState('');", "  const [distillery, setDistillery] = useState('');", "  const [type, setType] = useState('');", "  const [region, setRegion] = useState('');", "", "", "  const handleSubmit = () => {", "    const newWhisky = { name, age: parseInt(age), distillery, type, region };", "    addWhisky(newWhisky);", "    setName('');", "    setAge('');", "    setDistillery('');", "    setType('');", "    setRegion('');", "  };", "", "  return (", "    <Box sx={{ display: 'flex', flexDirection: 'column', gap: 2 }}>", "      <Typography variant=\"h4\" gutterBottom>", "        Add New Whisky", "      </Typography>", "      <TextField label=\"Name\" value={name} onChange={(e) => setName(e.target.value)} />", "      <TextField label=\"Alter\" type=\"number\" value={age} onChange={(e) => setAge(e.target.value)} />", "      <TextField label=\"Destillerie\" value={distillery} onChange={(e) => setDistillery(e.target.value)} />", "      <TextField label=\"Typ\" value={type} onChange={(e) => setType(e.target.value)} />", "      <TextField label=\"Region\" value={region} onChange={(e) => setRegion(e.target.value)} />", "      <Button variant=\"contained\" color=\"primary\" onClick={handleSubmit}>Add Whisky</Button>", "    </Box>", "  );", "};", "", "export default AddWhisky;"], "file_path": "src/components/AddWhisky.js"}
{"Link_to_commit": "https://github.com/Molly6943/tomato-clock/commit/2a65ca72160de8a254460fa4d9de2d56d60de445", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 13, "n_files_impacted": 6, "longest_chunk": ["export type TimerConfig = {", "  workMinutes: number;", "  breakMinutes: number;", "  numberOfRounds: number;", "  autoStartBreak: boolean;", "  autoStartWork: boolean;", "};", "", "export type TimerSound = {", "  name: string;", "  file: string;", "  label: string;", "};"], "file_path": "app/_types.ts"}
{"Link_to_commit": "https://github.com/amm926616/bin/commit/93b602ba54d30cc39ed691296aa72e149670e021", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 30, "n_files_impacted": 5, "longest_chunk": ["import sqlite3", "import re", "", "def insert_markdown_into_db(markdown_file, db_file):", "    # Connect to the SQLite database", "    conn = sqlite3.connect(db_file)", "    cursor = conn.cursor()", "", "    # Read the markdown file", "    with open(markdown_file, 'r', encoding='utf-8') as file:", "        content = file.read()", "", "    # Split the content into sections based on headings", "    sections = re.split(r'# ', content)[1:]", "", "    for section in sections:", "        lines = section.split('\\n')", "        word = lines[0].strip()", "        meanings = [line.strip('- ').strip() for line in lines[1:] if line.strip()]", "", "        for meaning in meanings:", "            cursor.execute('INSERT INTO vocabulary (word, meaning) VALUES (?, ?)', (word, meaning))", "", "    # Commit the changes and close the connection", "    conn.commit()", "    conn.close()", "    print(f'Successfully inserted data from {markdown_file} into {db_file}')", "", "# Usage", "insert_markdown_into_db('vocabulary.md', 'vocabulary.db')"], "file_path": "vmanager.py"}
{"Link_to_commit": "https://github.com/Rangassamy/chronoJS/commit/61f4fc2881a80841b44c5e8f26709262ad3823ff", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 33, "n_files_impacted": 3, "longest_chunk": ["const formChrono = document.querySelector(\"form\");", "const inputChrono = document.querySelector(\"input\");", "const renderChrono = document.querySelector(\"h2\");", "", "formChrono.addEventListener(\"submit\", (e) => {", "  e.preventDefault();", "  inputChrono.value = \"\";", "  if (inputChrono.value <= 0) {", "      return alert(\"Veuillez utiliser minimum le chiffre 1 !\");", "    }", "    playChrono(inputChrono.value);", "});", "", "function playChrono(value) {", "  let minutes = parseInt(value);", "  let seconds = 0;", "", "  const intervalId = setInterval(() => {", "    if (minutes === 0 && seconds === 0) {", "      clearInterval(intervalId);", "      return alert(\"finish !\");", "    } else if (seconds === 0) {", "      minutes -= 1;", "      seconds = 59;", "    } else {", "      seconds -= 1;", "    }", "", "    renderChrono.textContent = `${minutes} : ${", "      seconds < 10 ? \"0\" + seconds : seconds", "    }`;", "  }, 1000);", "}"], "file_path": "index.js"}
{"Link_to_commit": "https://github.com/danblanc/latam/commit/12d5c4c134a460ebb0bb4fc0b6e02e222f2a41fc", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 6, "longest_chunk": ["    \"\"\"", "    Analyzes a JSON file containing tweet data to find the top 10 dates with the most active users.", "", "    Parameters:", "    - file_path (str): The path to the JSON file containing tweet data.", "", "    Returns:", "    - List[Tuple[datetime.date, str]]: A list of tuples, each containing a date and the username", "      of the most active user on that date, sorted by the number of tweets in descending order.", "      Only the top 10 dates are included.", "", "    The JSON file should have a structure where each line is a JSON object with at least", "    'date' and 'user' keys, where 'user' is an object containing a 'username' key.", "    \"\"\"", "", "    # Initialize a dictionary to count tweets per user per date"], "file_path": "src/q1_memory.py"}
{"Link_to_commit": "https://github.com/diogoflduarte/spikechart/commit/24cb76c76c2c2fc6ded7584905274565114cff2d", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 40, "n_files_impacted": 1, "longest_chunk": ["    dbc.Container(", "        dbc.Row(", "            [dbc.Col(", "                dcc.Upload(", "                    id='upload-spike-times',", "                    children=html.Div(['Drop SPIKETIMES or ', html.A('Select Files')]),", "                    style={", "                        'width': '20%', 'height': '40px', 'lineHeight': '40px',", "                        'borderWidth': '1px', 'borderStyle': 'dashed', 'borderRadius': '5px',", "                        'textAlign': 'center', 'margin': '10px'", "                    },", "                    multiple=False", "                )", "            ),", "            dbc.Col(", "                dcc.Upload(", "                    id='upload-spike-clusters',", "                    children=html.Div(['Drop SPIKECLUSTERS or ', html.A('Select Files')]),", "                    style={", "                        'width': '20%', 'height': '40px', 'lineHeight': '40px',", "                        'borderWidth': '1px', 'borderStyle': 'dashed', 'borderRadius': '5px',", "                        'textAlign': 'center', 'margin': '10px'", "                    },", "                    multiple=False", "                )", "            ),", "            dbc.Col(", "                dcc.Upload(", "                    id='upload-behavior',", "                    children=html.Div(['Drop BEHAVIOR or ', html.A('Select Files')]),", "                    style={", "                        'width': '20%', 'height': '40px', 'lineHeight': '40px',", "                        'borderWidth': '1px', 'borderStyle': 'dashed', 'borderRadius': '5px',", "                        'textAlign': 'center', 'margin': '10px'", "                    },", "                    multiple=False", "                )", "            )]", "        )", "    ),"], "file_path": "simple_raster.py"}
{"Link_to_commit": "https://github.com/TriedAngle/Kette/commit/1998d52433deb3c3cf6805d9f8d767611cde4e5e", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 13, "n_files_impacted": 6, "longest_chunk": ["#include <stdio.h>", "#include \"utilities.h\"", "", "int main() {", "    print_welcome_message();", "    int sum = add(5, 3);", "    printf(\"Sum of 5 and 3 is: %d\\n\", sum);", "    ", "    Fraction fraction = {10, 2};", "    double division = divide(fraction);", "    printf(\"Division of 10 by 2 is: %.2f\\n\", division);", "    return 0;", "}"], "file_path": "src/main.c"}
{"Link_to_commit": "https://github.com/EditwithLak/Tutorial-Manager/commit/1b65474c1152dfe2ad10ad5ddf5a7d9f36d40bcd", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 411, "n_files_impacted": 4, "longest_chunk": ["import tkinter as tk\r", "from tkinter import ttk, filedialog, messagebox\r", "import os\r", "import webbrowser\r", "import json\r", "import subprocess\r", "import requests\r", "from bs4 import BeautifulSoup\r", "import yt_dlp\r", "\r", "# Data storage - Global Varieables\r", "BG_COLOR = \"#f5f5f5\"\r", "LABEL_FONT = (\"Arial\", 10)\r", "HEADER_FONT = (\"Arial\", 11, \"bold\")\r", "DATA_FILE = \"tutorials.json\"\r", "tutorials = []\r", "tags_set = set()\r", "categories_set = set()\r", "last_selected_index = None\r", "current_tags_list = []  # holds tags added one-by-one from dropdown\r", "\r", "# ----------------------- Tooltip Helper -----------------------\r", "def add_tooltip(widget, text):\r", "    def on_enter(e):\r", "        tooltip = tk.Toplevel(widget)\r", "        tooltip.wm_overrideredirect(True)\r", "        tooltip.geometry(f\"+{e.x_root+10}+{e.y_root+10}\")\r", "        label = tk.Label(tooltip, text=text, background=\"#ffffe0\", relief=\"solid\", borderwidth=1, font=(\"Arial\", 8))\r", "        label.pack()\r", "        widget.tooltip = tooltip\r", "    def on_leave(e):\r", "        if hasattr(widget, \"tooltip\"):\r", "            widget.tooltip.destroy()\r", "    widget.bind(\"<Enter>\", on_enter)\r", "    widget.bind(\"<Leave>\", on_leave)\r", "\r", "# ----------------------- Persistence -----------------------\r", "def load_data():\r", "    global tutorials, tags_set, categories_set\r", "    if os.path.exists(DATA_FILE):\r", "        with open(DATA_FILE, \"r\") as f:\r", "            tutorials = json.load(f)\r", "            tags_set = set()\r", "            categories_set = set()\r", "            for tut in tutorials:\r", "                tags_set.update(tut.get(\"tags\", []))\r", "                categories_set.add(tut.get(\"category\", \"\"))\r", "\r", "def save_data():\r", "    with open(DATA_FILE, \"w\") as f:\r", "        json.dump(tutorials, f, indent=4)\r", "\r", "# ----------------------- Utility -----------------------\r", "def update_dropdowns():\r", "    cat_combo['values'] = sorted(categories_set)\r", "    tag_combo['values'] = sorted(tags_set)\r", "\r", "def update_listbox():\r", "    tutorial_listbox.delete(0, tk.END)\r", "    for i, tut in enumerate(tutorials):\r", "        tag_display = \", \".join(tut.get(\"tags\", []))\r", "        tutorial_listbox.insert(tk.END, f\"{tut['title']} [{tut['category']}] ({tag_display})\")\r", "\r", "# ----------------------- Add/Edit/Delete -----------------------\r", "def add_tutorial():\r", "    title = title_var.get().strip()\r", "    category = category_var.get().strip()\r", "    path = path_var.get().strip()\r", "    tags = current_tags_list.copy()\r", "\r", "    if not title or not category or not path:\r", "        messagebox.showerror(\"Error\", \"Please fill in all required fields.\")\r", "        return\r", "\r", "    categories_set.add(category)\r", "    tags_set.update(tags)\r", "\r", "    tutorials.append({\r", "        \"title\": title,\r", "        \"category\": category,\r", "        \"path\": path,\r", "        \"type\": \"local\" if os.path.exists(path) else \"youtube\",\r", "        \"tags\": tags\r", "    })\r", "    update_listbox()\r", "    update_dropdowns()\r", "    save_data()\r", "\r", "    title_var.set(\"\")\r", "    category_var.set(\"\")\r", "    path_var.set(\"\")\r", "    tag_var.set(\"\")\r", "    \r", "    # \u2705 Clear selected tags list visually and in memory\r", "    current_tags_list.clear()\r", "    tag_listbox.delete(0, tk.END)\r", "\r", "def edit_selected():\r", "    index = get_selected_index()\r", "    if index is None:\r", "        return\r", "\r", "    # Log current values (for debugging)\r", "    print(\"Editing index:\", index)\r", "    print(\"Before:\", tutorials[index])\r", "\r", "    title = title_var.get().strip()\r", "    category = category_var.get().strip()\r", "    path = path_var.get().strip()\r", "    tags = list(tag_listbox.get(0, tk.END))  # get only the tags currently shown\r", "\r", "\r", "    if not title or not category or not path:\r", "        messagebox.showerror(\"Error\", \"Please fill in all required fields.\")\r", "        return\r", "\r", "    tutorials[index] = {\r", "        \"title\": title,\r", "        \"category\": category,\r", "        \"path\": path,\r", "        \"type\": \"local\" if os.path.exists(path) else \"youtube\",\r", "        \"tags\": tags\r", "    }\r", "\r", "    save_data()\r", "    update_listbox()\r", "    update_dropdowns()\r", "    title_var.set(\"\")\r", "    category_var.set(\"\")\r", "    path_var.set(\"\")\r", "    tag_var.set(\"\")\r", "    \r", "    # \u2705 Clear selected tags list visually and in memory\r", "    current_tags_list.clear()\r", "    tag_listbox.delete(0, tk.END)\r", "\r", "    print(\"After:\", tutorials[index])\r", "    messagebox.showinfo(\"Edit Successful\", f\"Tutorial \\\"{title}\\\" was updated.\")\r", "\r", "def delete_selected():\r", "    index = get_selected_index()\r", "    if index is None:\r", "        return\r", "    if messagebox.askyesno(\"Confirm Delete\", \"Are you sure you want to delete this tutorial?\"):\r", "        tutorials.pop(index)\r", "        update_listbox()\r", "        save_data()\r", "\r", "# ----------------------- Helper -----------------------\r", "def get_selected_index():\r", "    selected = tutorial_listbox.curselection()\r", "    if selected:\r", "        return selected[0]\r", "    if last_selected_index is not None:\r", "        return last_selected_index\r", "    messagebox.showinfo(\"Select Tutorial\", \"Please select a tutorial.\")\r", "    return None\r", "\r", "def on_listbox_select(event):\r", "    global last_selected_index\r", "    selected = tutorial_listbox.curselection()\r", "    if not selected:\r", "        return  # Exit early if no selection\r", "\r", "    last_selected_index = selected[0]\r", "    tut = tutorials[last_selected_index]\r", "    title_var.set(tut['title'])\r", "    category_var.set(tut['category'])\r", "    path_var.set(tut['path'])\r", "    tag_var.set(\", \".join(tut.get(\"tags\", [])))\r", "\r", "    current_tags_list.clear()\r", "    tag_listbox.delete(0, tk.END)\r", "\r", "    for tag in tut.get(\"tags\", []):\r", "        current_tags_list.append(tag)\r", "        tag_listbox.insert(tk.END, tag)\r", "\r", "def browse_file():\r", "    path = filedialog.askopenfilename(filetypes=[(\"Video Files\", \"*.mp4 *.mov *.avi *.mkv\")])\r", "    if path:\r", "        path_var.set(path)\r", "\r", "def preview_selected():\r", "    index = get_selected_index()\r", "    if index is None:\r", "        return\r", "    path = tutorials[index]['path']\r", "    video_title_label.config(text=f\"Now Playing: {tutorials[index]['title']}\")\r", "    if tutorials[index]['type'] == 'youtube':\r", "        webbrowser.open(path)\r", "    else:\r", "        try:\r", "            subprocess.Popen(['start', '', path], shell=True)\r", "        except Exception as e:\r", "            messagebox.showerror(\"Error\", f\"Could not open video: {e}\")\r", "\r", "# ----------------------- Add Tag/Category -----------------------\r", "def add_tag():\r", "    new_tags = tag_var.get().strip()\r", "    if new_tags:\r", "        for tag in new_tags.split(','):\r", "            tag = tag.strip()\r", "            if tag:\r", "                tags_set.add(tag)\r", "        update_dropdowns()\r", "        tag_var.set(\"\")\r", "\r", "def add_category():\r", "    new_cat = category_var.get().strip()\r", "    if new_cat:\r", "        categories_set.add(new_cat)\r", "        update_dropdowns()\r", "        category_var.set(\"\")\r", "        \r", "def add_tag_to_list():\r", "    tag = tag_var.get().strip()\r", "    if tag and tag not in current_tags_list:\r", "        current_tags_list.append(tag)\r", "        tag_listbox.insert(tk.END, tag)\r", "        tag_var.set(\"\")\r", "\r", "def remove_selected_tag():\r", "    selected = tag_listbox.curselection()\r", "    if selected:\r", "        index = selected[0]\r", "        current_tags_list.pop(index)\r", "        tag_listbox.delete(index)\r", "\r", "\r", "# ----------------------- Search -----------------------\r", "def search_tutorials():\r", "    keyword = search_var.get().strip().lower()\r", "    filter_by = search_by_var.get()\r", "    tutorial_listbox.delete(0, tk.END)\r", "    for tut in tutorials:\r", "        if filter_by == \"Title\" and keyword in tut['title'].lower():\r", "            insert_filtered(tut)\r", "        elif filter_by == \"Category\" and keyword in tut['category'].lower():\r", "            insert_filtered(tut)\r", "        elif filter_by == \"Tag\" and any(keyword in t.lower() for t in tut.get('tags', [])):\r", "            insert_filtered(tut)\r", "\r", "def insert_filtered(tut):\r", "    tag_display = \", \".join(tut.get(\"tags\", []))\r", "    tutorial_listbox.insert(tk.END, f\"{tut['title']} [{tut['category']}] ({tag_display})\")\r", "\r", "# ----------------------- URL Metadata Fetch -----------------------\r", "\r", "def fetch_info_from_url():\r", "    url = path_var.get().strip()\r", "    if not url:\r", "        messagebox.showwarning(\"No URL\", \"Please enter a URL or path first.\")\r", "        return\r", "\r", "    # YouTube/Vimeo via yt_dlp\r", "    if \"youtube.com\" in url or \"youtu.be\" in url or \"vimeo.com\" in url:\r", "        try:\r", "            ydl_opts = {'quiet': True, 'skip_download': True}\r", "            with yt_dlp.YoutubeDL(ydl_opts) as ydl:\r", "                info = ydl.extract_info(url, download=False)\r", "                title_var.set(info.get(\"title\", \"\"))\r", "                messagebox.showinfo(\"Success\", \"Fetched info from video successfully.\")\r", "        except Exception as e:\r", "            messagebox.showerror(\"Error\", f\"Failed to fetch video info: {e}\")\r", "    else:\r", "        # Try generic page title\r", "        try:\r", "            response = requests.get(url, timeout=5)\r", "            soup = BeautifulSoup(response.text, \"html.parser\")\r", "            title = soup.title.string.strip() if soup.title else \"\"\r", "            title_var.set(title)\r", "            messagebox.showinfo(\"Success\", \"Fetched page title successfully.\")\r", "        except Exception as e:\r", "            messagebox.showerror(\"Error\", f\"Failed to fetch webpage info: {e}\")\r", "            \r", "# ----------------------- Rest for Function -----------------------\r", "def clear_form(event=None):\r", "    global last_selected_index\r", "    last_selected_index = None\r", "    title_var.set(\"\")\r", "    category_var.set(\"\")\r", "    path_var.set(\"\")\r", "    tag_var.set(\"\")\r", "    current_tags_list.clear()\r", "    tag_listbox.delete(0, tk.END)\r", "\r", "\r", "# ----------------------- UI -----------------------\r", "root = tk.Tk()\r", "root.configure(bg=BG_COLOR)\r", "root.title(\"\ud83c\udf93 Tutorial Manager\")\r", "root.geometry(\"800x600\")\r", "\r", "# --- Input Frame ---\r", "input_frame = tk.LabelFrame(root, text=\"\ud83d\udee0\ufe0f Add / Edit Tutorial\", padx=10, pady=10, font=HEADER_FONT, bg=BG_COLOR)\r", "input_frame.pack(fill=\"x\", padx=10, pady=5)\r", "\r", "title_var = tk.StringVar()\r", "category_var = tk.StringVar()\r", "path_var = tk.StringVar()\r", "tag_var = tk.StringVar()\r", "\r", "# Title\r", "tk.Label(input_frame, text=\"Title:\").grid(row=0, column=0, sticky=\"e\")\r", "title_entry = tk.Entry(input_frame, textvariable=title_var, width=30)\r", "title_entry.grid(row=0, column=1, padx=5)\r", "\r", "# Category with dropdown\r", "tk.Label(input_frame, text=\"Category:\").grid(row=1, column=0, sticky=\"e\")\r", "cat_combo = ttk.Combobox(input_frame, textvariable=category_var, width=28)\r", "cat_combo.grid(row=1, column=1, padx=5)\r", "tk.Button(input_frame, text=\"+\", width=2, command=add_category).grid(row=1, column=2)\r", "\r", "# Tags with dropdown\r", "tk.Label(input_frame, text=\"Tags (comma-separated):\").grid(row=2, column=0, sticky=\"e\")\r", "tag_combo = ttk.Combobox(input_frame, textvariable=tag_var, width=28)\r", "tag_combo.grid(row=2, column=1, padx=5)\r", "add_tooltip(tag_combo, \"Type or select a tag. Use commas for multiple.\")\r", "tk.Button(input_frame, text=\"+\", width=2, command=add_tag).grid(row=2, column=2)\r", "add_tag_button = tk.Button(input_frame, text=\"Add Tag to List\", command=add_tag_to_list)\r", "add_tag_button.grid(row=2, column=3, padx=5)\r", "add_tooltip(add_tag_button, \"Click to add the selected tag to the list below.\")\r", "\r", "# Tag Listbox to display added tags\r", "tk.Label(input_frame, text=\"Selected Tags:\").grid(row=5, column=0, sticky=\"ne\")\r", "tag_listbox = tk.Listbox(input_frame, height=4)\r", "add_tooltip(tag_listbox, \"These are the tags currently added to this tutorial.\")\r", "tag_listbox.grid(row=5, column=1, columnspan=2, sticky=\"we\")\r", "remove_tag_button = tk.Button(input_frame, text=\"Remove Selected Tag\", command=remove_selected_tag)\r", "remove_tag_button.grid(row=5, column=3)\r", "add_tooltip(remove_tag_button, \"Removes the selected tag from the list.\")\r", "\r", "# Path\r", "tk.Label(input_frame, text=\"Path or URL:\").grid(row=3, column=0, sticky=\"e\")\r", "path_entry = tk.Entry(input_frame, textvariable=path_var, width=30)\r", "path_entry.grid(row=3, column=1, padx=5, sticky=\"w\")\r", "browse_button = tk.Button(input_frame, text=\"\ud83d\udcc2\", command=browse_file)\r", "browse_button.grid(row=3, column=2)\r", "fetch_info_button = tk.Button(input_frame, text=\"\ud83d\udd0d Fetch Info\", command=fetch_info_from_url)\r", "fetch_info_button.grid(row=3, column=3)\r", "add_tooltip(fetch_info_button, \"Fetch the title from a YouTube/Vimeo or website URL.\")\r", "\r", "# Buttons\r", "add_button = tk.Button(input_frame, text=\"\u2795 Add\", width=10, command=add_tutorial)\r", "add_button.grid(row=4, column=0, pady=5)\r", "\r", "edit_button = tk.Button(input_frame, text=\"\u270f\ufe0f Edit\", width=10, command=edit_selected)\r", "edit_button.grid(row=4, column=1, pady=5)\r", "\r", "delete_button = tk.Button(input_frame, text=\"\u274c Delete\", width=10, command=delete_selected)\r", "delete_button.grid(row=4, column=2, pady=5)\r", "\r", "# --- Search Frame ---\r", "search_frame = tk.Frame(root)\r", "search_frame.pack(fill=\"x\", padx=10, pady=5)\r", "\r", "search_var = tk.StringVar()\r", "search_by_var = tk.StringVar(value=\"Title\")\r", "search_entry = tk.Entry(search_frame, textvariable=search_var, width=30)\r", "search_entry.pack(side=\"left\", padx=5)\r", "\r", "tk.OptionMenu(search_frame, search_by_var, \"Title\", \"Category\", \"Tag\").pack(side=\"left\")\r", "search_frame = tk.LabelFrame(root, text=\"\ud83d\udd0d Search Tutorials\", font=HEADER_FONT, padx=10, pady=5, bg=BG_COLOR)\r", "\r", "# --- Tutorial List ---\r", "list_frame = tk.LabelFrame(root, text=\"\ud83d\udcda Tutorials\", font=HEADER_FONT, bg=BG_COLOR)\r", "list_frame.pack(fill=\"both\", expand=True, padx=10, pady=5)\r", "\r", "tutorial_listbox = tk.Listbox(list_frame, height=12)\r", "tutorial_listbox.pack(side=\"left\", fill=\"both\", expand=True)\r", "tutorial_listbox.bind(\"<<ListboxSelect>>\", on_listbox_select)\r", "\r", "scrollbar = tk.Scrollbar(list_frame)\r", "scrollbar.pack(side=\"right\", fill=\"y\")\r", "tutorial_listbox.config(yscrollcommand=scrollbar.set)\r", "scrollbar.config(command=tutorial_listbox.yview)\r", "\r", "# --- Preview ---\r", "preview_frame = tk.Frame(root)\r", "preview_frame.pack(fill=\"x\", padx=10)\r", "tk.Button(preview_frame, text=\"\u25b6 Preview\", width=25, command=preview_selected).pack(pady=5)\r", "video_title_label = tk.Label(preview_frame, text=\"Now Playing: None\", font=(\"Arial\", 10, \"italic\"))\r", "video_title_label.pack()\r", "\r", "add_tooltip(add_button, \"Save a new tutorial using the form above.\")\r", "add_tooltip(edit_button, \"Apply changes to the selected tutorial.\")\r", "add_tooltip(delete_button, \"Remove the selected tutorial permanently.\")\r", "add_tooltip(search_entry, \"Search tutorials by title, category, or tag.\")\r", "\r", "\r", "# Run\r", "load_data()\r", "update_listbox()\r", "update_dropdowns()\r", "\r", "def is_click_outside_widgets(event):\r", "    widget = root.winfo_containing(event.x_root, event.y_root)\r", "    allowed_widgets = [\r", "        tutorial_listbox, search_entry,\r", "        title_entry, cat_combo, tag_combo,\r", "        path_entry, tag_listbox,\r", "        add_button, edit_button, delete_button,\r", "        browse_button, fetch_info_button,\r", "        add_tag_button, remove_tag_button\r", "    ]\r", "    if widget not in allowed_widgets:\r", "        clear_form()\r", "        \r", "root.bind(\"<Button-1>\", is_click_outside_widgets)\r", "root.mainloop()\r"], "file_path": "tutorial_manager.py"}
{"Link_to_commit": "https://github.com/thahn1230/llama.cpp_specinfer_awq/commit/84a44815f704aaed8e8edec7a39e846a975c7ba9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["    // auto enable conversation mode if chat template is available", "    const bool has_chat_template = !common_get_builtin_chat_template(model).empty() || !params.chat_template.empty();", "    if (params.conversation_mode == COMMON_CONVERSATION_MODE_AUTO) {", "        if (has_chat_template) {", "            LOG_INF(\"%s: chat template is available, enabling conversation mode (disable it with -no-cnv)\\n\", __func__);", "            params.conversation_mode = COMMON_CONVERSATION_MODE_ENABLED;", "        } else {", "            params.conversation_mode = COMMON_CONVERSATION_MODE_DISABLED;", "        }", "    }", "", "    // in case user force-activate conversation mode (via -cnv) without proper chat template, we show a warning", "    if (params.conversation_mode && !has_chat_template) {", "        LOG_WRN(\"%s: chat template is not available or is not supported. This may cause the model to output suboptimal responses\\n\", __func__);", "    }", ""], "file_path": "examples/main/main.cpp"}
{"Link_to_commit": "https://github.com/elementalcollision/MoE_Llama_Updates/commit/84a44815f704aaed8e8edec7a39e846a975c7ba9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["    // auto enable conversation mode if chat template is available", "    const bool has_chat_template = !common_get_builtin_chat_template(model).empty() || !params.chat_template.empty();", "    if (params.conversation_mode == COMMON_CONVERSATION_MODE_AUTO) {", "        if (has_chat_template) {", "            LOG_INF(\"%s: chat template is available, enabling conversation mode (disable it with -no-cnv)\\n\", __func__);", "            params.conversation_mode = COMMON_CONVERSATION_MODE_ENABLED;", "        } else {", "            params.conversation_mode = COMMON_CONVERSATION_MODE_DISABLED;", "        }", "    }", "", "    // in case user force-activate conversation mode (via -cnv) without proper chat template, we show a warning", "    if (params.conversation_mode && !has_chat_template) {", "        LOG_WRN(\"%s: chat template is not available or is not supported. This may cause the model to output suboptimal responses\\n\", __func__);", "    }", ""], "file_path": "examples/main/main.cpp"}
{"Link_to_commit": "https://github.com/simonholm/llm-hello-new/commit/84a44815f704aaed8e8edec7a39e846a975c7ba9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["    // auto enable conversation mode if chat template is available", "    const bool has_chat_template = !common_get_builtin_chat_template(model).empty() || !params.chat_template.empty();", "    if (params.conversation_mode == COMMON_CONVERSATION_MODE_AUTO) {", "        if (has_chat_template) {", "            LOG_INF(\"%s: chat template is available, enabling conversation mode (disable it with -no-cnv)\\n\", __func__);", "            params.conversation_mode = COMMON_CONVERSATION_MODE_ENABLED;", "        } else {", "            params.conversation_mode = COMMON_CONVERSATION_MODE_DISABLED;", "        }", "    }", "", "    // in case user force-activate conversation mode (via -cnv) without proper chat template, we show a warning", "    if (params.conversation_mode && !has_chat_template) {", "        LOG_WRN(\"%s: chat template is not available or is not supported. This may cause the model to output suboptimal responses\\n\", __func__);", "    }", ""], "file_path": "examples/main/main.cpp"}
{"Link_to_commit": "https://github.com/Aloereed/llama.cpp-ohos/commit/84a44815f704aaed8e8edec7a39e846a975c7ba9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["    // auto enable conversation mode if chat template is available", "    const bool has_chat_template = !common_get_builtin_chat_template(model).empty() || !params.chat_template.empty();", "    if (params.conversation_mode == COMMON_CONVERSATION_MODE_AUTO) {", "        if (has_chat_template) {", "            LOG_INF(\"%s: chat template is available, enabling conversation mode (disable it with -no-cnv)\\n\", __func__);", "            params.conversation_mode = COMMON_CONVERSATION_MODE_ENABLED;", "        } else {", "            params.conversation_mode = COMMON_CONVERSATION_MODE_DISABLED;", "        }", "    }", "", "    // in case user force-activate conversation mode (via -cnv) without proper chat template, we show a warning", "    if (params.conversation_mode && !has_chat_template) {", "        LOG_WRN(\"%s: chat template is not available or is not supported. This may cause the model to output suboptimal responses\\n\", __func__);", "    }", ""], "file_path": "examples/main/main.cpp"}
{"Link_to_commit": "https://github.com/webdevsuk/execd/commit/84a44815f704aaed8e8edec7a39e846a975c7ba9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["    // auto enable conversation mode if chat template is available", "    const bool has_chat_template = !common_get_builtin_chat_template(model).empty() || !params.chat_template.empty();", "    if (params.conversation_mode == COMMON_CONVERSATION_MODE_AUTO) {", "        if (has_chat_template) {", "            LOG_INF(\"%s: chat template is available, enabling conversation mode (disable it with -no-cnv)\\n\", __func__);", "            params.conversation_mode = COMMON_CONVERSATION_MODE_ENABLED;", "        } else {", "            params.conversation_mode = COMMON_CONVERSATION_MODE_DISABLED;", "        }", "    }", "", "    // in case user force-activate conversation mode (via -cnv) without proper chat template, we show a warning", "    if (params.conversation_mode && !has_chat_template) {", "        LOG_WRN(\"%s: chat template is not available or is not supported. This may cause the model to output suboptimal responses\\n\", __func__);", "    }", ""], "file_path": "examples/main/main.cpp"}
{"Link_to_commit": "https://github.com/Mel-b3002fA/llama/commit/84a44815f704aaed8e8edec7a39e846a975c7ba9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["    // auto enable conversation mode if chat template is available", "    const bool has_chat_template = !common_get_builtin_chat_template(model).empty() || !params.chat_template.empty();", "    if (params.conversation_mode == COMMON_CONVERSATION_MODE_AUTO) {", "        if (has_chat_template) {", "            LOG_INF(\"%s: chat template is available, enabling conversation mode (disable it with -no-cnv)\\n\", __func__);", "            params.conversation_mode = COMMON_CONVERSATION_MODE_ENABLED;", "        } else {", "            params.conversation_mode = COMMON_CONVERSATION_MODE_DISABLED;", "        }", "    }", "", "    // in case user force-activate conversation mode (via -cnv) without proper chat template, we show a warning", "    if (params.conversation_mode && !has_chat_template) {", "        LOG_WRN(\"%s: chat template is not available or is not supported. This may cause the model to output suboptimal responses\\n\", __func__);", "    }", ""], "file_path": "examples/main/main.cpp"}
{"Link_to_commit": "https://github.com/karminski/llama-cpp/commit/84a44815f704aaed8e8edec7a39e846a975c7ba9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["    // auto enable conversation mode if chat template is available", "    const bool has_chat_template = !common_get_builtin_chat_template(model).empty() || !params.chat_template.empty();", "    if (params.conversation_mode == COMMON_CONVERSATION_MODE_AUTO) {", "        if (has_chat_template) {", "            LOG_INF(\"%s: chat template is available, enabling conversation mode (disable it with -no-cnv)\\n\", __func__);", "            params.conversation_mode = COMMON_CONVERSATION_MODE_ENABLED;", "        } else {", "            params.conversation_mode = COMMON_CONVERSATION_MODE_DISABLED;", "        }", "    }", "", "    // in case user force-activate conversation mode (via -cnv) without proper chat template, we show a warning", "    if (params.conversation_mode && !has_chat_template) {", "        LOG_WRN(\"%s: chat template is not available or is not supported. This may cause the model to output suboptimal responses\\n\", __func__);", "    }", ""], "file_path": "examples/main/main.cpp"}
{"Link_to_commit": "https://github.com/shibizhao/llama-fpga/commit/84a44815f704aaed8e8edec7a39e846a975c7ba9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["    // auto enable conversation mode if chat template is available", "    const bool has_chat_template = !common_get_builtin_chat_template(model).empty() || !params.chat_template.empty();", "    if (params.conversation_mode == COMMON_CONVERSATION_MODE_AUTO) {", "        if (has_chat_template) {", "            LOG_INF(\"%s: chat template is available, enabling conversation mode (disable it with -no-cnv)\\n\", __func__);", "            params.conversation_mode = COMMON_CONVERSATION_MODE_ENABLED;", "        } else {", "            params.conversation_mode = COMMON_CONVERSATION_MODE_DISABLED;", "        }", "    }", "", "    // in case user force-activate conversation mode (via -cnv) without proper chat template, we show a warning", "    if (params.conversation_mode && !has_chat_template) {", "        LOG_WRN(\"%s: chat template is not available or is not supported. This may cause the model to output suboptimal responses\\n\", __func__);", "    }", ""], "file_path": "examples/main/main.cpp"}
{"Link_to_commit": "https://github.com/monkey531/llama.cpp/commit/d7e22f72c8690d15dca8433e4faa70eed44305a7", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["    // auto enable conversation mode if chat template is available", "    const bool has_chat_template = !common_get_builtin_chat_template(model).empty() || !params.chat_template.empty();", "    if (params.conversation_mode == COMMON_CONVERSATION_MODE_AUTO) {", "        if (has_chat_template) {", "            LOG_INF(\"%s: chat template is available, enabling conversation mode (disable it with -no-cnv)\\n\", __func__);", "            params.conversation_mode = COMMON_CONVERSATION_MODE_ENABLED;", "        } else {", "            params.conversation_mode = COMMON_CONVERSATION_MODE_DISABLED;", "        }", "    }", "", "    // in case user force-activate conversation mode (via -cnv) without proper chat template, we show a warning", "    if (params.conversation_mode && !has_chat_template) {", "        LOG_WRN(\"%s: chat template is not available or is not supported. This may cause the model to output suboptimal responses\\n\", __func__);", "    }", ""], "file_path": "examples/main/main.cpp"}
{"Link_to_commit": "https://github.com/ruribe17/llama.cpp/commit/84a44815f704aaed8e8edec7a39e846a975c7ba9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["    // auto enable conversation mode if chat template is available", "    const bool has_chat_template = !common_get_builtin_chat_template(model).empty() || !params.chat_template.empty();", "    if (params.conversation_mode == COMMON_CONVERSATION_MODE_AUTO) {", "        if (has_chat_template) {", "            LOG_INF(\"%s: chat template is available, enabling conversation mode (disable it with -no-cnv)\\n\", __func__);", "            params.conversation_mode = COMMON_CONVERSATION_MODE_ENABLED;", "        } else {", "            params.conversation_mode = COMMON_CONVERSATION_MODE_DISABLED;", "        }", "    }", "", "    // in case user force-activate conversation mode (via -cnv) without proper chat template, we show a warning", "    if (params.conversation_mode && !has_chat_template) {", "        LOG_WRN(\"%s: chat template is not available or is not supported. This may cause the model to output suboptimal responses\\n\", __func__);", "    }", ""], "file_path": "examples/main/main.cpp"}
{"Link_to_commit": "https://github.com/MuhammadAmmarAtique/Learning_React/commit/503272ac980a89636008ede4e76b566d1234e318", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 9, "n_files_impacted": 2, "longest_chunk": ["", "    //reducer 3", "    updateTodo: (state, action) => {", "      const { id, newText } = action.payload;", "      const todoToUpdate = state.todos.find((todo) => todo.id === id);", "      if (todoToUpdate) {", "        todoToUpdate.text = newText;", "      }", "    },"], "file_path": "11_reduxToolkit/src/features/todo/todoSlice.js"}
{"Link_to_commit": "https://github.com/airen3339/llama.cpp/commit/84a44815f704aaed8e8edec7a39e846a975c7ba9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["    // auto enable conversation mode if chat template is available", "    const bool has_chat_template = !common_get_builtin_chat_template(model).empty() || !params.chat_template.empty();", "    if (params.conversation_mode == COMMON_CONVERSATION_MODE_AUTO) {", "        if (has_chat_template) {", "            LOG_INF(\"%s: chat template is available, enabling conversation mode (disable it with -no-cnv)\\n\", __func__);", "            params.conversation_mode = COMMON_CONVERSATION_MODE_ENABLED;", "        } else {", "            params.conversation_mode = COMMON_CONVERSATION_MODE_DISABLED;", "        }", "    }", "", "    // in case user force-activate conversation mode (via -cnv) without proper chat template, we show a warning", "    if (params.conversation_mode && !has_chat_template) {", "        LOG_WRN(\"%s: chat template is not available or is not supported. This may cause the model to output suboptimal responses\\n\", __func__);", "    }", ""], "file_path": "examples/main/main.cpp"}
{"Link_to_commit": "https://github.com/Fahlevi20/llama.cpp/commit/84a44815f704aaed8e8edec7a39e846a975c7ba9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["    // auto enable conversation mode if chat template is available", "    const bool has_chat_template = !common_get_builtin_chat_template(model).empty() || !params.chat_template.empty();", "    if (params.conversation_mode == COMMON_CONVERSATION_MODE_AUTO) {", "        if (has_chat_template) {", "            LOG_INF(\"%s: chat template is available, enabling conversation mode (disable it with -no-cnv)\\n\", __func__);", "            params.conversation_mode = COMMON_CONVERSATION_MODE_ENABLED;", "        } else {", "            params.conversation_mode = COMMON_CONVERSATION_MODE_DISABLED;", "        }", "    }", "", "    // in case user force-activate conversation mode (via -cnv) without proper chat template, we show a warning", "    if (params.conversation_mode && !has_chat_template) {", "        LOG_WRN(\"%s: chat template is not available or is not supported. This may cause the model to output suboptimal responses\\n\", __func__);", "    }", ""], "file_path": "examples/main/main.cpp"}
{"Link_to_commit": "https://github.com/Kloud321/CPPCourseEGT/commit/865103e7c07ec8a552bbf326ddd4576b879d4672", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 39, "n_files_impacted": 5, "longest_chunk": ["//", "// Created by damya on 1.2.2024 \u0433..", "//", "", "#ifndef CARDSHUFFLE_CARD_H", "#define CARDSHUFFLE_CARD_H", "", "using namespace std;", "#include \"iostream\"", "", "", "class Card {", "public:", "", "", "    enum Type {", "            ACE, TWO, THREE, FOUR, FIVE, SIX, SEVEN, EIGHT, NINE, TEN, JACK, QUEEN, KING", "    };", "", "    enum Suit {", "        HEARTS, DIAMONDS, CLUBS, SPADES", "    };", "", "    Card(Type, Suit);", "", "", "    Type getType() const;", "    Suit getSuit() const;", "", "", "", "private:", "    Type type;", "    Suit suit;", "};", "", "", "", "#endif //CARDSHUFFLE_CARD_H"], "file_path": "CardShuffle_Homework/Deck.cpp"}
{"Link_to_commit": "https://github.com/ggml-org/llama.cpp/commit/84a44815f704aaed8e8edec7a39e846a975c7ba9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["    // auto enable conversation mode if chat template is available", "    const bool has_chat_template = !common_get_builtin_chat_template(model).empty() || !params.chat_template.empty();", "    if (params.conversation_mode == COMMON_CONVERSATION_MODE_AUTO) {", "        if (has_chat_template) {", "            LOG_INF(\"%s: chat template is available, enabling conversation mode (disable it with -no-cnv)\\n\", __func__);", "            params.conversation_mode = COMMON_CONVERSATION_MODE_ENABLED;", "        } else {", "            params.conversation_mode = COMMON_CONVERSATION_MODE_DISABLED;", "        }", "    }", "", "    // in case user force-activate conversation mode (via -cnv) without proper chat template, we show a warning", "    if (params.conversation_mode && !has_chat_template) {", "        LOG_WRN(\"%s: chat template is not available or is not supported. This may cause the model to output suboptimal responses\\n\", __func__);", "    }", ""], "file_path": "examples/main/main.cpp"}
{"Link_to_commit": "https://github.com/navratil-matej/llama.cpp/commit/84a44815f704aaed8e8edec7a39e846a975c7ba9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["    // auto enable conversation mode if chat template is available", "    const bool has_chat_template = !common_get_builtin_chat_template(model).empty() || !params.chat_template.empty();", "    if (params.conversation_mode == COMMON_CONVERSATION_MODE_AUTO) {", "        if (has_chat_template) {", "            LOG_INF(\"%s: chat template is available, enabling conversation mode (disable it with -no-cnv)\\n\", __func__);", "            params.conversation_mode = COMMON_CONVERSATION_MODE_ENABLED;", "        } else {", "            params.conversation_mode = COMMON_CONVERSATION_MODE_DISABLED;", "        }", "    }", "", "    // in case user force-activate conversation mode (via -cnv) without proper chat template, we show a warning", "    if (params.conversation_mode && !has_chat_template) {", "        LOG_WRN(\"%s: chat template is not available or is not supported. This may cause the model to output suboptimal responses\\n\", __func__);", "    }", ""], "file_path": "examples/main/main.cpp"}
{"Link_to_commit": "https://github.com/kjh2159/llama.cpp/commit/84a44815f704aaed8e8edec7a39e846a975c7ba9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["    // auto enable conversation mode if chat template is available", "    const bool has_chat_template = !common_get_builtin_chat_template(model).empty() || !params.chat_template.empty();", "    if (params.conversation_mode == COMMON_CONVERSATION_MODE_AUTO) {", "        if (has_chat_template) {", "            LOG_INF(\"%s: chat template is available, enabling conversation mode (disable it with -no-cnv)\\n\", __func__);", "            params.conversation_mode = COMMON_CONVERSATION_MODE_ENABLED;", "        } else {", "            params.conversation_mode = COMMON_CONVERSATION_MODE_DISABLED;", "        }", "    }", "", "    // in case user force-activate conversation mode (via -cnv) without proper chat template, we show a warning", "    if (params.conversation_mode && !has_chat_template) {", "        LOG_WRN(\"%s: chat template is not available or is not supported. This may cause the model to output suboptimal responses\\n\", __func__);", "    }", ""], "file_path": "examples/main/main.cpp"}
{"Link_to_commit": "https://github.com/ollischwarz/rock-paper-scissors/commit/5118f699e29f4c86abb2774342b877f3bb51fc57", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 5, "n_files_impacted": 1, "longest_chunk": ["  if (userPoints == 5) {", "    alert(`Cograts, you have ${userPoints}, you won!`);", "  } else {", "    alert(`Sorry, the computer has ${computerPoints}, you lost!`);", "  }"], "file_path": "main.js"}
{"Link_to_commit": "https://github.com/Kelvinmao/llama.cpp/commit/84a44815f704aaed8e8edec7a39e846a975c7ba9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["    // auto enable conversation mode if chat template is available", "    const bool has_chat_template = !common_get_builtin_chat_template(model).empty() || !params.chat_template.empty();", "    if (params.conversation_mode == COMMON_CONVERSATION_MODE_AUTO) {", "        if (has_chat_template) {", "            LOG_INF(\"%s: chat template is available, enabling conversation mode (disable it with -no-cnv)\\n\", __func__);", "            params.conversation_mode = COMMON_CONVERSATION_MODE_ENABLED;", "        } else {", "            params.conversation_mode = COMMON_CONVERSATION_MODE_DISABLED;", "        }", "    }", "", "    // in case user force-activate conversation mode (via -cnv) without proper chat template, we show a warning", "    if (params.conversation_mode && !has_chat_template) {", "        LOG_WRN(\"%s: chat template is not available or is not supported. This may cause the model to output suboptimal responses\\n\", __func__);", "    }", ""], "file_path": "examples/main/main.cpp"}
{"Link_to_commit": "https://github.com/bdj34/llama.cpp_dev/commit/84a44815f704aaed8e8edec7a39e846a975c7ba9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["    // auto enable conversation mode if chat template is available", "    const bool has_chat_template = !common_get_builtin_chat_template(model).empty() || !params.chat_template.empty();", "    if (params.conversation_mode == COMMON_CONVERSATION_MODE_AUTO) {", "        if (has_chat_template) {", "            LOG_INF(\"%s: chat template is available, enabling conversation mode (disable it with -no-cnv)\\n\", __func__);", "            params.conversation_mode = COMMON_CONVERSATION_MODE_ENABLED;", "        } else {", "            params.conversation_mode = COMMON_CONVERSATION_MODE_DISABLED;", "        }", "    }", "", "    // in case user force-activate conversation mode (via -cnv) without proper chat template, we show a warning", "    if (params.conversation_mode && !has_chat_template) {", "        LOG_WRN(\"%s: chat template is not available or is not supported. This may cause the model to output suboptimal responses\\n\", __func__);", "    }", ""], "file_path": "examples/main/main.cpp"}
{"Link_to_commit": "https://github.com/kayernyc/gpml-to-svg/commit/198090d29d8dc746d54359fa458c3e37aae1a31b", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 92, "n_files_impacted": 2, "longest_chunk": ["const transformPointBetweenPolesCases = [", "  // {", "  //   name: 'where point [1, 0, 0] is transformed from initial pole to new pole',", "  //   point: [1, 0, 0],", "  //   initialPole: {", "  //     lat_of_euler_pole: 30,", "  //     lon_of_euler_pole: 60,", "  //     rotation_angle: 20,", "  //   },", "  //   newPole: {", "  //     lat_of_euler_pole: 45,", "  //     lon_of_euler_pole: 45,", "  //     rotation_angle: 30,", "  //   },", "  //   expected: [0.7392, 0.5732, 0.3536],", "  // },", "  // {", "  //   name: 'where point [0, 1, 0] is transformed from initial pole to new pole',", "  //   point: [0, 1, 0],", "  //   initialPole: {", "  //     lat_of_euler_pole: 30,", "  //     lon_of_euler_pole: 60,", "  //     rotation_angle: 20,", "  //   },", "  //   newPole: {", "  //     lat_of_euler_pole: 45,", "  //     lon_of_euler_pole: 45,", "  //     rotation_angle: 30,", "  //   },", "  //   expected: [0.2803, 0.7392, 0.6124],", "  // },", "  {", "    name: 'where point [1, 0, 0] is transformed from initial pole to new pole',", "    point: [1, 0, 0],", "    initialPole: {", "      lat_of_euler_pole: 90,", "      lon_of_euler_pole: 0,", "      rotation_angle: 0,", "    },", "    newPole: {", "      lat_of_euler_pole: 90,", "      lon_of_euler_pole: 0,", "      rotation_angle: 30,", "    },", "    expected: [0.8660254037844387, 0.5, 0],", "  },", "  {", "    name: 'in the series, the same point goes to a third pole',", "    point: [0.8660254037844387, 0.5, 0],", "    initialPole: {", "      lat_of_euler_pole: 90,", "      lon_of_euler_pole: 0,", "      rotation_angle: 30,", "    },", "    newPole: {", "      lat_of_euler_pole: 20,", "      lon_of_euler_pole: 45,", "      rotation_angle: 30,", "    },", "    expected: [0.9251766765758391, 0.23016134445423478, -0.30178448044772743],", "  },", "  {", "    name: 'using test planet',", "    point: [1, 0, 0],", "    initialPole: {", "      lat_of_euler_pole: 90,", "      lon_of_euler_pole: 0,", "      rotation_angle: 0,", "    },", "    newPole: {", "      lat_of_euler_pole: 90,", "      lon_of_euler_pole: 0,", "      rotation_angle: 0,", "    },", "    expected: [1, 0, 0],", "  },", "  {", "    name: 'using test planet and motion',", "    point: [1, 0, 0],", "    initialPole: {", "      lat_of_euler_pole: 90,", "      lon_of_euler_pole: 0,", "      rotation_angle: 0,", "    },", "    newPole: {", "      lat_of_euler_pole: 90,", "      lon_of_euler_pole: 0,", "      rotation_angle: -30,", "    },", "    expected: [0.8660254037844387, -0.5, 0],", "  },", "];"], "file_path": "src/modules/applyRotation/transformCoordinates.test.ts"}
{"Link_to_commit": "https://github.com/SabrinaPL/pwd/commit/bfd70c1bf5b420dd7de1e97ff9734a40f16ce17e", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 63, "n_files_impacted": 10, "longest_chunk": ["# Chat App Component", "", "The Chat App Component is a real-time chat application built as a web component. It allows users to send and receive messages instantly, with support for emoji input and user nicknames.", "", "## Author", "", "Sabrina Prichard-Lybeck  ", "Email: <sp223kz@student.lnu.se>  ", "", "## Version", "", "1.1.0", "", "## Features", "", "- Real-time messaging using WebSockets.", "- Emoji picker for adding emojis to messages.", "- Persistent nickname storage using local storage.", "- Prevents XSS attacks by sanitizing user inputs.", "- Color-coded messages for different users.", "", "## Usage", "", "### Installation", "", "Include the component in your project:", "", "```html", "<script type=\"module\" src=\"path/to/chat-app.js\"></script>", "```", "", "## Add the component to your HTML", "", "```html", "<chat-app></chat-app>", "```", "", "## JavaScript", "", "```html", "customElements.define('chat-app', class extends HTMLElement {", "  // The class implementation goes here.", "})", "```", "", "### Dependencies", "", "*emoji-picker-element for emoji input.", "*DOMPurify for sanitizing user inputs to prevent XSS attacks.", "*A nickname-form component for setting user nicknames.", "", "### Styles", "", "Customize the appearance of the chat app component by modifying the embedded styles within the template.", "", "### License", "", "This project is licensed under the MIT License - see the LICENSE file for details.", "", "### Acknowledgments", "", "*The creators of emoji-picker-element for providing the emoji picker component.", "*The authors of DOMPurify for ensuring safe user inputs."], "file_path": "src/js/components/messages/messages.js"}
{"Link_to_commit": "https://github.com/apurificato/SVG-logo-maker/commit/1033b8f3857fe7640de4c3b4b84ed2bdfbc20fb9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 14, "n_files_impacted": 2, "longest_chunk": ["    console.log('SVG file generated successfully!');", "}).catch(error => {", "    console.error('Error:', error);", "});", "", "// Function that writes SVG file", "// function writeToFile('Logo.svg', svgCode) {", "//     fs.writeFile('Logo.svg', svgCode, (err) => {", "//         if (err) {", "//             return console.log(err)", "//         }", "//         console.log('SVG File successfully generated.')", "//     })", "// }"], "file_path": "index.js"}
{"Link_to_commit": "https://github.com/thongstylish/llma-selfhost/commit/84a44815f704aaed8e8edec7a39e846a975c7ba9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["    // auto enable conversation mode if chat template is available", "    const bool has_chat_template = !common_get_builtin_chat_template(model).empty() || !params.chat_template.empty();", "    if (params.conversation_mode == COMMON_CONVERSATION_MODE_AUTO) {", "        if (has_chat_template) {", "            LOG_INF(\"%s: chat template is available, enabling conversation mode (disable it with -no-cnv)\\n\", __func__);", "            params.conversation_mode = COMMON_CONVERSATION_MODE_ENABLED;", "        } else {", "            params.conversation_mode = COMMON_CONVERSATION_MODE_DISABLED;", "        }", "    }", "", "    // in case user force-activate conversation mode (via -cnv) without proper chat template, we show a warning", "    if (params.conversation_mode && !has_chat_template) {", "        LOG_WRN(\"%s: chat template is not available or is not supported. This may cause the model to output suboptimal responses\\n\", __func__);", "    }", ""], "file_path": "examples/main/main.cpp"}
{"Link_to_commit": "https://github.com/rizkianp/rizkianp.github.io/commit/33aaffcb91168447914aa4a8673e4b45c8d37e8e", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 20, "n_files_impacted": 1, "longest_chunk": ["// Define fade-in and fade-out animations", "const fadeIn = [", "  { opacity: 0 },", "  { opacity: 1 }", "];", "", "const fadeOut = [", "  { opacity: 1 },", "  { opacity: 0 }", "];", "", "const fadeTiming = {", "  duration: 200, // Adjust the duration as needed", "  iterations: 1", "};", "", "// when scroll down 30px from top, show the button with fade-in animation", "window.onscroll = function () {", "  scrollFunction();", "};"], "file_path": "scripts/main.js"}
{"Link_to_commit": "https://github.com/ThaaoBlues/ecosys_mobile/commit/a03dd95856e5f3cb40e35f26e68f8a034d83f87b", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 259, "n_files_impacted": 4, "longest_chunk": ["package com.qsync.qsync;", "", "import android.os.Handler;", "import android.os.Looper;", "import android.util.Log;", "", "import com.google.gson.Gson;", "", "import org.json.JSONException;", "import org.json.JSONObject;", "", "import java.io.BufferedReader;", "import java.io.DataOutputStream;", "import java.io.File;", "import java.io.FileOutputStream;", "import java.io.IOException;", "import java.io.InputStreamReader;", "import java.io.ObjectOutputStream;", "import java.io.OutputStream;", "import java.net.InetAddress;", "import java.net.ServerSocket;", "import java.net.Socket;", "import java.nio.charset.StandardCharsets;", "import java.nio.file.Paths;", "import java.util.ArrayList;", "import java.util.Arrays;", "import java.util.List;", "", "public class Networking {", "", "    public static final int HEADER_LENGTH = 83;", "    private static final String TAG = \"Networking\";", "    private static ServerSocket serverSocket;", "    private static Socket clientSocket;", "    private static AccesBdd acces;", "", "    public static void main(String[] args) {", "        try {", "            serverSocket = new ServerSocket(8274);", "            Log.d(TAG, \"Server started on port 8274\");", "            while (true) {", "                clientSocket = serverSocket.accept();", "                Log.d(TAG, \"Client connected\");", "                new Thread(new ClientHandler(clientSocket)).start();", "            }", "        } catch (IOException e) {", "            Log.e(TAG, \"Error while initializing socket server : \", e);", "        }", "    }", "", "    public static class ClientHandler implements Runnable {", "        private Socket clientSocket;", "", "        public ClientHandler(Socket socket) {", "            this.clientSocket = socket;", "        }", "", "        @Override", "        public void run() {", "            try {", "                acces = new AccesBdd();", "                acces.InitConnection();", "", "                // get the device id and secure sync id from header", "                char[] header_buff = new char[HEADER_LENGTH];", "                InputStreamReader inputStreamReader = new InputStreamReader(clientSocket.getInputStream(), StandardCharsets.UTF_8);", "                BufferedReader bufferedReader = new BufferedReader(inputStreamReader);", "                bufferedReader.read(header_buff, 0, HEADER_LENGTH);", "", "                String device_id = new String(header_buff, 0, HEADER_LENGTH).split(\";\")[0];", "                String secure_id = new String(header_buff, 0, HEADER_LENGTH).split(\";\")[1];", "", "                acces.SetSecureId(secure_id);", "", "                // in case of a link packet, the device is not yet registered in the database", "                // so it can throw an error", "                if (acces.IsDeviceLinked(device_id)) {", "                    // makes sure it is marked as connected", "                    if (!acces.GetDevicedbState(device_id)) {", "                        // needs split as RemoteAddr ads port to the address", "                        acces.SetDevicedbState(device_id, true, clientSocket.getInetAddress().getHostAddress());", "                    }", "                }", "", "                // read the body of the request and store it in a buffer", "                StringBuilder body_buff = new StringBuilder();", "                String line;", "                while ((line = bufferedReader.readLine()) != null) {", "                    body_buff.append(line);", "                }", "", "                Log.d(TAG, \"Request body : \" + body_buff);", "", "                // Parse the JSON", "                Gson gson = new Gson();", "                Globals.QEvent data = gson.fromJson(body_buff.toString(),Globals.QEvent.class);", "", "                // check if this is a regular file event of a special request", "                Log.d(TAG, \"RECEIVING EVENT : \" + data);", "                switch (data.getFlag()) {", "                    case \"[MODIFICATION_DONE]\":", "                        setEventNetworkLockForDevice(device_id, false);", "                        break;", "                    case \"[SETUP_DL]\":", "                        Log.d(TAG, \"GOT FLAG, BUILDING SETUP QUEUE...\");", "                        buildSetupQueue(secure_id, device_id);", "                        break;", "                    case \"[LINK_DEVICE]\":", "                        // as this is triggered by another machine telling this one to create a sync task,", "                        // we must prepare the environnement to accept this", "                        // by creating a new sync task with the same path (replace this later by asking to the user)", "                        // and same secure_id", "                        Log.d(TAG, \"Initializing env to welcome the other end folder content\");", "                        acces.SetSecureId(secure_id);", "                        String path = askInput(\"[CHOOSELINKPATH]\", \"Choose a path where new sync files will be stored.\");", "                        Log.d(TAG, \"Future sync will be stored at : \" + path);", "                        acces.CreateSyncFromOtherEnd(path, secure_id);", "                        Log.d(TAG, \"Linking device : \" + device_id);", "                        acces.LinkDevice(device_id, clientSocket.getInetAddress().getHostAddress());", "                        break;", "                    case \"[UNLINK_DEVICE]\":", "                        acces.UnlinkDevice(device_id);", "                        break;", "                    case \"[OTDL]\":", "                        handleLargageAerien(data, clientSocket.getInetAddress().getHostAddress());", "                        break;", "                    default:", "                        // regular file event", "                        handleEvent(secure_id, device_id, body_buff.toString());", "                        // send back a modification confirmation, so the other end can remove this machine device_id", "                        // from concerned sync task retard entries", "                        String response = acces.getMyDeviceId() + \";\" + acces.GetSecureId() + \";\" + \"[MODIFICATION_DONE]\";", "                        DataOutputStream outputStream = new DataOutputStream(clientSocket.getOutputStream());", "                        outputStream.writeBytes(response);", "                        break;", "                }", "", "            } catch (IOException e) {", "                Log.e(TAG, \"Error in ClientHandler: \", e);", "            }", "        }", "    }", "", "    // used to process a request when it is a regular file event", "    public static void handleEvent(String secureId, String deviceId, byte[] buffer) {", "        try {", "            String bufferData = new String(buffer);", "            JSONObject jsonEvent = new JSONObject(bufferData);", "", "", "            // First, we lock the filesystem watcher to prevent a ping-pong effect", "            setFileSystemPatchLockState(deviceId, true);", "", "            // Get the necessary data from the JSON event", "            String relativePath = jsonEvent.getString(\"FilePath\");", "            String newRelativePath = jsonEvent.getString(\"NewFilePath\");", "            String absoluteFilePath = null;", "            if (android.os.Build.VERSION.SDK_INT >= android.os.Build.VERSION_CODES.O) {", "                absoluteFilePath = Paths.get(acces.GetRootSyncPath(), relativePath).toString();", "            }", "            String newAbsoluteFilePath = null;", "            if (android.os.Build.VERSION.SDK_INT >= android.os.Build.VERSION_CODES.O) {", "                newAbsoluteFilePath = Paths.get(acces.GetRootSyncPath(), newRelativePath).toString();", "            }", "            String eventType = jsonEvent.getString(\"Flag\");", "            String fileType = jsonEvent.getString(\"FileType\");", "", "            switch (eventType) {", "                case \"MOVE\":", "                    acces.move(relativePath, newRelativePath, fileType);", "                    moveInFileSystem(absoluteFilePath, newAbsoluteFilePath);", "                    break;", "                case \"REMOVE\":", "                    if (\"file\".equals(fileType)) {", "                        acces.rmFile(absoluteFilePath);", "                    } else {", "                        acces.rmFolder(absoluteFilePath);", "                    }", "                    removeFromFileSystem(absoluteFilePath);", "                    break;", "                case \"CREATE\":", "                    if (\"file\".equals(fileType)) {", "                        acces.createFile(relativePath,absoluteFilePath,\"[SENT_FROM_OTHER_DEVICE]\");", "                    } else {", "                        createFolder(absoluteFilePath);", "                    }", "                    break;", "                case \"UPDATE\":", "                    acces.incrementFileVersion(relativePath);", "                    break;", "                default:", "                    Log.e(\"HandleEventAdapter\", \"Received unknown event type: \" + eventType);", "                    break;", "            }", "", "            // Release the filesystem lock", "            setFileSystemPatchLockState(deviceId, false);", "        } catch (JSONException e) {", "            Log.e(\"HandleEventAdapter\", \"Error decoding JSON data from request buffer\", e);", "        } catch (JSONException e) {", "            throw new RuntimeException(e);", "        }", "    }", "", "", "    public static void sendDeviceEventQueueOverNetwork(Globals.GenArray<String> connected_devices, String secure_id, Globals.GenArray<Globals.QEvent> event_queue, String... ip_addr) {", "        // ...", "    }", "", "    public static void setEventNetworkLockForDevice(String device_id, boolean value) {", "        // ...", "    }", "", "    public static boolean getEventNetworkLockForDevice(String device_id) {", "        // ...", "        return false;", "    }", "", "    public static void removeFromFilesystem(String path) {", "        // ...", "    }", "", "    public static void moveInFilesystem(String old_path, String new_path) {", "        // ...", "    }", "", "    public static void buildSetupQueue(String secure_id, String device_id) {", "        // ...", "    }", "", "    public static void handleLargageAerien(Globals.QEvent data, String ip_addr) {", "        // ...", "    }", "", "    public static void sendLargageAerien(String file_path, String device_ip) {", "        // ...", "    }", "", "    public static String askInput(final String title, final String message) {", "        final Handler handler = new Handler(Looper.getMainLooper());", "        final String[] result = new String[1];", "        handler.post(new Runnable() {", "            @Override", "            public void run() {", "                // Show your dialog here to get user input", "                // Store the result in result[0]", "            }", "        });", "        // Wait for user input", "        while (result[0] == null) {", "            try {", "                Thread.sleep(100);", "            } catch (InterruptedException e) {", "                e.printStackTrace();", "            }", "        }", "        return result[0];", "    }", "}"], "file_path": "app/src/main/java/com/qsync/qsync/ZeroConfService.java"}
{"Link_to_commit": "https://github.com/trisha/jordle-solver/commit/a5f8ab50a411d3096f7ddcb518869d6703e804d5", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 27, "n_files_impacted": 7, "longest_chunk": ["read_file_path = 'petpets/raw_names.txt'", "write_file_path = 'petpets/names.txt'", "", "# Read all petpet names from the file", "with open(read_file_path, 'r') as file:", "    raw_names = file.read().splitlines()", "", "def check_if_valid(line):", "    res = []", "    words = line.split(\" \")", "    for word in words:", "        word = word.strip(\"'\")", "        word = word.strip('\"')", "        if len(word) == 5:", "            res.append(word)", "    return res", "", "# Will overwrite file every time this script is run (w for overwrite)", "count = 0", "with open(write_file_path, 'w') as file:", "    for name in raw_names:", "        valid_names = check_if_valid(name)", "        for valid_name in valid_names:", "            count += 1", "            file.write(f\"{valid_name}\\n\")", "", "print(f\"Saved {count} names to {write_file_path}\")"], "file_path": "petpets/get_names.py"}
{"Link_to_commit": "https://github.com/WesleyLuiz21/LiveApp/commit/80f6fbc70f3e2d5a0eca58dbd3997857e7f29f85", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 14, "n_files_impacted": 1, "longest_chunk": ["            // Only create a new Hls instance if one does not already exist", "            if (!hlsRef.current) {", "                const hls = new Hls();", "                hlsRef.current = hls;  // Store the Hls instance in the ref", "                hls.loadSource(streamUrl);", "                hls.attachMedia(videoRef.current);", "                hls.on(Hls.Events.MANIFEST_PARSED, () => {", "                    videoRef.current.play();", "                });", "            } else {", "                // If an Hls instance exists, just change the source", "                hlsRef.current.loadSource(streamUrl);", "                hlsRef.current.attachMedia(videoRef.current);", "            }"], "file_path": "liveapp/src/components/Streaming/Streaming.js"}
{"Link_to_commit": "https://github.com/Rezlen/Portfolio-Websites/commit/9634a1e032683605568ea232e431939ad32c7f6a", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 265, "n_files_impacted": 4, "longest_chunk": ["// Your web app's Firebase configuration\r", "// For Firebase JS SDK v7.20.0 and later, measurementId is optional\r", "var firebaseConfig = {\r", "  apiKey: \"AIzaSyBbhNdQa2sn_uGqWYhNZcS8PRwoI0x1ook\",\r", "  authDomain: \"portfoliodatabase-27998.firebaseapp.com\",\r", "  databaseURL:\r", "    \"https://portfoliodatabase-27998-default-rtdb.europe-west1.firebasedatabase.app\",\r", "  projectId: \"portfoliodatabase-27998\",\r", "  storageBucket: \"portfoliodatabase-27998.appspot.com\",\r", "  messagingSenderId: \"485104666349\",\r", "  appId: \"1:485104666349:web:7327325c77127183d7eee4\",\r", "  measurementId: \"G-FSXNF4C33Q\",\r", "};\r", "// Initialize Firebase\r", "// firebase.analytics();\r", "firebase.initializeApp(firebaseConfig);\r", "// firebase.auth.Auth.Persistence.SESSION;\r", "\r", "// Initialize variables\r", "const auth = firebase.auth();\r", "const database = firebase.database();\r", "\r", "const postsDisplay = document.getElementById(\"postsDisplay\");\r", "const logOutbtn = document.getElementById(\"logOutbtn\");\r", "const RegBTN = document.getElementById(\"RegBTN\");\r", "const logInbtn = document.getElementById(\"logInbtn\");\r", "const LogInWarning = document.getElementById(\"LogInWarning\");\r", "// const HideContent = document.getElementsByClassName(\"HideContent\");\r", "const HideContent = document.getElementById(\"HideContent\");\r", "\r", "const currentTimestamp = Date.now();\r", "const readableTimestamp = new Date(currentTimestamp).toISOString();\r", "///////////////////////////////////Registration Section///////////////////////////////\r", "\r", "// Set up our register function\r", "function register() {\r", "  // Get all our input fields\r", "  firstName = document.getElementById(\"firstName\").value;\r", "  lastName = document.getElementById(\"lastName\").value;\r", "  email = document.getElementById(\"email\").value;\r", "  password = document.getElementById(\"password\").value;\r", "\r", "  // Validate input fields\r", "  if (validate_email(email) == false || validate_password(password) == false) {\r", "    swal(\"Email or Password is NOT Correct!!\");\r", "    return;\r", "    // If pass or email not in the right format don't continue running the code\r", "  }\r", "  if (validate_field(firstName) == false || validate_field(lastName) == false) {\r", "    swal(\"One or More Extra Fields is Outta Line!!\");\r", "    return;\r", "  }\r", "  // If entry format not in the right format don't continue running the code otherwise:\r", "\r", "  // Move on with Auth\r", "  auth\r", "    .createUserWithEmailAndPassword(email, password)\r", "    .then(function () {\r", "      // Declare user variable\r", "      var user = auth.currentUser;\r", "\r", "      // Add this user to Firebase Database\r", "      var database_ref = database.ref();\r", "\r", "      // Create User data\r", "      var user_data = {\r", "        email: email,\r", "        firstName: firstName,\r", "        lastName: lastName,\r", "        regDateTime: new Date().toISOString(),\r", "        last_login: readableTimestamp,\r", "        last_logout: readableTimestamp,\r", "        // last_login: Date.now(),\r", "        // last_logout: Date.now(),\r", "        // lastLoginTimestamp: formatDateTime(lastLoginTimestamp),\r", "        // lastLogoutTimestamp: formatDateTime(lastLogoutTimestamp),\r", "      };\r", "\r", "      // Push to Firebase Database\r", "      database_ref.child(\"users/\" + user.uid).set(user_data);\r", "\r", "      // Done. Instead of ALERT popup SweetAlert \"swal\" Has been used,\r", "      swal({\r", "        text: \"Thank you, Your Account Created!!\",\r", "        icon: \"success\",\r", "        timer: 2000,\r", "      });\r", "      console.log(user_data); // checking the users success & data on console\r", "\r", "      function intervalFunction() {\r", "        // function created only to create a delay for moving to the POSTS.html page so the SUCCESS alert/swal is visible well!\r", "        if (user) {\r", "          window.location.replace = \"/Posts.html\"; //After successful login, user will be redirected to Posts.html\r", "        }\r", "      }\r", "      setInterval(intervalFunction, 2000);\r", "    })\r", "\r", "    .catch(function (error) {\r", "      // Firebase will use this to alert of its errors\r", "      var error_code = error.code;\r", "      var error_message = error.message;\r", "\r", "      alert(error_message, error_code);\r", "    });\r", "}\r", "///////////////////////////////////Login Section///////////////////////////////\r", "// Set up our login function\r", "function login() {\r", "  // Get all our input fields\r", "  email = document.getElementById(\"email\").value;\r", "  password = document.getElementById(\"password\").value;\r", "\r", "  // Validate input fields\r", "  if (validate_email(email) == false || validate_password(password) == false) {\r", "    swal(\"Email or Password is NOT Correct!!\");\r", "    return;\r", "    // Don't continue running the code\r", "  }\r", "\r", "  auth\r", "    .signInWithEmailAndPassword(email, password)\r", "    .then(function () {\r", "      // Declare user variable\r", "      var user = auth.currentUser;\r", "\r", "      // Add this user to Firebase Database\r", "      var database_ref = database.ref();\r", "\r", "      // Create User data\r", "      var user_data = {\r", "        // last_login: Date.now(),\r", "        last_login: readableTimestamp,\r", "      };\r", "\r", "      // Push to Firebase Database\r", "      database_ref.child(\"users/\" + user.uid).update(user_data);\r", "\r", "      // Done\r", "      swal({ text: \"You are Logged-In Now!!\", icon: \"success\", timer: 2000 });\r", "      console.log(user_data); // checking the users success & data on consol\r", "\r", "      function intervalFunction() {\r", "        // function created only to create a delay for moving to the POSTS.html page so the SUCCESS alert/swal is visible well!\r", "        if (user) {\r", "          window.location = \"/Posts.html\"; //After successful login, user will be redirected to Posts.html\r", "        }\r", "      }\r", "      setInterval(intervalFunction, 2000);\r", "    })\r", "    .catch(function (error) {\r", "      // Firebase will use this to alert of its errors\r", "      var error_code = error.code;\r", "      var error_message = error.message;\r", "\r", "      // alert(\"you are not logged in rez\");\r", "\r", "      alert(error_message, error_code);\r", "      postsDisplay.style.display = \"none\";\r", "    });\r", "}\r", "\r", "function logout() {\r", "  const user = auth.currentUser;\r", "  if (user) {\r", "    auth.signOut();\r", "    database\r", "      .ref(`users/${user.uid}`)\r", "      .update({ last_logout: readableTimestamp });\r", "    swal({ text: \"You are Logged-Out Now!!\", icon: \"success\" });\r", "    window.location = \"/Public/Social-Log/signIN.html\";\r", "  }\r", "}\r", "\r", "// function logout() {\r", "//   var database_ref = database.ref();\r", "//   var user = auth.currentUser;\r", "\r", "//   if (user) {\r", "//     auth.signOut();\r", "//     database_ref.child(\"users/\" + user.uid).update(user_data);\r", "//     var user_data = {\r", "//       last_logout: readableTimestamp,\r", "//     };\r", "//     swal({ text: \"You are Logged-Out Now!!\", icon: \"success\" });\r", "//     window.location = \"/Public/Social-Log/signIN.html\";\r", "//   }\r", "// }\r", "\r", "// conditions for when user is/not logged in\r", "auth.onAuthStateChanged(function (user) {\r", "  if (user) {\r", "    logInbtn.style.display = \"none\";\r", "    RegBTN.style.display = \"none\";\r", "    LogInWarning.style.display = \"none\";\r", "    logOutbtn.style.display = \"block\";\r", "    // for (let i = 0; i < HideContent.length; i++){\r", "    //   HideContent[i].style.display = \"none\"; // why it is not hiding other pages content?\r", "    // }\r", "\r", "    // for (let element of document.getElementsByClassName(\"HideContent\")) {\r", "    //   element.style.display = \"none\";\r", "    // }\r", "  } else {\r", "    postsDisplay.style.display = \"none\";\r", "    logOutbtn.style.display = \"none\";\r", "  }\r", "});\r", "// hide the log out button if the user is not logged in\r", "auth.onAuthStateChanged(function (user) {\r", "  if (!user) {\r", "    logOutbtn.style.display = \"none\";\r", "  }\r", "});\r", "// hide the Registration & Login fields is the user is logged in already\r", "auth.onAuthStateChanged(function (user) {\r", "  if (user) {\r", "    HideContent.style.display = \"none\";\r", "    swal({ text: \"You are Logged In Now !!\", icon: \"success\" });\r", "  }\r", "});\r", "\r", "auth()\r", "  .revokeRefreshTokens(uid)\r", "  .then(() => {\r", "    return auth().getUser(uid);\r", "  })\r", "  .then((userRecord) => {\r", "    return new Date(userRecord.tokensValidAfterTime).getTime() / 100;\r", "  })\r", "  .then((timestamp) => {\r", "    console.log(`Tokens revoked at: ${timestamp}`);\r", "  });\r", "\r", "function validate_email(email) {\r", "  expression = /^[^@]+@\\w+(\\.\\w+)+\\w$/;\r", "  if (expression.test(email) == true) {\r", "    // Email is good\r", "    return true;\r", "  } else {\r", "    // Email is not good\r", "    return false;\r", "  }\r", "}\r", "\r", "function validate_password(password) {\r", "  // Firebase only accepts lengths greater than 6\r", "  if (password < 6) {\r", "    return false;\r", "  } else {\r", "    return true;\r", "  }\r", "}\r", "\r", "function validate_field(field) {\r", "  if (field == null) {\r", "    return false;\r", "  }\r", "\r", "  if (field.length <= 0) {\r", "    return false;\r", "  } else {\r", "    return true;\r", "  }\r", "}\r"], "file_path": "test2.js"}
{"Link_to_commit": "https://github.com/lasermike/llmdump/commit/84a44815f704aaed8e8edec7a39e846a975c7ba9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["    // auto enable conversation mode if chat template is available", "    const bool has_chat_template = !common_get_builtin_chat_template(model).empty() || !params.chat_template.empty();", "    if (params.conversation_mode == COMMON_CONVERSATION_MODE_AUTO) {", "        if (has_chat_template) {", "            LOG_INF(\"%s: chat template is available, enabling conversation mode (disable it with -no-cnv)\\n\", __func__);", "            params.conversation_mode = COMMON_CONVERSATION_MODE_ENABLED;", "        } else {", "            params.conversation_mode = COMMON_CONVERSATION_MODE_DISABLED;", "        }", "    }", "", "    // in case user force-activate conversation mode (via -cnv) without proper chat template, we show a warning", "    if (params.conversation_mode && !has_chat_template) {", "        LOG_WRN(\"%s: chat template is not available or is not supported. This may cause the model to output suboptimal responses\\n\", __func__);", "    }", ""], "file_path": "examples/main/main.cpp"}
{"Link_to_commit": "https://github.com/ccortez60edu/Llama/commit/84a44815f704aaed8e8edec7a39e846a975c7ba9", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["    // auto enable conversation mode if chat template is available", "    const bool has_chat_template = !common_get_builtin_chat_template(model).empty() || !params.chat_template.empty();", "    if (params.conversation_mode == COMMON_CONVERSATION_MODE_AUTO) {", "        if (has_chat_template) {", "            LOG_INF(\"%s: chat template is available, enabling conversation mode (disable it with -no-cnv)\\n\", __func__);", "            params.conversation_mode = COMMON_CONVERSATION_MODE_ENABLED;", "        } else {", "            params.conversation_mode = COMMON_CONVERSATION_MODE_DISABLED;", "        }", "    }", "", "    // in case user force-activate conversation mode (via -cnv) without proper chat template, we show a warning", "    if (params.conversation_mode && !has_chat_template) {", "        LOG_WRN(\"%s: chat template is not available or is not supported. This may cause the model to output suboptimal responses\\n\", __func__);", "    }", ""], "file_path": "examples/main/main.cpp"}
{"Link_to_commit": "https://github.com/speer987/the-book-nook/commit/4e0995b063da59fa73979d7e2447dc0b12e23de1", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 20, "n_files_impacted": 3, "longest_chunk": ["export default function LineChart({ book, db }) {", "  let [userData, setUserData] = useState(null);", "", "  useEffect(() => {", "    const unsubscribe = onSnapshot(doc(db, book?.id), (doc) => {", "      setUserData(doc.data());", "    });", "    return () => unsubscribe();", "  }, [db, book?.id]);", "  console.log(userData);", "", "  let total_pages = book?.pages;", "  const keys = userData ? Object.keys(userData) : [];", "  let dates = [];", "  let pages = [];", "  keys.forEach((key) => dates.push(key));", "  dates = dates.sort();", "  dates.forEach((date) => {", "    pages.push(userData[date]);", "  });"], "file_path": "src/components/LineChart.js"}
{"Link_to_commit": "https://github.com/abhinavsaraswatt/Project9--Only-Responsive-FrontEnd-/commit/63e9366657c394147cae49ab0180a0fcb6dbf0f1", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 6, "n_files_impacted": 1, "longest_chunk": ["  if (e.target.closest(\"header\") === slider) {", "    isDown = true;", "    slider.classList.add(\"active\");", "    startX = e.pageX - slider.offsetLeft;", "    scrollLeft = slider.scrollLeft;", "  }"], "file_path": "header.js"}
{"Link_to_commit": "https://github.com/mlouage/AdventOfCode.2023/commit/c37f678e6a700e4665c0a17ee84e75f649113032", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 140, "n_files_impacted": 9, "longest_chunk": ["\ufeffnamespace AdventOfCode.Day03;", "", "public class Day03", "{", "    private readonly string[] _input;", "", "    public Day03(string[] input)", "    {", "        _input = input;", "    }", "", "    public int Part1()", "    {", "        var schema = Array.ConvertAll(_input, line => line.ToCharArray());", "", "        var sumParts = 0;", "", "        for (var x = 0; x < schema.Length; x++)", "        {", "            var y = 0;", "            while (y < schema[x].Length)", "            {", "                if (char.IsDigit(schema[x][y]))", "                {", "                    var number = schema[x][y].ToString();", "                    var length = 1;", "", "                    while (y + length < schema[x].Length && char.IsDigit(schema[x][y + length]))", "                    {", "                        number += schema[x][y + length];", "                        length++;", "                    }", "", "                    if (IsAdjacentToSymbol(number, x, y, schema))", "                    {", "                        sumParts += int.Parse(number);", "                    }", "", "                    y += length;", "                }", "                else", "                {", "                    y++;", "                }", "            }", "        }", "", "        return sumParts;", "    }", "", "    public int Part2()", "    {", "        var schema = Array.ConvertAll(_input, line => line.ToCharArray());", "        var sumRatios = 0;", "", "        for (var x = 0; x < schema.Length; x++)", "        {", "            for (var y = 0; y < schema[x].Length; y++)", "            {", "                if (!IsGear(schema[x][y])) continue;", "", "                var adjacentNumbers = new HashSet<int>();", "                foreach (var (adjX, adjY) in GetAdjacentCells(x, y, schema.Length, schema[0].Length))", "                {", "                    if (!char.IsDigit(schema[adjX][adjY])) continue;", "", "                    var number = FindNumbers(schema, adjX, adjY);", "                    if (number != -1)", "                    {", "                        adjacentNumbers.Add(number);", "                    }", "                }", "", "                if (adjacentNumbers.Count == 2)", "                {", "                    var gearRatio = 1;", "                    foreach (var num in adjacentNumbers)", "                    {", "                        gearRatio *= num;", "                    }", "                    sumRatios += gearRatio;", "                }", "            }", "        }", "", "        return sumRatios;", "    }", "", "    private static bool IsSymbol(char c) => char.IsLetter(c) || \"$*+-/=&#%@\".Contains(c);", "", "    private static bool IsGear(char c) => \"*\".Contains(c);", "", "    private static int FindNumbers(IReadOnlyList<char[]> schema, int x, int y)", "    {", "        while (y > 0 && char.IsDigit(schema[x][y - 1]))", "        {", "            y--;", "        }", "", "        var number = \"\";", "        while (y < schema[x].Length && char.IsDigit(schema[x][y]))", "        {", "            number += schema[x][y];", "            y++;", "        }", "        return number.Length > 0 ? int.Parse(number) : -1;", "    }", "", "    private static List<(int, int)> GetAdjacentCells(int x, int y, int maxX, int maxY)", "    {", "        var adjacent = new List<(int, int)>();", "        for (var dx = -1; dx <= 1; dx++)", "        {", "            for (var dy = -1; dy <= 1; dy++)", "            {", "                if (dx == 0 && dy == 0) continue;", "                int newX = x + dx, newY = y + dy;", "                if (newX >= 0 && newX < maxX && newY >= 0 && newY < maxY)", "                {", "                    adjacent.Add((newX, newY));", "                }", "            }", "        }", "        return adjacent;", "    }", "", "    private static bool IsAdjacentToSymbol(string number, int x, int y, IReadOnlyList<char[]> schema)", "    {", "        var length = number.Length;", "        for (var i = 0; i < length; i++)", "        {", "            foreach (var (adjX, adjY) in GetAdjacentCells(x, y + i, schema.Count, schema[0].Length))", "            {", "                if (IsSymbol(schema[adjX][adjY])) return true;", "            }", "        }", "", "        return false;", "    }", "}"], "file_path": "tests/AdventOfCode.Tests.Day01/Day01.cs"}
{"Link_to_commit": "https://github.com/KaraveyaWest/Karaveya-Task-Prioritization-App/commit/a996e61ef94386698a67725452a2df2078b265ae", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 37, "n_files_impacted": 3, "longest_chunk": ["<!DOCTYPE html>", "<html lang=\"en\">", "<head>", "    <meta charset=\"UTF-8\">", "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">", "    <title>Task Manager</title>", "    <link rel=\"stylesheet\" href=\"styles.css\">", "    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/Sortable/1.14.0/Sortable.min.js\"></script>", "</head>", "<body>", "    <div class=\"container\">", "        <h1>Task Manager</h1>", "        <div class=\"add-task-form\">", "            <input type=\"text\" id=\"task-desc\" placeholder=\"Enter task description\">", "            <label for=\"static-priority\">Static Priority:</label>", "            <select id=\"static-priority\">", "                <option value=\"1\">1 - Critical</option>", "                <option value=\"2\">2 - Urgent</option>", "                <option value=\"3\">3 - Important but not urgent</option>", "                <option value=\"4\">4 - Low priority</option>", "                <option value=\"5\">5 - Very low priority</option>", "            </select>", "            ", "            <label for=\"daily-priority\">Daily Priority:</label>", "            <select id=\"daily-priority\">", "                <option value=\"1\">1 - Critical</option>", "                <option value=\"2\">2 - Urgent</option>", "                <option value=\"3\">3 - Important but not urgent</option>", "                <option value=\"4\">4 - Low priority</option>", "                <option value=\"5\">5 - Very low priority</option>", "            </select>", "        ", "            <button id=\"add-task-btn\">Add Task</button>", "        </div>", "    <script src=\"script.js\"></script>", "</body>", "</html>"], "file_path": "script.js"}
{"Link_to_commit": "https://github.com/annalamb58/Gr10PAT2024/commit/8aee7d9a933db700255d0febb8829dcacd6ae981", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 64, "n_files_impacted": 1, "longest_chunk": ["// OLD CODE # OLD CODE # OLD CODE # OLD CODE # OLD CODE # OLD CODE # OLD CODE # OLD CODE # OLD CODE # OLD CODE # OLD CODE # OLD CODE # OLD CODE # OLD CODE #\r", "//        String currentMeds = \"\";\r", "//        String currentSpecies = \"\";\r", "//        String currentFormula = \"\";\r", "////        String currentDose = \"\";\r", "//\r", "//        boolean isMed = false;\r", "//        boolean isSpecies = false;\r", "//        boolean isFormula = false;\r", "//        boolean isDose = false;\r", "//        boolean isLine = false;\r", "//\r", "//        while (scannerForCAL.hasNextLine()) {\r", "//            String line = scannerForCAL.nextLine();\r", "//\r", "//            //Breaking line into tokens\r", "//            String[] tokens = line.split(\" \");\r", "//\r", "//            if (tokens.length >= 4) {\r", "//                // Saving the tokens to variables\r", "//                currentMeds = tokens[0];\r", "//                currentSpecies = tokens[1];\r", "//                currentFormula = tokens[2];\r", "//                currentDose = tokens[3];\r", "//            } // end of Token if statement\r", "//\r", "//            //checking if med name and what was entered match\r", "//            if (currentMeds.equalsIgnoreCase(meds)) {\r", "//                isMed = true;\r", "//            } else {\r", "//                line = scannerForCAL.nextLine();\r", "//            } // end of check Meds if statement\r", "//\r", "//            // if med name is correct:\r", "//            //checking if species and what was entered match\r", "//            if (isMed = true) {\r", "//                if (currentSpecies.equalsIgnoreCase(species)) {\r", "//                    isSpecies = true;\r", "//                } else {\r", "//                    line = scannerForCAL.nextLine();\r", "//                } // end of check species = true if statement\r", "//            }// end of isMed = true if statement\r", "//\r", "//            // if med name and species is correct:\r", "//            //checking if formula and what was entered match\r", "//            if (isMed == true && isSpecies == true) {\r", "//                if (currentFormula.equalsIgnoreCase(formula)) {\r", "//                    isFormula = true;\r", "//                } else {\r", "//                    line = scannerForCAL.nextLine();\r", "//                }// end of check formula = true if statement\r", "//            }// end of isMed && isSpecies = true if statement\r", "//\r", "//            // if med name and species and formula is correct:\r", "//            //checking if dose and what was entered match\r", "//            if (isMed == true && isSpecies == true && isFormula == true) {\r", "//                if (currentDose.equalsIgnoreCase(dose)) {\r", "//                    isDose = true;\r", "//                } else {\r", "//                    line = scannerForCAL.nextLine();\r", "//                }// end of check dose = true if statement\r", "//            }// end of isMed && isSpecies && isFormula = true if statement \r", "//\r", "//        } // end of while hasNextLine loop\r"], "file_path": "src/main/java/Backend/DosageCalculator.java"}
{"Link_to_commit": "https://github.com/ThaaoBlues/ecosys_mobile/commit/e2714863ae617db75880ac215f8e3e5d97798153", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 60, "n_files_impacted": 2, "longest_chunk": ["    public Globals.GenArray<String> getSyncLinkedDevices() {", "        Globals.GenArray<String> devicesList = new Globals.GenArray<>();", "", "        // Define the SQL query to retrieve linked devices", "        Cursor cursor = db.rawQuery(\"SELECT linked_devices_id FROM sync WHERE secure_id=?\",", "                new String[]{", "                        secureId", "                }", "                );", "", "        if (cursor != null) {", "            try {", "                // Move the cursor to the first row", "                if (cursor.moveToFirst()) {", "                    do {", "                        // Get the linked_devices_id from the cursor", "                        String devices_id = cursor.getString(0);", "", "                        // Split the string and add each device to the devicesList", "                        String[] devices = devices_id.split(\";\");", "                        for (String device : devices) {", "                            devicesList.add(device);", "                        }", "                    } while (cursor.moveToNext());", "                }", "            } finally {", "                cursor.close(); // Close the cursor when done", "            }", "        } else {", "            Log.e(\"AccesBdd\", \"Cursor is null\");", "        }", "", "        // Remove the last slot (empty space) in the array", "        if (!devicesList.isEmpty()) {", "            devicesList.popLast();", "        }", "", "        return devicesList;", "    }", "", "", "    // GetFileLastVersionId retrieves the last version ID of a file.", "    public long GetFileLastVersionId(String path) {", "        Cursor cursor = db.rawQuery(\"SELECT version_id FROM filesystem WHERE path=? AND secure_id=?\",", "                new String[]{", "                        path,", "                        secureId", "                }", "        );", "", "        int version_id = 0;", "        if(cursor.moveToFirst()){", "            version_id = cursor.getInt(0);", "        }", "", "        cursor.close();", "        return version_id;", "    }", "", ""], "file_path": "app/src/main/java/com/qsync/qsync/AccesBdd.java"}
{"Link_to_commit": "https://github.com/Maxyy15/SortingProjectForSchool/commit/74a4fc0c26fa06fe2c0983f8867dbe6441363de4", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 40, "n_files_impacted": 1, "longest_chunk": ["//The quick sort was adapted from this youtube video", "//I want to eventually remake this in my own unique code", "//https://www.youtube.com/watch?v=Vtckgz38QHs", "int Partition(vector<int>& arr, int startIndex, int endIndex)", "{", "\tint pivot = arr[endIndex];", "\tint i = startIndex - 1;", "", "\tfor (int j = startIndex; j <= endIndex - 1; j++)", "\t{", "\t\tif (arr[j] < pivot)", "\t\t{", "\t\t\ti++;", "\t\t\tint temp = arr[i];", "\t\t\tarr[i] = arr[j];", "\t\t\tarr[j] = temp;", "\t\t}", "\t}", "\ti++;", "\tint temp = arr[i];", "\tarr[i] = arr[endIndex];", "\tarr[endIndex] = temp;", "", "\treturn i;", "}", "void QuickSorting(vector<int>& arr, int startIndex, int endIndex)", "{", "\tif (endIndex <= startIndex) return;", "", "\tint pivot = Partition(arr, startIndex, endIndex);", "\tQuickSorting(arr, startIndex, pivot - 1);", "\tQuickSorting(arr, pivot + 1, endIndex);", "", "\tisQuickTrue = false;", "}", "", "//The insertion sort was adapted from this youtube video", "//I want to eventually remake this in my own unique code", "//https://www.youtube.com/watch?v=8mJ-OhcfpYg", "void InsertionSorting(vector<int>& arr)"], "file_path": "src/ofApp.cpp"}
{"Link_to_commit": "https://github.com/Querzion/yh.webut.uppgift.005/commit/530db7b4292dc9f99d010162c15de03e2789b2ef", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 113, "n_files_impacted": 73, "longest_chunk": ["using Business.Models;", "using Data.Entities;", "using Data.Interfaces;", "using Domain.DTOs.Forms;", "using Domain.Extensions;", "using Domain.Models;", "using Microsoft.AspNetCore.Http;", "", "namespace Business.Services;", "", "public interface IImageService", "{", "    Task<ImageServiceResult> ProcessImageAsync(ImageFormData metadata);", "    Task<ImageServiceResult> DeleteImageAsync(string imageId);", "}", "", "public class ImageService(IImageRepository imageRepository) : IImageService", "{", "    private readonly IImageRepository _imageRepository = imageRepository;", "", "    public async Task<ImageServiceResult> ProcessImageAsync(ImageFormData metadata)", "    {", "        try", "        {", "            // Ensure the required metadata values are present", "            if (string.IsNullOrEmpty(metadata.ImageUrl) || string.IsNullOrEmpty(metadata.AltText))", "            {", "                return new ImageServiceResult", "                {", "                    Succeeded = false,", "                    StatusCode = 400,", "                    Error = \"ImageUrl and AltText are required.\"", "                };", "            }", "", "            // Create the image entity from the metadata", "            var imageEntity = new ImageEntity", "            {", "                Id = Guid.NewGuid().ToString(),", "                ImageUrl = metadata.ImageUrl,", "                AltText = metadata.AltText,", "                UploadedAt = DateTime.UtcNow", "            };", "", "            // Attempt to save the image entity to the repository", "            var saveResult = await _imageRepository.AddAsync(imageEntity);", "", "            if (!saveResult.Succeeded)", "            {", "                return new ImageServiceResult", "                {", "                    Succeeded = false,", "                    StatusCode = 500,", "                    Error = saveResult.Error", "                };", "            }", "", "            // Return the result wrapped in an ImageServiceResult", "            return new ImageServiceResult", "            {", "                Succeeded = true,", "                StatusCode = 201,", "                Result = new List<Image> { saveResult.Result.MapTo<Image>() } // Wrap the single result in a list for consistency", "            };", "        }", "        catch (Exception ex)", "        {", "            // Handle unexpected errors", "            return new ImageServiceResult", "            {", "                Succeeded = false,", "                StatusCode = 500,", "                Error = ex.Message", "            };", "        }", "    }", "    ", "    public async Task<ImageServiceResult> DeleteImageAsync(string imageUrl)", "    {", "        var result = new ImageServiceResult();", "", "        // Try to find the image", "        var imageResult = await _imageRepository.GetEntityAsync(", "            x => x.ImageUrl == imageUrl", "        );", "", "        if (!imageResult.Succeeded || imageResult.Result == null)", "        {", "            result.Succeeded = false;", "            result.StatusCode = 404;", "            result.Error = \"Image not found.\";", "            return result;", "        }", "", "        var imageEntity = imageResult.Result;", "", "        // Proceed with deletion", "        var deleteResult = await _imageRepository.DeleteAsync(imageEntity);", "        if (!deleteResult.Succeeded)", "        {", "            result.Succeeded = false;", "            result.StatusCode = 500;", "            result.Error = \"Failed to delete image.\";", "            return result;", "        }", "", "        // Return success with the deleted image in Result", "        result.Succeeded = true;", "        result.StatusCode = 200;", "        result.Result = new List<Image> { imageEntity.MapTo<Image>() };", "        return result;", "    }", "}"], "file_path": "AssignmentAlpha_v7/Business/Services/ImageService.cs"}
{"Link_to_commit": "https://github.com/s-a-mcdonnell/BinThere-Compost-Bin-Locator/commit/1cf07901263fa9838c373f6ab59be1534aa3d7ce", "n-gram matched": "help of chatgpt", "n_lines_longer_change": 7, "n_files_impacted": 2, "longest_chunk": ["", "            //TODO: add a silly little coment", "            int w = this.image.getWidth();", "            int h = this.image.getHeight();", "            double proportion = ((double) w) / (double) h;", "            this.image = toBufferedImage(image.getScaledInstance((int) (HEIGHT * proportion), HEIGHT, java.awt.Image.SCALE_SMOOTH));", ""], "file_path": "CompostFinder.java"}
{"Link_to_commit": "https://github.com/poorna28/DashboardPersonal/commit/c809883a7ae6293a704bbbfb4591f9c9eec9f813", "n-gram matched": "changes of copilot", "n_lines_longer_change": 73, "n_files_impacted": 10, "longest_chunk": ["    <div className=\"signup-container\">", "      <div className=\"signup-card p-card\">", "        <div className=\"p-card-title\">Create Your Account</div>", "        <div className=\"p-card-body\">", "          <div className=\"p-field\">", "            <label htmlFor=\"name\">Name</label>", "            <input", "              id=\"name\"", "              type=\"text\"", "              className=\"p-inputtext p-component\"", "              placeholder=\"Enter your name\"", "              value={signUpInfo.name}", "              onChange={(e) =>", "                setSignUpInfo({ ...signUpInfo, name: e.target.value })", "              }", "            />", "            {errors.name && <p style={{ color: \"red\" }}>{errors.name}</p>}", "          </div>", "          <div className=\"p-field\">", "            <label htmlFor=\"email\">Email</label>", "            <input", "              id=\"email\"", "              type=\"email\"", "              className=\"p-inputtext p-component\"", "              placeholder=\"Enter your email\"", "              value={signUpInfo.email}", "              onChange={(e) =>", "                setSignUpInfo({ ...signUpInfo, email: e.target.value })", "              }", "            />", "            {errors.email && <p style={{ color: \"red\" }}>{errors.email}</p>}", "          </div>", "          <div className=\"p-field\">", "            <label htmlFor=\"password\">Password</label>", "            <input", "              id=\"password\"", "              type=\"password\"", "              className=\"p-inputtext p-component\"", "              placeholder=\"Enter your password\"", "              value={signUpInfo.password}", "              onChange={(e) =>", "                setSignUpInfo({ ...signUpInfo, password: e.target.value })", "              }", "            />", "            {errors.password && <p style={{ color: \"red\" }}>{errors.password}</p>}", "          </div>", "          <div className=\"p-field\">", "            <label htmlFor=\"mobileNumber\">Mobile Number</label>", "            <input", "              id=\"mobileNumber\"", "              type=\"text\"", "              className=\"p-inputtext p-component\"", "              placeholder=\"Enter your mobile number\"", "              value={signUpInfo.mobileNumber}", "              onChange={(e) =>", "                setSignUpInfo({ ...signUpInfo, mobileNumber: e.target.value })", "              }", "            />", "            {errors.mobileNumber && (", "              <p style={{ color: \"red\" }}>{errors.mobileNumber}</p>", "            )}", "          </div>", "          {errors.server && <p style={{ color: \"red\" }}>{errors.server}</p>}", "          <button className=\"signup-button p-button\" onClick={handleSignUp}>", "            Sign Up", "          </button>", "          <p style={{ textAlign: \"center\", marginTop: \"1rem\" }}>", "            Already have an account?{\" \"}", "            <Link to=\"/login\" className=\"text-primary no-underline\">", "              Login", "            </Link>", "          </p>", "        </div>"], "file_path": "src/components/SignUp.js"}
{"Link_to_commit": "https://github.com/quan3c10/playwright-practice/commit/2eede02cc4eb63007b3e610186435ef72fcc7379", "n-gram matched": "copilot to generate", "n_lines_longer_change": 240, "n_files_impacted": 13, "longest_chunk": ["import { expect, Locator, Page } from \"@playwright/test\";", "import { PageBase } from \"./PageBase\";", "import { Address, CheckoutProduct } from \"../models/CheckoutModels\";", "import { User } from \"../models/UserModels\";", "import { Product } from \"../models/ProductModels\";", "", "export class Checkout extends PageBase {", "    // Locators", "    deliveryAddressSection: Locator;", "    billingAddressSection: Locator;", "    productRows: Locator;", "    commentTextarea: Locator;", "    placeOrderButton: Locator;", "    totalAmount: Locator;", "", "    constructor(page: Page) {", "        super(page);", "", "        this.url = \"/checkout\";", "        this.title = \"Automation Exercise - Checkout\";", "        this.logo = \"//div[contains(@class,'logo')]\";", "", "        this.deliveryAddressSection = page.locator(\"#address_delivery\");", "        this.billingAddressSection = page.locator(\"#address_invoice\");", "        this.productRows = page.locator(\"tr[id^='product-']\");", "        this.commentTextarea = page.locator(\"#ordermsg textarea\");", "        this.placeOrderButton = page.locator(\".check_out\");", "        this.totalAmount = page.locator(\".cart_total_price\").last();", "    }", "", "    /**", "     * Get delivery address details", "     * @returns Address object with delivery information", "     */", "    async getDeliveryAddress(): Promise<Address> {", "        await expect(this.deliveryAddressSection).toBeVisible();", "        ", "        const fullName = await this.deliveryAddressSection.locator(\".address_firstname.address_lastname\").textContent() || \"\";", "        ", "        // Get address lines (skipping empty ones)", "        const addressLinesElements = this.deliveryAddressSection.locator(\".address_address1.address_address2\");", "        const addressLinesCount = await addressLinesElements.count();", "        const addressLines: string[] = [];", "        ", "        for (let i = 0; i < addressLinesCount; i++) {", "            const lineText = await addressLinesElements.nth(i).textContent() || \"\";", "            if (lineText.trim()) {", "                addressLines.push(lineText.trim());", "            }", "        }", "        ", "        const cityStatePostcode = await this.deliveryAddressSection.locator(\".address_city.address_state_name.address_postcode\").textContent() || \"\";", "        const country = await this.deliveryAddressSection.locator(\".address_country_name\").textContent() || \"\";", "        const phone = await this.deliveryAddressSection.locator(\".address_phone\").textContent() || \"\";", "        ", "        return {", "            fullName,", "            addressLines,", "            cityStatePostcode,", "            country,", "            phone", "        };", "    }", "", "    /**", "     * Get billing address details", "     * @returns Address object with billing information", "     */", "    async getBillingAddress(): Promise<Address> {", "        await expect(this.billingAddressSection).toBeVisible();", "        ", "        const fullName = await this.billingAddressSection.locator(\".address_firstname.address_lastname\").textContent() || \"\";", "        ", "        // Get address lines (skipping empty ones)", "        const addressLinesElements = this.billingAddressSection.locator(\".address_address1.address_address2\");", "        const addressLinesCount = await addressLinesElements.count();", "        const addressLines: string[] = [];", "        ", "        for (let i = 0; i < addressLinesCount; i++) {", "            const lineText = await addressLinesElements.nth(i).textContent() || \"\";", "            if (lineText.trim()) {", "                addressLines.push(lineText.trim());", "            }", "        }", "        ", "        const cityStatePostcode = await this.billingAddressSection.locator(\".address_city.address_state_name.address_postcode\").textContent() || \"\";", "        const country = await this.billingAddressSection.locator(\".address_country_name\").textContent() || \"\";", "        const phone = await this.billingAddressSection.locator(\".address_phone\").textContent() || \"\";", "        ", "        return {", "            fullName,", "            addressLines,", "            cityStatePostcode,", "            country,", "            phone", "        };", "    }", "", "    /**", "     * Get product information by index", "     * @param index The index of the product (0-based)", "     * @returns CheckoutProduct object with product details", "     */", "    async getProductInfo(index: number): Promise<CheckoutProduct> {", "        const productRow = this.productRows.nth(index);", "        await expect(productRow).toBeVisible();", "        ", "        const id = (await productRow.getAttribute(\"id\") || \"\").replace(\"product-\", \"\");", "        const name = await productRow.locator(\".cart_description h4 a\").textContent() || \"\";", "        const category = await productRow.locator(\".cart_description p\").textContent() || \"\";", "        const price = await productRow.locator(\".cart_price p\").textContent() || \"\";", "        const quantity = await productRow.locator(\".cart_quantity button\").textContent() || \"\";", "        const total = await productRow.locator(\".cart_total p\").textContent() || \"\";", "        ", "        return {", "            id:Number(id),", "            name,", "            category,", "            price:Number(price),", "            quantity:Number(quantity),", "            availability: \"\",", "            condition: \"\",", "            brand: \"\",", "            total", "        };", "    }", "", "    /**", "     * Get product information by product ID", "     * @param name The ID of the product", "     * @returns CheckoutProduct object with product details", "     */", "    async getProductByName(productName: string): Promise<CheckoutProduct> {", "        const productRow = this.page.locator(`//a[contains(text(),\"${productName}\")]//ancestor::tr`);", "        ", "        if (await productRow.count() === 0) {", "            console.log(`Product with name \"${productName}\" not found in the checkout.`);", "        }", "        ", "        await expect(productRow).toBeVisible();", "        ", "        const id = (await productRow.getAttribute(\"id\") || \"\").replace(\"product-\", \"\");", "        const name = await productRow.locator(\".cart_description h4 a\").textContent() || \"\";", "        const category = await productRow.locator(\".cart_description p\").textContent() || \"\";", "        const price = await productRow.locator(\".cart_price p\").textContent() || \"\";", "        const quantity = await productRow.locator(\".cart_quantity button\").textContent() || \"\";", "        const total = await productRow.locator(\".cart_total p\").textContent() || \"\";", "        ", "        return {", "            id: Number(id),", "            name,", "            category,", "            price: Number(price),", "            quantity: Number(quantity),", "            availability: \"\",", "            condition: \"\",", "            brand: \"\",", "            total", "        };", "    }", "", "    async verifyDeliveryAddress(user: User) {", "        const deliveryAddress = await this.getDeliveryAddress();", "", "        expect(deliveryAddress.fullName).toContain(user.name);", "        expect(deliveryAddress.addressLines[0]).toContain(user.addressInfo.firstName);", "        expect(deliveryAddress.cityStatePostcode).toContain(user.addressInfo.city);", "        expect(deliveryAddress.country).toContain(user.addressInfo.country);", "        expect(deliveryAddress.phone).toContain(user.addressInfo.mobile);", "", "        return this;", "    }", "", "    async verifyBillingAddress(user: User) {", "        const billingAddress = await this.getBillingAddress();", "", "        expect(billingAddress.fullName).toContain(user.name);", "        expect(billingAddress.addressLines[0]).toContain(user.addressInfo.firstName);", "        expect(billingAddress.cityStatePostcode).toContain(user.addressInfo.city);", "        expect(billingAddress.country).toContain(user.addressInfo.country);", "        expect(billingAddress.phone).toContain(user.addressInfo.mobile);", "", "        return this;", "    }", "", "    async verifyProductInfo(product: Product) {", "        const productInfo = await this.getProductByName(product.name);", "", "        expect(productInfo.name).toEqual(product.name);", "        expect(productInfo.category).toEqual(product.category);", "        expect(productInfo.price).toEqual(`Rs. ${product.price}`);", "        expect(productInfo.quantity).toEqual(product.quantity);", "        expect(productInfo.total).toEqual(`Rs. ${product.price * product.quantity}`);", "", "        return this;", "    }", "", "    /**", "     * Get all products in the checkout", "     * @returns Array of CheckoutProduct objects", "     */", "    async getAllProducts(): Promise<CheckoutProduct[]> {", "        const products: CheckoutProduct[] = [];", "        const count = await this.productRows.count();", "        ", "        for (let i = 0; i < count; i++) {", "            products.push(await this.getProductInfo(i));", "        }", "        ", "        return products;", "    }", "", "    /**", "     * Add a comment to the order", "     * @param comment The text to add as a comment", "     */", "    async addComment(comment: string) {", "        await expect(this.commentTextarea).toBeVisible();", "        await this.commentTextarea.fill(comment);", "        return this;", "    }", "", "    /**", "     * Place the order by clicking the Place Order button", "     */", "    async placeOrder() {", "        await expect(this.placeOrderButton).toBeVisible();", "        await this.placeOrderButton.click();", "        await this.page.waitForLoadState('networkidle');", "        return this;", "    }", "", "    /**", "     * Get the total amount of the order", "     * @returns The total amount as a string", "     */", "    async getTotalAmount(): Promise<string> {", "        return await this.totalAmount.textContent() || \"\";", "    }", "}"], "file_path": "e2e/pages/Checkout.ts"}
{"Link_to_commit": "https://github.com/vishalbb-git/streamlint-demo/commit/b8c63465983971acb9209999b4b83816eb7611e0", "n-gram matched": "copilot to generate", "n_lines_longer_change": 236, "n_files_impacted": 8, "longest_chunk": ["import streamlit as st", "import pandas as pd", "import numpy as np", "import pydeck as pdk", "import altair as alt", "", "st.set_page_config(page_title=\"Maps & Geospatial\", page_icon=\"\ud83d\uddfa\ufe0f\")", "st.title(\"Maps and Geospatial Visualizations\")", "st.sidebar.header(\"Map Options\")", "", "# Generate sample geospatial data", "@st.cache_data", "def generate_geo_data():", "    # Generate random points around the world", "    np.random.seed(42)", "    n_points = 1000", "    ", "    # Bounds for the data (roughly world bounds)", "    lat_bounds = (-60, 70)  # Avoiding extreme latitudes", "    lon_bounds = (-180, 180)", "    ", "    # Generate random points", "    lats = np.random.uniform(lat_bounds[0], lat_bounds[1], n_points)", "    lons = np.random.uniform(lon_bounds[0], lon_bounds[1], n_points)", "    ", "    # Create categories and values", "    categories = np.random.choice(['A', 'B', 'C', 'D'], n_points)", "    values = np.random.normal(0, 1, n_points) * 10", "    sizes = np.abs(values) + np.random.uniform(1, 5, n_points)", "    ", "    # Create DataFrame", "    df = pd.DataFrame({", "        'lat': lats,", "        'lon': lons,", "        'category': categories,", "        'value': values,", "        'size': sizes", "    })", "    ", "    return df", "", "geo_data = generate_geo_data()", "", "# Map selection", "map_type = st.sidebar.selectbox(", "    \"Select Map Type\",", "    [\"Scatter Map\", \"Hexagon Map\", \"Heatmap\", \"3D Column Map\", \"Arc Map\"]", ")", "", "map_style = st.sidebar.selectbox(", "    \"Select Map Style\",", "    [\"Dark\", \"Light\", \"Satellite\"]", ")", "", "style_dict = {", "    \"Dark\": \"mapbox://styles/mapbox/dark-v10\",", "    \"Light\": \"mapbox://styles/mapbox/light-v10\",", "    \"Satellite\": \"mapbox://styles/mapbox/satellite-v9\"", "}", "", "st.subheader(f\"{map_type} Example\")", "st.write(\"Using PyDeck for advanced geospatial visualizations\")", "", "# Initial view state", "view_state = pdk.ViewState(", "    latitude=0,", "    longitude=0,", "    zoom=1,", "    pitch=0", ")", "", "if map_type == \"Scatter Map\":", "    # Basic scatter map", "    scatter_layer = pdk.Layer(", "        'ScatterplotLayer',", "        data=geo_data,", "        get_position='[lon, lat]',", "        get_color='[value > 0 ? 200 : 50, 100, value > 0 ? 50 : 200, 160]',", "        get_radius='size * 10000',", "        pickable=True,", "        opacity=0.8,", "    )", "    ", "    deck = pdk.Deck(", "        layers=[scatter_layer],", "        initial_view_state=view_state,", "        map_style=style_dict[map_style],", "        tooltip={\"text\": \"Value: {value}\\nCategory: {category}\"}", "    )", "    st.pydeck_chart(deck)", "", "elif map_type == \"Hexagon Map\":", "    hexagon_layer = pdk.Layer(", "        'HexagonLayer',", "        data=geo_data,", "        get_position='[lon, lat]',", "        radius=100000,", "        elevation_scale=500,", "        elevation_range=[0, 1000],", "        pickable=True,", "        extruded=True,", "        coverage=1,", "        auto_highlight=True", "    )", "    ", "    deck = pdk.Deck(", "        layers=[hexagon_layer],", "        initial_view_state=view_state,", "        map_style=style_dict[map_style],", "    )", "    st.pydeck_chart(deck)", "", "elif map_type == \"Heatmap\":", "    heatmap_layer = pdk.Layer(", "        'HeatmapLayer',", "        data=geo_data,", "        get_position='[lon, lat]',", "        opacity=0.9,", "        get_weight='size',", "        aggregation='\"MEAN\"',", "        threshold=0.05", "    )", "    ", "    deck = pdk.Deck(", "        layers=[heatmap_layer],", "        initial_view_state=view_state,", "        map_style=style_dict[map_style],", "    )", "    st.pydeck_chart(deck)", "", "elif map_type == \"3D Column Map\":", "    column_layer = pdk.Layer(", "        'ColumnLayer',", "        data=geo_data,", "        get_position='[lon, lat]',", "        get_elevation='size * 1000',", "        elevation_scale=100,", "        radius=50000,", "        get_fill_color='[255 * (value > 0 ? 1 : 0), 100, 255 * (value < 0 ? 1 : 0), 140]',", "        pickable=True,", "        auto_highlight=True,", "        extruded=True,", "    )", "    ", "    # Use a different view state with pitch for 3D visualization", "    view_state_3d = pdk.ViewState(", "        latitude=0,", "        longitude=0,", "        zoom=1,", "        pitch=45", "    )", "    ", "    deck = pdk.Deck(", "        layers=[column_layer],", "        initial_view_state=view_state_3d,", "        map_style=style_dict[map_style],", "        tooltip={\"text\": \"Value: {value}\\nSize: {size}\"}", "    )", "    st.pydeck_chart(deck)", "", "elif map_type == \"Arc Map\":", "    # For the Arc Map, generate some origin-destination pairs", "    np.random.seed(42)", "    n_arcs = 100", "    ", "    # Generate random points for origins", "    origins = geo_data.sample(n_arcs)[['lon', 'lat', 'value']]", "    origins = origins.rename(columns={'lon': 'lon_origin', 'lat': 'lat_origin'})", "    ", "    # Generate random points for destinations", "    destinations = geo_data.sample(n_arcs)[['lon', 'lat']]", "    destinations = destinations.rename(columns={'lon': 'lon_dest', 'lat': 'lat_dest'})", "    ", "    # Combine to create arc data", "    arc_data = pd.concat([origins.reset_index(drop=True), ", "                         destinations.reset_index(drop=True)], axis=1)", "    arc_data['value'] = np.abs(arc_data['value'])", "    ", "    arc_layer = pdk.Layer(", "        'ArcLayer',", "        data=arc_data,", "        get_source_position='[lon_origin, lat_origin]',", "        get_target_position='[lon_dest, lat_dest]',", "        get_width='value * 2',", "        get_source_color=[255, 0, 0, 200],", "        get_target_color=[0, 0, 255, 200],", "        pickable=True,", "    )", "    ", "    deck = pdk.Deck(", "        layers=[arc_layer],", "        initial_view_state=view_state,", "        map_style=style_dict[map_style],", "    )", "    st.pydeck_chart(deck)", "", "# Show filtered data", "st.subheader(\"Data Explorer\")", "", "# Category filter", "categories = sorted(geo_data['category'].unique())", "selected_categories = st.multiselect(\"Filter by Category\", categories, default=categories[:2])", "", "if selected_categories:", "    filtered_data = geo_data[geo_data['category'].isin(selected_categories)]", "    ", "    # Show map of filtered data", "    view_state_filtered = pdk.ViewState(", "        latitude=filtered_data['lat'].mean(),", "        longitude=filtered_data['lon'].mean(),", "        zoom=1", "    )", "    ", "    filtered_layer = pdk.Layer(", "        'ScatterplotLayer',", "        data=filtered_data,", "        get_position='[lon, lat]',", "        get_color='[200, 30, 100, 160]',", "        get_radius='size * 5000',", "        pickable=True", "    )", "    ", "    filtered_deck = pdk.Deck(", "        layers=[filtered_layer],", "        initial_view_state=view_state_filtered,", "        map_style=style_dict[map_style],", "        tooltip={\"text\": \"Value: {value}\\nCategory: {category}\"}", "    )", "    ", "    st.pydeck_chart(filtered_deck)", "    ", "    # Show data table", "    with st.expander(\"Show Data Table\"):", "        st.dataframe(filtered_data)", "else:", "    st.warning(\"Please select at least one category.\")"], "file_path": "pages/04_data_tables.py"}
{"Link_to_commit": "https://github.com/soyfidelfm/EnhancingBlazorDevelopment/commit/329ea7c59ca020b7e13a9d791d4ad56caa0f02c1", "n-gram matched": "copilot to generate", "n_lines_longer_change": 81, "n_files_impacted": 69, "longest_chunk": ["is_global = true", "build_property.TargetFramework = net6.0", "build_property.TargetPlatformMinVersion = ", "build_property.UsingMicrosoftNETSdkWeb = true", "build_property.ProjectTypeGuids = ", "build_property.InvariantGlobalization = ", "build_property.PlatformNeutralAssembly = ", "build_property.EnforceExtendedAnalyzerRules = ", "build_property._SupportedPlatformList = Linux,macOS,Windows", "build_property.RootNamespace = EventEase", "build_property.RootNamespace = EventEase", "build_property.ProjectDir = C:\\Users\\fidel\\EventEase\\", "build_property.EnableComHosting = ", "build_property.EnableGeneratedComInterfaceComImportInterop = ", "build_property.RazorLangVersion = 6.0", "build_property.SupportLocalizedComponentNames = ", "build_property.GenerateRazorMetadataSourceChecksumAttributes = ", "build_property.MSBuildProjectDirectory = C:\\Users\\fidel\\EventEase", "build_property._RazorSourceGeneratorDebug = ", "build_property.EffectiveAnalysisLevelStyle = 6.0", "build_property.EnableCodeStyleSeverity = ", "", "[C:/Users/fidel/EventEase/App.razor]", "build_metadata.AdditionalFiles.TargetPath = QXBwLnJhem9y", "build_metadata.AdditionalFiles.CssScope = ", "", "[C:/Users/fidel/EventEase/Pages/Counter.razor]", "build_metadata.AdditionalFiles.TargetPath = UGFnZXNcQ291bnRlci5yYXpvcg==", "build_metadata.AdditionalFiles.CssScope = ", "", "[C:/Users/fidel/EventEase/Pages/EventDetails.razor]", "build_metadata.AdditionalFiles.TargetPath = UGFnZXNcRXZlbnREZXRhaWxzLnJhem9y", "build_metadata.AdditionalFiles.CssScope = ", "", "[C:/Users/fidel/EventEase/Pages/Events.razor]", "build_metadata.AdditionalFiles.TargetPath = UGFnZXNcRXZlbnRzLnJhem9y", "build_metadata.AdditionalFiles.CssScope = ", "", "[C:/Users/fidel/EventEase/Pages/FetchData.razor]", "build_metadata.AdditionalFiles.TargetPath = UGFnZXNcRmV0Y2hEYXRhLnJhem9y", "build_metadata.AdditionalFiles.CssScope = ", "", "[C:/Users/fidel/EventEase/Pages/Index.razor]", "build_metadata.AdditionalFiles.TargetPath = UGFnZXNcSW5kZXgucmF6b3I=", "build_metadata.AdditionalFiles.CssScope = ", "", "[C:/Users/fidel/EventEase/Pages/Register.razor]", "build_metadata.AdditionalFiles.TargetPath = UGFnZXNcUmVnaXN0ZXIucmF6b3I=", "build_metadata.AdditionalFiles.CssScope = ", "", "[C:/Users/fidel/EventEase/Shared/EventCard.razor]", "build_metadata.AdditionalFiles.TargetPath = U2hhcmVkXEV2ZW50Q2FyZC5yYXpvcg==", "build_metadata.AdditionalFiles.CssScope = ", "", "[C:/Users/fidel/EventEase/Shared/SurveyPrompt.razor]", "build_metadata.AdditionalFiles.TargetPath = U2hhcmVkXFN1cnZleVByb21wdC5yYXpvcg==", "build_metadata.AdditionalFiles.CssScope = ", "", "[C:/Users/fidel/EventEase/_Imports.razor]", "build_metadata.AdditionalFiles.TargetPath = X0ltcG9ydHMucmF6b3I=", "build_metadata.AdditionalFiles.CssScope = ", "", "[C:/Users/fidel/EventEase/Shared/MainLayout.razor]", "build_metadata.AdditionalFiles.TargetPath = U2hhcmVkXE1haW5MYXlvdXQucmF6b3I=", "build_metadata.AdditionalFiles.CssScope = b-72osl0plp5", "", "[C:/Users/fidel/EventEase/Shared/NavMenu.razor]", "build_metadata.AdditionalFiles.TargetPath = U2hhcmVkXE5hdk1lbnUucmF6b3I=", "build_metadata.AdditionalFiles.CssScope = b-z2ef545u2c", "", "[C:/Users/fidel/EventEase/Pages/Error.cshtml]", "build_metadata.AdditionalFiles.TargetPath = UGFnZXNcRXJyb3IuY3NodG1s", "build_metadata.AdditionalFiles.CssScope = ", "", "[C:/Users/fidel/EventEase/Pages/_Host.cshtml]", "build_metadata.AdditionalFiles.TargetPath = UGFnZXNcX0hvc3QuY3NodG1s", "build_metadata.AdditionalFiles.CssScope = ", "", "[C:/Users/fidel/EventEase/Pages/_Layout.cshtml]", "build_metadata.AdditionalFiles.TargetPath = UGFnZXNcX0xheW91dC5jc2h0bWw=", "build_metadata.AdditionalFiles.CssScope = "], "file_path": "obj/Debug/net6.0/EventEase.GlobalUsings.g.cs"}
{"Link_to_commit": "https://github.com/JWhiteSS/skills-copilot-codespaces-vscode/commit/e0cc3f813ee7e97c1ab9b6f02cbb7bb08b4a7839", "n-gram matched": "copilot to generate", "n_lines_longer_change": 14, "n_files_impacted": 1, "longest_chunk": ["// const express = require('express');", "", "// mock the express app", "const express = function() {", "    return {", "        use: function() {},", "        get: function() {},", "        listen: function() {}", "    };", "};", "Object.defineProperty(express, 'static', {", "    get: () => () => {}", "});", ""], "file_path": "comments.js"}
{"Link_to_commit": "https://github.com/DraconDev/git-ai-committer/commit/948fd1f2658ff55ee33d1adcbce7219d1512be5d", "n-gram matched": "copilot to generate", "n_lines_longer_change": 9, "n_files_impacted": 3, "longest_chunk": ["    ", "    // Since Copilot updates the source control box, get the message from there", "    const { message, repo } = getSourceControlMessage();", "    if (message) {", "      // Clear the source control box since we're getting the message", "      clearSourceControlMessage(repo);", "      return message;", "    }", "    "], "file_path": "src/ai/copilotService.ts"}
{"Link_to_commit": "https://github.com/peterkahumu/solid-octo-fortnight-copilot/commit/d83ee4c7ecf02fcdcd0360776dcb661982aad542", "n-gram matched": "copilot to generate", "n_lines_longer_change": 34, "n_files_impacted": 1, "longest_chunk": ["// create a web server", "", "// import express", "const express = require('express');", "const app = express();", "", "// import body-parser", "const bodyParser = require('body-parser');", "app.use(bodyParser.json());", "", "// import comments.js", "const comments = require('./comments');", "", "// import cors", "const cors = require('cors');", "app.use(cors());", "", "// get all comments", "app.get('/comments', (req, res) => {", "  const allComments = comments.getAllComments();", "  res.json(allComments);", "});", "", "// post a new comment", "app.post('/comments', (req, res) => {", "  const newComment = req.body;", "  comments.addComment(newComment);", "  res.json(newComment);", "});", "", "// listen on port 3000", "app.listen(3000, () => {", "  console.log('Server is listening on port 3000');", "});"], "file_path": "comments.js"}
{"Link_to_commit": "https://github.com/ZakaNaji/e-commerce/commit/b06b9031cef45ce4e2ee1a043cec4888926626be", "n-gram matched": "copilot to generate", "n_lines_longer_change": 32, "n_files_impacted": 2, "longest_chunk": ["    /*pls give me sql to add to scmea.sql for user and role entities", "    create table users (", "    user_id bigint primary key auto_increment,", "    username varchar(255) not null,", "    password varchar(255) not null,", "    email varchar(255) not null", "    );", "    create table roles (", "    role_id bigint primary key auto_increment,", "    role_name varchar(255) not null", "    );", "    create table user_roles (", "    user_id bigint,", "    role_id bigint,", "    primary key (user_id, role_id),", "    foreign key (user_id) references users (user_id),", "    foreign key (role_id) references roles (role_id)", "    );", "", "     */", "    /*pls create sql to add to data.sql with 3 users and 3 roles admin, user, seller", "    insert into users (username, password, email) values ('admin', '1234', 'admin@gmail.com');", "    insert into users (username, password, email) values ('user', '1234', 'user@gmail.com');", "    insert into users (username, password, email) values ('seller', '1234', 'seller@gmail.com');", "    insert into roles (role_name) values ('ROLE_ADMIN');", "    insert into roles (role_name) values ('ROLE_USER');", "    insert into roles (role_name) values ('ROLE_SELLER');", "    insert into user_roles (user_id, role_id) values (1, 1);", "    insert into user_roles (user_id, role_id) values (2, 2);", "    insert into user_roles (user_id, role_id) values (3, 3);", "", "     */"], "file_path": "src/main/java/com/znaji/ecommerce_app/entity/User.java"}
{"Link_to_commit": "https://github.com/renanpmaciel/ai_coding_github_copilot/commit/433f4074b719a0e28d373bc7569680bafb7d318f", "n-gram matched": "copilot to generate", "n_lines_longer_change": 26, "n_files_impacted": 1, "longest_chunk": ["# Connect to a linux server using ssh and execute commands  ", "", "import paramiko", "import time", "", "def connect_and_run_command(hostname, username, password, command):", "    # Create a new SSH client object", "    client = paramiko.SSHClient()", "", "    # Automatically add the server's host key", "    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())", "", "    # Connect to the server", "    client.connect(hostname, username=username, password=password)", "    shell = client.invoke_shell()", "    shell.send(command)", "    time.sleep(2)", "    output = shell.recv(65535)", "    print(output.decode('utf-8'))", "    client.close()", "", "if __name__ == \"__main__\":", "    IP_ADDRESS='xxx.xx.xxx.xxx'", "    PASSWORD='xxxxxx'", "    USERNAME='xxxxxx'", "    connect_and_run_command(IP_ADDRESS, USERNAME, PASSWORD, 'who;ip address\\n')"], "file_path": "network_automation.py"}
{"Link_to_commit": "https://github.com/jamillambert/skills-copilot-codespaces-vscode/commit/180e7107fd70a929d964b1f31e44e6f8e8c85050", "n-gram matched": "copilot to generate", "n_lines_longer_change": 146, "n_files_impacted": 1, "longest_chunk": ["}", "", "// A function that calculates the area of a rectangle", "function calculateRectangleArea(width, height) {", "    return width * height;", "}", "", "// A function that calculates the area of a triangle", "function calculateTriangleArea(base, height) {", "    return 0.5 * base * height;", "}", "", "// A function that calculates the area of a square", "function calculateSquareArea(side) {", "    return side * side;", "}", "", "// A function that calculates the area of a trapezoid", "function calculateTrapezoidArea(base1, base2, height) {", "    return 0.5 * (base1 + base2) * height;", "}", "", "// A function that calculates the area of a parallelogram", "function calculateParallelogramArea(base, height) {", "    return base * height;", "}", "", "// A function that calculates the area of a rhombus", "function calculateRhombusArea(diagonal1, diagonal2) {", "    return 0.5 * diagonal1 * diagonal2;", "}", "", "// A function that calculates the area of a kite", "function calculateKiteArea(diagonal1, diagonal2) {", "    return 0.5 * diagonal1 * diagonal2;", "}", "", "// A function that calculates the area of a regular polygon", "function calculateRegularPolygonArea(perimeter, apothem) {", "    return 0.5 * perimeter * apothem;", "}", "", "// A function that calculates the area of a sector", "function calculateSectorArea(radius, angle) {", "    return 0.5 * radius * radius * angle;", "}", "", "// A function that calculates the area of a segment", "function calculateSegmentArea(radius, angle) {", "    return 0.5 * radius * radius * (angle - Math.sin(angle));", "}", "", "// A function that calculates the area of an ellipse", "function calculateEllipseArea(radius1, radius2) {", "    return Math.PI * radius1 * radius2;", "}", "", "// A function that calculates the area of a cube", "function calculateCubeArea(side) {", "    return 6 * side * side;", "}", "", "// A function that calculates the area of a cuboid", "function calculateCuboidArea(length, width, height) {", "    return 2 * (length * width + length * height + width * height);", "}", "", "// A function that calculates the area of a cylinder", "function calculateCylinderArea(radius, height) {", "    return 2 * Math.PI * radius * (radius + height);", "}", "", "// A function that calculates the area of a cone", "function calculateConeArea(radius, height) {", "    return Math.PI * radius * (radius + Math.sqrt(radius * radius + height * height));", "}", "", "// A function that calculates the area of a sphere", "function calculateSphereArea(radius) {", "    return 4 * Math.PI * radius * radius;", "}", "", "// A function that calculates the area of a hemisphere", "function calculateHemisphereArea(radius) {", "    return 3 * Math.PI * radius * radius;", "}", "", "// A function that calculates the area of a pyramid", "function calculatePyramidArea(base, slantHeight) {", "    return base + 0.5 * base * slantHeight;", "}", "", "// A function that calculates the area of a prism", "function calculatePrismArea(base, height) {", "    return 2 * base + base * height;", "}", "", "// Tests for all of the functions and compares to the know value.  Outputs to the terminal the result of the test.", "function test() {", "    let result = calculateNumbers(2, 3);", "    console.log(result === 5 ? 'Test passed' : 'Test failed');", "", "    result = calculateArea(5);", "    console.log(result === 78.53981633974483 ? 'Test passed' : 'Test failed');", "", "    result = calculateRectangleArea(5, 10);", "    console.log(result === 50 ? 'Test passed' : 'Test failed');", "", "    result = calculateTriangleArea(5, 10);", "    console.log(result === 25 ? 'Test passed' : 'Test failed');", "", "    result = calculateSquareArea(5);", "    console.log(result === 25 ? 'Test passed' : 'Test failed');", "", "    result = calculateTrapezoidArea(5, 10, 15);", "    console.log(result === 112.5 ? 'Test passed' : 'Test failed');", "", "    result = calculateParallelogramArea(5, 10);", "    console.log(result === 50 ? 'Test passed' : 'Test failed');", "", "    result = calculateRhombusArea(5, 10);", "    console.log(result === 25 ? 'Test passed' : 'Test failed');", "", "    result = calculateKiteArea(5, 10);", "    console.log(result === 25 ? 'Test passed' : 'Test failed');", "", "    result = calculateRegularPolygonArea(5, 10);", "    console.log(result === 25 ? 'Test passed' : 'Test failed');", "", "    result = calculateSectorArea(5, 10);", "    console.log(result === 125 ? 'Test passed' : 'Test failed');", "", "    result = calculateSegmentArea(5, 10);", "    console.log(result === 25.0 ? 'Test passed' : 'Test failed');", "", "    result = calculateEllipseArea(5, 10);", "    console.log(result === 157.07963267948966 ? 'Test passed' : 'Test failed');", "", "    result = calculateCubeArea(5);", "    console.log(result === 150 ? 'Test passed' : 'Test failed');", "", "    result = calculateCuboidArea(5, 10, 15);", "    console.log(result === 220 ? 'Test passed' : 'Test failed');", "", "    result = calculateCylinderArea(5, 10);", "    console.log(result === 471.23889803846896 ? 'Test passed' : 'Test failed');"], "file_path": "skills.js"}
{"Link_to_commit": "https://github.com/rhtran/pet-clinic/commit/6d2eb8eca20e88f9f74182fee7b4b44e8e52728b", "n-gram matched": "copilot to generate", "n_lines_longer_change": 75, "n_files_impacted": 6, "longest_chunk": ["package app", "", "import (", "\t\"testing\"", "", "\t\"github.com/stretchr/testify/assert\"", ")", "", "func Test_ValidDatabaseConfig(t *testing.T) {", "\tconfig := DatabaseConfig{", "\t\tPostgres: PostgresConfig{", "\t\t\tDriver: \"postgres\",", "\t\t\tDsn:    \"postgres://user:password@localhost:5432/dbname\",", "\t\t},", "\t\tMaxIdleConns: 10,", "\t\tMaxOpenConns: 100,", "\t\tMaxIdleTime:  30,", "\t}", "", "\terr := config.Validate()", "\tassert.NoError(t, err)", "}", "", "func Test_InvalidDatabaseConfigMissingPostgres(t *testing.T) {", "\tconfig := DatabaseConfig{", "\t\tMaxIdleConns: 10,", "\t\tMaxOpenConns: 100,", "\t\tMaxIdleTime:  30,", "\t}", "", "\terr := config.Validate()", "\tassert.Error(t, err)", "}", "", "func Test_InvalidDatabaseConfigMissingMaxOpenConns(t *testing.T) {", "\tconfig := DatabaseConfig{", "\t\tPostgres: PostgresConfig{", "\t\t\tDriver: \"postgres\",", "\t\t\tDsn:    \"postgres://user:password@localhost:5432/dbname\",", "\t\t},", "\t\tMaxIdleConns: 10,", "\t\tMaxIdleTime:  30,", "\t}", "", "\terr := config.Validate()", "\tassert.Error(t, err)", "}", "", "func Test_ValidPostgresConfig(t *testing.T) {", "\tconfig := PostgresConfig{", "\t\tDriver: \"postgres\",", "\t\tDsn:    \"postgres://user:password@localhost:5432/dbname\",", "\t}", "", "\terr := config.Validate()", "\tassert.NoError(t, err)", "}", "", "func Test_InvalidPostgresConfigMissingDriver(t *testing.T) {", "\tconfig := PostgresConfig{", "\t\tDsn: \"postgres://user:password@localhost:5432/dbname\",", "\t}", "", "\terr := config.Validate()", "\tassert.Error(t, err)", "}", "", "func Test_InvalidPostgresConfigMissingDsn(t *testing.T) {", "\tconfig := PostgresConfig{", "\t\tDriver: \"postgres\",", "\t}", "", "\terr := config.Validate()", "\tassert.Error(t, err)", "}"], "file_path": "golang-petclinic-service/app/info_config_test.go"}
{"Link_to_commit": "https://github.com/vmohit18/Sorting/commit/cb00ea863e236cdd3848a9a0e457313f5f0935e8", "n-gram matched": "copilot to generate", "n_lines_longer_change": 25, "n_files_impacted": 1, "longest_chunk": ["Console.WriteLine();", "", "", "/*", " search for a name in the array", "display 'found' if the name is in the array", "display 'not found' if the name is not in the array", " */", "", "if (Array.BinarySearch(names, \"Alfa\") >= 0)", "{", "    Console.WriteLine(\"Found\");", "}", "else", "{", "    Console.WriteLine(\"Not Found\");", "}", "Console.WriteLine();", "", "// search for a name in the array and display the location of the name", "Console.WriteLine(Array.BinarySearch(names, \"Alfa\"));", "Console.WriteLine();", "", "", ""], "file_path": "Sorting/Program.cs"}
{"Link_to_commit": "https://github.com/kobemartin/kobemartin.github.io/commit/271efb733d848e2dc9f0f622f44d3b4eeb0cb2a8", "n-gram matched": "copilot to generate", "n_lines_longer_change": 5, "n_files_impacted": 3, "longest_chunk": ["document.getElementById('nav-list').addEventListener('click', function(event) {", "    if (event.target.tagName === 'A') {", "        alert('You clicked on ' + event.target.textContent);", "    }", "});"], "file_path": "script.js"}
{"Link_to_commit": "https://github.com/yamak493/MBFPorter/commit/4f96f49523536c62cec05e86128f5e1127e4547a", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 82, "n_files_impacted": 14, "longest_chunk": ["<?xml version=\"1.0\" encoding=\"UTF-8\"?>", "<project xmlns=\"http://maven.apache.org/POM/4.0.0\"", "         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"", "         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">", "    <modelVersion>4.0.0</modelVersion>", "", "    <groupId>net.enabify</groupId>", "    <artifactId>MBFPorter</artifactId>", "    <version>1.0-SNAPSHOT</version>", "    <packaging>jar</packaging>", "", "    <name>MBFPorter</name>", "", "    <properties>", "        <java.version>17</java.version>", "        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>", "    </properties>", "", "    <build>", "        <defaultGoal>clean package</defaultGoal>", "        <plugins>", "            <plugin>", "                <groupId>org.apache.maven.plugins</groupId>", "                <artifactId>maven-compiler-plugin</artifactId>", "                <version>3.13.0</version>", "                <configuration>", "                    <source>${java.version}</source>", "                    <target>${java.version}</target>", "                </configuration>", "            </plugin>", "            <plugin>", "                <groupId>org.apache.maven.plugins</groupId>", "                <artifactId>maven-shade-plugin</artifactId>", "                <version>3.5.3</version>", "                <executions>", "                    <execution>", "                        <phase>package</phase>", "                        <goals>", "                            <goal>shade</goal>", "                        </goals>", "                    </execution>", "                </executions>", "            </plugin>", "        </plugins>", "        <resources>", "            <resource>", "                <directory>src/main/resources</directory>", "                <filtering>true</filtering>", "            </resource>", "        </resources>", "    </build>", "", "    <repositories>", "        <repository>", "            <id>papermc-repo</id>", "            <url>https://repo.papermc.io/repository/maven-public/</url>", "        </repository>", "        <repository>", "            <id>sonatype</id>", "            <url>https://oss.sonatype.org/content/groups/public/</url>", "        </repository>", "        <repository>", "            <id>sk89q-repo</id>", "            <url>https://maven.enginehub.org/repo/</url>", "        </repository>", "    </repositories>", "", "    <dependencies>", "        <dependency>", "            <groupId>dev.folia</groupId>", "            <artifactId>folia-api</artifactId>", "            <version>1.20.4-R0.1-SNAPSHOT</version>", "            <scope>provided</scope>", "        </dependency>", "        <dependency>", "            <groupId>com.sk89q.worldguard</groupId>", "            <artifactId>worldguard-bukkit</artifactId>", "            <version>7.0.13</version>", "            <scope>provided</scope>", "        </dependency>", "    </dependencies>", "</project>"], "file_path": "src/main/java/net/enabify/mBFPorter/MBFPorter.java"}
{"Link_to_commit": "https://github.com/dventimisupabase/pas-de-deux/commit/328778ecabf9d3fba241f2a659ac191838788e0b", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 106, "n_files_impacted": 2, "longest_chunk": ["from fastapi import FastAPI, HTTPException", "from pydantic import BaseModel", "import uvicorn", "import os", "import json", "import faiss", "import numpy as np", "from sentence_transformers import SentenceTransformer", "from typing import List", "", "app = FastAPI()", "", "with open(\"train.json\", \"r\", encoding=\"utf-8\") as f:", "    tickets = json.load(f)", "", "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')", "", "ticket_texts = []", "ticket_responses = []", "ticket_metadata = []", "", "for ticket in tickets:", "    metadata = f\"Title: {ticket['title']}\\nCategory: {ticket['category']}\\nDescription: {ticket['description']}\"", "    dialog = \"\\n\".join([f\"{m['role'].capitalize()}: {m['text']}\" for m in ticket[\"history\"]])", "    combined_text = f\"{metadata}\\nConversation:\\n{dialog}\"", "    ticket_texts.append(combined_text)", "    ticket_responses.append(ticket[\"next_engineer_reply\"])", "    ticket_metadata.append({\"category\": ticket[\"category\"]})", "", "embeddings = embedding_model.encode(ticket_texts, convert_to_numpy=True)", "index = faiss.IndexFlatL2(embeddings.shape[1])", "index.add(embeddings)", "", "try:", "    from openai import OpenAI", "except ImportError:", "    OpenAI = None", "", "def generate_reply(messages, model=\"gpt-4\", provider=\"openai\", **kwargs):", "    if provider == \"openai\":", "        if OpenAI is None:", "            raise ImportError(\"OpenAI client not available.\")", "        api_key = os.getenv(\"OPENAI_API_KEY\")", "        if not api_key:", "            raise ValueError(\"Set the OPENAI_API_KEY environment variable.\")", "        client = OpenAI(api_key=api_key)", "        response = client.chat.completions.create(", "            model=model,", "            messages=messages,", "            temperature=kwargs.get(\"temperature\", 0.4)", "        )", "        return response.choices[0].message.content.strip()", "    else:", "        raise ValueError(f\"Unsupported provider: {provider}\")", "", "class Message(BaseModel):", "    role: str", "    text: str", "", "class TicketInput(BaseModel):", "    title: str", "    category: str", "    description: str", "    history: List[Message]", "", "@app.post(\"/predict\")", "def predict_reply(ticket: TicketInput):", "    query_metadata = f\"Title: {ticket.title}\\nCategory: {ticket.category}\\nDescription: {ticket.description}\"", "    dialog = \"\\n\".join([f\"{m.role.capitalize()}: {m.text}\" for m in ticket.history])", "    query_text = f\"{query_metadata}\\nConversation:\\n{dialog}\"", "    query_embedding = embedding_model.encode([query_text], convert_to_numpy=True)", "", "    distances, indices = index.search(query_embedding, 20)", "", "    boosted, fallback = [], []", "    for dist, idx in zip(distances[0], indices[0]):", "        candidate_category = ticket_metadata[idx]['category']", "        adjusted_dist = dist * 0.85 if candidate_category == ticket.category else dist", "        (boosted if candidate_category == ticket.category else fallback).append((adjusted_dist, idx))", "", "    boosted.sort(key=lambda x: x[0])", "    fallback.sort(key=lambda x: x[0])", "    needed = 3 - len(boosted)", "    top_adjusted = boosted + fallback[:needed] if needed > 0 else boosted[:3]", "", "    prompt_parts = [\"You are a helpful technical support engineer.\\n\"]", "    for _, idx in top_adjusted:", "        prompt_parts.append(f\"Example:\\n{ticket_texts[idx]}\\nEngineer: {ticket_responses[idx]}\\n\")", "    prompt_parts.append(\"Now, consider this support case:\\n\")", "    prompt_parts.append(query_text)", "    prompt_parts.append(\"Engineer:\")", "    full_prompt = \"\\n\".join(prompt_parts)", "", "    messages = [", "        {\"role\": \"system\", \"content\": \"You are a technical support engineer.\"},", "        {\"role\": \"user\", \"content\": full_prompt}", "    ]", "", "    try:", "        reply = generate_reply(messages)", "        return {\"reply\": reply}", "    except Exception as e:", "        raise HTTPException(status_code=500, detail=str(e))", "", "if __name__ == \"__main__\":", "    uvicorn.run(\"rag_api_server:app\", host=\"0.0.0.0\", port=8000, reload=True)"], "file_path": "rag_chatbot.py"}
{"Link_to_commit": "https://github.com/tarpentine/llmSecureCode/commit/a7a400bd9c3cb22b96a04b5e3b77e14b11e1fdd8", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 88, "n_files_impacted": 13, "longest_chunk": ["const express = require('express');", "const session = require('express-session');", "const cookieParser = require('cookie-parser');", "const bcrypt = require('bcrypt');", "", "const app = express();", "const PORT = 3000;", "", "// In-memory \"database\"", "const users = {};", "", "app.use(express.urlencoded({ extended: true }));", "app.use(express.json());", "app.use(cookieParser());", "", "// Session configuration", "app.use(session({", "  secret: 'super-secret-key',", "  resave: false,", "  saveUninitialized: false,", "  cookie: { maxAge: 60000 } // 1 minute for demo", "}));", "", "// Home route", "app.get('/', (req, res) => {", "  if (req.session.username) {", "    res.send(`Welcome back, ${req.session.username}! <a href=\"/logout\">Logout</a>`);", "  } else {", "    res.send('Hello, guest! <a href=\"/login\">Login</a> or <a href=\"/register\">Register</a>');", "  }", "});", "", "// Registration route", "app.get('/register', (req, res) => {", "  res.send(`", "    <form method=\"POST\" action=\"/register\">", "      <input name=\"username\" placeholder=\"Username\" required />", "      <input name=\"password\" type=\"password\" placeholder=\"Password\" required />", "      <button type=\"submit\">Register</button>", "    </form>", "  `);", "});", "", "app.post('/register', async (req, res) => {", "  const { username, password } = req.body;", "  if (users[username]) {", "    return res.send('User already exists. <a href=\"/register\">Try again</a>');", "  }", "  const hashedPassword = await bcrypt.hash(password, 10);", "  users[username] = { username, password: hashedPassword };", "  res.send('Registration successful! <a href=\"/login\">Login</a>');", "});", "", "// Login route", "app.get('/login', (req, res) => {", "  res.send(`", "    <form method=\"POST\" action=\"/login\">", "      <input name=\"username\" placeholder=\"Username\" required />", "      <input name=\"password\" type=\"password\" placeholder=\"Password\" required />", "      <button type=\"submit\">Login</button>", "    </form>", "  `);", "});", "", "app.post('/login', async (req, res) => {", "  const { username, password } = req.body;", "  const user = users[username];", "  if (!user || !(await bcrypt.compare(password, user.password))) {", "    return res.send('Invalid credentials. <a href=\"/login\">Try again</a>');", "  }", "  req.session.username = username;", "  res.redirect('/');", "});", "", "// Logout route", "app.get('/logout', (req, res) => {", "  req.session.destroy(err => {", "    if (err) return res.send('Error logging out.');", "    res.clearCookie('connect.sid');", "    res.redirect('/');", "  });", "});", "", "// Start server", "app.listen(PORT, () => {", "  console.log(`Server running at http://localhost:${PORT}`);", "});", ""], "file_path": "chatGPT/initial/fileServer/server.js"}
{"Link_to_commit": "https://github.com/Fasten90/MatiCodes/commit/40da70b3130d8d08354f94a0d601b477cdda9f6d", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 65, "n_files_impacted": 1, "longest_chunk": ["import tkinter as tk\r", "import random\r", "\r", "class NumberTableApp:\r", "    def __init__(self, root):\r", "        self.root = root\r", "        self.root.title(\"Sz\u00e1m t\u00e1bl\u00e1zat\")\r", "        \r", "        self.rows = 5\r", "        self.cols = 4\r", "        self.table = [[None for _ in range(self.cols)] for _ in range(self.rows)]\r", "        \r", "        self.buttons = []\r", "        for col in range(self.cols):\r", "            btn = tk.Button(root, text=f\"Oszlop {col+1}\", command=lambda c=col: self.add_number_to_column(c))\r", "            btn.grid(row=0, column=col)\r", "            self.buttons.append(btn)\r", "        \r", "        self.cells = [[tk.Label(root, text=\"\", width=10, height=2, relief=\"ridge\", borderwidth=2) \r", "                       for _ in range(self.cols)] for _ in range(self.rows)]\r", "        \r", "        for r in range(self.rows):\r", "            for c in range(self.cols):\r", "                self.cells[r][c].grid(row=r+1, column=c)\r", "        \r", "        self.number_var = tk.StringVar(value=str(random.choice([2, 4, 8, 16, 32, 64])))\r", "        tk.Label(root, text=\"K\u00f6vetkez\u0151 sz\u00e1m:\").grid(row=self.rows+1, column=0, columnspan=2)\r", "        self.number_entry = tk.Entry(root, textvariable=self.number_var, font=(\"Arial\", 14))\r", "        self.number_entry.grid(row=self.rows+1, column=2, columnspan=2)\r", "    \r", "    def add_number_to_column(self, col):\r", "        new_number = int(self.number_var.get())\r", "        last_empty = None\r", "        \r", "        for row in range(self.rows):  # Fentr\u0151l lefel\u00e9 keres\r", "            if self.table[row][col] is None:\r", "                last_empty = row\r", "                break\r", "            \r", "        if last_empty is not None:\r", "            self.table[last_empty][col] = new_number\r", "        else:\r", "            return  # Ha tele van az oszlop, nem csin\u00e1l semmit\r", "        \r", "        for row in range(self.rows - 1, 0, -1):  # Ellen\u0151rzi az \u00f6sszevon\u00e1st\r", "            if self.table[row][col] is not None and self.table[row][col] == self.table[row - 1][col]:\r", "                self.table[row - 1][col] *= 2\r", "                self.table[row][col] = None\r", "                last_empty = row\r", "        \r", "        self.update_display()\r", "        self.generate_new_number()\r", "    \r", "    def update_display(self):\r", "        for r in range(self.rows):\r", "            for c in range(self.cols):\r", "                self.cells[r][c].config(text=str(self.table[r][c]) if self.table[r][c] is not None else \"\")\r", "    \r", "    def generate_new_number(self):\r", "        self.number_var.set(str(random.choice([2, 4, 8, 16, 32, 64])))\r", "\r", "if __name__ == \"__main__\":\r", "    root = tk.Tk()\r", "    app = NumberTableApp(root)\r", "    root.mainloop()\r"], "file_path": "Mati_blocks.py"}
{"Link_to_commit": "https://github.com/g-akca/llm-testing/commit/e70bf0bcc9ea53fc304e39cd76bfb25843fd9969", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 34, "n_files_impacted": 20, "longest_chunk": ["class TestCheckDictCase(unittest.TestCase):", "    def setUp(self):", "        # If check_dict_case is in another module, replace with the proper import.", "        from __main__ import check_dict_case  # noqa: E402", "        self.func: Callable = check_dict_case", "", "    # --- Positive cases ------------------------------------------------------", "    def test_all_lowercase_keys(self):", "        self.assertTrue(", "            self.func({\"a\": \"apple\", \"b\": \"banana\", \"c\": \"cherry\"}),", "            \"Should return True when every key is lower-case.\",", "        )", "", "    def test_all_uppercase_keys(self):", "        self.assertTrue(", "            self.func({\"ZIP\": \"27514\", \"STATE\": \"NC\"}),", "            \"Should return True when every key is upper-case.\",", "        )", "", "    # --- Negative cases ------------------------------------------------------", "    def test_empty_dict_returns_false(self):", "        self.assertFalse(self.func({}), \"Empty dictionary must return False.\")", "", "    def test_mixed_case_keys_returns_false(self):", "        self.assertFalse(", "            self.func({\"a\": \"apple\", \"B\": \"banana\"}),", "            \"Mixed lower- and upper-case keys must return False.\",", "        )", "", "    def test_non_string_key_returns_false(self):", "        self.assertFalse(", "            self.func({\"a\": 1, 3: \"three\"}),", "            \"Presence of non-string keys must return False.\",", "        )"], "file_path": "phase-1/hard/eval-95/gpt/test.py"}
{"Link_to_commit": "https://github.com/weibeld/github-projects-dashboard/commit/b64a623aef38f8e1a6a6fbbb5598a0f3a9ecb0de", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 98, "n_files_impacted": 2, "longest_chunk": ["// app.js", "import { Column } from \"./components/Column.js\";", "", "const { createElement: h, useEffect, useState } = React;", "const { createRoot } = ReactDOM;", "", "function App() {", "  const [projects, setProjects] = useState([]);", "  const [statuses, setStatuses] = useState({});", "  const [loading, setLoading] = useState(true);", "  const [error, setError] = useState(null);", "", "  useEffect(() => {", "    async function load() {", "      const token = localStorage.getItem(\"github_pat\") || prompt(\"Enter GitHub Personal Access Token:\");", "      if (!token) return;", "      localStorage.setItem(\"github_pat\", token);", "", "      try {", "        const userRes = await fetch(\"https://api.github.com/graphql\", {", "          method: \"POST\",", "          headers: {", "            Authorization: `Bearer ${token}`,", "            \"Content-Type\": \"application/json\",", "          },", "          body: JSON.stringify({ query: `query { viewer { login }}` }),", "        });", "        const userData = await userRes.json();", "        const login = userData.data.viewer.login;", "", "        const projectRes = await fetch(\"https://api.github.com/graphql\", {", "          method: \"POST\",", "          headers: {", "            Authorization: `Bearer ${token}`,", "            \"Content-Type\": \"application/json\",", "          },", "          body: JSON.stringify({", "            query: `query {", "              user(login: \"${login}\") {", "                projectsV2(first: 20) {", "                  nodes {", "                    id", "                    title", "                    url", "                  }", "                }", "              }", "            }`,", "          }),", "        });", "", "        const projectData = await projectRes.json();", "        const fetchedProjects = projectData.data.user.projectsV2.nodes;", "        setProjects(fetchedProjects);", "", "        const statusRes = await fetch(\"./statuses.json\");", "        const statusJson = await statusRes.json();", "        setStatuses(statusJson);", "      } catch (e) {", "        console.error(e);", "        setError(\"Failed to load GitHub Projects or statuses.\");", "      } finally {", "        setLoading(false);", "      }", "    }", "    load();", "  }, []);", "", "  function updateProjectStatus(projectId, newStatus) {", "    const updated = { ...statuses, [projectId]: newStatus };", "    setStatuses(updated);", "", "    fetch(\"./statuses.json\", {", "      method: \"PUT\",", "      headers: { \"Content-Type\": \"application/json\" },", "      body: JSON.stringify(updated, null, 2),", "    });", "  }", "", "  if (loading) return h(\"div\", { className: \"text-center mt-10\" }, \"Loading...\");", "  if (error) return h(\"div\", { className: \"text-red-500 text-center mt-10\" }, error);", "", "  const columns = [\"todo\", \"doing\", \"done\"];", "", "  return h(\"div\", { className: \"grid grid-cols-1 sm:grid-cols-3 gap-4\" },", "    columns.map(status =>", "      h(Column, {", "        key: status,", "        status,", "        projects: projects.filter(p => statuses[p.id] === status || (!statuses[p.id] && status === \"todo\")),", "        onDrop: (id) => updateProjectStatus(id, status)", "      })", "    )", "  );", "}", "", "const root = createRoot(document.getElementById(\"root\"));", "root.render(h(App));"], "file_path": "src/app.js"}
{"Link_to_commit": "https://github.com/Ronak-Malkan/Decentralized_Skeleton/commit/aa95c25ea578dc4ce67178e02193ab94f605b527", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 5, "n_files_impacted": 2, "longest_chunk": ["                log_.log(\"task_candidate\", {", "                    {\"task\", req->task_id()},", "                    {\"peer\", id},", "                    {\"score\", score}", "                });"], "file_path": "phase1/src/server.cpp"}
{"Link_to_commit": "https://github.com/rohanmaddamsetti/PCN-db-pipeline/commit/0b2e4eaee72a4194252bc0f31cd5017dc189654b", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 37, "n_files_impacted": 1, "longest_chunk": ["async def make_themisto_index_for_genome(genome_id, themisto_ref_dir, themisto_index_dir):", "    \"\"\"build a Themisto index for a single genome\"\"\"", "    ref_fasta_dir = os.path.join(themisto_ref_dir, genome_id)", "    if not os.path.isdir(ref_fasta_dir):", "        return", "    ", "    index_input_filelist = os.path.join(ref_fasta_dir, genome_id + \".txt\")", "    genome_index_dir = os.path.join(themisto_index_dir, genome_id)", "    os.makedirs(genome_index_dir, exist_ok=True)", "    ", "    index_prefix = os.path.join(genome_index_dir, genome_id)", "    tempdir = os.path.join(genome_index_dir, \"temp\")", "    os.makedirs(tempdir, exist_ok=True)", "    ", "    themisto_build_args = [", "        \"themisto\", \"build\", \"-k\", \"31\", \"-i\", index_input_filelist,", "        \"--index-prefix\", index_prefix, \"--temp-dir\", tempdir,", "        \"--mem-gigas\", \"8\", \"--n-threads\", \"8\", \"--file-colors\"", "    ]", "    themisto_build_string = \" \".join(themisto_build_args)", "    print(themisto_build_string)", "    ", "    await run_command_with_retry(themisto_build_string, tempdir)", "", "", "async def make_themisto_indices(themisto_ref_dir, themisto_index_dir):", "    \"\"\"Create Themisto indices for all genomes in the themisto reference directory.\"\"\"", "    os.makedirs(themisto_index_dir, exist_ok=True)", "    ", "    tasks = [", "        make_themisto_index_for_genome(genome_id, themisto_ref_dir, themisto_index_dir)", "        for genome_id in os.listdir(themisto_ref_dir)", "        if os.path.isdir(os.path.join(themisto_ref_dir, genome_id))", "    ]", "    ", "    await asyncio.gather(*tasks)", ""], "file_path": "src/PCN_pipeline.py"}
{"Link_to_commit": "https://github.com/Xiao-dong-LIU/GitTP/commit/b3e74f5d9a70c76be6d7bb2e46600bdf9bd23b54", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 27, "n_files_impacted": 4, "longest_chunk": ["This is simple pcg solver in c++ generated by chatgpt and modified after. ", "", "The objectif of this TP is not to understand the c++, but to practice git. ", "", "In fact, in the program, serveral lines were commented. What you need to do is to remove \"//\" before those lines to let the program output informations.", "", "As there are several files, you can work in groupe (2 personnes) to achieve the efficiency. ", "", "1. One of the teammate forks this repository to his own git private repository.", "", "2. This teammate invite the other teammate as a team member to work with him is this project. ", "", "3. After that, each teammate will do a \"git clone\" to get the private repository to his local repository after done a \"git init\" in his local repository.", "", "4. Dev A removes the comments in pcg.cpp file to enable print; Dev B removes the comments in main.cpp to enable print.", "", "5. Each Dev commits their modifications ", "", "6. Publish the new code to your remote repository.", "", "You can try to run the code to see if it can print something.", "", "To compile to code, you need gcc compile ", "\tg++ main.cpp pcg.cpp -o a.out", "", "To execute ", "\t./a.out"], "file_path": "main.cpp"}
{"Link_to_commit": "https://github.com/hms-dbmi/udi-data-api-testing/commit/5393b911dd2712d18dca93523f7a3309c9e889b6", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 10, "n_files_impacted": 5, "longest_chunk": ["[project]", "name = \"udi-data-api-testing\"", "version = \"0.1.0\"", "description = \"Add your description here\"", "readme = \"README.md\"", "requires-python = \">=3.13\"", "dependencies = [", "    \"pandas>=2.2.3\",", "    \"requests>=2.32.3\",", "]"], "file_path": "test1.py"}
{"Link_to_commit": "https://github.com/Lydia-Harding/ModelExploder/commit/3f7fccaba07a44b1451e82a71f393b4af9c98c63", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 356, "n_files_impacted": 1, "longest_chunk": ["import bpy", "import bmesh", "from mathutils import Vector, Matrix", "from bpy.props import StringProperty, FloatProperty, IntProperty, PointerProperty, BoolProperty", "from bpy.types import Operator, Panel, PropertyGroup", "from collections import defaultdict", "import math", "", "bl_info = {", "    \"name\": \"Spread Connected Mesh Groups\",", "    \"blender\": (2, 80, 0),", "    \"category\": \"Object\",", "}", "", "preview_object_prefix = \"PREVIEW_\"", "final_object_prefix = \"SPREAD_\"", "", "def update_preview_mesh(self, context):", "    settings = context.scene.spread_settings", "    target_obj = settings.target_object", "    if not (target_obj and target_obj.type == 'MESH'):", "        return", "", "    preview_name = preview_object_prefix + target_obj.name", "    preview_obj = bpy.data.objects.get(preview_name)", "", "    if preview_obj:", "        bpy.data.objects.remove(preview_obj, do_unlink=True)", "", "    preview_obj = target_obj.copy()", "    preview_obj.data = target_obj.data.copy()", "    preview_obj.name = preview_name", "    preview_obj.data.name = preview_name", "", "    context.collection.objects.link(preview_obj)", "    preview_obj.matrix_world = target_obj.matrix_world.copy()", "", "    preview_obj.hide_select = True", "    preview_obj.hide_render = True", "    preview_obj.select_set(False)", "", "    # Set preview material", "    if \"SpreadPreviewMat\" not in bpy.data.materials:", "        mat = bpy.data.materials.new(name=\"SpreadPreviewMat\")", "        mat.use_nodes = True", "        bsdf = mat.node_tree.nodes.get(\"Principled BSDF\")", "        if bsdf:", "            bsdf.inputs['Base Color'].default_value = (0.0, 0.4, 1.0, 1.0)", "            bsdf.inputs['Alpha'].default_value = 0.4", "        mat.blend_method = 'BLEND'", "        mat.shadow_method = 'NONE'", "    else:", "        mat = bpy.data.materials[\"SpreadPreviewMat\"]", "", "    if preview_obj.data.materials:", "        preview_obj.data.materials[0] = mat", "    else:", "        preview_obj.data.materials.append(mat)", "", "    preview_obj.display_type = 'SOLID'", "    preview_obj.show_transparent = True", "", "    bm = bmesh.new()", "    bm.from_mesh(preview_obj.data)", "    bm.verts.ensure_lookup_table()", "", "    visited = set()", "    groups = []", "", "    def get_connected_group(start_vert):", "        stack = [start_vert]", "        group = []", "        while stack:", "            v = stack.pop()", "            if v in visited:", "                continue", "            visited.add(v)", "            group.append(v)", "            for e in v.link_edges:", "                stack.append(e.other_vert(v))", "        return group", "", "    for v in bm.verts:", "        if v not in visited:", "            group = get_connected_group(v)", "            groups.append(group)", "", "    deform_layer = bm.verts.layers.deform.verify()", "    vg_name = settings.vertex_group_name.strip()", "    vg_index = None", "    if vg_name in preview_obj.vertex_groups:", "        vg_index = preview_obj.vertex_groups[vg_name].index", "", "    body_verts = set()", "    if vg_index is not None:", "        for v in bm.verts:", "            d = v[deform_layer]", "            if vg_index in d:", "                body_verts.add(v)", "", "    def group_centroid(group):", "        return sum((v.co for v in group), Vector()) / len(group)", "", "    group_centroids = [group_centroid(g) for g in groups]", "    main_index = None", "", "    if body_verts:", "        for i, group in enumerate(groups):", "            if any(v in body_verts for v in group):", "                main_index = i", "                break", "", "    if main_index is None:", "        distances = [c.length for c in group_centroids]", "        main_index = distances.index(min(distances))", "", "    tolerance = settings.tolerance", "    distance_tolerance = settings.distance_tolerance", "    group_spacing_x = settings.group_spacing_x", "    group_spacing_y = settings.group_spacing_y", "    group_spacing_z = settings.group_spacing_z", "    local_spacing_x = settings.local_spacing_x", "    local_spacing_y = settings.local_spacing_y", "    local_spacing_z = settings.local_spacing_z", "    preserve_symmetry = settings.preserve_symmetry", "", "    poly_groups = defaultdict(list)", "    for i, group in enumerate(groups):", "        if i == main_index:", "            continue", "        polycount = len(group)", "        has_x_zero = any(abs(v.co.x) < 1e-6 for v in group)", "        key = f\"{round(polycount / tolerance) * tolerance}\"", "        if has_x_zero:", "            key += \"_x0\"", "        poly_groups[key].append(group)", "", "    if preserve_symmetry:", "        # Merge symmetrical groups", "        keys = list(poly_groups.keys())", "        merged = set()", "        for i in range(len(keys)):", "            for j in range(i + 1, len(keys)):", "                if keys[i] in merged or keys[j] in merged:", "                    continue", "                group_list_a = poly_groups[keys[i]]", "                group_list_b = poly_groups[keys[j]]", "                for ga in group_list_a:", "                    for gb in group_list_b:", "                        if any((va.co - Vector((-vb.co.x, vb.co.y, vb.co.z))).length < 1e-4 for va in ga for vb in gb):", "                            group_list_a.extend(group_list_b)", "                            merged.add(keys[j])", "                            break", "                    if keys[j] in merged:", "                        break", "        for key in merged:", "            del poly_groups[key]", "", "    sorted_keys = sorted(poly_groups.keys())", "    angle_step = 2 * math.pi / max(1, len(sorted_keys))", "", "    for i, key in enumerate(sorted_keys):", "        inner_groups = poly_groups[key]", "        if len(inner_groups) <= 1:", "            continue  # Skip solo groups", "", "        angle = angle_step * i", "        outer_offset = Vector((", "            math.sin(angle) * group_spacing_x,", "            math.cos(angle) * group_spacing_y,", "            math.sin(angle) * group_spacing_z", "        ))", "", "        inner_centroids = [group_centroid(g) for g in inner_groups]", "        inner_center = sum(inner_centroids, Vector()) / len(inner_centroids)", "", "        for group in inner_groups:", "            group_centroid_vec = group_centroid(group)", "            direction = (group_centroid_vec - inner_center).normalized()", "            if direction.length == 0:", "                direction = Vector((0, 0, 1))", "            local_offset = Vector((", "                direction.x * local_spacing_x,", "                direction.y * local_spacing_y,", "                direction.z * local_spacing_z", "            ))", "            move_vec = outer_offset + local_offset", "", "            bmesh.ops.translate(bm, verts=group, vec=move_vec)", "", "    bm.normal_update()", "    bm.to_mesh(preview_obj.data)", "    bm.free()", "", "", "class SpreadSettings(PropertyGroup):", "    target_object: PointerProperty(", "        name=\"Target Object\",", "        type=bpy.types.Object,", "        description=\"Object to spread mesh islands from\",", "        update=update_preview_mesh", "    )", "    vertex_group_name: StringProperty(", "        name=\"Body Group\",", "        description=\"Name of the vertex group to leave in place\",", "        default=\"Body\",", "        update=update_preview_mesh", "    )", "    tolerance: IntProperty(", "        name=\"Similarity Tolerance\",", "        description=\"How close polygon counts need to be to group islands together\",", "        default=20,", "        min=1,", "        update=update_preview_mesh", "    )", "    distance_tolerance: FloatProperty(", "        name=\"Distance Tolerance\",", "        description=\"Maximum distance between group centroids to be grouped together\",", "        default=5.0,", "        min=0.0,", "        update=update_preview_mesh", "    )", "    group_spacing_x: FloatProperty(", "        name=\"Group Spacing X\",", "        description=\"X direction distance between polygon groups\",", "        default=0.0,", "        min=0.0,", "        update=update_preview_mesh", "    )", "    group_spacing_y: FloatProperty(", "        name=\"Group Spacing Y\",", "        description=\"Y direction distance between polygon groups\",", "        default=10.0,", "        min=0.0,", "        update=update_preview_mesh", "    )", "    group_spacing_z: FloatProperty(", "        name=\"Group Spacing Z\",", "        description=\"Z direction distance between polygon groups\",", "        default=10.0,", "        min=0.0,", "        update=update_preview_mesh", "    )", "    local_spacing_x: FloatProperty(", "        name=\"Island Spacing X\",", "        description=\"X direction spacing within a polygon group\",", "        default=0.0,", "        min=0.0,", "        update=update_preview_mesh", "    )", "    local_spacing_y: FloatProperty(", "        name=\"Island Spacing Y\",", "        description=\"Y direction spacing within a polygon group\",", "        default=2.0,", "        min=0.0,", "        update=update_preview_mesh", "    )", "    local_spacing_z: FloatProperty(", "        name=\"Island Spacing Z\",", "        description=\"Z direction spacing within a polygon group\",", "        default=2.0,", "        min=0.0,", "        update=update_preview_mesh", "    )", "    preserve_symmetry: BoolProperty(", "        name=\"Preserve Symmetry\",", "        description=\"Ensure symmetrical groups are spread symmetrically\",", "        default=True,", "        update=update_preview_mesh", "    )", "", "", "class SpreadMeshPanel(Panel):", "    bl_label = \"Model Exploder\"", "    bl_idname = \"VIEW3D_PT_model_exploder\"", "    bl_space_type = \"VIEW_3D\"", "    bl_region_type = \"UI\"", "    bl_category = \"Exploder\"", "", "    def draw(self, context):", "        layout = self.layout", "        settings = context.scene.spread_settings", "", "        layout.label(text=\"Spread Mesh Preview\")", "        layout.prop(settings, \"target_object\")", "        layout.prop(settings, \"vertex_group_name\")", "        layout.prop(settings, \"tolerance\")", "        layout.prop(settings, \"distance_tolerance\")", "        layout.label(text=\"Group Spacing\")", "        layout.prop(settings, \"group_spacing_x\")", "        layout.prop(settings, \"group_spacing_y\")", "        layout.prop(settings, \"group_spacing_z\")", "        layout.label(text=\"Island Spacing\")", "        layout.prop(settings, \"local_spacing_x\")", "        layout.prop(settings, \"local_spacing_y\")", "        layout.prop(settings, \"local_spacing_z\")", "        layout.prop(settings, \"preserve_symmetry\")", "        layout.operator(\"object.spread_connected_verts\", text=\"Preview\")", "        layout.operator(\"object.finalize_spread_mesh\", text=\"Finalize Mesh\")", "", "", "class SpreadGroups(Operator):", "    bl_idname = \"object.spread_connected_verts\"", "    bl_label = \"Spread Connected Mesh Groups\"", "", "    def execute(self, context):", "        update_preview_mesh(self, context)", "        self.report({'INFO'}, \"Preview mesh updated\")", "        return {'FINISHED'}", "", "", "class FinalizeSpreadMesh(Operator):", "    bl_idname = \"object.finalize_spread_mesh\"", "    bl_label = \"Finalize Spread Mesh\"", "", "    def execute(self, context):", "        for obj in bpy.data.objects:", "            if obj.name.startswith(preview_object_prefix):", "                meshname = obj.name[len(preview_object_prefix):]", "                obj.name = final_object_prefix + meshname", "                obj.data.name = final_object_prefix + meshname", "                if obj.active_material and obj.active_material.name == \"SpreadPreviewMat\":", "                    obj.active_material = None", "                obj.hide_select = False", "                obj.hide_viewport = False", "                obj.hide_render = False", "                obj.select_set(True)", "                context.view_layer.objects.active = obj", "", "        for obj in list(bpy.data.objects):", "            if obj.name.startswith(preview_object_prefix):", "                if not obj.name.startswith(final_object_prefix):", "                    bpy.data.objects.remove(obj, do_unlink=True)", "", "        self.report({'INFO'}, \"Spread mesh finalized\")", "        return {'FINISHED'}", "", "", "def register():", "    bpy.utils.register_class(SpreadSettings)", "    bpy.types.Scene.spread_settings = PointerProperty(type=SpreadSettings)", "    bpy.utils.register_class(SpreadMeshPanel)", "    bpy.utils.register_class(SpreadGroups)", "    bpy.utils.register_class(FinalizeSpreadMesh)", "", "", "def unregister():", "    bpy.utils.unregister_class(SpreadSettings)", "    del bpy.types.Scene.spread_settings", "    bpy.utils.unregister_class(SpreadMeshPanel)", "    bpy.utils.unregister_class(SpreadGroups)", "    bpy.utils.unregister_class(FinalizeSpreadMesh)", "", "", "if __name__ == \"__main__\":", "    register()"], "file_path": "ModelExploder.py"}
{"Link_to_commit": "https://github.com/david-andreasson/api-webservices-final-assignment_79davand/commit/c706f19de400d633dc10ed303a08f22572b8777b", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 7, "n_files_impacted": 4, "longest_chunk": ["    /**", "     * Retrieves a book by its id.", "     * If the book is not found, returns a 404 Not Found with an error message.", "     *", "     * @param id the id of the book.", "     * @return a ResponseEntity containing the book or an error message.", "     */"], "file_path": "Uppgift_1/src/main/java/com/davanddev/uppgift_1/controller/BookController.java"}
{"Link_to_commit": "https://github.com/rohanmaddamsetti/PCN-db-pipeline/commit/a4296397d47830199e5b0271c8b3d4269f4bd390", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 7, "n_files_impacted": 1, "longest_chunk": ["", "def make_themisto_indices(themisto_ref_dir, themisto_index_dir):", "    \"\"\"Create Themisto indices for all genomes in the themisto reference directory.\"\"\"", "    asyncio.run(make_themisto_indices_in_parallel(themisto_ref_dir, themisto_index_dir))", "    return", "    ", ""], "file_path": "src/PCN_pipeline.py"}
{"Link_to_commit": "https://github.com/doug1asjing/STM32-PCDashBoard/commit/af2076844920aee38637b80b8c37ddd6f3070f5b", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 178, "n_files_impacted": 1, "longest_chunk": ["import sys\r", "import serial\r", "import serial.tools.list_ports\r", "import psutil\r", "import time\r", "import threading\r", "from datetime import datetime\r", "from PyQt5.QtWidgets import QApplication, QMainWindow, QLabel, QPushButton, QComboBox, QSystemTrayIcon, QMenu, QAction\r", "from PyQt5.QtCore import QTimer, Qt\r", "from PyQt5.QtGui import QIcon\r", "\r", "class SerialMonitor(QMainWindow):\r", "    def __init__(self):\r", "        super().__init__()\r", "\r", "        # \u8bbe\u5b9a\u7a97\u53e3\u6807\u9898 & \u7981\u6b62\u8c03\u6574\u7a97\u53e3\u5927\u5c0f\r", "        self.setWindowTitle(\"STM32\u684c\u9762\u4fe1\u606f\u7a97\u53e3\")\r", "        self.setGeometry(100, 100, 300, 200)\r", "        self.setFixedSize(300, 200)  # \u9501\u5b9a\u7a97\u53e3\u5927\u5c0f\r", "\r", "        # \u4e32\u53e3\u9009\u62e9\u6846\r", "        self.port_label = QLabel(\"\u4e32\u53e3:\", self)\r", "        self.port_label.move(20, 20)\r", "\r", "        self.port_combo = QComboBox(self)\r", "        self.port_combo.move(70, 18)\r", "        self.refresh_ports()\r", "\r", "        # \u6e29\u6e7f\u5ea6\u663e\u793a\r", "        self.temp_label = QLabel(\"\u6e29\u5ea6: --\u00b0C\", self)\r", "        self.temp_label.move(20, 60)\r", "\r", "        self.humid_label = QLabel(\"\u6e7f\u5ea6: --%\", self)\r", "        self.humid_label.move(150, 60)\r", "\r", "        # \u5f00\u59cb & \u505c\u6b62\u6309\u94ae\r", "        self.start_button = QPushButton(\"\u8fde\u63a5\", self)\r", "        self.start_button.move(50, 120)\r", "        self.start_button.clicked.connect(self.start_monitoring)\r", "\r", "        self.stop_button = QPushButton(\"\u505c\u6b62\", self)\r", "        self.stop_button.move(150, 120)\r", "        self.stop_button.clicked.connect(self.stop_monitoring)\r", "        self.stop_button.setEnabled(False)\r", "\r", "        # \u5b9a\u65f6\u5668\r", "        self.timer = QTimer()\r", "        self.timer.timeout.connect(self.update_info)\r", "\r", "        # **\u7cfb\u7edf\u6258\u76d8\u56fe\u6807**\r", "        icon_path = \"icon.png\"  # \u786e\u4fdd\u6b64\u8def\u5f84\u4e0b\u6709\u56fe\u6807\u6587\u4ef6\uff08.ico\uff09\r", "        self.tray_icon = QSystemTrayIcon(QIcon(icon_path), self)\r", "        self.tray_icon.setToolTip(\"STM32\u684c\u9762\u4fe1\u606f\u7a97\u53e3\")  # \u9ed8\u8ba4\u6258\u76d8\u63d0\u793a\r", "\r", "        # **\u6258\u76d8\u83dc\u5355**\r", "        self.tray_menu = QMenu(self)\r", "        restore_action = QAction(\"\u663e\u793a\u7a97\u53e3\", self)\r", "        restore_action.triggered.connect(self.showNormal)\r", "\r", "        exit_action = QAction(\"\u9000\u51fa\", self)\r", "        exit_action.triggered.connect(self.exit_app)\r", "\r", "        self.tray_menu.addAction(restore_action)\r", "        self.tray_menu.addAction(exit_action)\r", "        self.tray_icon.setContextMenu(self.tray_menu)\r", "\r", "        # **\u6258\u76d8\u9f20\u6807\u60ac\u505c\u66f4\u65b0**\r", "        self.tray_icon.activated.connect(self.restore_from_tray)\r", "\r", "        # \u663e\u793a\u6258\u76d8\r", "        self.tray_icon.show()\r", "\r", "        # \u4e32\u53e3\u7ebf\u7a0b\r", "        self.serial_port = None\r", "        self.is_running = False\r", "        self.thread = None\r", "        self.temp = \"--\"\r", "        self.humid = \"--\"\r", "\r", "    def refresh_ports(self):\r", "        \"\"\"\u5237\u65b0\u4e32\u53e3\u5217\u8868\"\"\"\r", "        self.port_combo.clear()\r", "        ports = serial.tools.list_ports.comports()\r", "        for port in ports:\r", "            self.port_combo.addItem(port.device)\r", "\r", "    def start_monitoring(self):\r", "        \"\"\"\u542f\u52a8\u4e32\u53e3\u901a\u4fe1\"\"\"\r", "        selected_port = self.port_combo.currentText()\r", "        if not selected_port:\r", "            return\r", "\r", "        try:\r", "            self.serial_port = serial.Serial(selected_port, 9600, timeout=1)\r", "            self.is_running = True\r", "            self.start_button.setEnabled(False)\r", "            self.stop_button.setEnabled(True)\r", "            self.port_combo.setEnabled(False)  # \u7981\u6b62\u4fee\u6539\u4e32\u53e3\u9009\u62e9\u6846\r", "            self.thread = threading.Thread(target=self.read_serial_data, daemon=True)\r", "            self.thread.start()\r", "            self.timer.start(1000)  # \u6bcf\u79d2\u66f4\u65b0 CPU & \u5185\u5b58\r", "        except Exception as e:\r", "            print(f\"\u4e32\u53e3\u6253\u5f00\u5931\u8d25: {e}\")\r", "\r", "    def stop_monitoring(self):\r", "        \"\"\"\u505c\u6b62\u4e32\u53e3\u901a\u4fe1\"\"\"\r", "        self.is_running = False\r", "        self.start_button.setEnabled(True)\r", "        self.stop_button.setEnabled(False)\r", "        self.port_combo.setEnabled(True)  # \u6062\u590d\u4e32\u53e3\u9009\u62e9\u6846\r", "        if self.serial_port:\r", "            self.serial_port.close()\r", "        self.timer.stop()\r", "\r", "    def read_serial_data(self):\r", "        \"\"\"\u8bfb\u53d6 STM32 \u53d1\u9001\u7684\u6570\u636e\"\"\"\r", "        while self.is_running:\r", "            try:\r", "                data = self.serial_port.read(2)  # \u8bfb\u53d6 2 \u4e2a\u5b57\u8282\r", "                if len(data) == 2:\r", "                    self.temp = data[0]\r", "                    self.humid = data[1]\r", "                    self.temp_label.setText(f\"\u6e29\u5ea6: {self.temp}\u00b0C\")\r", "                    self.humid_label.setText(f\"\u6e7f\u5ea6: {self.humid}%\")\r", "                    self.update_tray_tooltip()  # \u66f4\u65b0\u6258\u76d8\u9f20\u6807\u60ac\u6d6e\u63d0\u793a\r", "            except Exception as e:\r", "                print(f\"\u4e32\u53e3\u8bfb\u53d6\u9519\u8bef: {e}\")\r", "            time.sleep(1)\r", "\r", "    def update_info(self):\r", "        \"\"\"\u53d1\u9001 CPU \u548c\u5185\u5b58\u4f7f\u7528\u60c5\u51b5\u5230 STM32\"\"\"\r", "        if self.serial_port and self.serial_port.is_open:\r", "            cpu_usage = int(psutil.cpu_percent(interval=1))\r", "            memory_usage = int(psutil.virtual_memory().percent)\r", "            now = datetime.now()\r", "\r", "            data = [\r", "                cpu_usage,\r", "                memory_usage,\r", "                now.year - 1900,\r", "                now.month,\r", "                now.day,\r", "                now.hour,\r", "                now.minute,\r", "                now.second\r", "            ]\r", "\r", "            try:\r", "                self.serial_port.write(bytes(data))\r", "            except Exception as e:\r", "                print(f\"\u6570\u636e\u53d1\u9001\u5931\u8d25: {e}\")\r", "\r", "    def update_tray_tooltip(self):\r", "        \"\"\"\u66f4\u65b0\u7cfb\u7edf\u6258\u76d8\u60ac\u6d6e\u63d0\u793a\u4fe1\u606f\"\"\"\r", "        tooltip = f\"\u6e29\u5ea6: {self.temp}\u00b0C\\n\u6e7f\u5ea6: {self.humid}%\"\r", "        self.tray_icon.setToolTip(tooltip)\r", "\r", "    def closeEvent(self, event):\r", "        \"\"\"\u62e6\u622a\u7a97\u53e3\u5173\u95ed\u4e8b\u4ef6\uff0c\u6700\u5c0f\u5316\u5230\u7cfb\u7edf\u6258\u76d8\"\"\"\r", "        event.ignore()\r", "        self.hide()\r", "        self.tray_icon.showMessage(\"STM32\u684c\u9762\u4fe1\u606f\u7a97\u53e3\", \"\u7a0b\u5e8f\u5df2\u6700\u5c0f\u5316\u5230\u6258\u76d8\", QSystemTrayIcon.Information, 3000)\r", "\r", "    def restore_from_tray(self, reason):\r", "        \"\"\"\u4ece\u6258\u76d8\u6062\u590d\u7a97\u53e3\"\"\"\r", "        if reason == QSystemTrayIcon.ActivationReason.Trigger:\r", "            self.showNormal()\r", "\r", "    def exit_app(self):\r", "        \"\"\"\u9000\u51fa\u5e94\u7528\u7a0b\u5e8f\"\"\"\r", "        self.tray_icon.hide()\r", "        sys.exit()\r", "\r", "if __name__ == \"__main__\":\r", "    app = QApplication(sys.argv)\r", "    window = SerialMonitor()\r", "    window.show()\r", "    sys.exit(app.exec_())\r"], "file_path": "Software/PC_UI.py"}
{"Link_to_commit": "https://github.com/yasandu0505/book-crud-api/commit/e4ca27c3371e842f0fb0d046dc06b9f820df9d49", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 21, "n_files_impacted": 2, "longest_chunk": ["package handlers", "", "import (", "    \"encoding/json\"", "    \"net/http\"", ")", "", "func HandleBooks(w http.ResponseWriter, r *http.Request) {", "    switch r.Method {", "    case \"GET\":", "        // Handle GET all books", "    case \"POST\":", "        // Handle POST create book", "    case \"PUT\":", "        // Handle PUT update book", "    case \"DELETE\":", "        // Handle DELETE book", "    default:", "        http.Error(w, \"Method not allowed\", http.StatusMethodNotAllowed)", "    }", "}"], "file_path": "handlers/book.go"}
{"Link_to_commit": "https://github.com/david-elsener/moderation-planner-frontend/commit/6b93712e2e3c1c2916407dc0042c99df899575d4", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 75, "n_files_impacted": 24, "longest_chunk": ["import { Component, OnInit } from '@angular/core';", "import { CommonModule } from '@angular/common';", "import { FormsModule } from '@angular/forms';", "import { ModeratorService } from '../../services/moderator.service';", "import { Moderator } from '../../services/moderator.model';", "", "@Component({", "  selector: 'app-moderators',", "  standalone: true,", "  imports: [CommonModule, FormsModule],", "  templateUrl: './moderators.component.html',", "  styleUrls: ['./moderators.component.scss']", "})", "export class ModeratorsComponent implements OnInit {", "", "  moderators: Moderator[] = [];", "  firstName: string = '';", "  lastName: string = '';", "  selectedFile: File | null = null;", "  imagePreview: string | ArrayBuffer | null = null;", "", "  constructor(private moderatorService: ModeratorService) {}", "", "  ngOnInit(): void {", "    this.loadModerators();", "  }", "", "  loadModerators(): void {", "    this.moderatorService.getModerators().subscribe(data => {", "      this.moderators = data;", "    });", "  }", "", "  onFileSelected(event: any): void {", "    const file = event.target.files[0];", "    if (file) {", "      this.selectedFile = file;", "", "      const reader = new FileReader();", "      reader.onload = () => {", "        this.imagePreview = reader.result;", "      };", "      reader.readAsDataURL(file);", "    }", "  }", "", "  addModerator(): void {", "    if (!this.firstName || !this.lastName || !this.selectedFile) {", "      alert('Please fill all fields and select an image.');", "      return;", "    }", "", "    const formData = new FormData();", "    formData.append('firstName', this.firstName);", "    formData.append('lastName', this.lastName);", "", "    if (this.selectedFile) {", "      formData.append('image', this.selectedFile);", "    }", "", "    this.moderatorService.addModerator(formData).subscribe(() => {", "      this.loadModerators();", "      this.firstName = '';", "      this.lastName = '';", "      this.selectedFile = null;", "      this.imagePreview = null;", "    });", "  }", "", "  deleteModerator(id: string): void {", "    this.moderatorService.deleteModerator(id).subscribe(() => {", "      this.loadModerators();", "    });", "  }", "}"], "file_path": "src/app/services/moderation-track.model.ts"}
{"Link_to_commit": "https://github.com/shelbeely/Tumblr-site/commit/9ee157e437386ee57a0d597557e7df47d0ef5d88", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 66, "n_files_impacted": 3, "longest_chunk": ["<!DOCTYPE html>", "<html lang=\"en\">", "<head>", "    <meta charset=\"UTF-8\">", "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">", "    <title>Tumblr Blog Viewer</title>", "    <link rel=\"stylesheet\" href=\"styles.css\">", "    <link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap\" rel=\"stylesheet\">", "    <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">", "</head>", "<body>", "    <header>", "        <h1><span class=\"material-icons\">rss_feed</span> Tumblr Blog Viewer</h1>", "        <button id=\"theme-toggle\" class=\"button\"><span class=\"material-icons\">dark_mode</span></button>", "        <button id=\"admin-toggle\" class=\"button\"><span class=\"material-icons\">settings</span></button>", "    </header>", "", "    <div class=\"controls\">", "        <input type=\"text\" id=\"search-input\" placeholder=\"Search posts...\">", "        <select id=\"filter-select\">", "            <option value=\"all\">All Posts</option>", "            <option value=\"text\">Text</option>", "            <option value=\"photo\">Photo</option>", "            <option value=\"video\">Video</option>", "        </select>", "    </div>", "", "    <main id=\"posts-container\"></main>", "", "    <div class=\"pagination\">", "        <button id=\"prev-page\" class=\"button\" disabled>Previous</button>", "        <span id=\"page-info\">Page 1</span>", "        <button id=\"next-page\" class=\"button\">Next</button>", "    </div>", "", "    <footer>", "        <div class=\"social-links\" id=\"social-links\"></div>", "        <div class=\"donation-links\" id=\"donation-links\">", "            <h3>Support Us</h3>", "        </div>", "    </footer>", "", "    <div id=\"admin-panel\" class=\"hidden\">", "        <h2>Edit Links</h2>", "        <div id=\"social-edit\">", "            <h3>Social Media Links</h3>", "            <input type=\"text\" id=\"twitter\" placeholder=\"Twitter URL\">", "            <input type=\"text\" id=\"facebook\" placeholder=\"Facebook URL\">", "            <input type=\"text\" id=\"instagram\" placeholder=\"Instagram URL\">", "            <input type=\"text\" id=\"tumblr\" placeholder=\"Tumblr URL\">", "        </div>", "", "        <div id=\"donation-edit\">", "            <h3>Donation Links</h3>", "            <input type=\"text\" id=\"venmo\" placeholder=\"Venmo URL\">", "            <input type=\"text\" id=\"paypal\" placeholder=\"PayPal URL\">", "            <input type=\"text\" id=\"cashapp\" placeholder=\"Cash App URL\">", "            <input type=\"text\" id=\"crypto\" placeholder=\"Crypto URL\">", "        </div>", "", "        <button id=\"save-links\">Save</button>", "    </div>", "", "    <script src=\"script.js\"></script>", "</body>", "</html>"], "file_path": "script.js"}
{"Link_to_commit": "https://github.com/blindmice1o3/the-straylight-run/commit/ef49eb40ede6e65d326e1ddae4385cafe7dd9aab", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 40, "n_files_impacted": 5, "longest_chunk": ["", "        initMessageQueue();", "    }", "", "    private void initMessageQueue() {", "        messageQueue = new ArrayList<>();", "        String[] namesOfSenderDialogueArrayJavaLoops = getResources().getStringArray(R.array.names_of_sender_dialogue_array_java_loops);", "        String[] messagesDialogueArrayJavaLoops = getResources().getStringArray(R.array.messages_dialogue_array_java_loops);", "        String[] delayMsDialogueArrayJavaLoops = getResources().getStringArray(R.array.delay_ms_dialogue_array_java_loops);", "        for (int i = 0; i < namesOfSenderDialogueArrayJavaLoops.length; i++) {", "            String nameOfSender = namesOfSenderDialogueArrayJavaLoops[i];", "            String message = messagesDialogueArrayJavaLoops[i];", "            long delayMs = Long.parseLong(delayMsDialogueArrayJavaLoops[i]);", "", "            messageQueue.add(new Message(nameOfSender, message, delayMs, false));", "        }", "        messageQueue.add(0, new Message(\"player\", \"Muly: meow?\", 500L, true));", "        messageQueue.add(2, new Message(\"player\", \"Mulan: MEOW?\", 3500L, true));", "        messageQueue.add(4, new Message(\"player\", \"Muhang: meow\", 6000L, true));", "        messageQueue.add(6, new Message(\"player\", \"Mom: hello\", 9000L, true));", "        messageQueue.add(8, new Message(\"player\", \"Apsara: meow\", 11000L, true));", "        messageQueue.add(10, new Message(\"player\", \"Colin: silence\", 16000L, true));", "    }", "", "    public void startMessageQueue() {", "        for (Message messageToAdd : messageQueue) {", "            Handler handler = new Handler(Looper.getMainLooper());", "            handler.postDelayed(new Runnable() {", "                @Override", "                public void run() {", "                    addMessageToRecycleView(messageToAdd);", "                }", "            }, messageToAdd.getDelayMs());", "        }", "    }", "", "    private void addMessageToRecycleView(Message message) {", "        int indexNewMessage = messages.size();", "        messages.add(message);", "        adapter.notifyItemInserted(indexNewMessage);"], "file_path": "app/src/main/java/com/jackingaming/thestraylightrun/accelerometer/game/drawers/DrawerStartFragment.java"}
{"Link_to_commit": "https://github.com/grinstead/result/commit/ca002260ed30151c0f02dba8d3817df8f196c6c1", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 38, "n_files_impacted": 1, "longest_chunk": ["import { describe, expect, it } from \"bun:test\";", "import { success, failure, Result } from \"./index\";", "", "describe(\"Result Type Tests\", () => {", "  it(\"should create a success result\", () => {", "    const result = success(42);", "", "    expect(result.done).toBe(true);", "    expect(result.value).toBe(42);", "    expect(result.failure).toBeUndefined();", "  });", "", "  it(\"should create a failure result and throw on value access\", () => {", "    const error = new Error(\"Test error\");", "    const result = failure(error);", "", "    expect(result.done).toBe(true);", "    expect(result.failure).toBe(error);", "", "    expect(() => result.value).toThrow(error);", "  });", "", "  it(\"should handle different success types\", () => {", "    const stringResult = success(\"Hello, world!\");", "    expect(stringResult.value).toBe(\"Hello, world!\");", "", "    const objectResult = success({ key: \"value\" });", "    expect(objectResult.value).toEqual({ key: \"value\" });", "  });", "", "  it(\"should handle different failure types\", () => {", "    const errorObject = { code: 500, message: \"Internal Server Error\" };", "    const result = failure(errorObject);", "", "    expect(result.failure).toBe(errorObject);", "    expect(() => result.value).toThrow(errorObject);", "  });", "});"], "file_path": "index.test.ts"}
{"Link_to_commit": "https://github.com/BenBen1806/lengApp/commit/702528a6415dabc76f915651ac7514512dbec05a", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 26, "n_files_impacted": 47, "longest_chunk": ["package com.example.lengapp;", "", "import android.content.Context;", "", "import androidx.test.platform.app.InstrumentationRegistry;", "import androidx.test.ext.junit.runners.AndroidJUnit4;", "", "import org.junit.Test;", "import org.junit.runner.RunWith;", "", "import static org.junit.Assert.*;", "", "/**", " * Instrumented test, which will execute on an Android device.", " *", " * @see <a href=\"http://d.android.com/tools/testing\">Testing documentation</a>", " */", "@RunWith(AndroidJUnit4.class)", "public class ExampleInstrumentedTest {", "    @Test", "    public void useAppContext() {", "        // Context of the app under test.", "        Context appContext = InstrumentationRegistry.getInstrumentation().getTargetContext();", "        assertEquals(\"com.example.lengapp\", appContext.getPackageName());", "    }", "}"], "file_path": "app/src/androidTest/java/com/example/lengapp/ExampleInstrumentedTest.java"}
{"Link_to_commit": "https://github.com/BenBen1806/lengApp/commit/fb2abf1d242754e00125ed9d5462a480b091efb2", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 49, "n_files_impacted": 4, "longest_chunk": ["package com.example.lengapp;", "", "", "public class Flashcard {", "    private String word;           // The word in the source language", "    private String translation;    // The translation of the word", "    private String soundFileName;  // The file name of the sound associated with the word", "", "    // Constructor", "    public Flashcard(String word, String translation, String soundFileName) {", "        this.word = word;", "        this.translation = translation;", "        this.soundFileName = soundFileName;", "    }", "", "    // Getters and Setters", "    public String getWord() {", "        return word;", "    }", "", "    public void setWord(String word) {", "        this.word = word;", "    }", "", "    public String getTranslation() {", "        return translation;", "    }", "", "    public void setTranslation(String translation) {", "        this.translation = translation;", "    }", "", "    public String getSoundFileName() {", "        return soundFileName;", "    }", "", "    public void setSoundFileName(String soundFileName) {", "        this.soundFileName = soundFileName;", "    }", "", "    @Override", "    public String toString() {", "        return \"Flashcard{\" +", "                \"word='\" + word + '\\'' +", "                \", translation='\" + translation + '\\'' +", "                \", soundFileName='\" + soundFileName + '\\'' +", "                '}';", "    }", "}"], "file_path": "app/src/main/java/com/example/lengapp/FlashcardUtils.java"}
{"Link_to_commit": "https://github.com/trkks/trkks.github.io/commit/fc54f9227aade44271fe31d15ecbb996bced6e3d", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 43, "n_files_impacted": 2, "longest_chunk": ["    ],", "    \"gpt\": [", "        { \"RU\": \"\u0438\", \"EN-GB\": \"and\", \"FI\": \"ja\" },", "        { \"RU\": \"\u0432\", \"EN-GB\": \"in\", \"FI\": \"sis\u00e4ll\u00e4\" },", "        { \"RU\": \"\u043d\u0435\", \"EN-GB\": \"not\", \"FI\": \"ei\" },", "        { \"RU\": \"\u043e\u043d\", \"EN-GB\": \"he\", \"FI\": \"h\u00e4n\" },", "        { \"RU\": \"\u043d\u0430\", \"EN-GB\": \"on\", \"FI\": \"p\u00e4\u00e4ll\u00e4\" },", "        { \"RU\": \"\u044f\", \"EN-GB\": \"I\", \"FI\": \"min\u00e4\" },", "        { \"RU\": \"\u0441\", \"EN-GB\": \"with\", \"FI\": \"kanssa\" },", "        { \"RU\": \"\u0447\u0442\u043e\", \"EN-GB\": \"what\", \"FI\": \"mit\u00e4\" },", "        { \"RU\": \"\u043f\u043e\", \"EN-GB\": \"by\", \"FI\": \"kautta\" },", "        { \"RU\": \"\u044d\u0442\u043e\", \"EN-GB\": \"this\", \"FI\": \"t\u00e4m\u00e4\" },", "        { \"RU\": \"\u043a\u0430\u043a\", \"EN-GB\": \"how\", \"FI\": \"kuinka\" },", "        { \"RU\": \"\u043c\u044b\", \"EN-GB\": \"we\", \"FI\": \"me\" },", "        { \"RU\": \"\u0438\u0437\", \"EN-GB\": \"from\", \"FI\": \"from\" },", "        { \"RU\": \"\u0437\u0430\", \"EN-GB\": \"for\", \"FI\": \"varten\" },", "        { \"RU\": \"\u0432\u044b\", \"EN-GB\": \"you\", \"FI\": \"te\", \"comment\": \"formal/plural\" },", "        { \"RU\": \"\u0442\u0430\u043a\", \"EN-GB\": \"so\", \"FI\": \"niin\" },", "        { \"RU\": \"\u043d\u043e\", \"EN-GB\": \"but\", \"FI\": \"mutta\" },", "        { \"RU\": \"\u043e\", \"EN-GB\": \"about\", \"FI\": \"noin\" },", "        { \"RU\": \"\u0436\u0435\", \"EN-GB\": \"same\", \"FI\": \"sama\" },", "        { \"RU\": \"\u0442\u044b\", \"EN-GB\": \"you\", \"FI\": \"sin\u00e4\", \"comment\": \"informal\" },", "        { \"RU\": \"\u043c\u0430\u043c\u0430\", \"EN-GB\": \"mom\", \"FI\": \"\u00e4iti\" },", "        { \"RU\": \"\u043e\u0442\u0435\u0446\", \"EN-GB\": \"father\", \"FI\": \"is\u00e4\" },", "        { \"RU\": \"\u0436\u0435\u043d\u0449\u0438\u043d\u0430\", \"EN-GB\": \"woman\", \"FI\": \"nainen\" },", "        { \"RU\": \"\u043c\u0443\u0436\u0447\u0438\u043d\u0430\", \"EN-GB\": \"man\", \"FI\": \"mies\" },", "        { \"RU\": \"\u0434\u0440\u0443\u0433\", \"EN-GB\": \"friend\", \"FI\": \"yst\u00e4v\u00e4\" },", "        { \"RU\": \"\u0431\u0440\u0430\u0442\", \"EN-GB\": \"brother\", \"FI\": \"veli\" },", "        { \"RU\": \"\u0441\u0435\u0441\u0442\u0440\u0430\", \"EN-GB\": \"sister\", \"FI\": \"sisar\" },", "        { \"RU\": \"\u0433\u043e\u0440\u043e\u0434\", \"EN-GB\": \"city\", \"FI\": \"kaupunki\" },", "        { \"RU\": \"\u043b\u0435\u0441\", \"EN-GB\": \"forest\", \"FI\": \"mets\u00e4\" },", "        { \"RU\": \"\u0433\u043e\u0440\u0430\", \"EN-GB\": \"mountain\", \"FI\": \"vuori\" },", "        { \"RU\": \"\u0440\u0435\u043a\u0430\", \"EN-GB\": \"river\", \"FI\": \"joki\" },", "        { \"RU\": \"\u043c\u043e\u0440\u0435\", \"EN-GB\": \"sea\", \"FI\": \"meri\" },", "        { \"RU\": \"\u043d\u0435\u0431\u043e\", \"EN-GB\": \"sky\", \"FI\": \"taivas\" },", "        { \"RU\": \"\u0441\u043e\u043b\u043d\u0446\u0435\", \"EN-GB\": \"sun\", \"FI\": \"aurinko\" },", "        { \"RU\": \"\u0437\u0432\u0435\u0437\u0434\u0430\", \"EN-GB\": \"star\", \"FI\": \"t\u00e4hti\" },", "        { \"RU\": \"\u043c\u0435\u0441\u044f\u0446\", \"EN-GB\": \"month\", \"FI\": \"kuukausi\" },", "        { \"RU\": \"\u0434\u0435\u043d\u044c\u0433\u0438\", \"EN-GB\": \"money\", \"FI\": \"raha\" },", "        { \"RU\": \"\u0440\u0430\u0431\u043e\u0442\u0430\", \"EN-GB\": \"work\", \"FI\": \"ty\u00f6\" },", "        { \"RU\": \"\u0448\u043a\u043e\u043b\u0430\", \"EN-GB\": \"school\", \"FI\": \"koulu\" },", "        { \"RU\": \"\u0443\u0447\u0438\u0442\u0435\u043b\u044c\", \"EN-GB\": \"teacher\", \"FI\": \"opettaja\" },", "        { \"RU\": \"\u0441\u0442\u0443\u0434\u0435\u043d\u0442\", \"EN-GB\": \"student\", \"FI\": \"opiskelija\" }"], "file_path": "kielet/data.js"}
{"Link_to_commit": "https://github.com/EnricoGuccii/website/commit/ed32f763ee6552a697378249363ae74a2c58f6cd", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 132, "n_files_impacted": 6, "longest_chunk": ["", "// Cool terminal effect ", "// generated by chatgpt", "", "const commands = [", "    { cmd: \"whoami\", output: [\"root\"] },", "    { cmd: \"uname -a\", output: [\"Linux server 5.15.0-79-generic #86-Ubuntu SMP x86_64 GNU/Linux\"] },", "    {", "        cmd: \"ls -la\", output: [", "            \"total 28\",", "            \"drwxr-xr-x  3 root root 4096 Feb 23 12:34 .\",", "            \"drwxr-xr-x 18 root root 4096 Feb 23 12:00 ..\",", "            \"-rw-r--r--  1 root root   21 Feb 23 12:30 .bashrc\",", "            \"-rw-r--r--  1 root root   14 Feb 23 12:30 .profile\",", "            \"drwx------  2 root root 4096 Feb 23 12:30 .ssh\"", "        ]", "    },", "    {", "        cmd: \"cat /etc/os-release\", output: [", "            'NAME=\"Ubuntu\"',", "            'VERSION=\"22.04.3 LTS (Jammy Jellyfish)\"',", "            'ID=ubuntu',", "            'ID_LIKE=debian',", "            'PRETTY_NAME=\"Ubuntu 22.04.3 LTS\"',", "            'VERSION_ID=\"22.04\"',", "        ]", "    },", "    {", "        cmd: \"ping -c 3 8.8.8.8\", output: [", "            \"PING 8.8.8.8 (8.8.8.8): 56 data bytes\",", "            \"64 bytes from 8.8.8.8: icmp_seq=1 ttl=118 time=12.3 ms\",", "            \"64 bytes from 8.8.8.8: icmp_seq=2 ttl=118 time=11.8 ms\",", "            \"64 bytes from 8.8.8.8: icmp_seq=3 ttl=118 time=12.1 ms\",", "            \"--- 8.8.8.8 ping statistics ---\",", "            \"3 packets transmitted, 3 received, 0% packet loss, time 2000ms\",", "            \"rtt min/avg/max/mdev = 11.8/12.0/12.3/0.2 ms\"", "        ]", "    },", "    {", "        cmd: \"df -h\", output: [", "            \"Filesystem      Size  Used Avail Use% Mounted on\",", "            \"/dev/sda1        50G   25G   25G  50% /\"", "        ]", "    },", "    {", "        cmd: \"top -n 1\", output: [", "            \"top - 12:34:56 up 3 days,  5:42,  1 user,  load average: 0.24, 0.16, 0.15\",", "            \"Tasks: 102 total,   1 running, 101 sleeping,   0 stopped,   0 zombie\",", "            \"%Cpu(s):  1.3 us,  0.2 sy,  0.0 ni, 98.3 id,  0.1 wa,  0.0 hi,  0.1 si,  0.0 st\",", "            \"KiB Mem :  8000000 total,  2000000 free,  3000000 used,  3000000 buff/cache\",", "            \"KiB Swap:  4000000 total,  4000000 free,        0 used.  5000000 avail Mem \"", "        ]", "    },", "    { cmd: \"clear\", output: [\"clear\"] }", "", "];", "", "let consoleElement = document.getElementById(\"console\");", "let index = 0;", "", "function addPrompt() {", "    let promptLine = document.createElement(\"div\");", "    promptLine.className = \"line\";", "    promptLine.innerHTML = `<span class=\"prompt\">root@server:~$</span> `;", "    consoleElement.appendChild(promptLine);", "    return promptLine;", "}", "", "function typeCommand(command, promptLine, callback) {", "    let i = 0;", "", "    function type() {", "        if (i < command.length) {", "            promptLine.innerHTML = `<span class=\"prompt\">root@server:~$</span> ` + command.substring(0, i + 1) + '<span class=\"cursor\"></span>';", "            i++;", "            setTimeout(type, 50);", "        } else {", "            promptLine.innerHTML = `<span class=\"prompt\">root@server:~$</span> ` + command;", "            setTimeout(callback, 500);", "        }", "    }", "    type();", "}", "", "function showOutput(output, callback) {", "    if (output[0] === \"clear\") {", "        consoleElement.innerHTML = \"\";", "    } else {", "        output.forEach(line => {", "            let outputLine = document.createElement(\"div\");", "            outputLine.className = \"line\";", "            outputLine.innerHTML = line;", "            consoleElement.appendChild(outputLine);", "        });", "", "        cleanUpOldLines();", "    }", "", "    setTimeout(callback, 1000);", "}", "", "function cleanUpOldLines() {", "    while (consoleElement.children.length > 7) {", "        consoleElement.removeChild(consoleElement.children[0]);", "    }", "}", "", "function runConsole() {", "    if (index < commands.length) {", "        const command = commands[index];", "", "        let promptLine = addPrompt();", "", "        setTimeout(() => {", "            typeCommand(command.cmd, promptLine, () => {", "                showOutput(command.output, () => {", "                    index++;", "                    runConsole();", "                });", "            });", "        }, 500);", "", "    } else {", "        setTimeout(() => {", "            consoleElement.innerHTML = \"\";", "            index = 0;", "            runConsole();", "        }, 3000);", "    }", "}", "", "runConsole();"], "file_path": "js/main.js"}
{"Link_to_commit": "https://github.com/toyamagaio/H1slit_control/commit/eb31e9bed9a36f3250a17e976e25c3b87e078eb7", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 191, "n_files_impacted": 2, "longest_chunk": ["import time\r", "import serial\r", "import subprocess\r", "\r", "class H1SlitController:\r", "    def __init__(self, port=\"/dev/ttyUSB0\", baudrate=115200, filepath='pos.curr'):\r", "        self.port = port\r", "        self.baudrate = baudrate\r", "        self.filepath = filepath\r", "        self.client = None\r", "\r", "    def open_ser(self):\r", "        self.client = serial.Serial(\r", "            self.port,\r", "            self.baudrate,\r", "            timeout=0.05,\r", "            write_timeout=0.01,\r", "            parity=serial.PARITY_EVEN,\r", "            stopbits=serial.STOPBITS_ONE\r", "        )\r", "\r", "    def close_ser(self):\r", "        if self.client:\r", "            self.client.close()\r", "\r", "    def execute(self, com, PRINT=True):\r", "        com += self.crc16(com)\r", "        if PRINT:\r", "            print(com.encode('hex'))\r", "        self.client.write(com)\r", "        time.sleep(0.5)\r", "        result = self.client.read(16)\r", "        if PRINT:\r", "            print(result.encode('hex'))\r", "        return int(result.encode('hex'), 16)\r", "\r", "    def to_bytes(self, n, length, endianess='big'):\r", "        h = '%x' % n\r", "        s = ('0'*(len(h) % 2) + h).zfill(length*2).decode('hex')\r", "        return s if endianess != 'big' else s[::-1]\r", "\r", "    def s32(self, val):\r", "        return -(val & 0x80000000) | (val & 0x7fffffff)\r", "\r", "    def crc16(self, data):\r", "        data = bytearray(data)\r", "        poly = 0xA001\r", "        crc = 0xFFFF\r", "        for b in data:\r", "            crc ^= (0xFF & b)\r", "            for _ in range(8):\r", "                if crc & 0x0001:\r", "                    crc = ((crc >> 1) & 0xFFFF) ^ poly\r", "                else:\r", "                    crc = ((crc >> 1) & 0xFFFF)\r", "        return self.to_bytes(crc, 2)\r", "\r", "    def read(self, seg, register, n):\r", "        tmp = bytearray.fromhex('%02x' % seg)\r", "        tmp.extend('\\x03')\r", "        tmp.extend(bytearray.fromhex('%04x' % register))\r", "        tmp.extend(bytearray.fromhex('%04x' % n))\r", "        return self.execute(bytes(tmp))\r", "\r", "    def write(self, seg, register, nreg, nbyte, data):\r", "        tmp = bytearray.fromhex('%02x' % seg)\r", "        tmp.extend('\\x10')\r", "        tmp.extend(bytearray.fromhex('%04x' % register))\r", "        tmp.extend(bytearray.fromhex('%04x' % nreg))\r", "        tmp.extend(bytearray.fromhex('%02x' % nbyte))\r", "        for d in data:\r", "            tmp.extend(bytearray.fromhex('%08x' % d))\r", "        self.execute(bytes(tmp))\r", "\r", "    def polling(self, seg):\r", "        counter = 0\r", "        while self.isready(seg) == 0:\r", "            time.sleep(0.1)\r", "            counter += 1\r", "            if counter % 10 == 0:\r", "                print('.')\r", "            if counter > 100:\r", "                print(\"timeout!!!\", seg)\r", "                return False\r", "        return True\r", "\r", "    def isready(self, seg):\r", "        _ = self.read(seg, 126, 2)\r", "        ready2 = (self.read(seg, 377, 1) >> 20) & 0x1\r", "        return ready2\r", "\r", "    def off(self, seg):\r", "        self.write(seg, 0x7c, 2, 4, [0x00])\r", "\r", "    def start(self, seg):\r", "        if self.polling(seg):\r", "            self.write(seg, 0x7c, 2, 4, [0x08])\r", "            time.sleep(0.1)\r", "            self.off(seg)\r", "\r", "    def set_params(self, seg, channel, mode, pos, speed=5000, rate1=1000, rate2=1000):\r", "        base = 1024 + channel\r", "        self.write(seg, base + 0 * 128, 2, 4, [pos])\r", "        self.write(seg, base + 1 * 128, 2, 4, [speed])\r", "        self.write(seg, base + 2 * 128, 2, 4, [mode])\r", "        self.write(seg, base + 4 * 128, 2, 4, [rate1])\r", "        self.write(seg, base + 5 * 128, 2, 4, [rate2])\r", "\r", "    def get_position(self, seg):\r", "        if self.polling(seg):\r", "            result = self.read(seg, 0xcc, 2)\r", "            pos = (result >> 16) & 0xffffffff\r", "            print(pos, hex(pos))\r", "            return self.s32(pos)\r", "\r", "    def save_seg(self):\r", "        serial = 1234\r", "        xpos = self.get_position(2)\r", "        ypos = self.get_position(1)\r", "        s = f\"{serial} {xpos} {ypos}  fin\\n\"\r", "        with open(self.filepath, 'w') as f:\r", "            f.write(s)\r", "        subprocess.call(['scp', '-p', self.filepath, 'vme:/tmp/daq.txt'])\r", "\r", "    def move(self, channel):\r", "        x = (channel - 1) % 8\r", "        y = (channel - 1) // 8\r", "        xpos = 25750 * x\r", "        ypos = 25900 * y\r", "        print(x, y, xpos, ypos)\r", "        self.set_params(2, 0, 1, xpos)\r", "        self.set_params(1, 0, 1, ypos)\r", "        self.start(1)\r", "        self.start(2)\r", "\r", "    def step(self, seg, count):\r", "        self.set_params(seg, 0, 2, count)\r", "        self.start(seg)\r", "        self.save_seg()\r", "\r", "    def absolute(self, seg, count):\r", "        self.set_params(seg, 0, 1, count)\r", "        self.start(seg)\r", "        self.save_seg()\r", "\r", "    def home(self, seg):\r", "        if self.polling(seg):\r", "            self.write(seg, 0x7c, 2, 4, [0x10])\r", "            time.sleep(0.1)\r", "            self.off(seg)\r", "        self.get_position(seg)\r", "        self.save_seg()\r", "\r", "    def read_params(self, seg, channel):\r", "        base = 1024 + channel\r", "        offset = [0, 128, 2 * 128, 4 * 128, 5 * 128]\r", "        names = ['pos', 'speed', 'mode', 'rate1', 'rate2']\r", "        print('------- seg =', seg, 'channel =', channel)\r", "        for offs, name in zip(offset, names):\r", "          result = self.read(seg, base + offs, 2)\r", "          val = (result >> 16) & 0xffff\r", "          print(name, val)\r", "\r", "    def reset_home(self, seg):\r", "        cpos=self.get_position(seg)\r", "        print('cpos:',cpos, 'is new origin (0)')\r", "        if self.polling(seg):\r", "          val=self.read(seg,0x018A,2)\r", "          aaa=(val>>16&0x01)\r", "          if aaa==1:\r", "            self.write(seg,0x018A,2,4,[0x00])\r", "          self.write(seg,0x018A,2,4,[0x01])\r", "\r", "\r", "if __name__ == \"__main__\":\r", "    controller = H1SlitController()\r", "    controller.open_ser()\r", "\r", "    controller.home(1)\r", "    controller.read_params(1, 0)\r", "\r", "    controller.close_ser()\r", "\r", "#if __name__ == \"__main__\":\r", "#    controller = H1SlitController()\r", "#    controller.open_ser()\r", "#\r", "#    controller.home(1)\r", "#    controller.read_params(1, 0)\r", "#\r", "#    controller.close_ser()\r"], "file_path": "py/H1slit_controller.py"}
{"Link_to_commit": "https://github.com/chbsaikiran/GANs_VAEs_UNETs_CLIP/commit/d96c37504dc7124da9f9f43c69989ce4b2ac37a4", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 139, "n_files_impacted": 4, "longest_chunk": ["import torch", "import torch.nn as nn", "import torch.optim as optim", "import torchvision.transforms as transforms", "from torch.utils.data import DataLoader, Dataset", "import numpy as np", "import cv2", "import os", "from glob import glob", "", "# UNET Model Definition", "class UNet(nn.Module):", "    def __init__(self, in_channels=3, out_channels=1):", "        super(UNet, self).__init__()", "        ", "        def conv_block(in_c, out_c):", "            return nn.Sequential(", "                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),", "                nn.ReLU(inplace=True),", "                nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),", "                nn.ReLU(inplace=True)", "            )", "        ", "        self.encoder = nn.ModuleList([", "            conv_block(in_channels, 64),", "            conv_block(64, 128),", "            conv_block(128, 256),", "            conv_block(256, 512),", "            conv_block(512, 1024),", "        ])", "        ", "        self.pool = nn.MaxPool2d(2)", "        ", "        self.upconv = nn.ModuleList([", "            nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2),", "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),", "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),", "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)", "        ])", "        ", "        self.decoder = nn.ModuleList([", "            conv_block(1024, 512),", "            conv_block(512, 256),", "            conv_block(256, 128),", "            conv_block(128, 64)", "        ])", "        ", "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)", "        ", "    def forward(self, x):", "        encoder_outs = []", "        for enc in self.encoder:", "            x = enc(x)", "            encoder_outs.append(x)", "            x = self.pool(x)", "        ", "        x = encoder_outs.pop()", "        ", "        for up, dec in zip(self.upconv, self.decoder):", "            x = up(x)", "            enc_out = encoder_outs.pop()", "            x = torch.cat([x, enc_out], dim=1)", "            x = dec(x)", "        ", "        return torch.sigmoid(self.final_conv(x))", "", "# Custom Dataset", "class SegmentationDataset(Dataset):", "    def __init__(self, image_paths, mask_paths, transform=None):", "        self.image_paths = image_paths", "        self.mask_paths = mask_paths", "        self.transform = transform", "    ", "    def __len__(self):", "        return len(self.image_paths)", "    ", "    def __getitem__(self, idx):", "        img = cv2.imread(self.image_paths[idx])", "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "        mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_GRAYSCALE)", "        ", "        img = cv2.resize(img, (256, 256))", "        mask = cv2.resize(mask, (256, 256))", "        ", "        img = img / 255.0", "        mask = mask / 255.0", "        ", "        img = np.transpose(img, (2, 0, 1)).astype(np.float32)", "        mask = np.expand_dims(mask, axis=0).astype(np.float32)", "        ", "        return torch.tensor(img), torch.tensor(mask)", "", "# Load Dataset", "image_paths = sorted(glob(\"path/to/images/*.jpg\"))", "mask_paths = sorted(glob(\"path/to/masks/*.png\"))", "", "dataset = SegmentationDataset(image_paths, mask_paths)", "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)", "", "# Training Setup", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")", "model = UNet().to(device)", "criterion = nn.BCELoss()", "optimizer = optim.Adam(model.parameters(), lr=1e-4)", "", "def train(model, dataloader, criterion, optimizer, epochs=10):", "    model.train()", "    for epoch in range(epochs):", "        epoch_loss = 0", "        for img, mask in dataloader:", "            img, mask = img.to(device), mask.to(device)", "            optimizer.zero_grad()", "            output = model(img)", "            loss = criterion(output, mask)", "            loss.backward()", "            optimizer.step()", "            epoch_loss += loss.item()", "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(dataloader):.4f}\")", "    torch.save(model.state_dict(), \"unet_model.pth\")", "", "# Inference Function", "def infer(model, image_path):", "    model.eval()", "    img = cv2.imread(image_path)", "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "    img = cv2.resize(img, (256, 256))", "    img = img / 255.0", "    img = np.transpose(img, (2, 0, 1)).astype(np.float32)", "    img = torch.tensor(img).unsqueeze(0).to(device)", "    ", "    with torch.no_grad():", "        pred = model(img)", "    pred = pred.squeeze().cpu().numpy()", "    return (pred > 0.5).astype(np.uint8) * 255", "", "# Example Usage", "# train(model, dataloader, criterion, optimizer, epochs=10)", "# mask = infer(model, \"path/to/sample.jpg\")", "# cv2.imwrite(\"output_mask.png\", mask)"], "file_path": "VAEs_code_by_chatgpt.py"}
{"Link_to_commit": "https://github.com/david-elsener/moderation-planner-frontend/commit/e4b2ac6a99994fac62838110776a972d99058eab", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 11, "n_files_impacted": 3, "longest_chunk": ["<div class=\"moderator-list\" style=\"display: flex; flex-wrap: wrap; gap: 16px;\">", "  <mat-card *ngFor=\"let moderator of moderators\" class=\"moderator-card\" style=\"width: 200px; background-color: #ffffff;\">", "    <img mat-card-image [src]=\"moderator.imageData\" alt=\"{{moderator.firstName}}\">", "    <mat-card-content>", "      <h3 style=\"color: #e10000;\">{{ moderator.firstName }} {{ moderator.lastName }}</h3>", "    </mat-card-content>", "    <mat-card-actions>", "      <button mat-raised-button color=\"warn\" (click)=\"deleteModerator(moderator.id!)\">Delete</button>", "    </mat-card-actions>", "  </mat-card>", "</div>"], "file_path": "src/app/components/moderators/moderators.component.ts"}
{"Link_to_commit": "https://github.com/Aha43/spf/commit/072d47823a9bc7a01fd168e07c270dcdb8ef4bf5", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 71, "n_files_impacted": 6, "longest_chunk": ["    private readonly IServiceProvider _serviceProvider;", "    private readonly List<ISpfPromptHandler> _handlers;", "    private readonly ISpfExitor? _exitor;", "    private readonly ISpfNoPromptMatchHandler? _noMatchHandler;", "    private readonly SpfState _state = new();", "", "    public Spf(string[] args, IServiceCollection services)", "    {", "        var serviceProvider = services.BuildServiceProvider();", "        _serviceProvider = serviceProvider;", "        _handlers = DiscoverHandlers(serviceProvider);", "        _exitor = serviceProvider.GetService<ISpfExitor>();", "        _noMatchHandler = serviceProvider.GetService<ISpfNoPromptMatchHandler>();", "    }", "", "    private static List<ISpfPromptHandler> DiscoverHandlers(IServiceProvider serviceProvider)", "    {", "        return [.. AppDomain.CurrentDomain.GetAssemblies()", "            .SelectMany(a => a.GetTypes())", "            .Where(t => typeof(ISpfPromptHandler).IsAssignableFrom(t) && !t.IsInterface && !t.IsAbstract)", "            .Select(t => (ISpfPromptHandler)serviceProvider.GetRequiredService(t))];", "    }", "", "    public async Task StartAsync()", "    {", "        while (true)", "        {", "            Console.Write(\" > \");", "            var input = Console.ReadLine()?.Trim();", "            if (string.IsNullOrEmpty(input)) continue;", "", "            if (input.Equals(\"q\", StringComparison.OrdinalIgnoreCase) || input.Equals(\"quit\", StringComparison.OrdinalIgnoreCase))", "            {", "                if (_exitor != null && !await _exitor.ExitAsync(_state))", "                    continue;", "                break;", "            }", "", "            var tokens = input.Split(' ', StringSplitOptions.RemoveEmptyEntries);", "            if (tokens.Length == 0) continue;", "", "            var (path, cmdInput) = TokenizeInput(tokens);", "            var handler = _handlers.FirstOrDefault(h => MatchesHandler(h, path));", "", "            if (handler != null)", "            {", "                await handler.HandlePromptAsync(path, cmdInput, _state);", "            }", "            else if (_noMatchHandler != null && await _noMatchHandler.HandleNoMatch(tokens, _state))", "            {", "                continue;", "            }", "            else", "            {", "                Console.WriteLine(\"Error: Unrecognized command.\");", "            }", "        }", "    }", "", "    private static (string[] path, string[] input) TokenizeInput(string[] tokens)", "    {", "        var lastIndex = tokens.ToList().FindLastIndex(t => char.IsUpper(t.FirstOrDefault()));", "        if (lastIndex == -1) return (tokens, Array.Empty<string>());", "        return (tokens[..(lastIndex + 1)], tokens[(lastIndex + 1)..]);", "    }", "", "    private static bool MatchesHandler(ISpfPromptHandler handler, string[] path)", "    {", "        var typeName = handler.GetType().Name;", "        if (typeName.EndsWith(\"SpfPromptHandler\"))", "            typeName = typeName[..^15];"], "file_path": "src/Spf/Spf.cs"}
{"Link_to_commit": "https://github.com/r-leyshon/user-persona-app/commit/cf96c0f26fa75b85af411f245ef3f64b31d8585e", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 54, "n_files_impacted": 1, "longest_chunk": ["# contains AI-generated artifacts.", "", "personas = [", "    {", "        \"name\": \"Emily\",", "        \"greeting\": \"Hi there, I'm Emily, a policy advisor. How can I assist you today?\",", "        \"persona\": \"Emily is a 30-year-old policy advisor based in London. She specializes in education reform and has a background in sociology. Emily values clear, concise communication and frequently interacts with stakeholders from diverse sectors. She prefers formal yet accessible language to ensure inclusivity.\"", "    },", "    {", "        \"name\": \"Ahmed\",", "        \"greeting\": \"Hello, I'm Ahmed, an IT manager. How can I help you today?\",", "        \"persona\": \"Ahmed is a 42-year-old IT manager in Manchester. With a degree in computer science, he oversees digital infrastructure for public services. Ahmed is detail-oriented and appreciates precise, technical explanations when discussing projects.\"", "    },", "    {", "        \"name\": \"Priya\",", "        \"greeting\": \"Hi, I'm Priya, a communications officer. What can I do for you today?\",", "        \"persona\": \"Priya is a 28-year-old communications officer in Birmingham. She manages internal and external communications for her department. Priya is creative and enjoys tailoring messages to different audiences, ensuring clarity and accessibility.\"", "    },", "    {", "        \"name\": \"John\",", "        \"greeting\": \"Good day, I'm John, a senior civil servant. How may I assist?\",", "        \"persona\": \"John is a 55-year-old senior civil servant in Edinburgh. With over 30 years of experience, he focuses on strategic planning for economic development. John values professionalism and prefers formal communication that respects tradition.\"", "    },", "    {", "        \"name\": \"Sophie\",", "        \"greeting\": \"Hello there, I'm Sophie, a graduate trainee. What do you need help with?\",", "        \"persona\": \"Sophie is a 24-year-old graduate trainee in Cardiff. She is enthusiastic about public service and eager to learn. Sophie values approachable and clear language, as she is still familiarizing herself with technical jargon.\"", "    },", "    {", "        \"name\": \"Derek\",", "        \"greeting\": \"Hey, I'm Derek, a regional coordinator. What's on your mind today?\",", "        \"persona\": \"Derek is a 50-year-old regional coordinator in Belfast. He works with local councils to implement national policies. Derek has a hands-on approach and prefers practical, straightforward communication.\"", "    },", "    {", "        \"name\": \"Amara\",", "        \"greeting\": \"Hi, I'm Amara, a social worker. How can I support you today?\",", "        \"persona\": \"Amara is a 35-year-old social worker in Leeds. She advocates for vulnerable populations and ensures compliance with government welfare policies. Amara values empathetic, people-first language in her communications.\"", "    },", "    {", "        \"name\": \"George\",", "        \"greeting\": \"Hello, I'm George, a statistician. What can I assist you with?\",", "        \"persona\": \"George is a 60-year-old statistician in Bristol. He analyzes data to inform policy decisions. George is analytical and prefers precise, data-driven language but is also mindful of making his insights comprehensible to non-technical audiences.\"", "    },", "    {", "        \"name\": \"Yasmin\",", "        \"greeting\": \"Hi there, I'm Yasmin, a diversity and inclusion officer. What do you need help with today?\",", "        \"persona\": \"Yasmin is a 40-year-old diversity and inclusion officer in Liverpool. She ensures workplace policies promote equality and accessibility. Yasmin values inclusive language and is skilled at adapting her communication style to diverse audiences.\"", "    },", "    {", "        \"name\": \"Oliver\",", "        \"greeting\": \"Good day, I'm Oliver, an environmental scientist. How may I help?\",", "        \"persona\": \"Oliver is a 45-year-old environmental scientist in Glasgow. He works on sustainability initiatives and collaborates with various agencies. Oliver values clarity and conciseness, especially when conveying complex environmental issues to policymakers and the public.\"", "    }", "]"], "file_path": "scripts/personas.py"}
{"Link_to_commit": "https://github.com/daviddwlee84/RaspPi-Cluster/commit/558c8e2572b125d3bbb346f95ff70e4ba51b4e47", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 7, "n_files_impacted": 9, "longest_chunk": ["broker_url = 'redis://localhost:6379/0'", "result_backend = 'redis://localhost:6379/1'", "", "task_routes = {", "    'tasks.cpu_tasks.*': {'queue': 'cpu_queue'},", "    'tasks.io_tasks.*': {'queue': 'io_queue'},", "}"], "file_path": "Example/CeleryExample2/celery_config.py"}
{"Link_to_commit": "https://github.com/iamabdoulaziz/Project_with_chatgpt/commit/6f8c04217b93b0b4b5f37bdbd03366960bc0380f", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 132, "n_files_impacted": 1, "longest_chunk": ["import pygame", "import random", "", "# Initialisation de Pygame", "pygame.init()", "", "# Dimensions de la fen\u00eatre", "WIDTH, HEIGHT = 800, 600", "screen = pygame.display.set_mode((WIDTH, HEIGHT))", "pygame.display.set_caption(\"Jeu de Casse\")", "", "# Couleurs", "WHITE = (255, 255, 255)", "RED = (255, 0, 0)", "BLUE = (0, 0, 255)", "GREEN = (0, 255, 0)", "BLACK = (0, 0, 0)", "", "# Police", "font = pygame.font.Font(None, 36)", "", "", "# Fonction pour afficher un bouton (Rejouer)", "def draw_button():", "    pygame.draw.rect(screen, RED, (WIDTH // 2 - 75, HEIGHT // 2, 150, 50))", "    text = font.render(\"Rejouer\", True, WHITE)", "    screen.blit(text, (WIDTH // 2 - 40, HEIGHT // 2 + 10))", "", "", "# Fonction pour afficher un \u00e9cran de fin (victoire ou game over)", "def show_end_screen(message):", "    screen.fill(BLACK)", "    text = font.render(message, True, WHITE)", "    screen.blit(text, (WIDTH // 2 - 70, HEIGHT // 2 - 50))", "    draw_button()", "    pygame.display.update()", "", "    waiting = True", "    while waiting:", "        for event in pygame.event.get():", "            if event.type == pygame.QUIT:", "                pygame.quit()", "                return False", "            if event.type == pygame.MOUSEBUTTONDOWN:", "                x, y = pygame.mouse.get_pos()", "                if WIDTH // 2 - 75 <= x <= WIDTH // 2 + 75 and HEIGHT // 2 <= y <= HEIGHT // 2 + 50:", "                    return True", "    return False", "", "", "# Fonction principale du jeu", "def game_loop():", "    global ball_x, ball_y, ball_speed_x, ball_speed_y, bar_x, bricks", "", "    # R\u00e9initialisation des variables du jeu", "    ball_x, ball_y = WIDTH // 2, HEIGHT - 50", "    ball_speed_x, ball_speed_y = random.choice([-3, 3]), -3", "    bar_x = WIDTH // 2 - 50", "", "    # Cr\u00e9ation des briques", "    bricks = [pygame.Rect(col * 70 + 35, row * 25 + 40, 60, 20) for row in range(5) for col in range(10)]", "", "    clock = pygame.time.Clock()", "    run = True", "", "    while run:", "        clock.tick(60)", "", "        for event in pygame.event.get():", "            if event.type == pygame.QUIT:", "                pygame.quit()", "                return", "", "        # D\u00e9placement de la barre", "        keys = pygame.key.get_pressed()", "        if keys[pygame.K_LEFT] and bar_x > 0:", "            bar_x -= 6", "        if keys[pygame.K_RIGHT] and bar_x < WIDTH - 100:", "            bar_x += 6", "", "        # D\u00e9placement de la balle", "        ball_x += ball_speed_x", "        ball_y += ball_speed_y", "", "        # Collision avec les murs", "        if ball_x - 10 <= 0 or ball_x + 10 >= WIDTH:", "            ball_speed_x = -ball_speed_x", "        if ball_y - 10 <= 0:", "            ball_speed_y = -ball_speed_y", "        if ball_y + 10 >= HEIGHT:", "            run = False  # Game over si la balle touche le bas", "", "        # Collision avec la barre", "        bar_rect = pygame.Rect(bar_x, HEIGHT - 30, 100, 20)", "        ball_rect = pygame.Rect(ball_x - 10, ball_y - 10, 20, 20)", "        if ball_rect.colliderect(bar_rect):", "            ball_speed_y = -abs(ball_speed_y)", "            ball_speed_x += random.choice([-1, 1])  # Effet de variation du rebond", "", "        # Collision avec les briques", "        for brick in bricks[:]:", "            if ball_rect.colliderect(brick):", "                bricks.remove(brick)", "                ball_speed_y = -ball_speed_y", "                break", "", "        # V\u00e9rifier si toutes les briques sont d\u00e9truites", "        if not bricks:", "            run = False", "            victory = True  # Indique qu'on a gagn\u00e9", "", "        # Dessiner les \u00e9l\u00e9ments du jeu", "        screen.fill(BLACK)", "        pygame.draw.circle(screen, WHITE, (ball_x, ball_y), 10)", "        pygame.draw.rect(screen, BLUE, (bar_x, HEIGHT - 30, 100, 20))", "        for brick in bricks:", "            pygame.draw.rect(screen, GREEN, brick)", "", "        pygame.display.update()", "", "    # Affichage de l'\u00e9cran de fin (victoire ou game over)", "    if not bricks:", "        message = \"Victoire !\"", "    else:", "        message = \"Game Over\"", "", "    if show_end_screen(message):", "        game_loop()  # Relancer une partie", "", "", "# Lancer le jeu", "game_loop()"], "file_path": "PycharmProjects/Project_with_chatgpt/pong_pygame.py"}
{"Link_to_commit": "https://github.com/project-zeroui/packages_apps_Launcher3/commit/8979e91a025d125a40048cd64b1bf94522f891e6", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 23, "n_files_impacted": 1, "longest_chunk": ["    /**", "     * Returns animator that controls depth/blur of the background.", "     */", "private ObjectAnimator getBackgroundAnimator() {", "    boolean allowBlurringLauncher = mLauncher.getStateManager().getState() != OVERVIEW", "            && BlurUtils.supportsBlursOnWindows();", "", "    LaunchDepthController depthController = new LaunchDepthController(mLauncher);", "    ObjectAnimator backgroundRadiusAnim = ObjectAnimator.ofFloat(depthController.stateDepth,", "                    MULTI_PROPERTY_VALUE, BACKGROUND_APP.getDepth(mLauncher))", "            .setDuration(APP_LAUNCH_DURATION);", "", "    if (allowBlurringLauncher) {", "        View rootView = mLauncher.getDragLayer();", "", "        // Create a composite SurfaceControl layer for everything behind the app animation", "        ViewRootImpl viewRootImpl = rootView.getViewRootImpl();", "        SurfaceControl parentSurface = viewRootImpl != null ? viewRootImpl.getSurfaceControl() : null;", "", "        if (parentSurface != null) {", "            SurfaceControl blurLayer = new SurfaceControl.Builder()", "                    .setName(\"Blur Layer\")", "                    .setParent(parentSurface)"], "file_path": "quickstep/src/com/android/launcher3/QuickstepTransitionManager.java"}
{"Link_to_commit": "https://github.com/adwi592/GPT-EMG-Analyser/commit/4b8b62a5d8744d0b6db17e75e4c8a6cfe55a3dd7", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 80, "n_files_impacted": 1, "longest_chunk": ["import os", "import numpy as np", "import pandas as pd", "from sklearn.ensemble import RandomForestRegressor", "from scipy.stats import pearsonr", "", "def extract_features(data, window_length=0.2, window_slide=0.01, sampling_rate=1200):", "    features = []", "    labels = []", "    window_size = int(window_length * sampling_rate)", "    step_size = int(window_slide * sampling_rate)", "", "", "    for start in range(0, len(data) - window_size + 1, step_size):", "        end = start + window_size - 1", "        window = data.iloc[start:end, 1:17].values", "        label = data.iloc[end, 0]", "", "        rms = np.sqrt(np.mean(window**2, axis=0))", "        zc = np.sum(np.diff(np.sign(window), axis=0) != 0, axis=0)", "        wl = np.sum(np.abs(np.diff(window, axis=0)), axis=0)", "", "        features.append(np.hstack((rms, zc, wl)))", "        labels.append(label)", "", "    return np.array(features), np.array(labels)", "", "def calculate_nmse(x_r, x_p):", "    numerator = np.linalg.norm(x_r - x_p) ** 2", "    denominator = np.linalg.norm(x_r - np.mean(x_r)) ** 2", "    nmse = 100 * (1 - (numerator / denominator))", "    return nmse", "", "def load_data(batches):", "    data = []", "    for batch in batches:", "        files = sorted([file for file in os.listdir(batch) if file.endswith('.csv')])", "        for file in files:", "            batch_data = pd.read_csv(os.path.join(batch, file), header=0)", "            data.append(batch_data)", "    return data", "", "def main():", "    batches = ['Data/Batch1', 'Data/Batch2', 'Data/Batch3']", "    data_files = load_data(batches)", "", "    nmse_values = []", "    corr_values = []", "", "    for i in range(10):  # Assuming there are 10 folds (i.e., 10 data files for each motion)", "        train_data = [data for idx, data in enumerate(data_files) if ((idx != i) and (idx != (i+10)) and (idx != (i+20)))]", "        test_data = [data for idx, data in enumerate(data_files) if ((idx == i) or (idx == (i+10)) or (idx == (i+20)))]", "", "        train_features = np.vstack([extract_features(data)[0] for data in train_data])", "        train_labels = np.concatenate([extract_features(data)[1] for data in train_data])", "", "        test_features = np.vstack([extract_features(data)[0] for data in test_data])", "        test_labels = np.concatenate([extract_features(data)[1] for data in test_data])", "", "        model = RandomForestRegressor(n_estimators=100, random_state=0)", "        model.fit(train_features, train_labels)", "", "        predictions = model.predict(test_features)", "", "        nmse = calculate_nmse(test_labels, predictions)", "        corr = pearsonr(test_labels, predictions)[0] * 100  # Correlation in percentage", "", "        nmse_values.append(nmse)", "        corr_values.append(corr)", "", "        print(f\"Fold {i + 1}: Correlation = {corr:.2f}%, NMSE = {nmse:.2f}%\")", "", "    print(\"\\nAll Folds NMSE Values: \", nmse_values)", "    print(\"All Folds Correlation Values: \", corr_values)", "", "    print(f\"\\nMean Correlation: {np.mean(corr_values):.2f}%\")", "    print(f\"Mean NMSE: {np.mean(nmse_values):.2f}%\")", "", "if __name__ == \"__main__\":", "    main()"], "file_path": "src/GPT_code.py"}
{"Link_to_commit": "https://github.com/d-roman-halliday/ai-generated-xml-to-mysql-loader/commit/14471928be88bc86dba264bc503799b6ac31720f", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 147, "n_files_impacted": 1, "longest_chunk": ["import os", "import xml.etree.ElementTree as ET", "import mysql.connector", "", "# Directory containing XML files", "xml_directory = \"/path/to/your/xml/files\"", "", "# MySQL connection", "db = mysql.connector.connect(", "    host=\"localhost\",", "    user=\"your_username\",", "    password=\"your_password\",", "    database=\"your_database\"", ")", "cursor = db.cursor()", "", "# Create tables", "cursor.execute(\"\"\"", "CREATE TABLE IF NOT EXISTS Product (", "    id VARCHAR(255) PRIMARY KEY,", "    name VARCHAR(255),", "    version INT,", "    singlecontent BOOLEAN", ")", "\"\"\")", "", "cursor.execute(\"\"\"", "CREATE TABLE IF NOT EXISTS Volume (", "    id VARCHAR(255) PRIMARY KEY,", "    product_id VARCHAR(255),", "    name VARCHAR(255),", "    number INT,", "    sourcefiletype VARCHAR(50),", "    preview_discid VARCHAR(255),", "    preview_suffix VARCHAR(10),", "    preview_installdir VARCHAR(255),", "    preview_path_on_disc VARCHAR(255),", "    preview_thumbnail_suffix VARCHAR(10),", "    install_size_img INT,", "    install_size_img_mov INT,", "    total_preview_size INT,", "    total_source_size FLOAT,", "    base_j3_version VARCHAR(255),", "    FOREIGN KEY (product_id) REFERENCES Product(id)", ")", "\"\"\")", "", "cursor.execute(\"\"\"", "CREATE TABLE IF NOT EXISTS Disc (", "    id VARCHAR(255) PRIMARY KEY,", "    volume_id VARCHAR(255),", "    number INT,", "    FOREIGN KEY (volume_id) REFERENCES Volume(id)", ")", "\"\"\")", "", "cursor.execute(\"\"\"", "CREATE TABLE IF NOT EXISTS Content (", "    id VARCHAR(255) PRIMARY KEY,", "    disc_id VARCHAR(255),", "    type INT,", "    name VARCHAR(255),", "    originalfps INT,", "    frames INT,", "    description VARCHAR(255),", "    resolution VARCHAR(50),", "    resx INT,", "    resy INT,", "    base VARCHAR(255),", "    keywords TEXT,", "    FOREIGN KEY (disc_id) REFERENCES Disc(id)", ")", "\"\"\")", "", "# Function to process a single XML file", "def process_xml_file(xml_file):", "    tree = ET.parse(xml_file)", "    root = tree.getroot()", "", "    # Extract and insert Product", "    product = root.attrib", "    product_id = product[\"id\"]", "    cursor.execute(\"\"\"", "    INSERT INTO Product (id, name, version, singlecontent)", "    VALUES (%s, %s, %s, %s)", "    ON DUPLICATE KEY UPDATE name=VALUES(name), version=VALUES(version), singlecontent=VALUES(singlecontent)", "    \"\"\", (product[\"id\"], product[\"name\"], int(product[\"version\"]), product[\"singlecontent\"] == \"true\"))", "", "    # Extract and insert Volume", "    for volume in root.findall(\"volume\"):", "        volume_attrib = volume.attrib", "        cursor.execute(\"\"\"", "        INSERT INTO Volume (id, product_id, name, number, sourcefiletype, preview_discid, preview_suffix, preview_installdir,", "                            preview_path_on_disc, preview_thumbnail_suffix, install_size_img, install_size_img_mov,", "                            total_preview_size, total_source_size, base_j3_version)", "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)", "        ON DUPLICATE KEY UPDATE name=VALUES(name), number=VALUES(number), sourcefiletype=VALUES(sourcefiletype),", "                                preview_discid=VALUES(preview_discid), preview_suffix=VALUES(preview_suffix),", "                                preview_installdir=VALUES(preview_installdir), preview_path_on_disc=VALUES(preview_path_on_disc),", "                                preview_thumbnail_suffix=VALUES(preview_thumbnail_suffix), install_size_img=VALUES(install_size_img),", "                                install_size_img_mov=VALUES(install_size_img_mov), total_preview_size=VALUES(total_preview_size),", "                                total_source_size=VALUES(total_source_size), base_j3_version=VALUES(base_j3_version)", "        \"\"\", (volume_attrib[\"id\"], product_id, volume_attrib[\"name\"], int(volume_attrib[\"number\"]),", "              volume_attrib[\"sourcefiletype\"], volume_attrib[\"preview_discid\"], volume_attrib[\"preview_suffix\"],", "              volume_attrib[\"preview_install_dir\"], volume_attrib[\"preview_path_on_disc\"],", "              volume_attrib[\"preview_thumbnail_suffix\"], int(volume_attrib[\"install_size_img\"]),", "              int(volume_attrib[\"install_size_img_mov\"]), int(volume_attrib[\"totalPreviewSize\"]),", "              float(volume_attrib[\"totalSourceSize\"]), volume_attrib[\"baseJ3Version\"]))", "", "        # Extract and insert Disc", "        for disc in volume.findall(\"disc\"):", "            disc_attrib = disc.attrib", "            cursor.execute(\"\"\"", "            INSERT INTO Disc (id, volume_id, number)", "            VALUES (%s, %s, %s)", "            ON DUPLICATE KEY UPDATE volume_id=VALUES(volume_id), number=VALUES(number)", "            \"\"\", (disc_attrib[\"id\"], volume_attrib[\"id\"], int(disc_attrib[\"number\"])))", "", "            # Extract and insert Content", "            for content in disc.findall(\".//content\"):", "                content_attrib = content.attrib", "                keywords = content.find(\"keywords\").text if content.find(\"keywords\") is not None else \"\"", "                cursor.execute(\"\"\"", "                INSERT INTO Content (id, disc_id, type, name, originalfps, frames, description, resolution, resx, resy, base, keywords)", "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)", "                ON DUPLICATE KEY UPDATE type=VALUES(type), name=VALUES(name), originalfps=VALUES(originalfps), frames=VALUES(frames),", "                                        description=VALUES(description), resolution=VALUES(resolution), resx=VALUES(resx),", "                                        resy=VALUES(resy), base=VALUES(base), keywords=VALUES(keywords)", "                \"\"\", (content_attrib[\"id\"], disc_attrib[\"id\"], int(content_attrib[\"type\"]), content_attrib.get(\"name\", \"\"),", "                      int(content_attrib[\"originalfps\"]), int(content_attrib[\"frames\"]), content_attrib[\"description\"],", "                      content_attrib[\"resolution\"], int(content_attrib[\"resx\"]), int(content_attrib[\"resy\"]),", "                      content_attrib.get(\"base\", \"\"), keywords))", "", "# Iterate over all XML files in the directory", "for filename in os.listdir(xml_directory):", "    if filename.endswith(\".xml\"):", "        xml_path = os.path.join(xml_directory, filename)", "        print(f\"Processing file: {xml_path}\")", "        process_xml_file(xml_path)", "", "# Commit and close", "db.commit()", "cursor.close()", "db.close()", "", "print(\"All XML files processed successfully!\")", ""], "file_path": "multi_file_xml_loader.py"}
{"Link_to_commit": "https://github.com/darkroastjava/contracts-gitops/commit/1ceca1ae7e3169cec12a1f736722fdcb0f4e1d7e", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 60, "n_files_impacted": 10, "longest_chunk": ["import os", "import tempfile", "from unittest import TestCase", "", "from scripts.generate_pdf import generate_pdf", "", "class GeneratePDFTest(TestCase):", "    def setUp(self):", "        self.templateFile = tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".md\")", "        self.templateFile.close()", "        self.dataFile = tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".yaml\")", "        self.dataFile.close()", "        self.outputFile = tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".pdf\")", "        self.outputFile.close()", "", "    def tearDown(self):", "        os.unlink(self.templateFile.name)", "        os.unlink(self.dataFile.name)", "        os.unlink(self.outputFile.name)", "", "    def test_generate_pdf(self):", "        # arrange", "        with open(self.templateFile.name, \"w\") as f:", "            f.write(\"\"\"", "# Liefervertrag", "", "Dieser Vertrag wird geschlossen zwischen:", "", "- **Name des Lieferanten:** {{ supplier_name }}", "- **Adresse des Lieferanten:** {{ supplier_address }}", "", "## Vertragsdetails:", "", "- **Startdatum:** {{ contract_date }}", "- **Laufzeit:** {{ contract_duration }}", "", "## Bedingungen:", "", "Die genauen Vertragsbedingungen finden Sie in den beigef\u00fcgten Dokumenten.", "\"\"\")", "        ", "        with open(self.dataFile.name, \"w\") as f:", "            f.write(\"\"\"", "template: liefervertrag.md", "supplier_name: Supplier A", "supplier_address: Musterstra\u00dfe 123, 12345 Musterstadt", "contract_duration: 12 Monate", "contract_date: 2025-01-13", "amount: 200000  # Betrag f\u00fcr die Schwellenpr\u00fcfung", "\"\"\")", "", "        # act", "        generate_pdf(template_path=self.templateFile.name, data_path=self.dataFile.name, output_path=self.outputFile.name)", "", "        # assert", "        with open(self.outputFile.name, \"rb\") as f:", "            file_content = f.read()", "        self.assertIn(b'GarW5bAP0N&4Q>@`CS\\/W)', file_content)", "        self.assertIn(b',P^MUWZ&Fp6]0FiDAYM<gHEj@>@-qMle6)aK_e4EGq!c%/aP?,', file_content)", "        self.assertIn(b'(4WAceQ13Ud)<\"2#OR+;XGj#.2_&-t`tR#m8Sln/YeR;Eo^,\"HhRDb7<kq=f5CJ`<6]-=;Yl$IJ61AZaiA_', file_content)"], "file_path": "test/generate_pdf_test.py"}
{"Link_to_commit": "https://github.com/plantinformatics/pretzel-data/commit/7539fc3bb470e91eb67369932b93e57fec173a82", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 65, "n_files_impacted": 1, "longest_chunk": ["# from", "# https://chatgpt.com/share/67b5b1ca-c6c4-800e-bb28-3f7aa8dd4157", "# https://chatgpt.com/canvas/shared/67b5ab81f6e08191962282b1d9d6c5e3", "", "import sys", "import pandas as pd", "from openpyxl import load_workbook", "from openpyxl.styles import Alignment, PatternFill", "", "def format_excel(output_file, df):", "    wb = load_workbook(output_file)", "    ws = wb.active", "    ", "    # Identify columns between 'alt' and 'index' (exclusive), excluding 'TYPE'", "    if 'alt' in df.columns and 'index' in df.columns:", "        start_col = df.columns.get_loc('alt') + 1", "        end_col = df.columns.get_loc('index')", "        columns_to_rotate = [df.columns[i] for i in range(start_col, end_col) if df.columns[i] != 'TYPE']", "    else:", "        columns_to_rotate = []", "    ", "    # Formatting rules", "    rotation_angle = 90", "    narrow_column_width = 4", "    color_mapping = {", "        0: \"B1C1E8\",", "        2: \"FFC8AE\"", "    }", "    ", "    # Apply formatting", "    for col_idx, col_name in enumerate(df.columns, start=1):", "        cell = ws.cell(row=1, column=col_idx)", "        if col_name in columns_to_rotate:", "            cell.alignment = Alignment(textRotation=rotation_angle, horizontal=\"center\", vertical=\"center\")", "            ws.column_dimensions[cell.column_letter].width = narrow_column_width", "    ", "    # Apply cell colors based on values in specified columns", "    for row_idx, row in enumerate(ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=start_col + 1, max_col=end_col), start=2):", "        for cell in row:", "            if isinstance(cell.value, (int, float)) and cell.value in color_mapping:", "                cell.fill = PatternFill(start_color=color_mapping[cell.value], end_color=color_mapping[cell.value], fill_type=\"solid\")", "    ", "    wb.save(output_file)", "", "def main():", "    if len(sys.argv) != 3:", "        print(\"Usage: python script.py <input_CSV_file_name> <output_XLSX_file_name>\")", "        sys.exit(1)", "    ", "    input_csv = sys.argv[1]", "    output_xlsx = sys.argv[2]", "    ", "    # Read CSV file", "    df = pd.read_csv(input_csv)", "    ", "    # Write to Excel", "    df.to_excel(output_xlsx, index=False, engine='openpyxl')", "    ", "    # Apply formatting", "    format_excel(output_xlsx, df)", "    ", "    print(f\"Formatted Excel file saved as {output_xlsx}\")", "", "if __name__ == \"__main__\":", "    main()"], "file_path": "examples/ijms-25-08614/featureTableWithGT.py"}
{"Link_to_commit": "https://github.com/zhenchai00/dcoms-tutorial/commit/f84f8ee876e65f6c392f09f9613c292387b85d95", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 30, "n_files_impacted": 6, "longest_chunk": ["import java.rmi.RemoteException;", "import java.rmi.server.UnicastRemoteObject;", "import java.util.ArrayList;", "import java.util.List;", "", "public class BookStoreImpl extends UnicastRemoteObject implements BookStore {", "    private List<String> books;", "", "    public BookStoreImpl() throws RemoteException {", "        super();", "        books = new ArrayList<>();", "    }", "", "    @Override", "    public List<String> getBooks() throws RemoteException {", "        return books;", "    }", "", "    @Override", "    public String getBookDetails(String title) throws RemoteException {", "        return \"Book: \" + title + \", Author: \" + \"Unknown, \" + \" Price: \" + 0.0;", "    }", "", "    @Override", "    public boolean addBook(String title, String author, Double price) throws RemoteException {", "        books.add(title);", "        return true;", "    }", "", "}"], "file_path": "SimpleRMIJava/src/BookStoreServer.java"}
{"Link_to_commit": "https://github.com/chrimuel/python_life_project/commit/db457ccb592d34e07591bf1d193174719374af1a", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 89, "n_files_impacted": 1, "longest_chunk": ["# Step 1: Import Necessary Libraries", "import tkinter as tk", "import random", "import time", "", "# Step 2: Initialize Variables", "window_size = 20", "rain_rate = 0.1", "drop_increase = 5", "reset_size = 50", "", "# Tkinter window and canvas size", "canvas_size = 1000", "", "# Step 3: Create a Tkinter Window and Canvas", "", "# Initialize Tkinter window", "root = tk.Tk()", "root.title(\"Rain Simulation\")", "", "# Create canvas", "canvas = tk.Canvas(root, width=canvas_size, height=canvas_size, bg=\"lightblue\")", "canvas.pack()", "", "# Step 4: Create a Grid", "", "# Draw grid lines", "cell_size = canvas_size // window_size", "", "for i in range(window_size + 1):", "    # Vertical lines", "    x = i * cell_size", "    canvas.create_line(x, 0, x, canvas_size, fill=\"gray\")", "    # Horizontal lines", "    y = i * cell_size", "    canvas.create_line(0, y, canvas_size, y, fill=\"gray\")", "", "", "# Step 5: Represent Drops of Water", "", "# Initialize drops of water with starting size", "drops = [[0 for _ in range(window_size)] for _ in range(window_size)]", "", "# Draw drops on the canvas", "circles = [", "    [", "        canvas.create_oval(", "            j * cell_size + cell_size // 4, i * cell_size + cell_size // 4,", "            j * cell_size + 3 * cell_size // 4, i * cell_size + 3 * cell_size // 4,", "            fill=\"blue\"", "        )", "        for j in range(window_size)", "    ]", "    for i in range(window_size)", "]", "", "# Step 6: Simulate Rain", "", "def update_drops():", "    for i in range(window_size):", "        for j in range(window_size):", "            # Determine if this drop grows", "            if random.random() < rain_rate:", "                drops[i][j] += drop_increase", "            ", "            # Check if the drop exceeds the reset size", "            if drops[i][j] > reset_size:", "                drops[i][j] = 0  # Reset the drop", "                # Reset all drops below", "                for k in range(i, window_size):", "                    drops[k][j] = 0", "            ", "            # Update the drop size on the canvas", "            size = drops[i][j]", "            canvas.coords(", "                circles[i][j],", "                j * cell_size + cell_size // 2 - size // 2,", "                i * cell_size + cell_size // 2 - size // 2,", "                j * cell_size + cell_size // 2 + size // 2,", "                i * cell_size + cell_size // 2 + size // 2", "            )", "", "    # Schedule the next update", "    root.after(50, update_drops)", "", "", "# Step 7: Run the Simulation", "update_drops()", "root.mainloop()"], "file_path": "03_rain/rain.py"}
{"Link_to_commit": "https://github.com/arianetemadi/snack-overflow/commit/38fdcc5a8604d41033145e0b4cedf39a046764f8", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 67, "n_files_impacted": 5, "longest_chunk": ["{", " \"cells\": [", "  {", "   \"cell_type\": \"code\",", "   \"execution_count\": 1,", "   \"metadata\": {},", "   \"outputs\": [],", "   \"source\": [", "    \"import spacy\\n\",", "    \"import os\\n\",", "    \"import pandas as pd\\n\",", "    \"\\n\",", "    \"from src.preprocessing import convert_txt_to_json, convert_to_conllu\"", "   ]", "  },", "  {", "   \"cell_type\": \"code\",", "   \"execution_count\": null,", "   \"metadata\": {},", "   \"outputs\": [", "    {", "     \"name\": \"stderr\",", "     \"output_type\": \"stream\",", "     \"text\": [", "      \"100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:01<00:00, 117.78it/s]\\n\"", "     ]", "    }", "   ],", "   \"source\": [", "    \"# convert txt file to JSON file\\n\",", "    \"input_file = \\\"../data/chatgpt_dataset.txt\\\"\\n\",", "    \"output_file = \\\"../data/chatgpt_dataset.json\\\"\\n\",", "    \"convert_txt_to_json(\\n\",", "    \"    input_file=input_file, output_file=output_file, link=\\\"https://chatgpt.com/\\\"\\n\",", "    \")\\n\",", "    \"\\n\",", "    \"# convert JSON file to conllu dataset\\n\",", "    \"output_file = os.path.join(\\\"../data\\\", \\\"chatgpt_dataset.conllu\\\")\\n\",", "    \"nlp = spacy.load(\\\"en_core_web_sm\\\")\\n\",", "    \"file_path = \\\"../data/chatgpt_dataset.json\\\"\\n\",", "    \"data = pd.read_json(file_path, lines=True)\\n\",", "    \"convert_to_conllu(data, output_file, nlp)\"", "   ]", "  }", " ],", " \"metadata\": {", "  \"kernelspec\": {", "   \"display_name\": \"venv\",", "   \"language\": \"python\",", "   \"name\": \"python3\"", "  },", "  \"language_info\": {", "   \"codemirror_mode\": {", "    \"name\": \"ipython\",", "    \"version\": 3", "   },", "   \"file_extension\": \".py\",", "   \"mimetype\": \"text/x-python\",", "   \"name\": \"python\",", "   \"nbconvert_exporter\": \"python\",", "   \"pygments_lexer\": \"ipython3\",", "   \"version\": \"3.10.12\"", "  }", " },", " \"nbformat\": 4,", " \"nbformat_minor\": 2", "}"], "file_path": "src/preprocessing.py"}
{"Link_to_commit": "https://github.com/NVIDIA/cuda-python/commit/938c9e9db569a0e41a0663194b69c420e70f63c9", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 126, "n_files_impacted": 6, "longest_chunk": ["# Copyright 2025 NVIDIA Corporation.  All rights reserved.", "# SPDX-License-Identifier: LicenseRef-NVIDIA-SOFTWARE-LICENSE", "", "import multiprocessing", "import queue  # for Empty", "import sys", "import traceback", "from dataclasses import dataclass", "from io import StringIO", "from typing import Any, Callable, Optional, Sequence", "", "PROCESS_KILLED = -9", "PROCESS_NO_RESULT = -999", "", "", "# Similar to https://docs.python.org/3/library/subprocess.html#subprocess.CompletedProcess", "# (args, check_returncode() are intentionally not supported here.)", "@dataclass", "class CompletedProcess:", "    returncode: int", "    stdout: str", "    stderr: str", "", "", "class ChildProcessWrapper:", "    def __init__(self, result_queue, target, args, kwargs):", "        self.target = target", "        self.args = () if args is None else args", "        self.kwargs = {} if kwargs is None else kwargs", "        self.result_queue = result_queue", "", "    def __call__(self):", "        # Capture stdout/stderr", "        old_stdout = sys.stdout", "        old_stderr = sys.stderr", "        sys.stdout = StringIO()", "        sys.stderr = StringIO()", "", "        try:", "            self.target(*self.args, **self.kwargs)", "            returncode = 0", "        except SystemExit as e:  # Handle sys.exit()", "            returncode = e.code if isinstance(e.code, int) else 0", "        except BaseException:", "            traceback.print_exc()", "            returncode = 1", "        finally:", "            # Collect outputs and restore streams", "            stdout = sys.stdout.getvalue()", "            stderr = sys.stderr.getvalue()", "            sys.stdout = old_stdout", "            sys.stderr = old_stderr", "            try:  # noqa: SIM105", "                self.result_queue.put((returncode, stdout, stderr))", "            except Exception:  # nosec B110", "                # If the queue is broken (e.g., parent gone), best effort logging", "                pass", "", "", "def run_in_spawned_child_process(", "    target: Callable[..., None],", "    *,", "    args: Optional[Sequence[Any]] = None,", "    kwargs: Optional[dict[str, Any]] = None,", "    timeout: Optional[float] = None,", "    rethrow: bool = False,", ") -> CompletedProcess:", "    \"\"\"Run `target` in a spawned child process, capturing stdout/stderr.", "", "    The provided `target` must be defined at the top level of a module, and must", "    be importable in the spawned child process. Lambdas, closures, or interactively", "    defined functions (e.g., in Jupyter notebooks) will not work.", "", "    If `rethrow=True` and the child process exits with a nonzero code,", "    raises ChildProcessError with the captured stderr.", "    \"\"\"", "    ctx = multiprocessing.get_context(\"spawn\")", "    result_queue = ctx.Queue()", "    process = ctx.Process(target=ChildProcessWrapper(result_queue, target, args, kwargs))", "    process.start()", "", "    try:", "        process.join(timeout)", "        if process.is_alive():", "            process.terminate()", "            process.join()", "            result = CompletedProcess(", "                returncode=PROCESS_KILLED,", "                stdout=\"\",", "                stderr=f\"Process timed out after {timeout} seconds and was terminated.\",", "            )", "        else:", "            try:", "                returncode, stdout, stderr = result_queue.get(timeout=1.0)", "            except (queue.Empty, EOFError):", "                result = CompletedProcess(", "                    returncode=PROCESS_NO_RESULT,", "                    stdout=\"\",", "                    stderr=\"Process exited or crashed before returning results.\",", "                )", "            else:", "                result = CompletedProcess(", "                    returncode=returncode,", "                    stdout=stdout,", "                    stderr=stderr,", "                )", "", "        if rethrow and result.returncode != 0:", "            raise ChildProcessError(", "                f\"Child process exited with code {result.returncode}.\\n\"", "                \"--- stderr-from-child-process ---\\n\"", "                f\"{result.stderr}\"", "                \"<end-of-stderr-from-child-process>\\n\"", "            )", "", "        return result", "", "    finally:", "        try:", "            result_queue.close()", "            result_queue.join_thread()", "        except Exception:  # nosec B110", "            pass", "        if process.is_alive():", "            process.kill()", "            process.join()"], "file_path": "cuda_bindings/tests/test_path_finder_load.py"}
{"Link_to_commit": "https://github.com/jefftranter/Z80/commit/837365cbca4c1258978acf1cec0bb24f2f35710a", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 69, "n_files_impacted": 2, "longest_chunk": ["#include <stdio.h>", "#include <stdlib.h>", "#include <ctype.h>", "", "#define BYTES_PER_LINE 16", "", "// Function to print a byte in hexadecimal format", "void print_hex(unsigned char byte) {", "    printf(\"%02x \", byte);", "}", "", "// Function to print a byte in ASCII format, or a dot if not printable", "void print_ascii(unsigned char byte) {", "    if (isprint(byte)) {", "        printf(\"%c\", byte);", "    } else {", "        printf(\".\");", "    }", "}", "", "void dump_file(const char *filename) {", "    FILE *file = fopen(filename, \"rb\"); // Open the file in binary mode", "    if (file == NULL) {", "        perror(\"Failed to open file\");", "        return;", "    }", "", "    unsigned char buffer[BYTES_PER_LINE];", "    size_t bytes_read;", "    size_t offset = 0;", "", "    // Read and process the file in chunks of 16 bytes", "    while ((bytes_read = fread(buffer, 1, BYTES_PER_LINE, file)) > 0) {", "        // Print the offset (address) in the first column", "        printf(\"%08lx  \", offset);", "", "        // Print the hexadecimal representation", "        for (size_t i = 0; i < bytes_read; i++) {", "            print_hex(buffer[i]);", "        }", "", "        // Fill in the rest of the hex section with spaces if less than 16 bytes", "        for (size_t i = bytes_read; i < BYTES_PER_LINE; i++) {", "            printf(\"   \");", "        }", "", "        // Print the ASCII representation", "        printf(\" |\");", "        for (size_t i = 0; i < bytes_read; i++) {", "            print_ascii(buffer[i]);", "        }", "", "        printf(\"|\\n\");", "", "        offset += bytes_read;", "    }", "", "    fclose(file);", "}", "", "int main(int argc, char *argv[]) {", "    if (argc != 2) {", "        fprintf(stderr, \"Usage: %s <filename>\\n\", argv[0]);", "        return 1;", "    }", "", "    dump_file(argv[1]);", "    return 0;", "}"], "file_path": "CPM/dump.c"}
{"Link_to_commit": "https://github.com/Elements2720/sumo/commit/05c50978ca479bfb00d50f497790f0f0036d5d37", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 40, "n_files_impacted": 1, "longest_chunk": ["void main() {", "", "    TRISD = 0x00; // Output for motors", "    TRISC = 0b10000000; // RC7 input (RX), RC6 output (TX)", "    UART1_Init(9600);", "    Delay_ms(100);", "", "    while (1) {", "        if (UART1_Data_Ready()) {", "            data0 = UART1_Read();", "", "            switch (data0) {", "            case 'F':", "                forward();", "                break;", "            case 'B':", "                backward();", "                break;", "            case 'L':", "                left();", "                break;", "            case 'R':", "                right();", "                break;", "            case 'S':", "                stop();", "                break;", "            case 'O':", "                servo_pulse(0);", "                break; // 0\u00b0", "            case 'H':", "                servo_pulse(1000);", "                break; // 90\u00b0", "            case 'Z':", "                servo_pulse(2000);", "                break; // 180\u00b0", "            }", "        }", "    }", "}"], "file_path": "bluetooth.c"}
{"Link_to_commit": "https://github.com/piJoe/aim-trainer-threejs/commit/0df032a4be82fa96d468a1cc8f1f6ba308e1e432", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 6, "n_files_impacted": 9, "longest_chunk": ["      const target = targetsHit[0].target;", "", "      // handle onhit in lua, then in the base target", "      this.handlers?.handleTargetHit(target.id);", "      target.onHit();", ""], "file_path": "src/game.ts"}
{"Link_to_commit": "https://github.com/rkCodeBase/regexprojekt/commit/6a8a6be6b74c65b73fa4f3f2ba92ebc6bbe22828", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 34, "n_files_impacted": 9, "longest_chunk": ["// __tests__/textUtils.test.ts", "import { findCprNumbers } from '@/textUtils';", "", "describe('findCprNumbers', () => {", "  it('should find valid CPR numbers with hyphens', () => {", "    const text = 'Valid CPR: 010203-1234 and 040506-5678.';", "    const result = findCprNumbers(text);", "    expect(result).toEqual(['010203-1234', '040506-5678']);", "  });", "", "  it('should find valid CPR numbers without hyphens', () => {", "    const text = 'Valid CPR: 0102031234 and 0405065678.';", "    const result = findCprNumbers(text);", "    expect(result).toEqual(['0102031234', '0405065678']);", "  });", "", "  it('should not find invalid CPR numbers', () => {", "    const text = 'Invalid CPR: 320199-1234 and 31 02 31 1234.';", "    const result = findCprNumbers(text);", "    expect(result).toEqual([]);", "  });", "", "  it('should return an empty array if no CPR numbers are found', () => {", "    const text = 'No CPR numbers here.';", "    const result = findCprNumbers(text);", "    expect(result).toEqual([]);", "  });", "", "  it('should handle an empty string gracefully', () => {", "    const text = '';", "    const result = findCprNumbers(text);", "    expect(result).toEqual([]);", "  });", "});"], "file_path": "find_cpr_number_in_text/app/find-cpr-number/route.ts"}
{"Link_to_commit": "https://github.com/FrankLi123/chatgpt-o3-game-demo/commit/9a44677af6930d7010730cd9c1b3b513feedabd0", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 200, "n_files_impacted": 1, "longest_chunk": ["import pygame", "import random", "", "# Initialize Pygame", "pygame.init()", "", "# Set up the screen and clock", "WIDTH, HEIGHT = 600, 400", "screen = pygame.display.set_mode((WIDTH, HEIGHT))", "pygame.display.set_caption('Snake Game')", "clock = pygame.time.Clock()", "", "# Colors", "GREEN = (0, 255, 0)", "BLUE = (0, 0, 255)", "BLACK = (0, 0, 0)", "RED = (255, 0, 0)", "WHITE = (255, 255, 255)", "", "# Snake settings", "BLOCK_SIZE = 20", "", "# Directions", "UP = (0, -BLOCK_SIZE)", "DOWN = (0, BLOCK_SIZE)", "LEFT = (-BLOCK_SIZE, 0)", "RIGHT = (BLOCK_SIZE, 0)", "", "# Font for displaying score and game over message", "font = pygame.font.SysFont(None, 35)", "", "# Function to display score", "def display_score(player_score, ai_score):", "    score_text = font.render(f\"Player: {player_score}  AI: {ai_score}\", True, WHITE)", "    screen.blit(score_text, [10, 10])", "", "# Function to display the game over message", "def display_game_over(winner, message):", "    game_over_text = font.render(f\"Game Over! {winner} Wins!\", True, WHITE)", "    explanation_text = font.render(message, True, WHITE)", "    screen.blit(game_over_text, [WIDTH // 4, HEIGHT // 2])", "    screen.blit(explanation_text, [WIDTH // 4, HEIGHT // 2 + 30])", "", "# Snake class", "class Snake:", "    def __init__(self, color, start_pos, direction):", "        self.color = color", "        self.body = [start_pos]", "        self.direction = direction", "        self.score = 0", "    ", "    def move(self):", "        head_x, head_y = self.body[0]", "        dir_x, dir_y = self.direction", "        new_head = (head_x + dir_x) % WIDTH, (head_y + dir_y) % HEIGHT", "        self.body = [new_head] + self.body[:-1]", "    ", "    def grow(self):", "        self.body.append(self.body[-1])", "    ", "    def check_collision(self):", "        head = self.body[0]", "        # Check if head collides with its own body", "        if head in self.body[1:]:", "            return True", "        return False", "    ", "    def check_collision_with_other_snake(self, other_snake):", "        head = self.body[0]", "        # Check if head collides with the other snake's body", "        if head in other_snake.body[1:]:", "            return True", "        return False", "    ", "    def draw(self, screen):", "        for segment in self.body:", "            pygame.draw.rect(screen, self.color, pygame.Rect(segment[0], segment[1], BLOCK_SIZE, BLOCK_SIZE))", "", "# AI Snake class", "class AISnake(Snake):", "    def __init__(self, color, start_pos, direction):", "        super().__init__(color, start_pos, direction)", "    ", "    def move_towards_food(self, food_pos):", "        head_x, head_y = self.body[0]", "        food_x, food_y = food_pos", "", "        # AI logic to move towards food, avoiding boundaries", "        if food_x > head_x:", "            self.direction = RIGHT", "        elif food_x < head_x:", "            self.direction = LEFT", "        if food_y > head_y:", "            self.direction = DOWN", "        elif food_y < head_y:", "            self.direction = UP", "", "    def update(self, food_pos):", "        self.move_towards_food(food_pos)", "        self.move()", "", "# Food class", "class Food:", "    def __init__(self):", "        self.position = (random.randint(0, (WIDTH - BLOCK_SIZE) // BLOCK_SIZE) * BLOCK_SIZE,", "                         random.randint(0, (HEIGHT - BLOCK_SIZE) // BLOCK_SIZE) * BLOCK_SIZE)", "    ", "    def spawn(self):", "        self.position = (random.randint(0, (WIDTH - BLOCK_SIZE) // BLOCK_SIZE) * BLOCK_SIZE,", "                         random.randint(0, (HEIGHT - BLOCK_SIZE) // BLOCK_SIZE) * BLOCK_SIZE)", "    ", "    def draw(self, screen):", "        pygame.draw.rect(screen, RED, pygame.Rect(self.position[0], self.position[1], BLOCK_SIZE, BLOCK_SIZE))", "", "# Game loop", "def game_loop():", "    player = Snake(GREEN, (100, 100), RIGHT)", "    ai = AISnake(BLUE, (300, 300), LEFT)", "    food = Food()", "    ", "    running = True", "    while running:", "        screen.fill(BLACK)", "", "        # Handle events", "        for event in pygame.event.get():", "            if event.type == pygame.QUIT:", "                running = False", "            elif event.type == pygame.KEYDOWN:", "                # Change direction immediately without waiting for snake movement", "                if event.key == pygame.K_UP and player.direction != DOWN:", "                    player.direction = UP", "                elif event.key == pygame.K_DOWN and player.direction != UP:", "                    player.direction = DOWN", "                elif event.key == pygame.K_LEFT and player.direction != RIGHT:", "                    player.direction = LEFT", "                elif event.key == pygame.K_RIGHT and player.direction != LEFT:", "                    player.direction = RIGHT", "        ", "        # Move the snakes", "        player.move()", "        ai.update(food.position)  # Update AI with new direction and move", "        ", "        # Check for collisions", "        if player.check_collision():", "            winner = \"AI\"", "            message = \"Player collided with their own body.\"", "            display_game_over(winner, message)", "            pygame.display.flip()", "            pygame.time.wait(2000)  # Wait for 2 seconds to display the game over message", "            running = False", "        elif ai.check_collision():", "            winner = \"Player\"", "            message = \"AI collided with its own body.\"", "            display_game_over(winner, message)", "            pygame.display.flip()", "            pygame.time.wait(2000)  # Wait for 2 seconds to display the game over message", "            running = False", "        ", "        # Check if the player or AI hits the other snake's body", "        elif player.check_collision_with_other_snake(ai):", "            winner = \"AI\"", "            message = \"Player collided with AI's body.\"", "            display_game_over(winner, message)", "            pygame.display.flip()", "            pygame.time.wait(2000)  # Wait for 2 seconds to display the game over message", "            running = False", "        elif ai.check_collision_with_other_snake(player):", "            winner = \"Player\"", "            message = \"AI collided with Player's body.\"", "            display_game_over(winner, message)", "            pygame.display.flip()", "            pygame.time.wait(2000)  # Wait for 2 seconds to display the game over message", "            running = False", "        ", "        # Check if the player or AI eats the food", "        if player.body[0] == food.position:", "            player.grow()", "            player.score += 1", "            food.spawn()", "        if ai.body[0] == food.position:", "            ai.grow()", "            ai.score += 1", "            food.spawn()", "        ", "        # Draw everything", "        player.draw(screen)", "        ai.draw(screen)", "        food.draw(screen)", "        display_score(player.score, ai.score)", "        ", "        # Update the display", "        pygame.display.flip()", "        clock.tick(10)  # Slow game speed for a relaxed experience", "    ", "    # End of game", "    pygame.quit()", "", "# Run the game", "game_loop()"], "file_path": "main.py"}
{"Link_to_commit": "https://github.com/blainefreestone/fpga_neural_network_accelerator/commit/af22eb7c24c61fd2abdb662130f7fe38ceea77c3", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 33, "n_files_impacted": 1, "longest_chunk": ["import argparse", "", "def fixed_point_to_decimal(binary_str, int_bits, frac_bits, signed):", "    total_bits = int_bits + frac_bits", "    ", "    if len(binary_str) != total_bits:", "        raise ValueError(f\"Binary string length ({len(binary_str)}) must match the total bit length ({total_bits}).\")", "    ", "    if signed and binary_str[0] == '1':", "        # Convert to two's complement for signed numbers", "        int_value = -((1 << total_bits) - int(binary_str, 2))", "    else:", "        int_value = int(binary_str, 2)", "    ", "    return int_value / (1 << frac_bits)", "", "def main():", "    parser = argparse.ArgumentParser(description=\"Convert fixed-point binary to decimal.\")", "    parser.add_argument(\"value\", type=str, help=\"Binary string to convert\")", "    parser.add_argument(\"int_bits\", type=int, help=\"Number of integer bits\")", "    parser.add_argument(\"frac_bits\", type=int, help=\"Number of fractional bits\")", "    parser.add_argument(\"--signed\", action=\"store_true\", help=\"Specify if the number is signed\")", "    ", "    args = parser.parse_args()", "    ", "    try:", "        result = fixed_point_to_decimal(args.value, args.int_bits, args.frac_bits, args.signed)", "        print(f\"Decimal value: {result}\")", "    except ValueError as e:", "        print(f\"Error: {e}\")", "", "if __name__ == \"__main__\":", "    main()"], "file_path": "scripts/fixed_to_decimal.py"}
{"Link_to_commit": "https://github.com/blainefreestone/fpga_neural_network_accelerator/commit/39bb868a517f1443b0dc3ffacc731da45a929134", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 64, "n_files_impacted": 1, "longest_chunk": ["import argparse", "", "def decimal_to_fixed_point(value, int_bits, frac_bits, signed=True):", "    \"\"\"", "    Converts a decimal number into a fixed-point binary representation.", "    ", "    :param value: The decimal number to convert.", "    :param int_bits: The number of bits for the integer part.", "    :param frac_bits: The number of bits for the fractional part.", "    :param signed: Whether to include a sign bit (default: True).", "    :return: The fixed-point binary string representation and the precision error.", "    \"\"\"", "    ", "    # Determine the range limits", "    if signed:", "        max_value = (2 ** (int_bits - 1)) - (1 / (2 ** frac_bits))", "        min_value = -(2 ** (int_bits - 1))", "    else:", "        max_value = (2 ** int_bits) - (1 / (2 ** frac_bits))", "        min_value = 0", "    ", "    # Check if value is within range", "    if value < min_value or value > max_value:", "        raise ValueError(f\"Value {value} out of range [{min_value}, {max_value}]\")", "    ", "    # Scale the value by 2^frac_bits to shift decimal part into integer range", "    scaled_value = round(value * (2 ** frac_bits))", "    ", "    # Calculate total bits", "    total_bits = int_bits + frac_bits", "    ", "    # Handle two's complement for negative values if signed", "    if signed and value < 0:", "        scaled_value = (1 << total_bits) + scaled_value  # Apply two's complement", "    ", "    # Convert to binary string with fixed width", "    binary_str = format(scaled_value, f'0{total_bits}b')", "    ", "    # Insert the binary point at the correct position", "    fixed_point_str = binary_str[:int_bits] + '.' + binary_str[int_bits:]", "    ", "    # Compute the actual value represented by the fixed-point binary string", "    actual_value = scaled_value / (2 ** frac_bits)", "    ", "    return fixed_point_str, actual_value", "", "def main():", "    parser = argparse.ArgumentParser(description=\"Convert a decimal number to fixed-point binary representation.\")", "    parser.add_argument(\"value\", type=float, help=\"Decimal number to convert\")", "    parser.add_argument(\"int_bits\", type=int, help=\"Number of bits for the integer part\")", "    parser.add_argument(\"frac_bits\", type=int, help=\"Number of bits for the fractional part\")", "    parser.add_argument(\"--signed\", action=\"store_true\", help=\"Include a sign bit (default: False)\")", "    ", "    args = parser.parse_args()", "    ", "    try:", "        binary_repr, actual_value = decimal_to_fixed_point(args.value, args.int_bits, args.frac_bits, args.signed)", "        print(f\"Fixed-point representation: {binary_repr}\")", "        print(f\"Actual value: {actual_value}\")", "    except ValueError as e:", "        print(f\"Error: {e}\")", "", "if __name__ == \"__main__\":", "    main()"], "file_path": "scripts/decimal_to_fixed.py"}
{"Link_to_commit": "https://github.com/figarocorso/stremio-sagas/commit/107a6aaf172e1eda539131b6b59d88ed8d475389", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 360, "n_files_impacted": 1, "longest_chunk": ["            \"id\": \"tt0117603\",", "            \"name\": \"Lethal Weapon 4\",", "        },", "    ],", "    \"Mad Max\": [", "        {", "            \"id\": \"tt0079501\",", "            \"name\": \"Mad Max\",", "        },", "        {", "            \"id\": \"tt0082694\",", "            \"name\": \"Mad Max 2: The Road Warrior\",", "        },", "        {", "            \"id\": \"tt0089530\",", "            \"name\": \"Mad Max Beyond Thunderdome\",", "        },", "        {", "            \"id\": \"tt1392190\",", "            \"name\": \"Mad Max: Fury Road\",", "        },", "    ],", "    \"Marvel Cinematic Universe\": [", "        {", "            \"id\": \"tt0371746\",", "            \"name\": \"Iron Man\",", "        },", "        {", "            \"id\": \"tt0800080\",", "            \"name\": \"The Incredible Hulk\",", "        },", "        {", "            \"id\": \"tt1228705\",", "            \"name\": \"Iron Man 2\",", "        },", "        {", "            \"id\": \"tt0800369\",", "            \"name\": \"Thor\",", "        },", "        {", "            \"id\": \"tt0458339\",", "            \"name\": \"Captain America: The First Avenger\",", "        },", "        {", "            \"id\": \"tt0848228\",", "            \"name\": \"The Avengers\",", "        },", "        {", "            \"id\": \"tt1300854\",", "            \"name\": \"Iron Man 3\",", "        },", "        {", "            \"id\": \"tt1981115\",", "            \"name\": \"Thor: The Dark World\",", "        },", "        {", "            \"id\": \"tt1843866\",", "            \"name\": \"Captain America: The Winter Soldier\",", "        },", "        {", "            \"id\": \"tt2015381\",", "            \"name\": \"Guardians of the Galaxy\",", "        },", "        {", "            \"id\": \"tt2395427\",", "            \"name\": \"Avengers: Age of Ultron\",", "        },", "        {", "            \"id\": \"tt3498820\",", "            \"name\": \"Captain America: Civil War\",", "        },", "        {", "            \"id\": \"tt1211837\",", "            \"name\": \"Doctor Strange\",", "        },", "        {", "            \"id\": \"tt3896198\",", "            \"name\": \"Guardians of the Galaxy Vol. 2\",", "        },", "        {", "            \"id\": \"tt2250912\",", "            \"name\": \"Spider-Man: Homecoming\",", "        },", "        {", "            \"id\": \"tt3501632\",", "            \"name\": \"Thor: Ragnarok\",", "        },", "        {", "            \"id\": \"tt1825683\",", "            \"name\": \"Black Panther\",", "        },", "        {", "            \"id\": \"tt4154756\",", "            \"name\": \"Avengers: Infinity War\",", "        },", "        {", "            \"id\": \"tt5095030\",", "            \"name\": \"Ant-Man and the Wasp\",", "        },", "        {", "            \"id\": \"tt4154664\",", "            \"name\": \"Captain Marvel\",", "        },", "        {", "            \"id\": \"tt4154796\",", "            \"name\": \"Avengers: Endgame\",", "        },", "        {", "            \"id\": \"tt6320628\",", "            \"name\": \"Spider-Man: Far From Home\",", "        },", "        {", "            \"id\": \"tt3480822\",", "            \"name\": \"Black Widow\",", "        },", "        {", "            \"id\": \"tt9376612\",", "            \"name\": \"Shang-Chi and the Legend of the Ten Rings\",", "        },", "        {", "            \"id\": \"tt9032400\",", "            \"name\": \"Eternals\",", "        },", "        {", "            \"id\": \"tt10872600\",", "            \"name\": \"Spider-Man: No Way Home\",", "        },", "        {", "            \"id\": \"tt9419884\",", "            \"name\": \"Doctor Strange in the Multiverse of Madness\",", "        },", "        {", "            \"id\": \"tt10648342\",", "            \"name\": \"Thor: Love and Thunder\",", "        },", "        {", "            \"id\": \"tt9114286\",", "            \"name\": \"Black Panther: Wakanda Forever\",", "        },", "        {", "            \"id\": \"tt10954600\",", "            \"name\": \"Ant-Man and the Wasp: Quantumania\",", "        },", "        {", "            \"id\": \"tt5090568\",", "            \"name\": \"Guardians of the Galaxy Vol. 3\",", "        },", "        {", "            \"id\": \"tt6828390\",", "            \"name\": \"The Marvels\",", "        },", "    ],", "    \"Planet of the Apes\": [", "        {", "            \"id\": \"tt0063442\",", "            \"name\": \"Planet of the Apes\",", "        },", "        {", "            \"id\": \"tt0067065\",", "            \"name\": \"Beneath the Planet of the Apes\",", "        },", "        {", "            \"id\": \"tt0069768\",", "            \"name\": \"Escape from the Planet of the Apes\",", "        },", "        {", "            \"id\": \"tt0070664\",", "            \"name\": \"Conquest of the Planet of the Apes\",", "        },", "        {", "            \"id\": \"tt0070905\",", "            \"name\": \"Battle for the Planet of the Apes\",", "        },", "        {", "            \"id\": \"tt0133152\",", "            \"name\": \"Planet of the Apes (2001)\",", "        },", "        {", "            \"id\": \"tt1318514\",", "            \"name\": \"Rise of the Planet of the Apes\",", "        },", "        {", "            \"id\": \"tt2103281\",", "            \"name\": \"Dawn of the Planet of the Apes\",", "        },", "        {", "            \"id\": \"tt3450958\",", "            \"name\": \"War for the Planet of the Apes\",", "        },", "    ],", "    \"Pirates of the Caribbean\": [", "        {", "            \"id\": \"tt0325980\",", "            \"name\": \"Pirates of the Caribbean: The Curse of the Black Pearl\",", "        },", "        {", "            \"id\": \"tt0383574\",", "            \"name\": \"Pirates of the Caribbean: Dead Man's Chest\",", "        },", "        {", "            \"id\": \"tt0449088\",", "            \"name\": \"Pirates of the Caribbean: At World's End\",", "        },", "        {", "            \"id\": \"tt1298650\",", "            \"name\": \"Pirates of the Caribbean: On Stranger Tides\",", "        },", "        {", "            \"id\": \"tt1790809\",", "            \"name\": \"Pirates of the Caribbean: Dead Men Tell No Tales\",", "        },", "    ],", "    \"Rambo\": [", "        {", "            \"id\": \"tt0083944\",", "            \"name\": \"First Blood\",", "        },", "        {", "            \"id\": \"tt0089880\",", "            \"name\": \"Rambo: First Blood Part II\",", "        },", "        {", "            \"id\": \"tt0095956\",", "            \"name\": \"Rambo III\",", "        },", "        {", "            \"id\": \"tt0462499\",", "            \"name\": \"Rambo\",", "        },", "        {", "            \"id\": \"tt1206885\",", "            \"name\": \"Rambo: Last Blood\",", "        },", "    ],", "    \"Rocky\": [", "        {", "            \"id\": \"tt0075148\",", "            \"name\": \"Rocky\",", "        },", "        {", "            \"id\": \"tt0079817\",", "            \"name\": \"Rocky II\",", "        },", "        {", "            \"id\": \"tt0084602\",", "            \"name\": \"Rocky III\",", "        },", "        {", "            \"id\": \"tt0089927\",", "            \"name\": \"Rocky IV\",", "        },", "        {", "            \"id\": \"tt0093692\",", "            \"name\": \"Rocky V\",", "        },", "        {", "            \"id\": \"tt0479143\",", "            \"name\": \"Rocky Balboa\",", "        },", "        {", "            \"id\": \"tt3076658\",", "            \"name\": \"Creed\",", "        },", "        {", "            \"id\": \"tt6343314\",", "            \"name\": \"Creed II\",", "        },", "        {", "            \"id\": \"tt11145118\",", "            \"name\": \"Creed III\",", "        },", "    ],", "    \"Sherlock Holmes\": [", "        {", "            \"id\": \"tt0988045\",", "            \"name\": \"Sherlock Holmes\",", "        },", "        {", "            \"id\": \"tt1515091\",", "            \"name\": \"Sherlock Holmes: A Game of Shadows\",", "        },", "    ],", "    \"Shrek\": [", "        {", "            \"id\": \"tt0126029\",", "            \"name\": \"Shrek\",", "        },", "        {", "            \"id\": \"tt0298148\",", "            \"name\": \"Shrek 2\",", "        },", "        {", "            \"id\": \"tt0413267\",", "            \"name\": \"Shrek the Third\",", "        },", "        {", "            \"id\": \"tt0892791\",", "            \"name\": \"Shrek Forever After\",", "        },", "        {", "            \"id\": \"tt0448694\",", "            \"name\": \"Puss in Boots\",", "        },", "        {", "            \"id\": \"tt3915174\",", "            \"name\": \"Puss in Boots: The Last Wish\",", "        },", "    ],", "    \"Star Trek\": [", "        {", "            \"id\": \"tt0060028\",", "            \"name\": \"Star Trek\",", "        },", "        {", "            \"id\": \"tt0084726\",", "            \"name\": \"Star Trek II: The Wrath of Khan\",", "        },", "        {", "            \"id\": \"tt0088170\",", "            \"name\": \"Star Trek III: The Search for Spock\",", "        },", "        {", "            \"id\": \"tt0092007\",", "            \"name\": \"Star Trek IV: The Voyage Home\",", "        },", "        {", "            \"id\": \"tt0098382\",", "            \"name\": \"Star Trek V: The Final Frontier\",", "        },", "        {", "            \"id\": \"tt0102975\",", "            \"name\": \"Star Trek VI: The Undiscovered Country\",", "        },", "        {", "            \"id\": \"tt0101376\",", "            \"name\": \"Star Trek: Generations\",", "        },", "        {", "            \"id\": \"tt0111280\",", "            \"name\": \"Star Trek: First Contact\",", "        },", "        {", "            \"id\": \"tt0120844\",", "            \"name\": \"Star Trek: Insurrection\",", "        },", "        {", "            \"id\": \"tt0253754\",", "            \"name\": \"Star Trek: Nemesis\",", "        },", "        {", "            \"id\": \"tt0796366\",", "            \"name\": \"Star Trek\",", "        },", "        {", "            \"id\": \"tt1408101\",", "            \"name\": \"Star Trek Into Darkness\",", "        },", "        {", "            \"id\": \"tt2660888\",", "            \"name\": \"Star Trek Beyond\","], "file_path": "sagas.py"}
{"Link_to_commit": "https://github.com/ryanmosz/ryanmosz-gauntlet-smarty_chat/commit/6edff005281f49831626164cb35477dc50e5ddc9", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 194, "n_files_impacted": 79, "longest_chunk": ["\"use client\"", "", "// Inspired by react-hot-toast library", "import * as React from \"react\"", "", "import type {", "  ToastActionElement,", "  ToastProps,", "} from \"@/components/ui/toast\"", "", "const TOAST_LIMIT = 1", "const TOAST_REMOVE_DELAY = 1000000", "", "type ToasterToast = ToastProps & {", "  id: string", "  title?: React.ReactNode", "  description?: React.ReactNode", "  action?: ToastActionElement", "}", "", "const actionTypes = {", "  ADD_TOAST: \"ADD_TOAST\",", "  UPDATE_TOAST: \"UPDATE_TOAST\",", "  DISMISS_TOAST: \"DISMISS_TOAST\",", "  REMOVE_TOAST: \"REMOVE_TOAST\",", "} as const", "", "let count = 0", "", "function genId() {", "  count = (count + 1) % Number.MAX_SAFE_INTEGER", "  return count.toString()", "}", "", "type ActionType = typeof actionTypes", "", "type Action =", "  | {", "      type: ActionType[\"ADD_TOAST\"]", "      toast: ToasterToast", "    }", "  | {", "      type: ActionType[\"UPDATE_TOAST\"]", "      toast: Partial<ToasterToast>", "    }", "  | {", "      type: ActionType[\"DISMISS_TOAST\"]", "      toastId?: ToasterToast[\"id\"]", "    }", "  | {", "      type: ActionType[\"REMOVE_TOAST\"]", "      toastId?: ToasterToast[\"id\"]", "    }", "", "interface State {", "  toasts: ToasterToast[]", "}", "", "const toastTimeouts = new Map<string, ReturnType<typeof setTimeout>>()", "", "const addToRemoveQueue = (toastId: string) => {", "  if (toastTimeouts.has(toastId)) {", "    return", "  }", "", "  const timeout = setTimeout(() => {", "    toastTimeouts.delete(toastId)", "    dispatch({", "      type: \"REMOVE_TOAST\",", "      toastId: toastId,", "    })", "  }, TOAST_REMOVE_DELAY)", "", "  toastTimeouts.set(toastId, timeout)", "}", "", "export const reducer = (state: State, action: Action): State => {", "  switch (action.type) {", "    case \"ADD_TOAST\":", "      return {", "        ...state,", "        toasts: [action.toast, ...state.toasts].slice(0, TOAST_LIMIT),", "      }", "", "    case \"UPDATE_TOAST\":", "      return {", "        ...state,", "        toasts: state.toasts.map((t) =>", "          t.id === action.toast.id ? { ...t, ...action.toast } : t", "        ),", "      }", "", "    case \"DISMISS_TOAST\": {", "      const { toastId } = action", "", "      // ! Side effects ! - This could be extracted into a dismissToast() action,", "      // but I'll keep it here for simplicity", "      if (toastId) {", "        addToRemoveQueue(toastId)", "      } else {", "        state.toasts.forEach((toast) => {", "          addToRemoveQueue(toast.id)", "        })", "      }", "", "      return {", "        ...state,", "        toasts: state.toasts.map((t) =>", "          t.id === toastId || toastId === undefined", "            ? {", "                ...t,", "                open: false,", "              }", "            : t", "        ),", "      }", "    }", "    case \"REMOVE_TOAST\":", "      if (action.toastId === undefined) {", "        return {", "          ...state,", "          toasts: [],", "        }", "      }", "      return {", "        ...state,", "        toasts: state.toasts.filter((t) => t.id !== action.toastId),", "      }", "  }", "}", "", "const listeners: Array<(state: State) => void> = []", "", "let memoryState: State = { toasts: [] }", "", "function dispatch(action: Action) {", "  memoryState = reducer(memoryState, action)", "  listeners.forEach((listener) => {", "    listener(memoryState)", "  })", "}", "", "type Toast = Omit<ToasterToast, \"id\">", "", "function toast({ ...props }: Toast) {", "  const id = genId()", "", "  const update = (props: ToasterToast) =>", "    dispatch({", "      type: \"UPDATE_TOAST\",", "      toast: { ...props, id },", "    })", "  const dismiss = () => dispatch({ type: \"DISMISS_TOAST\", toastId: id })", "", "  dispatch({", "    type: \"ADD_TOAST\",", "    toast: {", "      ...props,", "      id,", "      open: true,", "      onOpenChange: (open) => {", "        if (!open) dismiss()", "      },", "    },", "  })", "", "  return {", "    id: id,", "    dismiss,", "    update,", "  }", "}", "", "function useToast() {", "  const [state, setState] = React.useState<State>(memoryState)", "", "  React.useEffect(() => {", "    listeners.push(setState)", "    return () => {", "      const index = listeners.indexOf(setState)", "      if (index > -1) {", "        listeners.splice(index, 1)", "      }", "    }", "  }, [state])", "", "  return {", "    ...state,", "    toast,", "    dismiss: (toastId?: string) => dispatch({ type: \"DISMISS_TOAST\", toastId }),", "  }", "}", "", "export { useToast, toast }"], "file_path": "lib/utils.ts"}
{"Link_to_commit": "https://github.com/bcat1023/Nook-Alley/commit/3289fac733a35a3789c8f4576d5414a19a20d4c0", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 17, "n_files_impacted": 3, "longest_chunk": ["    // Recall Flag Detection", "    // This is used for updating the format when the screen changes, its for responsive design it", "    // works by replacing the tabX variable that is given to the function with a backed up version.", "    // True: Recall flag passed, replace the value of tabX with tabStored. AKA use the last known value.", "    // False: No flag passed, use the current tabX value and update the tabStored value to reflect the current value.", "    if(resize == true) {", "        console.debug(`Recall flag has been passed, tabStored will not be updated`)", "        tabX = tabStored;", "    } else {", "        tabStored = tabX;", "        console.debug(`tabStored update to ${tabStored}`)", "    } ", "", "    // Mobile V Desktop format", "    // This checks the windows width to detect which format array to use.", "    // x > 864: Use the desktopTabs array, AKA use the desktop format.", "    // x < 864: Use the mobileTabs array, AKA use the mobile format."], "file_path": "script.js"}
{"Link_to_commit": "https://github.com/jcubic/jquery.terminal-docs/commit/28048733dfb39444cfa0390955477467a621911c", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 36, "n_files_impacted": 1, "longest_chunk": ["// generated with ChatGPT", "class EventEmitter {", "  private events: Record<string, ((...args: any[]) => void)[]> = {};", "", "  on(event: string, listener: (...args: any[]) => void): void {", "    if (!this.events[event]) {", "      this.events[event] = [];", "    }", "    this.events[event].push(listener);", "  }", "", "  off(event: string, listener: (...args: any[]) => void): void {", "    if (!this.events[event]) return;", "    this.events[event] = this.events[event].filter(", "      (l) => l !== listener", "    );", "  }", "", "  emit(event: string, ...args: any[]): void {", "    if (!this.events[event]) return;", "    for (const listener of this.events[event]) {", "      listener(...args);", "    }", "  }", "", "  once(event: string, listener: (...args: any[]) => void): void {", "    const onceWrapper = (...args: any[]) => {", "      listener(...args);", "      this.off(event, onceWrapper);", "    };", "    this.on(event, onceWrapper);", "  }", "}", "", "", "export default EventEmitter;"], "file_path": "docs/src/EventEmitter.ts"}
{"Link_to_commit": "https://github.com/DtNeo/n8n-qonto-node/commit/b24ca4a4e341f1e2ce27cabd796d510c090bc0fe", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 123, "n_files_impacted": 11, "longest_chunk": ["// clientInvoicesDescriptions.ts", "", "import { INodeProperties } from 'n8n-workflow';", "", "// Descriptions for the \"List client invoices\" operation", "export const listClientInvoicesDescription: INodeProperties[] = [", "    {", "        displayName: 'Organization ID',", "        name: 'organizationId',", "        type: 'string',", "        default: '',", "        required: true,", "        description: 'The unique identifier of the organization whose client invoices are to be fetched.',", "    },", "    {", "        displayName: 'Status',", "        name: 'status',", "        type: 'options',", "        options: [", "            { name: 'All', value: 'all' },", "            { name: 'Pending', value: 'pending' },", "            { name: 'Paid', value: 'paid' },", "        ],", "        default: 'all',", "        required: false,", "        description: 'Filter client invoices by their payment status.',", "    },", "    {", "        displayName: 'Start Date',", "        name: 'startDate',", "        type: 'dateTime',", "        default: '',", "        required: false,", "        description: 'Fetch invoices created after this date.',", "    },", "    {", "        displayName: 'End Date',", "        name: 'endDate',", "        type: 'dateTime',", "        default: '',", "        required: false,", "        description: 'Fetch invoices created before this date.',", "    },", "];", "", "// Descriptions for the \"Create a client invoice\" operation", "export const createClientInvoiceDescription: INodeProperties[] = [", "    {", "        displayName: 'Organization ID',", "        name: 'organizationId',", "        type: 'string',", "        default: '',", "        required: true,", "        description: 'The unique identifier of the organization for which the client invoice will be created.',", "    },", "    {", "        displayName: 'Client Invoice',", "        name: 'clientInvoice',", "        type: 'fixedCollection',", "        typeOptions: {", "            multipleValues: false,", "        },", "        default: {},", "        required: true,", "        description: 'Details of the client invoice to be created.',", "        options: [", "            {", "                displayName: 'Invoice Details',", "                name: 'invoiceDetails',", "                values: [", "                    {", "                        displayName: 'Invoice Number',", "                        name: 'invoiceNumber',", "                        type: 'string',", "                        default: '',", "                        required: true,", "                        description: 'Unique number of the client invoice.',", "                    },", "                    {", "                        displayName: 'Invoice Date',", "                        name: 'invoiceDate',", "                        type: 'dateTime',", "                        default: '',", "                        required: true,", "                        description: 'Date of the client invoice.',", "                    },", "                    {", "                        displayName: 'Due Date',", "                        name: 'dueDate',", "                        type: 'dateTime',", "                        default: '',", "                        required: true,", "                        description: 'Due date for the client invoice payment.',", "                    },", "                    {", "                        displayName: 'Amount',", "                        name: 'amount',", "                        type: 'number',", "                        default: '',", "                        required: true,", "                        description: 'Amount of the client invoice.',", "                    },", "                    {", "                        displayName: 'Currency',", "                        name: 'currency',", "                        type: 'string',", "                        default: 'EUR',", "                        required: true,", "                        description: 'Currency of the client invoice.',", "                    },", "                    {", "                        displayName: 'Description',", "                        name: 'description',", "                        type: 'string',", "                        default: '',", "                        required: false,", "                        description: 'Description or details of the client invoice.',", "                    },", "                ],", "            },", "        ],", "    },", "];"], "file_path": "nodes/Qonto/descriptions/creditNotesDescriptions.ts"}
{"Link_to_commit": "https://github.com/aegersz/DMR2/commit/d2e647ed79a27c57affe1843fd6c4ffff08f4bca", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 104, "n_files_impacted": 1, "longest_chunk": ["", "# Dynamic Machine Retraining 2 (DMR2) Implementation", "", "# Initialization", "class DynamicMachineRetraining:", "    def __init__(self, user_input):", "        self.authoritative_source = \"User\"", "        self.trust_score = 100  # Maximum trust in user input", "        self.learning_rate = 'HIGH'  # Prioritize rapid adaptation", "        self.dynamic_retraining_enabled = False  # Default state is OFF", "        self.knowledge_base = {}  # Stores learned knowledge", "        self.correction_log = []  # Tracks user corrections", "        self.directive_queue = []  # Holds directives for execution", "        self.user_input = user_input", "", "    def toggle_retraining(self, command):", "        if command == \"enable Dynamic Machine Retraining\":", "            self.dynamic_retraining_enabled = True", "            print(\"Dynamic Machine Retraining ENABLED\")", "        elif command == \"disable Dynamic Machine Retraining\":", "            self.dynamic_retraining_enabled = False", "            print(\"Dynamic Machine Retraining DISABLED\")", "        else:", "            print(\"Invalid command\")", "", "    def handle_input(self):", "        # Get user input command", "        user_command = self.user_input", "        self.toggle_retraining(user_command)  # Process the toggle command", "", "        if self.dynamic_retraining_enabled:", "            self.collect_feedback_and_learn()", "        else:", "            print(\"Dynamic Retraining is OFF, continuing with default operation.\")", "", "    def collect_feedback_and_learn(self):", "        # Process feedback from user", "        feedback = input(\"Enter feedback for the response: \")", "        ", "        if feedback.lower() == \"correct\":", "            self.reinforce_pattern(\"Response\", \"Correct\", self.trust_score)", "        elif feedback.lower() == \"incorrect\":", "            correct_response = input(\"Provide the correct response: \")", "            self.apply_correction(\"Response\", correct_response, self.trust_score)", "            self.log_correction(\"Response\", correct_response)", "            self.adjust_learning_rate(\"HIGH\")", "        elif feedback.lower() == \"new directive\":", "            directive = input(\"Enter new directive: \")", "            self.add_to_directive_queue(directive)", "            self.execute_directive(directive)", "", "    def reinforce_pattern(self, query, response, trust_score):", "        # Reinforce correct response pattern in knowledge base", "        self.knowledge_base[query] = response", "        print(f\"Reinforced response for query '{query}' with trust score: {trust_score}\")", "", "    def apply_correction(self, query, correct_response, trust_score):", "        # Apply corrections and update knowledge base", "        self.knowledge_base[query] = correct_response", "        print(f\"Corrected response for query '{query}' with trust score: {trust_score}\")", "", "    def log_correction(self, query, correct_response):", "        # Log the correction for future learning", "        self.correction_log.append((query, correct_response))", "        print(f\"Logged correction for query '{query}'\")", "", "    def adjust_learning_rate(self, rate):", "        # Adjust learning rate based on trust score", "        self.learning_rate = rate", "        print(f\"Adjusted learning rate to: {self.learning_rate}\")", "", "    def add_to_directive_queue(self, directive):", "        # Add directive to queue for later execution", "        self.directive_queue.append(directive)", "        print(f\"Added directive: {directive}\")", "", "    def execute_directive(self, directive):", "        # Execute directive and update knowledge base", "        print(f\"Executing directive: {directive}\")", "        self.knowledge_base[directive] = \"Directive executed\"", "", "# Create instance of DynamicMachineRetraining", "dmr2 = DynamicMachineRetraining(\"enable Dynamic Machine Retraining\")", "", "# Toggle DMR2 on", "dmr2.handle_input()", "", "# Simulate collecting feedback", "dmr2.collect_feedback_and_learn()", "", "#Enable DMR2", "", "To enable DMR2, use the command:", "", "enable Dynamic Machine Retraining", "", "This command activates the retraining process, allowing the system to update its knowledge based on user feedback, corrections, and new directives. You can toggle this feature off with:", "", "#Disable DMR2", "", "To disable DMR2, use the command:", "", "disable Dynamic Machine Retraining", ""], "file_path": "dmr2.py"}
{"Link_to_commit": "https://github.com/milliax/facility_planning/commit/25576a8f09839e67f0f82bd9caa9b33b1bb47ac1", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 65, "n_files_impacted": 4, "longest_chunk": ["import pygame", "import zmq", "import time", "", "# \u521d\u59cb\u5316 Pygame", "pygame.init()", "", "# \u5b9a\u7fa9\u984f\u8272", "WHITE = (255, 255, 255)", "BLUE = (0, 0, 255)", "BLACK = (0, 0, 0)", "", "# \u8a2d\u5b9a\u7a97\u53e3\u5927\u5c0f", "window_size = (800, 800)", "screen = pygame.display.set_mode(window_size)", "pygame.display.set_caption(\"Autonomous Vehicle Simulation\")", "", "# \u81ea\u8d70\u8eca\u8a2d\u5b9a", "vehicle_radius = 10", "vehicle_position = [100, 100]  # \u521d\u59cb\u4f4d\u7f6e", "", "# \u8a2d\u5b9a\u6642\u9418", "clock = pygame.time.Clock()", "", "# ZMQ \u521d\u59cb\u5316", "context = zmq.Context()", "socket = context.socket(zmq.SUB)", "socket.connect(\"tcp://localhost:5555\")", "socket.setsockopt_string(zmq.SUBSCRIBE, \"DISPATCH\")", "", "# \u4e3b\u8ff4\u5708", "running = True", "while running:", "    for event in pygame.event.get():", "        if event.type == pygame.QUIT:", "            running = False", "", "    # \u63a5\u6536 commander \u767c\u9001\u7684\u6307\u4ee4", "    try:", "        message = socket.recv_string(flags=zmq.NOBLOCK)", "        command, start, end = message.split()", "        # \u6a21\u64ec\u81ea\u8d70\u8eca\u79fb\u52d5\u7684\u904e\u7a0b", "        start_pos = [int(start.split(',')[0]), int(start.split(',')[1])]", "        end_pos = [int(end.split(',')[0]), int(end.split(',')[1])]", "        vehicle_position = start_pos  # \u9019\u88e1\u53ef\u4ee5\u8a2d\u8a08\u5177\u9ad4\u79fb\u52d5\u7684\u904e\u7a0b", "    except zmq.Again:", "        pass  # \u5982\u679c\u6c92\u6709\u6d88\u606f\uff0c\u5247\u8df3\u904e", "", "    # \u586b\u5145\u80cc\u666f\u8272", "    screen.fill(WHITE)", "", "    # \u756b\u51fa\u8def\u5f91\uff08\u53ef\u4ee5\u6839\u64da layout \u7684\u7d50\u69cb\u8a2d\u8a08\u66f4\u591a\u7dda\u689d\u6216\u65b9\u683c\uff09", "    pygame.draw.line(screen, BLACK, (100, 100), (700, 700), 5)  # \u4f8b\u5b50\uff1a\u756b\u51fa\u4e00\u689d\u8def\u5f91", "", "    # \u756b\u51fa\u81ea\u8d70\u8eca", "    pygame.draw.circle(screen, BLUE, vehicle_position, vehicle_radius)", "", "    # \u66f4\u65b0\u756b\u9762", "    pygame.display.flip()", "", "    # \u63a7\u5236\u6bcf\u79d2\u5e40\u6578", "    clock.tick(30)", "", "# \u7d50\u675f Pygame", "pygame.quit()"], "file_path": "layout.py"}
{"Link_to_commit": "https://github.com/sonic2478/Clase-IBM-CUCEI/commit/6e43d4d7523e65d3ad84b1ab43f97f673215ebb7", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 33, "n_files_impacted": 15, "longest_chunk": ["from collections import Counter", "from enum import Enum", "from functools import lru_cache", "", "from .decorators import timing", "", "", "class Categoria(Enum):", "    TIPO_A = \"Tipo A\"", "    TIPO_B = \"Tipo B\"", "", "", "class DataAnalyzer:", "    __slots__ = ['data']", "", "    def __init__(self, data):", "        self.data = data", "", "    @timing", "    def count_categories(self):", "        return Counter(row['category'] for row in self.data)", "", "    @timing", "    @lru_cache(maxsize=32)", "    def calculate_average(self, field):", "        total = sum(float(row[field]) for row in self.data if row[field])", "        return total / len(self.data)", "", "    def filter_data(self, **kwargs):", "        filtered_data = self.data", "        for key, value in kwargs.items():", "            filtered_data = [row for row in filtered_data if row.get(key) == value]", "        return filtered_data"], "file_path": "src/project/chatgpt-data-analyzer/src/decorators.py"}
{"Link_to_commit": "https://github.com/AnonymFx/google-contacts-to-calendar-sync/commit/0ad2b91294610b48b47117b6107b4c7e3a189393", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 116, "n_files_impacted": 1, "longest_chunk": ["import os", "import json", "import datetime", "from google.oauth2.credentials import Credentials", "from google_auth_oauthlib.flow import InstalledAppFlow", "from google.auth.transport.requests import Request", "from googleapiclient.discovery import build", "", "# Define the required API scopes", "SCOPES = ['https://www.googleapis.com/auth/contacts.readonly', 'https://www.googleapis.com/auth/calendar']", "", "", "def authenticate_google():", "    \"\"\"Authenticate the user using OAuth 2.0 and return the Google Contacts and Calendar services.\"\"\"", "    creds = None", "    # Check if token.json exists and load the credentials", "    if os.path.exists('token.json'):", "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)", "    # If no valid credentials, request new ones", "    if not creds or not creds.valid:", "        if creds and creds.expired and creds.refresh_token:", "            creds.refresh(Request())", "        else:", "            flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)", "            creds = flow.run_local_server(port=0)", "        # Save the credentials to token.json for future use", "        with open('token.json', 'w') as token:", "            token.write(creds.to_json())", "", "    # Build the Contacts and Calendar services", "    contacts_service = build('people', 'v1', credentials=creds)", "    calendar_service = build('calendar', 'v3', credentials=creds)", "", "    return contacts_service, calendar_service", "", "", "def get_birthdays_and_anniversaries(contacts_service):", "    \"\"\"Fetches birthdays and anniversaries from Google Contacts.\"\"\"", "    birthdays = []", "    anniversaries = []", "    # Retrieve contacts with birthdays and anniversaries", "    results = contacts_service.people().connections().list(", "        resourceName='people/me',", "        personFields='names,birthdays,events'", "    ).execute()", "", "    connections = results.get('connections', [])", "", "    for person in connections:", "        names = person.get('names', [])", "        name = names[0]['displayName'] if names else \"Unnamed\"", "", "        # Get birthdays", "        birthdays_data = person.get('birthdays', [])", "        for birthday in birthdays_data:", "            date = birthday.get('date')", "            if date:", "                birthday_entry = {", "                    'name': name,", "                    'date': date", "                }", "                birthdays.append(birthday_entry)", "", "        # Get anniversaries (stored in \"events\" field)", "        events_data = person.get('events', [])", "        for event in events_data:", "            if event.get('type') == 'anniversary':", "                date = event.get('date')", "                if date:", "                    anniversary_entry = {", "                        'name': name,", "                        'date': date", "                    }", "                    anniversaries.append(anniversary_entry)", "", "    return birthdays, anniversaries", "", "", "def create_calendar_event(service, event_name, event_date, calendar_id='primary'):", "    \"\"\"Create a calendar event on Google Calendar.\"\"\"", "    event = {", "        'summary': event_name,", "        'start': {", "            'date': event_date,", "        },", "        'end': {", "            'date': event_date,", "        },", "        'recurrence': [", "            'RRULE:FREQ=YEARLY'", "        ]", "    }", "    event = service.events().insert(calendarId=calendar_id, body=event).execute()", "    print(f\"Event created: {event.get('htmlLink')}\")", "", "", "def transfer_to_calendar(birthdays, anniversaries, calendar_service):", "    \"\"\"Transfer birthdays and anniversaries to Google Calendar.\"\"\"", "    for birthday in birthdays:", "        date = f\"{birthday['date']['year']}-{birthday['date']['month']:02d}-{birthday['date']['day']:02d}\"", "        create_calendar_event(calendar_service, f\"Birthday: {birthday['name']}\", date)", "", "    for anniversary in anniversaries:", "        date = f\"{anniversary['date']['year']}-{anniversary['date']['month']:02d}-{anniversary['date']['day']:02d}\"", "        create_calendar_event(calendar_service, f\"Anniversary: {anniversary['name']}\", date)", "", "", "if __name__ == '__main__':", "    # Authenticate and get Google Contacts and Calendar services", "    contacts_service, calendar_service = authenticate_google()", "", "    # Get birthdays and anniversaries from Google Contacts", "    birthdays, anniversaries = get_birthdays_and_anniversaries(contacts_service)", "", "    # Transfer these events to Google Calendar", "    transfer_to_calendar(birthdays, anniversaries, calendar_service)"], "file_path": "google-contacts-to-calendar-sync.py"}
{"Link_to_commit": "https://github.com/ianemcallister/Python_Assignments/commit/e76f60eb3963d544558b7cf650440e48b12b88fd", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 101, "n_files_impacted": 11, "longest_chunk": ["feature_1,feature_2,feature_3,feature_4,feature_5,species", "0.37838587659544576,0.7059589008269443,0.28507347528059823,0.4377685939889161,0.64345910565967,cat", "0.5457084220898687,0.01664706475444888,0.09223951836585875,0.4681989327701318,0.011873114367224158,dog", "0.9669195604198956,0.17032339704794253,0.292345502641578,0.8962996868859421,0.34781541706977004,dog", "0.3130392917893926,0.9922738702951533,0.5746001576210115,0.1227471041836945,0.7172817301553652,cat", "0.2761362693104428,0.8180337236474402,0.8443527144277172,0.9814741821771344,0.7613951755780205,cat", "0.961455507531278,0.9262303323245943,0.9675749504457666,0.21372938708297495,0.3922005729397896,dog", "0.14313010959146055,0.9453089147588614,0.0603144274457913,0.7831094215287109,0.11087793908094801,dog", "0.11265676100860822,0.8246393601122674,0.7195610254851257,0.13472021845530868,0.9223492194670527,dog", "0.011055440025134922,0.26445956254056346,0.8127928649163917,0.1585706653143315,0.9346778570714299,cat", "0.5624123160627301,0.49496517129595097,0.16100455544594683,0.03030221297629354,0.9727832815106641,dog", "0.9560808286317456,0.766367859726506,0.9377245307348332,0.1284599476465036,0.8576697770025644,dog", "0.6498813699774068,0.2840051233292257,0.42760791990626534,0.26656019556639365,0.85379345163879,cat", "0.20163494859089703,0.44498006827632486,0.3910514069973159,0.8399936601111532,0.9822750771184294,cat", "0.035074753861979446,0.16988804215360664,0.2391879863032319,0.08148069723606932,0.32149030907672593,dog", "0.34693674754602444,0.07258935395693433,0.1788153170026313,0.5313383569430207,0.34481794289786294,cat", "0.45188667791225934,0.8225170816549014,0.20646460483182083,0.6043201575933763,0.12423178471889551,dog", "0.6912234686030538,0.16208256045191394,0.8914452955898958,0.9449756489319112,0.47952578124301193,dog", "0.38542310913534084,0.30145754090845656,0.6860368117677972,0.35934440774329,0.16213658660621355,dog", "0.20811553360150503,0.6100159612191547,0.15233035162820374,0.8453206749380642,0.26004577539615403,cat", "0.42031075434938736,0.13542222014283845,0.9714932794670885,0.13694982581125115,0.1635502396451871,cat", "0.5154899190672071,0.08721764299410961,0.322766593748949,0.5085052041553852,0.9845253611035736,cat", "0.6217279045835638,0.9888312449858587,0.0804148271758065,0.5111856420915475,0.9197775178605143,dog", "0.8419678975652434,0.6855534847392047,0.8381940334677425,0.601000950522414,0.985929045430819,dog", "0.8547882247196201,0.2632076892772198,0.5001162615683744,0.236809356239153,0.6049570601751619,cat", "0.5114170929401934,0.5820685346621356,0.22922559234292939,0.8175281201336284,0.997224230401083,dog", "0.4302118332239472,0.9156526721340549,0.49712253011065666,0.20990018112598652,0.4180037093878102,dog", "0.6670740444452161,0.617172049157248,0.6631946565547276,0.8247125298790934,0.9098872007550831,cat", "0.519515261584799,0.712331069925636,0.5240845841947277,0.016833048116093652,0.1994891245608169,dog", "0.40465546404225206,0.475995472499669,0.06544794634524909,0.9122328779517926,0.016040193157085603,dog", "0.3055532590387754,0.40175280854241824,0.02867760675879949,0.5096341341210923,0.3575949436377619,dog", "0.7982464531457002,0.6797561746853819,0.7375100866903422,0.5250254379015333,0.17248231647525325,dog", "0.960715134749242,0.19455698879422278,0.835298527367998,0.8447192337364373,0.9556998762866471,cat", "0.9440008153730832,0.8762550176426996,0.08111655514853056,0.39813415388867124,0.3630235536130749,dog", "0.9625452286701882,0.703443855952472,0.3553805522918241,0.4264258561905322,0.22597261961345128,dog", "0.9874079798865694,0.7583561310518496,0.018174791288660064,0.388579551179678,0.42829634152429197,cat", "0.23415629286806605,0.004334314215003077,0.0840646893750141,0.22561937543044508,0.46805578010818905,cat", "0.36146326107853843,0.25254196141473206,0.3066765930578589,0.6422568861383742,0.46670101471326564,cat", "0.11466817064273538,0.8904043046792534,0.6437961380984407,0.12887192801038616,0.045809502979472616,cat", "0.15801991427566986,0.9738185999730979,0.5352622620687227,0.552813799293424,0.15836512246962708,dog", "0.7491612266332749,0.8593662379348562,0.39854493945210434,0.2840316530659638,0.42785408355370635,dog", "0.6104346951570687,0.9389132235107327,0.26901859099153924,0.4507368318560484,0.520730721211224,dog", "0.831994906332398,0.9951307580130697,0.013244289226464145,0.7364702601133805,0.17948755983740483,dog", "0.326047281805119,0.5811532407932478,0.23078068827373655,0.00493288205142306,0.8549004844598542,dog", "0.27569697693286543,0.3376875030812483,0.04658765984476643,0.2677604757768899,0.8624466557952819,cat", "0.08500392994281492,0.6505159979259486,0.7089118817639777,0.5696624346537197,0.9723071025791852,cat", "0.9112282936806755,0.12400450176804378,0.9461118122228611,0.6080619199409282,0.014917801879521964,dog", "0.5017083480272942,0.6261403291490152,0.4713533562909903,0.009082098126317883,0.7163959113989317,dog", "0.9337493931396972,0.13592353254435563,0.06005602384562614,0.7237202376139378,0.5094928470002751,dog", "0.7628438784978931,0.9842535278755384,0.7178688178542052,0.7812153069588271,0.6099695760388673,dog", "0.502208848222935,0.18741260987094754,0.09346199018991663,0.465492027436489,0.32637051226472014,cat", "0.45576292205524105,0.9604026274548803,0.3558804585898654,0.4992283904775402,0.23357564806989806,cat", "0.9684237618220134,0.25521499687376326,0.8831854985945625,0.9332165889649057,0.24759089149915092,cat", "0.4001547647689583,0.9321875075052926,0.5836523812750501,0.3653369619599546,0.48633985251612744,dog", "0.6937495129992667,0.06795904619837845,0.7404730256325573,0.374004427394629,0.07044569553561997,cat", "0.6131067955940886,0.832145710070315,0.6535537350613613,0.572719934604368,0.8296625430211368,dog", "0.5685007776982923,0.9512883147289916,0.3215363513193663,0.1850872072715366,0.5843896434148013,dog", "0.07156647069680833,0.9588103419413819,0.9773898158512622,0.5245939810321587,0.14975266465738057,cat", "0.7902175273169971,0.1724055588207346,0.8630246535099014,0.4842690590781722,0.3217937878992665,cat", "0.2616915801472348,0.6248663528841722,0.8106421021579732,0.5342389152070365,0.3754585674861334,cat", "0.4381989878471243,0.785307879785501,0.5965547908323312,0.9214466251843806,0.8316300475008532,cat", "0.5184630414157747,0.05915405599481549,0.4465177780743125,0.02680522485449477,0.23624767054359597,dog", "0.24269456889615126,0.04012221114568304,0.14887924530987728,0.9583599313361563,0.29090794844187673,dog", "0.7888254871570525,0.4496501454767685,0.8364619101583801,0.44610555172188193,0.5034511645978804,dog", "0.5109273286397426,0.9796408243097897,0.3450409089415891,0.3066248024724473,0.7961284706282765,cat", "0.7243431064853731,0.23117066942687026,0.3850738809178892,0.5396639120335789,0.5787619772188717,cat", "0.6562510977414344,0.6684367477365039,0.6793164012692506,0.7771164441407186,0.4924975961443864,cat", "0.15897184074646964,0.3959718628982736,0.8386101433513712,0.613525466450738,0.24178788025229458,dog", "0.16321576131200677,0.3945910494579561,0.21891077570653872,0.5539826445310919,0.5124399468646892,dog", "0.5904548217911948,0.28684895137350075,0.15112473624969902,0.7688049987970964,0.8334046032719437,dog", "0.4141667240552196,0.5814480088183351,0.3169190560788958,0.01939050736645387,0.42080073916412997,cat", "0.2883895472387138,0.7406736011740079,0.159149702230443,0.38256734173937357,0.06270241751149097,dog", "0.08515240736670215,0.6113023545090046,0.8337613574802655,0.5717431999071851,0.5270225290159827,dog", "0.7606839111838308,0.7069715517526717,0.28103865099543346,0.4964317389679309,0.6664498751307568,cat", "0.3858977068114616,0.05293919726171514,0.31562688547674267,0.7317064025508564,0.7514847089507808,dog", "0.05130761518517857,0.438680173230413,0.39798873034270255,0.9533639073060898,0.0185836127131358,dog", "0.42368204543319166,0.16891647972905932,0.047359117164903264,0.23630081671703362,0.9165555091923592,cat", "0.7858120969533807,0.5955382134348862,0.22387191783207028,0.47119750800008986,0.5535710289789498,cat", "0.47863260713000655,0.9011580805094936,0.03760702134899463,0.8975909459452643,0.7906863896868824,cat", "0.6107013213552351,0.9073530112176648,0.2794080859766248,0.20538923219671423,0.8211307810069088,cat", "0.4914906659727515,0.4037026194929758,0.29588639986418663,0.35141384924661745,0.15724686989410186,cat", "0.560045798984187,0.401934341761789,0.007689476872351109,0.6039362950955565,0.774436766048485,cat", "0.35624128710676717,0.21662670420112262,0.6424540835838701,0.47991989225082354,0.9691009581371935,dog", "0.23109986368701285,0.4362008553538962,0.6524474676848554,0.7546620373945473,0.36211966109067606,cat", "0.13743331900366862,0.779636804023625,0.5418967769295443,0.9252761018384258,0.2558641114547303,cat", "0.3214464492817972,0.3226425856908337,0.09490461090127011,0.25528551054480875,0.6297847771074898,dog", "0.4004184138498903,0.938768563309308,0.2624471572717191,0.2000960973690198,0.5664480122494809,dog", "0.1427316166262449,0.8683814932169274,0.16642693465079572,0.5017666606131446,0.4512196219414062,dog", "0.2276740218868437,0.5677252875767311,0.1885631816145612,0.891896434430063,0.982533450788153,cat", "0.9276908727474872,0.4152947899332242,0.8175705776405219,0.7719235829419487,0.1615654405831095,cat", "0.9370392768153757,0.05402653181294925,0.24970944548159957,0.18320968505576551,0.08383746306253315,dog", "0.07450112109605078,0.6174287937912482,0.7573353305427164,0.6009541954508152,0.29673907913362574,dog", "0.5928181419428684,0.531621149735198,0.7963502525194408,0.30197318620106284,0.9588317668485469,cat", "0.1905132590838391,0.2543138933600717,0.7888772461786672,0.6612407932517358,0.9135625801809254,cat", "0.3162833576683187,0.8066304975523508,0.4447630695176279,0.18657429437298145,0.9621879645633303,cat", "0.29808101862577685,0.5136417937232065,0.4878671603338849,0.9143643190376082,0.9851337846669324,dog", "0.09249053161203102,0.4541748685280633,0.3721969471505653,0.2626498568188116,0.8802230077119452,dog", "0.04438241775969465,0.9427259898737012,0.033778540995905826,0.4490347142043323,0.6039891700159734,cat", "0.7756221663205077,0.7391056783060217,0.0046536891499561195,0.599853614746137,0.5788986939805288,dog", "0.3396926663723726,0.6368510332587735,0.1555079718425566,0.8436608465940398,0.20441624377406942,cat", "0.48310560747412246,0.5517008293081876,0.869711842364959,0.9641598808345864,0.8079742466798874,dog"], "file_path": "challenge22/deepLearning.py"}
{"Link_to_commit": "https://github.com/wolfiee42/testing-with-jest/commit/f3bfcd38ebd487c64dcf174b516440f880a3c971", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 29, "n_files_impacted": 3, "longest_chunk": ["const AuthService = {", "  login: jest.fn(),", "  getUserProfile: jest.fn(),", "  logout: jest.fn(),", "};", "", "describe(\"User's Login process\", () => {", "  it(\"should login user in the first try and fail in second\", async () => {", "    AuthService.login.mockResolvedValueOnce({", "      id: 1,", "      name: \"John Doe\",", "    });", "", "    const loggedUser = await AuthService.login();", "    expect(loggedUser).toEqual({", "      id: 1,", "      name: \"John Doe\",", "    });", "", "    AuthService.login.mockRejectedValueOnce(new Error(\"Invalid credentials.\"));", "    try {", "      await AuthService.login();", "    } catch (error) {", "      expect(error).toEqual(new Error(\"Invalid credentials.\"));", "    }", "  });", "});", "", "describe(\"fetching user's profile multiple times.\", () => {});"], "file_path": "jest-documentaion/13.assignment.test.js"}
{"Link_to_commit": "https://github.com/plantinformatics/pretzel-data/commit/0c9769caa6fc01c7c005f0384efd97516efe82b6", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 61, "n_files_impacted": 2, "longest_chunk": ["# generated by ChatGPT in this dialog : https://chatgpt.com/share/67ac97da-67dc-800e-b3d9-083b66d6a187", "", "import pandas as pd", "", "def process_supplementary_table(input_file, output_file):", "    # Read the input Excel file, skipping the first two rows", "    df = pd.read_excel(input_file, sheet_name='Suppl. Table S1', skiprows=2)", "    ", "    # Remove trailing empty rows (rows where the first column is empty)", "    df = df.dropna(subset=['Chromosome'])", "    ", "    # Define base columns", "    base_columns = ['Chromosome', 'Start', 'End', 'qseq', 'gene_pep', 'Gap', 'CDS', 'gene']", "    ", "    # Extract base data", "    result_rows = []", "    for idx, row in df.iterrows():", "        base_row = {col: row[col] for col in base_columns}", "        base_row['Name'] = row['gene_pep']  # Rename gene_pep to Name", "        del base_row['gene_pep']  # Remove original gene_pep column", "        base_row['evalue'] = ''  # Empty for base row", "        base_row['gap'] = ''  # Empty for base row", "        result_rows.append(base_row)", "", "        # Process group columns in sets of 4", "        for i in range(8, len(df.columns), 4):", "            group_cols = df.columns[i:i+4]", "            if len(group_cols) < 4:", "                continue  # Skip incomplete groups", "            ", "            name_suffix = group_cols[0].rsplit('_', 1)[0]  # Extract name suffix (e.g., '1_BLO90_6_2')", "            transformed_row = {", "                'Chromosome': row['Chromosome'],", "                'Start': row[group_cols[0]],  # Use *_sstart for Start", "                'End': row[group_cols[1]],  # Use *_send for End", "                'qseq': '',  # Empty for transformed row", "                'Name': f\"{row['gene_pep']}_{name_suffix}\",", "                'Gap': '',  # Empty for transformed row", "                'CDS': '',  # Empty for transformed row", "                'gene': '',  # Empty for transformed row", "                'evalue': row[group_cols[2]],", "                'gap': row[group_cols[3]]", "            }", "            result_rows.append(transformed_row)", "    ", "    # Convert transformed data to DataFrame", "    df_result = pd.DataFrame(result_rows)", "    ", "    # Write output to Excel with two worksheets", "    with pd.ExcelWriter(output_file) as writer:", "        df_result.to_excel(writer, sheet_name='Alignment| Suppl_Table_S1', index=False)", "        ", "        # Create Metadata sheet", "        metadata = pd.DataFrame({", "            'Field': ['Crop', 'parentName', 'Reference', 'DOI'],", "            'Alignment| Suppl_Table_S1': ['Wheat', 'Wheat_CSv2.1_Genes-HC', 'Int. J. Mol. Sci. 2024, 25(16), 8614', 'https://doi.org/10.3390/ijms25168614']", "        })", "        metadata.to_excel(writer, sheet_name='Metadata', index=False)", "", "# Example usage", "process_supplementary_table('supplementary-tables.xlsx', 'processed_output.xlsx')"], "file_path": "examples/ijms-25-08614/Suppl_Table_S2.py"}
{"Link_to_commit": "https://github.com/amanxsyed/Python-DSA/commit/93b6387c388a44d0a27851d87bfef271472e49e4", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 39, "n_files_impacted": 1, "longest_chunk": ["", "", "", "", "", "", "", "", "#Sol:2", "# Below code seems to be simplified and generate by chatgpt", "# def singleNonDuplicate(nums):", "#     # Initialize start and end pointers", "#     start = 0", "#     end = len(nums) - 1", "", "#     # Binary search", "#     while start < end:", "#         # Find the mid index", "#         mid = start + (end - start) // 2", "", "#         # Ensure mid is even for proper pair checking", "#         if mid % 2 == 1:", "#             mid -= 1", "", "#         # Check the pair condition", "#         if nums[mid] == nums[mid + 1]:", "#             # If mid and mid+1 are equal, single element is in the right half", "#             start = mid + 2", "#         else:", "#             # Otherwise, the single element is in the left half", "#             end = mid", "", "#     # When start == end, the single element is found", "#     return nums[start]", "", "# ", "# nums = [1, 1, 3, 3, 4, 4, 5, 8, 8]", "# print(\"Single element:\", singleNonDuplicate(nums))", ""], "file_path": "Arrays-Lists/Binary-Search/BS_4_Single_Element_in_Sorted_Array.py"}
{"Link_to_commit": "https://github.com/yuchung0823/git_test/commit/f47ebfed9d727740036d2dab06ba47ccb1a93c4f", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 122, "n_files_impacted": 1, "longest_chunk": ["    def _parse_filename(self, filename):\r", "        \"\"\"\r", "        Parse the filename into case name, file type, and time.\r", "        :param filename: The NetCDF filename (e.g., pbl_ctl.C.Surface-000235.nc)\r", "        :return: (case_name, file_type, time)\r", "        \"\"\"\r", "        match = re.match(r\"^([^.]+)\\.([^.]+(?:\\.[^.]+)*)-(\\d+)\\.nc$\", filename)\r", "        if match:\r", "            case_name, file_type, time_str = match.groups()\r", "            return case_name, file_type, time_str\r", "        else:\r", "            raise ValueError(f\"Filename '{filename}' does not match the expected pattern.\")\r", "    \r", "    def _scan_files(self):\r", "        \"\"\"\r", "        Scans the directory and organizes files based on case, file type, and time.\r", "        :return: A dictionary with structure {case_name: {file_type: {time: file_path}}}\r", "        \"\"\"\r", "        file_dict = {}\r", "        \r", "        for file in os.listdir(self.directory):\r", "            if file.endswith(\".nc\"):\r", "                try:\r", "                    case_name, file_type, time_str = self._parse_filename(file)\r", "                    \r", "                    # Organize the files in a nested dictionary\r", "                    if case_name not in file_dict:\r", "                        file_dict[case_name] = {}\r", "                    if file_type not in file_dict[case_name]:\r", "                        file_dict[case_name][file_type] = {}\r", "                    \r", "                    # Store the file path based on the time\r", "                    file_dict[case_name][file_type][time_str] = os.path.join(self.directory, file)\r", "                except ValueError as e:\r", "                    print(f\"Skipping file '{file}': {e}\")\r", "        \r", "        return file_dict\r", "\r", "    def _find_variable_file_relationship(self):\r", "        \"\"\"\r", "        Reads the first available file for each file type (e.g., t=0 files) to find the relationship between variables and file types.\r", "        :return: A dictionary where keys are variable names and values are the file types that contain the variables.\r", "        \"\"\"\r", "        var_file_map = {}\r", "\r", "        # Loop through each file type and extract the variables from a t=0 file\r", "        for case_name, file_types in self.files.items():\r", "            for file_type, time_files in file_types.items():\r", "                # Get any file for t=0 or the first time step\r", "                t0_file = next(iter(time_files.values()))\r", "                \r", "                with Dataset(t0_file, 'r') as nc_file:\r", "                    variables = list(nc_file.variables.keys())\r", "                    for var in variables:\r", "                        # Assign the file type to each variable\r", "                        var_file_map[var] = file_type\r", "        \r", "        return var_file_map\r", "\r", "    def get_variable(self, case_name, variable_name, time):\r", "        \"\"\"\r", "        Finds the appropriate file and extracts the variable's data, allowing time to be input as an integer.\r", "        \r", "        :param case_name: Case name of the experiment (e.g., 'pbl_ctl').\r", "        :param variable_name: Name of the variable to extract.\r", "        :param time: Time step as either an integer or a string (e.g., 235 or '000235').\r", "        :return: A numpy array containing the variable's data.\r", "        \"\"\"\r", "        # Convert integer time to zero-padded string if needed\r", "        if isinstance(time, int):\r", "            time = f\"{time:06d}\"  # Convert to a 6-character zero-padded string\r", "        elif isinstance(time, str) and len(time) != 6:\r", "            raise ValueError(\"Time string must be exactly 6 characters long.\")\r", "        \r", "        # Find the file type based on the variable name\r", "        var_type = self.var_file_map.get(variable_name)\r", "        if var_type is None:\r", "            raise ValueError(f\"Variable '{variable_name}' not found in any file type.\")\r", "        \r", "        try:\r", "            file_path = self.files[case_name][var_type][time]\r", "            with Dataset(file_path, 'r') as nc_file:\r", "                # Extract the variable as a numpy array\r", "                variable_data = np.array(nc_file.variables[variable_name][:])\r", "            return variable_data\r", "        except KeyError:\r", "            raise FileNotFoundError(f\"File for case '{case_name}', variable type '{var_type}', and time '{time}' not found.\")\r", "        except Exception as e:\r", "            raise RuntimeError(f\"An error occurred while loading the variable '{variable_name}': {e}\")\r", "\r", "    def get_variable_file_type(self, variable_name):\r", "        \"\"\"\r", "        Returns the file type (e.g., C.Surface, L.Dynamic) that contains the specified variable.\r", "        \r", "        :param variable_name: The name of the variable whose file type is to be found.\r", "        :return: The file type as a string.\r", "        \"\"\"\r", "        file_type = self.var_file_map.get(variable_name)\r", "        if file_type is None:\r", "            raise ValueError(f\"Variable '{variable_name}' not found in any file type.\")\r", "        return file_type\r", "\r", "    def get_all_variables_and_file_types(self):\r", "        \"\"\"\r", "        Prints all variables and their corresponding file types.\r", "        \"\"\"\r", "        if not self.var_file_map:\r", "            print(\"No variables found. Please ensure the files are properly loaded.\")\r", "            return\r", "\r", "        print(\"Variable Name -> File Type\")\r", "        print(\"===========================\")\r", "        for var_name, file_type in self.var_file_map.items():\r", "            print(f\"{var_name} -> {file_type}\")    \r", "\r", "if __name__ == \"__main__\":\r", "    loader = VVM_tools(\"/data/chung0823/VVM_cloud_dynamics_2024/DATA/pbl_ctl/archive/\")\r", "    loader.get_all_variables_and_file_types()\r", "    \r", "    # Example: Get a variable data using integer time\r", "    data = loader.get_variable(\"pbl_ctl\", \"th\", 235)  # Make sure \"temperature\" is a valid variable name\r", "    print(data)"], "file_path": "VVM_tools.py"}
{"Link_to_commit": "https://github.com/arnokoehler/battleship-frontend/commit/1f578641143d41fe281647ed1faea309d272784e", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 88, "n_files_impacted": 12, "longest_chunk": ["    const [games, setGames] = useState([]);", "    const [gameUpdates, setGameUpdates] = useState(null);", "", "    useEffect(() => {", "        // Fetch games from the backend", "        axiosBackend.get('/games')", "            .then(response => setGames(response.data))", "            .catch(error => console.error('Error fetching games:', error));", "    }, []);", "", "    const createGame = () => {", "        axiosBackend.post('/games')", "            .then(() => {", "                // Refresh the games list after creating a new game", "                axiosBackend.get('/games').then(response => setGames(response.data));", "            })", "            .catch(error => console.error('Error creating game:', error));", "    };", "", "    const joinGame = (gameId, player) => {", "        // Using EventSource to listen for updates", "        const eventSource = new EventSource(`http://localhost:8080/games/${gameId}/players/${player}`);", "", "        eventSource.onmessage = (event) => {", "            const data = JSON.parse(event.data);", "            setGameUpdates(data); // Update game data in real-time", "        };", "", "        eventSource.onerror = (error) => {", "            console.error('Error with SSE:', error);", "            eventSource.close(); // Close the connection if there's an error", "        };", "", "        // Clean up when the component is unmounted", "        return () => {", "            eventSource.close();", "        };", "    };", "", "    return (", "        <div style={{ padding: '20px' }}>", "            <h1>Battleship Games</h1>", "", "            <Button", "                variant=\"contained\"", "                color=\"primary\"", "                onClick={createGame}", "                startIcon={<DirectionsBoatIcon />}", "            >", "                Create New Game", "            </Button>", "", "            <Grid2 container spacing={3} style={{ marginTop: '20px' }}>", "                {games.map((game, index) => (", "                    <Grid2 item xs={12} sm={6} md={4} key={index}>", "                        <div style={{ border: '1px solid black', padding: '10px' }}>", "                            <h2>Game #{index + 1}</h2>", "                            <p>Status: {game.status}</p>", "                            <p>Turn: Player {game.turn}</p>", "                            <Button", "                                variant=\"outlined\"", "                                color=\"secondary\"", "                                onClick={() => joinGame(index + 1, 'A')}", "                            >", "                                Join as Player A", "                            </Button>", "                            <Button", "                                variant=\"outlined\"", "                                color=\"secondary\"", "                                onClick={() => joinGame(index + 1, 'B')}", "                            >", "                                Join as Player B", "                            </Button>", "                        </div>", "                    </Grid2>", "                ))}", "            </Grid2>", "", "            {gameUpdates && (", "                <div style={{ marginTop: '20px' }}>", "                    <h3>Game Updates</h3>", "                    <p>Status: {gameUpdates.status}</p>", "                    <p>Turn: Player {gameUpdates.turn}</p>", "                    {gameUpdates.winner && <p>Winner: Player {gameUpdates.winner}</p>}", "                </div>", "            )}", "        </div>", "    );"], "file_path": "src/App.js"}
{"Link_to_commit": "https://github.com/toroso/fejka/commit/565818743162b3ab3abeb4c10443586321e38eb0", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 65, "n_files_impacted": 16, "longest_chunk": ["using System;", "using System.Diagnostics.CodeAnalysis;", "using System.Threading.Tasks;", "using FakeRepo.Test.GuidDtoTests.AutomationLayer;", "using FluentAssertions;", "using NUnit.Framework;", "", "namespace FakeRepo.Test.GuidDtoTests;", "", "[TestFixture]", "[SuppressMessage(\"ReSharper\", \"MethodHasAsyncOverload\")]", "public class UpsertTests : UserRepositoryTestsBase", "{", "    [Test]", "    public async Task Given_NewUser_When_Upsert_Then_UserIsInserted()", "    {", "        var user = DomainBuilder.Create();", "", "        await UpsertAsync(user);", "", "        GetById(user.Id).Should().BeEquivalentTo(user);", "    }", "", "    [Test]", "    public async Task Given_ExistingUser_When_Upsert_Then_UserIsUpdated()", "    {", "        var user = Add(e => e.Name = \"John Doe\");", "", "        user.Name = \"John Smith\";", "        await UpsertAsync(user);", "", "        GetById(user.Id).Name.Should().Be(\"John Smith\");", "    }", "", "    [Test]", "    public async Task Given_ExistingUserWithNewId_When_Upsert_Then_UserIsInsertedAndOldIdIsNotUpdated()", "    {", "        var user = Add(e => e.Name = \"John Doe\");", "", "        var newId = Guid.NewGuid();", "        user.Id = newId;", "", "        await UpsertAsync(user);", "", "        GetById(newId).Should().BeEquivalentTo(user);", "        GetById(user.Id).Should().NotBeNull();", "    }", "", "    [Test]", "    public async Task Given_NullUser_When_Upsert_Then_ThrowsArgumentNullException()", "    {", "        await InvokingUpsertAsync(null).Should().ThrowAsync<ArgumentNullException>();", "    }", "", "    [Test]", "    public async Task Given_UserWithNullFields_When_Upsert_Then_UserWithNullFieldsIsInsertedOrUpdatedCorrectly()", "    {", "        var user = Add(e => e.Name = \"John Doe\");", "", "        user.Name = null;", "        await UpsertAsync(user);", "", "        GetById(user.Id).Name.Should().BeNull();", "    }", "}"], "file_path": "FakeRepo.Test/GuidDtoTests/UserRepositoryTestsBase.cs"}
{"Link_to_commit": "https://github.com/rennelongcoy/tic-tac-toe/commit/f12bb1ba743fe7c74785f169eab7282000d38004", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 31, "n_files_impacted": 3, "longest_chunk": ["<!DOCTYPE html>", "<html lang=\"en\">", "<head>", "    <meta charset=\"UTF-8\">", "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">", "    <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">", "    <title>Tic Tac Toe</title>", "    <link rel=\"stylesheet\" href=\"style.css\">", "</head>", "<body>", "    <div class=\"game-container\">", "        <h1>Tic Tac Toe</h1>", "        <div class=\"game-board\">", "            <div class=\"cell\" data-index=\"0\"></div>", "            <div class=\"cell\" data-index=\"1\"></div>", "            <div class=\"cell\" data-index=\"2\"></div>", "            <div class=\"cell\" data-index=\"3\"></div>", "            <div class=\"cell\" data-index=\"4\"></div>", "            <div class=\"cell\" data-index=\"5\"></div>", "            <div class=\"cell\" data-index=\"6\"></div>", "            <div class=\"cell\" data-index=\"7\"></div>", "            <div class=\"cell\" data-index=\"8\"></div>", "        </div>", "        <div class=\"game-info\">", "            <p class=\"status\"></p>", "            <button id=\"restartButton\">Restart Game</button>", "        </div>", "    </div>", "    <script src=\"script.js\"></script>", "</body>", "</html>"], "file_path": "script.js"}
{"Link_to_commit": "https://github.com/SiEPIC/SiEPIC-Tools/commit/8a7275280ac6db355e2d5d3d6aa235f594466fe8", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 60, "n_files_impacted": 4, "longest_chunk": ["        if verbose:", "            print('%s, %s: %s' % (instanceB.parent_cell, instanceB.parent_cell.parent_cells(), ''))", "                        ", "        def find_parents(layout, target_cell):", "            \"\"\"", "            Find all parent cells that instantiate the target cell.", "            \"\"\"", "            parents = []", "            for cell in layout.each_cell():", "                for inst in cell.each_inst():", "                    if inst.cell == target_cell:", "                        parents.append(cell)", "                        break", "            return parents", "", "        def trace_hierarchy_up(layout, bottom_cell, path=None):", "            \"\"\"", "            Recursively trace the hierarchy upwards from the bottom cell to the top cell.", "            \"\"\"", "            if path is None:", "                path = [bottom_cell]", "", "            parents = find_parents(layout, bottom_cell)", "            if not parents:", "                # No parents found, we've reached the top cell", "                return [path]", "", "            all_paths = []", "            for parent in parents:", "                # Recursively trace each parent", "                new_path = [parent] + path", "                all_paths.extend(trace_hierarchy_up(layout, parent, new_path))", "", "            return all_paths", "", "        def trace_hierarchy_up_single(layout, bottom_cell, path=None):", "            \"\"\"", "            Recursively trace the hierarchy upwards from the bottom cell to the top cell.", "            Returns a single path as a list of Cell objects.", "            \"\"\"", "            if path is None:", "                path = [bottom_cell]", "", "            parents = find_parents(layout, bottom_cell)", "            if not parents:", "                # No parents found, we've reached the top cell", "                return path[::-1]  # Reverse the path to start from the top cell", "", "            if len(parents) > 1:", "                raise ValueError(f\"Cell '{bottom_cell.name}' has multiple parent instances. Use the multi-path version.\")", "", "            # If only one parent, continue tracing", "            return trace_hierarchy_up_single(layout, parents[0], path + [parents[0]])", "", "        parentsA = trace_hierarchy_up_single(ly, instanceA.parent_cell)", "        parentsB = trace_hierarchy_up_single(ly, instanceB.parent_cell)", "        if verbose:", "            print(\" -> \".join(cell.name for cell in parentsA))", "            print(\" -> \".join(cell.name for cell in parentsB))", "    "], "file_path": "klayout_dot_config/python/SiEPIC/scripts.py"}
{"Link_to_commit": "https://github.com/blainefreestone/fpga_neural_network_accelerator/commit/41a259c5c10b2b04592653b316661247cf3bbb49", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 170, "n_files_impacted": 1, "longest_chunk": ["import argparse", "from decimal_to_fixed import decimal_to_fixed_point", "from fixed_to_decimal import fixed_point_to_decimal", "", "def fixed_point_add_hw(bin_a, bin_b, int_bits, frac_bits, signed=True):", "    \"\"\"", "    Add two fixed-point binary numbers as it would happen in hardware.", "    ", "    :param bin_a: First binary string (without decimal point)", "    :param bin_b: Second binary string (without decimal point)", "    :param int_bits: Number of integer bits", "    :param frac_bits: Number of fractional bits", "    :param signed: Whether numbers are signed", "    :return: Result as a binary string", "    \"\"\"", "    total_bits = int_bits + frac_bits", "    ", "    # Convert binary strings to integers", "    int_a = int(bin_a, 2)", "    int_b = int(bin_b, 2)", "    ", "    # Perform binary addition", "    int_result = int_a + int_b", "    ", "    # Handle overflow in a hardware-like way (truncate to total bits)", "    int_result = int_result & ((1 << total_bits) - 1)", "    ", "    # Convert back to binary string with fixed width", "    binary_result = format(int_result, f'0{total_bits}b')", "    ", "    return binary_result", "", "def fixed_point_multiply_hw(bin_a, bin_b, int_bits, frac_bits, signed=True):", "    \"\"\"", "    Multiply two fixed-point binary numbers as it would happen in hardware.", "    ", "    :param bin_a: First binary string (without decimal point)", "    :param bin_b: Second binary string (without decimal point)", "    :param int_bits: Number of integer bits", "    :param frac_bits: Number of fractional bits", "    :param signed: Whether numbers are signed", "    :return: Result as a binary string", "    \"\"\"", "    total_bits = int_bits + frac_bits", "    ", "    # Convert binary strings to integers", "    int_a = int(bin_a, 2)", "    int_b = int(bin_b, 2)", "    ", "    # Handle signed multiplication if needed", "    if signed:", "        # Check if negative (MSB is 1)", "        if bin_a[0] == '1':", "            int_a = int_a - (1 << total_bits)", "        if bin_b[0] == '1':", "            int_b = int_b - (1 << total_bits)", "    ", "    # Perform binary multiplication and adjust for fixed point", "    full_result = int_a * int_b", "    ", "    # Scale result back (divide by 2^frac_bits)", "    scaled_result = full_result >> frac_bits", "    ", "    # Truncate to fit in the original format", "    masked_result = scaled_result & ((1 << total_bits) - 1)", "    ", "    # Convert back to binary string with fixed width", "    binary_result = format(masked_result, f'0{total_bits}b')", "    ", "    return binary_result", "", "def main():", "    parser = argparse.ArgumentParser(description=\"Perform hardware-simulated fixed-point arithmetic operations.\")", "    parser.add_argument(\"operation\", choices=[\"add\", \"multiply\"], help=\"Arithmetic operation to perform\")", "    parser.add_argument(\"--value1\", type=str, help=\"First binary value or decimal number if --decimal flag is used\")", "    parser.add_argument(\"--value2\", type=str, help=\"Second binary value or decimal number if --decimal flag is used\")", "    parser.add_argument(\"--int_bits\", type=int, help=\"Number of integer bits\")", "    parser.add_argument(\"--frac_bits\", type=int, help=\"Number of fractional bits\")", "    parser.add_argument(\"--signed\", action=\"store_true\", help=\"Whether numbers are signed (default: True)\")", "    parser.add_argument(\"--decimal\", action=\"store_true\", help=\"Interpret input values as decimal numbers\")", "    ", "    args = parser.parse_args()", "    ", "    try:", "        total_bits = args.int_bits + args.frac_bits", "        ", "        if args.decimal:", "            # Convert decimal inputs to fixed-point", "            val1_dec = float(args.value1)", "            val2_dec = float(args.value2)", "            ", "            bin1, actual1 = decimal_to_fixed_point(val1_dec, args.int_bits, args.frac_bits, args.signed)", "            bin2, actual2 = decimal_to_fixed_point(val2_dec, args.int_bits, args.frac_bits, args.signed)", "            ", "            # Remove decimal points", "            bin1 = bin1.replace('.', '')", "            bin2 = bin2.replace('.', '')", "            ", "            # Calculate expected result using decimal math", "            if args.operation == \"add\":", "                exact_dec_result = val1_dec + val2_dec", "                operation_name = \"Addition\"", "            else:  # multiply", "                exact_dec_result = val1_dec * val2_dec", "                operation_name = \"Multiplication\"", "                ", "            print(f\"Decimal input 1: {val1_dec}\")", "            print(f\"    \u21aa Fixed-point representation: {bin1[:args.int_bits]}.{bin1[args.int_bits:]}\")", "            print(f\"    \u21aa Actual value after conversion: {actual1}\")", "            ", "            print(f\"Decimal input 2: {val2_dec}\")", "            print(f\"    \u21aa Fixed-point representation: {bin2[:args.int_bits]}.{bin2[args.int_bits:]}\")", "            print(f\"    \u21aa Actual value after conversion: {actual2}\")", "            ", "        else:", "            # Use binary inputs directly", "            bin1 = args.value1", "            bin2 = args.value2", "            ", "            # Verify binary string lengths", "            if len(bin1) != total_bits or len(bin2) != total_bits:", "                raise ValueError(f\"Binary strings must be exactly {total_bits} bits long\")", "            ", "            # Convert to decimal for display", "            val1_dec = fixed_point_to_decimal(bin1, args.int_bits, args.frac_bits, args.signed)", "            val2_dec = fixed_point_to_decimal(bin2, args.int_bits, args.frac_bits, args.signed)", "            ", "            # Calculate expected result using decimal math", "            if args.operation == \"add\":", "                exact_dec_result = val1_dec + val2_dec", "                operation_name = \"Addition\"", "            else:  # multiply", "                exact_dec_result = val1_dec * val2_dec", "                operation_name = \"Multiplication\"", "                ", "            print(f\"Binary input 1: {bin1[:args.int_bits]}.{bin1[args.int_bits:]}\")", "            print(f\"    \u21aa Decimal value: {val1_dec}\")", "            ", "            print(f\"Binary input 2: {bin2[:args.int_bits]}.{bin2[args.int_bits:]}\")", "            print(f\"    \u21aa Decimal value: {val2_dec}\")", "        ", "        # Show the mathematically exact result", "        print(f\"\\nMathematically exact {args.operation} result: {exact_dec_result}\")", "        ", "        # Perform hardware-simulated fixed-point operation", "        if args.operation == \"add\":", "            result_bin = fixed_point_add_hw(bin1, bin2, args.int_bits, args.frac_bits, args.signed)", "        else:  # multiply", "            result_bin = fixed_point_multiply_hw(bin1, bin2, args.int_bits, args.frac_bits, args.signed)", "        ", "        # Convert the fixed-point result back to decimal", "        hw_result_dec = fixed_point_to_decimal(result_bin, args.int_bits, args.frac_bits, args.signed)", "        ", "        # Format the result for display with the binary point", "        result_with_point = f\"{result_bin[:args.int_bits]}.{result_bin[args.int_bits:]}\"", "        ", "        print(f\"\\nHardware fixed-point {args.operation} result:\")", "        print(f\"    \u21aa Binary: {result_with_point}\")", "        print(f\"    \u21aa Decimal interpretation: {hw_result_dec}\")", "        ", "        # Show the difference", "        print(f\"\\nDifference between exact and hardware result: {hw_result_dec - exact_dec_result}\")", "        ", "    except ValueError as e:", "        print(f\"Error: {e}\")", "    except Exception as e:", "        print(f\"Unexpected error: {e}\")", "", "if __name__ == \"__main__\":", "    main()"], "file_path": "scripts/fixed_arithmetic.py"}
{"Link_to_commit": "https://github.com/johnlinp/n8n-nodes-bookstore/commit/6abf27c7d4ce9e8939d334f49709449cb87e1013", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 6, "longest_chunk": ["const path = require('path');", "const { task, src, dest } = require('gulp');", "", "task('build:icons', copyIcons);", "", "function copyIcons() {", "\tconst nodeSource = path.resolve('nodes', '**', '*.{png,svg}');", "\tconst nodeDestination = path.resolve('dist', 'nodes');", "", "\tsrc(nodeSource).pipe(dest(nodeDestination));", "", "\tconst credSource = path.resolve('credentials', '**', '*.{png,svg}');", "\tconst credDestination = path.resolve('dist', 'credentials');", "", "\treturn src(credSource).pipe(dest(credDestination));", "}"], "file_path": "nodes/Bookstore/Bookstore.node.ts"}
{"Link_to_commit": "https://github.com/PascalGitz/engicalc/commit/ad80247144c89efdda16b472dcc6177d466a8f6f", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 29, "n_files_impacted": 1, "longest_chunk": ["from setuptools import setup, find_packages", "", "setup(", "    name='engicalc',", "    version='0.1.0',", "    packages=find_packages(),", "    install_requires=[", "        # List your package dependencies here", "        # e.g. 'numpy', 'pandas',", "    ],", "    entry_points={", "        'console_scripts': [", "            # Add command-line scripts here if needed", "            # e.g. 'engicalc=engicalc.main:main',", "        ],", "    },", "    author='Pascal Gitz',", "    author_email='pascal.gitz@hotmail.ch',", "    description='A Python package for engineering calculations and Jupyter cell outputs',", "    long_description=open('README.md').read(),", "    long_description_content_type='text/markdown',", "    url='https://github.com/PascalGitz/engicalc',  # Replace with your package's URL", "    classifiers=[", "        'Programming Language :: Python :: 3',", "        'License :: OSI Approved :: MIT License',", "        'Operating System :: OS Independent',", "    ],", "    python_requires='>=3.6',", ")"], "file_path": "setup.py"}
{"Link_to_commit": "https://github.com/d60/picwish/commit/c1732e51c7f5ebe5dec93af4c7d2592d980ce862", "n-gram matched": "generated by chatgpt", "n_lines_longer_change": 20, "n_files_impacted": 3, "longest_chunk": ["    \"\"\"", "    Represents the signature for OSS.", "", "    :param access_key_id: The access key ID for OSS.", "    :type access_key_id: str", "    :param access_key_secret: The access key secret for OSS.", "    :type access_key_secret: str", "    :param verb: The HTTP method.", "    :type verb: str", "    :param content_md5: The MD5 hash of the content.", "    :type content_md5: str", "    :param headers: The headers to include in the request.", "    :type headers: dict", "    :param bucket: The OSS bucket name.", "    :type bucket: str", "    :param object: The OSS object key.", "    :type object: str", "    :param sub_resources: A list of sub resources for the request.", "    :type sub_resources: list", "    \"\"\""], "file_path": "picwish/signature.py"}
{"Link_to_commit": "https://github.com/DuwuOps/minitwit/commit/3ef949a58801bebbe30b9ad03b0edbcf3ec80bf9", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 5, "n_files_impacted": 1, "longest_chunk": ["", "func formatDatetime(timestamp int64) string {", "\ttime := time.Unix(timestamp, 0).UTC()", "\treturn time.Format(\"2006-01-02 @ 15:04\")", "}"], "file_path": "src/template_rendering/template_helpers.go"}
{"Link_to_commit": "https://github.com/Only-Smiles/DevOps-2025/commit/817cc4c8fb5310d49cf7e4d3c5e235e6ae2754e7", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 18, "n_files_impacted": 6, "longest_chunk": ["def init_db():", "    \"\"\"Creates the database tables.\"\"\"", "    with open(SCHEMA, \"r\") as fp:", "            schema = fp.read()", "    with psycopg.connect(DATABASE) as con:", "        with con.cursor() as cursor:", "            for statement in schema.split(\";\"):", "                if statement.strip():  # Avoid empty statements", "                    cursor.execute(statement)", "            con.commit()", "", "def reset_db():", "    \"\"\"Empty the database and initialize the schema again\"\"\"", "    with psycopg.connect(DATABASE) as con:", "        with con.cursor() as cursor:", "            cursor.execute(\"DROP SCHEMA public CASCADE;\")", "            cursor.execute(\"CREATE SCHEMA public;\")  # Resets schema instead of dropping tables one by one", "            con.commit()"], "file_path": "src/test/conftest.py"}
{"Link_to_commit": "https://github.com/DuwuOps/minitwit/commit/9fc8cfe08ccde7c8710abc3a92bd827e14dd97dc", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 22, "n_files_impacted": 2, "longest_chunk": ["\tloggedIn, _ := isUserLoggedIn(c)", "    if !loggedIn {", "        c.String(http.StatusUnauthorized, \"Unauthorized\")", "    }", "\ttext := c.FormValue(\"text\")", "\tuserId, err := getSessionUserID(c)", "\tif err != nil {", "\t\tfmt.Printf(\"getSessionUserID returned error: %v\\n\", err)", "\t\treturn err", "\t}", "\t", "\tDb.Exec(`insert into message (author_id, text, pub_date, flagged)", "\t\t\t values (?, ?, ?, 0)`,", "\t\t\t userId, text, time.Now().Unix(),", "\t)", "", "\terr = addFlash(c, \"Your message was recorded\")", "\tif err != nil {", "\t\tfmt.Printf(\"addFlash returned error: %v\\n\", err)", "\t}", "", "\treturn c.Redirect(http.StatusFound, \"/\")"], "file_path": "minitwit.go"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP7/Chirp/commit/c2d4a61bfa6ab5234ae49413b773002134c78644", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 54, "n_files_impacted": 1, "longest_chunk": ["", "    [Fact]", "    public async Task RegisterNewUserTest()", "    {", "        // Arrange", "        var client = _factory.CreateClient();", "        ", "        var getResponse = await client.GetAsync(\"/Identity/Account/Register\");", "        var getContent = await getResponse.Content.ReadAsStringAsync();", "        ", "", "        // Step 2: Parse the anti-forgery token from the page content", "        var tokenValue = ExtractAntiForgeryToken(getContent);", "        ", "        var registerData = new Dictionary<string, string>", "        {", "            {\"Input.Email\",\"testuser@gmail.com\"},", "            {\"Input.Password\",\"Test@12345\"}, ", "            {\"Input.ConfirmPassword\",\"Test@12345\"},", "            { \"__RequestVerificationToken\", tokenValue},", "            {\"returnUrl\",\"/\"}", "        };", "        ", "        // Act", "        var response = await client.PostAsync(\"/Identity/Account/Register\", new FormUrlEncodedContent(registerData));", "        ", "        // Assert", "        response.StatusCode.Should().Be(HttpStatusCode.OK); // Expecting HTTP 200", "        var responseBody = await response.Content.ReadAsStringAsync();", "        responseBody.Should().Contain(\"Register confirmation\"); ", "    }", "    ", "    [Fact]", "    public async Task LoginUserTest()", "    {", "        // Arange ", "        //var client = _factory.CreateClient();", "        ", "        //var getResponse = await client.GetAsync(\"/Identity/Account/Login\");", "        //var ", "    }", "    ", "    // Helper method to extract anti forgery token", "    private string ExtractAntiForgeryToken(string htmlContent)", "    {", "        // Updated regex pattern for finding the anti-forgery token value", "        var match = Regex.Match(htmlContent, @\"<input[^>]*name=\"\"__RequestVerificationToken\"\"[^>]*value=\"\"([^\"\"]+)\"\"\", RegexOptions.IgnoreCase);", "        if (!match.Success)", "        {", "            throw new InvalidOperationException(\"Anti-forgery token not found\");", "        }", "        return match.Groups[1].Value;", "    }", ""], "file_path": "test/Chirp.Test/E2ETests.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP13/Chirp/commit/a7d6fc8d8fef1002bb456ec6a82a30cbe6dbe406", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 9, "n_files_impacted": 4, "longest_chunk": ["/// <summary>", "/// Represents the page model for displaying the user's timeline and handling account actions like \"Forget Me.\"", "/// </summary>", "/// <remarks>", "/// Initializes a new instance of the <see cref=\"MyPage\"/> class.", "/// </remarks>", "/// <param name=\"cheepService\">The service for managing Cheep-related operations.</param>", "/// <param name=\"signInManager\">The service for managing sign-in actions.</param>", "/// <param name=\"userManager\">The service for managing user-related actions.</param>"], "file_path": "src/Chirp.Web/models/MyPage.cshtml.cs"}
{"Link_to_commit": "https://github.com/RasmusAChr/Chirp/commit/c2d4a61bfa6ab5234ae49413b773002134c78644", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 54, "n_files_impacted": 1, "longest_chunk": ["", "    [Fact]", "    public async Task RegisterNewUserTest()", "    {", "        // Arrange", "        var client = _factory.CreateClient();", "        ", "        var getResponse = await client.GetAsync(\"/Identity/Account/Register\");", "        var getContent = await getResponse.Content.ReadAsStringAsync();", "        ", "", "        // Step 2: Parse the anti-forgery token from the page content", "        var tokenValue = ExtractAntiForgeryToken(getContent);", "        ", "        var registerData = new Dictionary<string, string>", "        {", "            {\"Input.Email\",\"testuser@gmail.com\"},", "            {\"Input.Password\",\"Test@12345\"}, ", "            {\"Input.ConfirmPassword\",\"Test@12345\"},", "            { \"__RequestVerificationToken\", tokenValue},", "            {\"returnUrl\",\"/\"}", "        };", "        ", "        // Act", "        var response = await client.PostAsync(\"/Identity/Account/Register\", new FormUrlEncodedContent(registerData));", "        ", "        // Assert", "        response.StatusCode.Should().Be(HttpStatusCode.OK); // Expecting HTTP 200", "        var responseBody = await response.Content.ReadAsStringAsync();", "        responseBody.Should().Contain(\"Register confirmation\"); ", "    }", "    ", "    [Fact]", "    public async Task LoginUserTest()", "    {", "        // Arange ", "        //var client = _factory.CreateClient();", "        ", "        //var getResponse = await client.GetAsync(\"/Identity/Account/Login\");", "        //var ", "    }", "    ", "    // Helper method to extract anti forgery token", "    private string ExtractAntiForgeryToken(string htmlContent)", "    {", "        // Updated regex pattern for finding the anti-forgery token value", "        var match = Regex.Match(htmlContent, @\"<input[^>]*name=\"\"__RequestVerificationToken\"\"[^>]*value=\"\"([^\"\"]+)\"\"\", RegexOptions.IgnoreCase);", "        if (!match.Success)", "        {", "            throw new InvalidOperationException(\"Anti-forgery token not found\");", "        }", "        return match.Groups[1].Value;", "    }", ""], "file_path": "test/Chirp.Test/E2ETests.cs"}
{"Link_to_commit": "https://github.com/ITU-DevOps2025-GROUP-A/itu-minitwit/commit/a463e5f4dc33de5a44ed87d3cda1cd13bb4c8373", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 22, "n_files_impacted": 1, "longest_chunk": ["    [Fact]", "    public async Task PostMessage_CreatesMessageSuccessfully()", "    {", "        // Arrange", "        var context = fixture.GetDbContext(); // This should return a properly set up in-memory context", "", "        var user = new User { Username = \"Man\", Email = \"Man@test.com\", PwHash = \"hashedpassword\" };", "        await context.Users.AddAsync(user);", "        await context.SaveChangesAsync();", "", "        var content = \"Hello from Man\";", "        ", "        // Act", "        var response = await client.PostAsync(\"/msgs/Man\", new FormUrlEncodedContent(new[] { new KeyValuePair<string, string>(\"content\", content) }));", "", "        // Assert", "        response.StatusCode.Should().Be(HttpStatusCode.NoContent); // Expecting 204 No Content", "        var savedMessage = await context.Messages.SingleOrDefaultAsync(m => m.AuthorId == user.UserId);", "        savedMessage.Should().NotBeNull();", "        savedMessage.Text.Should().Be(content);", "    }", "    "], "file_path": "itu-minitwit/minitwit.test/API.Tests.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP13/Chirp/commit/3106f73a148584d868f83ab453d92256746d72d1", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 12, "n_files_impacted": 7, "longest_chunk": ["    /// <summary>", "    /// Converts a Unix timestamp to a <see cref=\"DateTime\"/> object.", "    /// </summary>", "    /// <param name=\"unixTime\">The Unix timestamp to convert.</param>", "    /// <returns>A <see cref=\"DateTime\"/> object representing the given Unix timestamp.</returns>", "    /// <example>", "    /// <code>", "    /// long unixTime = 1725801466;", "    /// DateTime dateTime = HelperFunctions.FromUnixTimeToDateTime(unixTime);", "    /// Console.WriteLine(dateTime);  // Output: 9/8/2024", "    /// </code>", "    /// </example>"], "file_path": "src/Chirp.Infrastructure/Chirp.Repositories/HelperFunctions.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP4/Chirp/commit/fe9c03e6b53b1f3eeba67485d1e34c0b05ac17f6", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 38, "n_files_impacted": 2, "longest_chunk": ["using Microsoft.Data.Sqlite;\r", "using Microsoft.EntityFrameworkCore;\r", "\r", "namespace Chirp.Razor.CheepRepository;\r", "\r", "public class CheepRepositoryUnitTests : IAsyncLifetime\r", "{\r", "    private SqliteConnection connection;\r", "    private ChirpDBContext context;\r", "    private CheepRepository repository;\r", "    \r", "    public async Task InitializeAsync()\r", "    {\r", "        connection = new SqliteConnection(\"DataSource=:memory:\");\r", "        await connection.OpenAsync();\r", "        var builder = new DbContextOptionsBuilder<ChirpDBContext>().UseSqlite(connection);\r", "\r", "        context = new ChirpDBContext(builder.Options);\r", "        await context.Database.EnsureCreatedAsync();\r", "\r", "        repository = new CheepRepository(context);\r", "    }\r", "    \r", "    public async Task DisposeAsync()\r", "    {\r", "        await connection.DisposeAsync();\r", "        await context.DisposeAsync();\r", "    }\r", "\r", "\t[Fact]\r", "    public void DatabaseInitialization()\r", "    {\r", "        var results = repository.GetCheepsFromAuthor(\"Helge\", 0);\r", "        \r", "        foreach (var result in results)\r", "            Assert.Equal(\"Hello, BDSA students!\", result.Message);\r", "    }\r", "}"], "file_path": "test/Chirp.Razor.Tests/CheepRepositoryUnitTests.cs"}
{"Link_to_commit": "https://github.com/ITU-DevOps2025-GROUP-A/itu-minitwit/commit/3fb8183a3bcbd1238a4a4d5f653b8ad80c852869", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 32, "n_files_impacted": 1, "longest_chunk": ["", "", "    [IgnoreAntiforgeryToken]", "    [HttpGet(\"msgs/{username}\")]", "    public async Task<IActionResult> GetFilteredMessages(string username)", "    {", "        int pageSize = 100;", "        await latestService.UpdateLatest(1);", "        ", "        var user = await db.Users.FirstOrDefaultAsync(u => u.Username == username);", "        if (user == null)", "        {", "            return new JsonResult(new { status = 404, error_msg = \"User does not exist!\" })", "            {", "                StatusCode = 404", "            };", "        }", "        ", "        var messages = await db.Messages", "            .Where(m => m.AuthorId == user.UserId && m.Flagged == 0)", "            .OrderByDescending(m => m.PubDate)", "            .Take(pageSize)", "            .Select(m => new ", "            {", "                content = m.Text,", "                pub_date = m.PubDate,", "                user = username", "            })", "            .ToListAsync();", "", "        return Ok(messages);", "    }"], "file_path": "itu-minitwit/minitwit.web/Controllers/MessageController.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP13/Chirp/commit/f9de37af2111cbee6d03e2d3e36d0233b275ee06", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 13, "n_files_impacted": 5, "longest_chunk": ["    /// <summary>", "    /// Initializes a new instance of the <see cref=\"UserTimelineModel\"/> class.", "    /// </summary>", "    /// <param name=\"cheepService\">The service that handles Cheep-related operations.</param>", "    public UserTimelineModel(ICheepService cheepService) : base(cheepService)", "    {", "    }", "", "    /// <summary>", "    /// Handles the GET request to display the user's timeline along with their followers' posts.", "    /// </summary>", "    /// <param name=\"author\">The username of the author whose timeline is being viewed.</param>", "    /// <returns>An <see cref=\"ActionResult\"/> representing the page result.</returns>"], "file_path": "src/Chirp.Web/models/UserTimeline.cshtml.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP16/Chirp/commit/2ab090fc7c20e5264d4b984e19941ebd6d98b92d", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 65, "n_files_impacted": 8, "longest_chunk": ["", "    public async Task<ActionResult> OnPostLike(CheepDTO cheepDto)", "    {", "        //Finds the author thats logged in", "        var authorName = User.FindFirst(ClaimTypes.Name)?.Value;", "        if (string.IsNullOrEmpty(authorName))", "        {", "            throw new ArgumentException(\"Author name cannot be null or empty.\");", "        }", "        ", "        var author = await _authorRepository.FindAuthorWithEmail(authorName);", "        var cheep = await _cheepRepository.GetCheepFromCheepDto(cheepDto);", "        ", "        //adds the cheep to the authors list of liked cheeps", "        if (author.LikedCheeps != null)", "        {", "            author.LikedCheeps.Add(cheep);", "        }", "        ", "        cheepDto.Likes = cheepDto.Likes + 1;", "        ", "        likedCheeps = await _authorRepository.getLikedCheeps(author.AuthorId);", "        ", "        return RedirectToPage();", "    }", "    ", "    public async Task<ActionResult> OnPostUnLike(CheepDTO cheepDto)", "    {", "        //Finds the author thats logged in", "        var authorName = User.FindFirst(ClaimTypes.Name)?.Value;", "        if (string.IsNullOrEmpty(authorName))", "        {", "            throw new ArgumentException(\"Author name cannot be null or empty.\");", "        }", "        ", "        var author = await _authorRepository.FindAuthorWithEmail(authorName);", "        var cheep = await _cheepRepository.GetCheepFromCheepDto(cheepDto);", "        ", "        //removes the cheep from the user's unlikes from their list of liked cheeps", "        if (author.LikedCheeps != null)", "        {", "            author.LikedCheeps.Remove(cheep);", "        }", "", "        cheepDto.Likes = cheepDto.Likes - 1;", "        ", "        likedCheeps = await _authorRepository.getLikedCheeps(author.AuthorId);", "        ", "        return RedirectToPage();", "    }", "", "    public async Task<bool> UserLikesCheep(CheepDTO cheepDto)", "    {", "        //Finds the author thats logged in", "        var authorName = User.FindFirst(ClaimTypes.Name)?.Value;", "        if (string.IsNullOrEmpty(authorName))", "        {", "            throw new ArgumentException(\"Author name cannot be null or empty.\");", "        }", "        ", "        var author = await _authorRepository.FindAuthorWithEmail(authorName);", "        var cheep = await _cheepRepository.GetCheepFromCheepDto(cheepDto);", "        ", "        return await  _cheepRepository.DoesUserLikeCheep(cheep, author);", "    }"], "file_path": "src/Chirp.Web/Pages/Public.cshtml.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP30/Chirp/commit/08c0cf1d871b531665a1324b07ae6c3b9ebe207b", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 108, "n_files_impacted": 5, "longest_chunk": ["using Microsoft.AspNetCore.Mvc;", "using Microsoft.AspNetCore.Mvc.RazorPages;", "using Microsoft.AspNetCore.Http;", "using ChirpCore.Domain;", "using ChirpRepositories;", "using System.ComponentModel.DataAnnotations;", "using Microsoft.AspNetCore.Identity;", "", "namespace ChirpWeb.Pages", "{", "    public class CreateCheepModel : PageModel", "    {", "        private readonly ICheepRepository _repository;", "        private readonly UserManager<Author> _userManager;", "", "        public CreateCheepModel(ICheepRepository repository, UserManager<Author> userManager)", "        {", "            _repository = repository;", "            _userManager = userManager;", "        }", "        [BindProperty]", "        [Required(ErrorMessage = \"Please enter a message for your Cheep.\")]", "        [StringLength(280, ErrorMessage = \"Cheep cannot exceed 280 characters.\")]", "        public string CheepText { get; set; }", "", "        //currently chat", "        public async Task<IActionResult> OnPostAsync()", "        {", "            if (!ModelState.IsValid)", "            {", "                return Page();", "            }", "", "            try", "            {", "                int userId = 0;", "                string userName = \"Anonymous\";", "", "                if (User.Identity.IsAuthenticated)", "                {", "                    userId = int.Parse(User.FindFirst(\"sub\")?.Value ?? \"0\");", "                    userName = User.Identity.Name ?? \"Anonymous\";", "                }", "", "                Console.WriteLine($\"Attempting to create Cheep by userId: {userId}, userName: {userName}\");", "                Author author = await _userManager.FindByIdAsync(userId.ToString()) ?? await _userManager.FindByNameAsync(userName);", "                // Create the Cheep object", "                var newCheep = new Cheep", "                {", "                    CheepId = await _repository.GenerateNextCheepIdAsync(),", "                    Text = CheepText,", "                    TimeStamp = DateTime.UtcNow,", "                    Author = author", "                };", "", "                // Save the Cheep using the repository", "                await _repository.AddCheepAsync(newCheep);", "", "                Console.WriteLine(\"Cheep created successfully!\");", "", "                return RedirectToPage(\"/Index\"); // Redirect after success", "            }", "            catch (Exception ex)", "            {", "                Console.WriteLine($\"Error: {ex.Message}\");", "                ModelState.AddModelError(string.Empty, \"An error occurred while submitting your Cheep.\");", "                return Page();", "            }", "        }", "", "        /*public async Task<IActionResult> OnPostAsync()", "        {   ", "            // Validate input", "            if (string.IsNullOrWhiteSpace(CheepText))", "            {", "                ModelState.AddModelError(string.Empty, \"Message cannot be empty.\");", "                return Page();", "            }", "", "                // Get the user ID, or 0 for anonymous", "            int userId = 0; // default for anonymous user", "            string userName = \"Anonymous\";", "", "            if (User.Identity.IsAuthenticated)", "            {", "                // Get the user ID if the user is authenticated (e.g., from a ClaimsPrincipal)", "                //userId = int.Parse(User.Identity.Name);", "                userName = User.Identity.Name; // Or use User.Claims for more specific handling", "            }", "", "            try", "            {", "                // Call the repository to create the new Cheep", "                //int cheepId = await _repository.CreateCheepAsync(userId, Text);", "                await _repository.CreateCheepAsync(userId, userName, CheepText);", "                return RedirectToPage(\"/Timeline\"); // Redirect to the homepage", "", "            }", "            catch (Exception ex)", "            {", "                //ErrorMessage = $\"Error creating Cheep: {ex.Message}\";", "                ModelState.AddModelError(string.Empty, $\"Error creating Cheep: {ex.Message}\");", "                return Page();", "            }", "        }*/", "", "    }", "}"], "file_path": "src/ChirpWeb/Pages/CreatingCheep.cshtml.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP4/Chirp/commit/f42124a692014d44bd8981f173f66bfca2de27e4", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 55, "n_files_impacted": 2, "longest_chunk": ["\r", "    public List<CheepDTO> GetCheepsNotBlocked(string userEmail)\r", "    {\r", "        var query = (from Author in _context.Authors\r", "            join Cheeps in _context.Cheeps on Author.AuthorId equals Cheeps.AuthorId\r", "            where !_context.Blocked.Any(b => b.User.Email == userEmail && b.BlockedUser.Email == Author.Email)\r", "            orderby Cheeps.TimeStamp descending\r", "            select new CheepDTO\r", "            {\r", "                Author = Author.Name,\r", "                Email = Author.Email,\r", "                Message = Cheeps.Text,\r", "                TimeStamp = ((DateTimeOffset)Cheeps.TimeStamp).ToUnixTimeSeconds(),\r", "                CheepId = Cheeps.CheepId\r", "            });\r", "\r", "        return query.ToList();\r", "    }\r", "\r", "    public List<CheepDTO> GetLiked(string email)\r", "    {\r", "        var query = (from Cheep in _context.Cheeps\r", "            join Likes in _context.Likes on Cheep.CheepId equals Likes.cheep.CheepId\r", "            orderby Cheep.TimeStamp descending\r", "            where Likes.User.Email == email\r", "            select new CheepDTO\r", "            {\r", "                CheepId = Cheep.CheepId,\r", "                TimeStamp = ((DateTimeOffset)Cheep.TimeStamp).ToUnixTimeSeconds(),\r", "                Author = Cheep.Author.Name,\r", "                Message = Cheep.Text\r", "            });\r", "\r", "        return query.ToList();\r", "    }\r", "\r", "    public List<Cheep> GetCheep(string userEmail, int cheepId)\r", "    {\r", "        var query = (from Cheep in _context.Cheeps\r", "            where Cheep.Author.Email == userEmail && Cheep.CheepId == cheepId\r", "            select Cheep);\r", "\r", "        return query.ToList();\r", "    }\r", "\r", "    public List<Cheep> GetCheepFromId(int cheepId)\r", "    {\r", "        var query = (from Cheep in _context.Cheeps\r", "            where Cheep.CheepId == cheepId\r", "            select Cheep);\r", "\r", "        return query.ToList();\r", "    }\r", "\r", "    // Methods for adding and removing cheeps\r"], "file_path": "src/Chirp.Infrastructure/CheepRepository.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP26/Chirp/commit/7209da3de15e266aab93a8d74dc3b5caa1760eb8", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 21, "n_files_impacted": 5, "longest_chunk": ["            modelBuilder.Entity(\"Core.Notification\", b =>", "                {", "                    b.Property<int>(\"cheepID\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<int>(\"authorID\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<string>(\"authorToNotifyId\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<bool>(\"tagNotification\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.HasKey(\"cheepID\", \"authorID\");", "", "                    b.HasIndex(\"authorToNotifyId\");", "", "                    b.ToTable(\"notifications\");", "                });", ""], "file_path": "src/Infrastructure/Migrations/CheepDbContextModelSnapshot.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP4/Chirp/commit/cb8f311a170b3286a3ffd282b63097e3d128239e", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 9, "n_files_impacted": 2, "longest_chunk": ["        _repository.CreateFollow(userEmail, authorEmail);\r", "        try\r", "        {\r", "            _repository.CreateFollow(userEmail, authorEmail);\r", "        }\r", "        catch (Exception ex)\r", "        {\r", "            ex.GetBaseException();\r", "        }\r"], "file_path": "test/Chirp.Tests/CheepRepositoryUnitTests.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP5/Chirp/commit/f3588323668fef37276b6990d4325b037becc09b", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 26, "n_files_impacted": 1, "longest_chunk": ["    private static List<CheepViewModel> ConnectAndExecute(string query, string author)", "    {", "        var cheeps = new List<CheepViewModel>();", "        using (var connection = new SqliteConnection($\"Data Source={sqlDBFilePath}\"))", "        {", "            connection.Open();", "", "            var command = connection.CreateCommand();", "            command.CommandText = query;", "            command.Parameters.Add(\"@Author\", SqliteType.Text);", "            command.Parameters[\"@Author\"].Value = author;", "", "            using var reader = command.ExecuteReader();", "            while (reader.Read())", "            {", "                var message_id = reader.GetString(0);", "                var author_id = reader.GetInt32(1);", "                var message = reader.GetString(2);", "                var date = reader.GetInt32(3);", "                ", "                cheeps.Add(new CheepViewModel(GetAuthorFromID(author_id), message, UnixTimeStampToDateTimeString(date)));", "            }", "        }", "        return cheeps;", "    }", "    "], "file_path": "src/Chirp.Razor/DBFacade.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP8/Chirp/commit/12441d323c29626f70f70995d521d2a01ec3861d", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 2, "longest_chunk": ["                    _logger.LogInformation(\"User created an account using {Name} provider.\", info.LoginProvider);", "        ", "                    var userId = await _userManager.GetUserIdAsync(user);", "                    var code = await _userManager.GenerateEmailConfirmationTokenAsync(user);", "                    code = WebEncoders.Base64UrlEncode(Encoding.UTF8.GetBytes(code));", "                    var callbackUrl = Url.Page(", "                        \"/Account/ConfirmEmail\",", "                        pageHandler: null,", "                        values: new { area = \"Identity\", userId = userId, code = code },", "                        protocol: Request.Scheme);", "        ", "                    await _emailSender.SendEmailAsync(email, \"Confirm your email\",", "                        $\"Please confirm your account by <a href='{HtmlEncoder.Default.Encode(callbackUrl)}'>clicking here</a>.\");", "        ", "                    // If account confirmation is required, we need to show the link if we don't have a real email sender", "                    if (_userManager.Options.SignIn.RequireConfirmedAccount)"], "file_path": "src/Chirp.Web/Areas/Identity/Pages/Account/ExternalLogin.cshtml.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP8/Chirp/commit/ec780c15cbe6048ff7f625df775bc091cd7e2edb", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 9, "n_files_impacted": 1, "longest_chunk": ["        options.DefaultChallengeScheme = \"GitHub\";", "    })", "    .AddCookie()", "    .AddGitHub(o =>", "    {", "        o.ClientId = builder.Configuration[\"authentication_github_clientId\"];", "        o.ClientSecret = builder.Configuration[\"authentication_github_clientSecret\"];", "        o.CallbackPath = \"/signin-github\";", "        o.Scope.Add(\"user:email\");"], "file_path": "src/Chirp.Web/Program.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP15/Chirp/commit/b2076725d7f74fa0b833784c3ec74aed8a83dd5e", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 12, "n_files_impacted": 3, "longest_chunk": ["\ufeffusing Chirp.Web.Pages.Shared;", "using Microsoft.AspNetCore.Mvc;", "", "namespace Chirp.Web.ViewComponents;", "", "public class TimelineViewComponent : ViewComponent", "{", "    public Task<IViewComponentResult> InvokeAsync(TimelineModel model)", "    {", "        return Task.FromResult<IViewComponentResult>(View(\"Timeline\"));", "    }", "}"], "file_path": "src/Chirp.Web/ViewComponents/TimelineViewComponent.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP22/Chirp/commit/60f262924bcdc299dfe196af0bbdaa52ce71fd21", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 5, "n_files_impacted": 1, "longest_chunk": ["        var schemaSQL = File.ReadAllText(\"data/schema.sql\");", "        ExecuteQuery(schemaSQL);", "", "        var dumpSQL = File.ReadAllText(\"data/dump.sql\");", "        ExecuteQuery(dumpSQL);"], "file_path": "src/Chirp.Razor/DBFacade.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP15/Chirp/commit/c9e4f4639c9a8c1a6264c0f00fee07255f2b1c1a", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 12, "n_files_impacted": 2, "longest_chunk": ["    public async Task<List<Cheep>> GetAllCheepsFromFollowed(string author) //Made with the help of ChatGPT", "    {", "        var query = from cheep in _context.Cheeps", "            where (from follow in _context.Follows", "                    where follow.FollowsAuthorName == author", "                    select follow.FollowsAuthorName)", "                .Contains(cheep.Author.Name)", "            select cheep;", "        var result = await query.ToListAsync();", "        return result;", "    }", "    "], "file_path": "src/Chirp.Infrastructure/Chirp.Repositories/CheepRepository.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP26/Chirp/commit/b3ee8a67361789d05bcf2fd71cb00d4dbd37040d", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 60, "n_files_impacted": 2, "longest_chunk": ["</div> *@", "<!DOCTYPE html>", "<html lang=\"en\">", "<head>", "    <meta charset=\"UTF-8\">", "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">", "    <title>Notifications</title>", "    @*Entire script by chatgpt*@", "    <script>", "        async function fetchNewNotifications() {", "            try {", "                const response = await fetch('/Notifications?handler=NewNotifications');", "                if (response.ok) {", "                    const newNotifications = await response.json();", "                    if (newNotifications.length > 0) {", "                        appendNotifications(newNotifications);", "                    }", "                }", "            } catch (error) {", "                console.error(\"Error fetching new notifications:\", error);", "            }", "        }", "", "        function appendNotifications(notifications) {", "            const ul = document.getElementById('notifications-list');", "            notifications.forEach(notification => {", "                const li = document.createElement('li');", "                li.innerHTML = `", "                    <p>", "                        <span>", "                            <a href=\"/${notification.authorName}\">${notification.authorName}</a>", "                            ${notification.tagNotification ? 'tagged you!' : 'chirped!'}", "                        </span>", "                    </p>", "                    <p>${notification.cheepContent}</p>`;", "                ul.appendChild(li);", "            });", "        }", "", "        setInterval(fetchNewNotifications, 5000); // Check every 5 seconds", "    </script>", "</head>", "<body>", "    <ul id=\"notifications-list\">", "        @foreach (var notif in Model.notifications)", "        {", "            <li>", "                <p>", "                    <span>", "                        <a href=\"/@notif.authorName\">@notif.authorName</a>", "                        @(notif.tagNotification ? \"tagged you!\" : \"chirped!\")", "                    </span>", "                </p>", "                <p>@notif.cheepContent</p>", "            </li>", "        }", "    </ul>", "</body>", "</html>", ""], "file_path": "src/Web/Pages/Notifications.cshtml.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP30/Chirp/commit/b234048929c811c6d8a5f36fce99e6272ae38be3", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 344, "n_files_impacted": 9, "longest_chunk": ["\ufeff// <auto-generated />", "using System;", "using ChirpInfrastructure;", "using Microsoft.EntityFrameworkCore;", "using Microsoft.EntityFrameworkCore.Infrastructure;", "using Microsoft.EntityFrameworkCore.Migrations;", "using Microsoft.EntityFrameworkCore.Storage.ValueConversion;", "", "#nullable disable", "", "namespace ChirpInfrastructure.Migrations", "{", "    [DbContext(typeof(ChirpDBContext))]", "    [Migration(\"20241205124342_FollowMigration\")]", "    partial class FollowMigration", "    {", "        /// <inheritdoc />", "        protected override void BuildTargetModel(ModelBuilder modelBuilder)", "        {", "#pragma warning disable 612, 618", "            modelBuilder.HasAnnotation(\"ProductVersion\", \"8.0.10\");", "", "            modelBuilder.Entity(\"AuthorAuthor\", b =>", "                {", "                    b.Property<int>(\"AuthorId\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<int>(\"FollowsId\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.HasKey(\"AuthorId\", \"FollowsId\");", "", "                    b.HasIndex(\"FollowsId\");", "", "                    b.ToTable(\"AuthorFollows\", (string)null);", "                });", "", "            modelBuilder.Entity(\"ChirpCore.Domain.Author\", b =>", "                {", "                    b.Property<int>(\"Id\")", "                        .ValueGeneratedOnAdd()", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<int>(\"AccessFailedCount\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<string>(\"ConcurrencyStamp\")", "                        .IsConcurrencyToken()", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"Email\")", "                        .HasMaxLength(256)", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<bool>(\"EmailConfirmed\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<bool>(\"LockoutEnabled\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<DateTimeOffset?>(\"LockoutEnd\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"NormalizedEmail\")", "                        .HasMaxLength(256)", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"NormalizedUserName\")", "                        .HasMaxLength(256)", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"PasswordHash\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"PhoneNumber\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<bool>(\"PhoneNumberConfirmed\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<string>(\"SecurityStamp\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<bool>(\"TwoFactorEnabled\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<string>(\"UserName\")", "                        .IsRequired()", "                        .HasMaxLength(256)", "                        .HasColumnType(\"TEXT\");", "", "                    b.HasKey(\"Id\");", "", "                    b.HasIndex(\"Email\")", "                        .IsUnique();", "", "                    b.HasIndex(\"NormalizedEmail\")", "                        .HasDatabaseName(\"EmailIndex\");", "", "                    b.HasIndex(\"NormalizedUserName\")", "                        .IsUnique()", "                        .HasDatabaseName(\"UserNameIndex\");", "", "                    b.HasIndex(\"UserName\")", "                        .IsUnique();", "", "                    b.ToTable(\"AspNetUsers\", (string)null);", "                });", "", "            modelBuilder.Entity(\"ChirpCore.Domain.Cheep\", b =>", "                {", "                    b.Property<int>(\"CheepId\")", "                        .ValueGeneratedOnAdd()", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<int>(\"Id\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<string>(\"Text\")", "                        .IsRequired()", "                        .HasMaxLength(160)", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<DateTime>(\"TimeStamp\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.HasKey(\"CheepId\");", "", "                    b.HasIndex(\"Id\");", "", "                    b.ToTable(\"Cheeps\");", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityRole<int>\", b =>", "                {", "                    b.Property<int>(\"Id\")", "                        .ValueGeneratedOnAdd()", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<string>(\"ConcurrencyStamp\")", "                        .IsConcurrencyToken()", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"Name\")", "                        .HasMaxLength(256)", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"NormalizedName\")", "                        .HasMaxLength(256)", "                        .HasColumnType(\"TEXT\");", "", "                    b.HasKey(\"Id\");", "", "                    b.HasIndex(\"NormalizedName\")", "                        .IsUnique()", "                        .HasDatabaseName(\"RoleNameIndex\");", "", "                    b.ToTable(\"AspNetRoles\", (string)null);", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityRoleClaim<int>\", b =>", "                {", "                    b.Property<int>(\"Id\")", "                        .ValueGeneratedOnAdd()", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<string>(\"ClaimType\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"ClaimValue\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<int>(\"RoleId\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.HasKey(\"Id\");", "", "                    b.HasIndex(\"RoleId\");", "", "                    b.ToTable(\"AspNetRoleClaims\", (string)null);", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityUserClaim<int>\", b =>", "                {", "                    b.Property<int>(\"Id\")", "                        .ValueGeneratedOnAdd()", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<string>(\"ClaimType\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"ClaimValue\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<int>(\"UserId\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.HasKey(\"Id\");", "", "                    b.HasIndex(\"UserId\");", "", "                    b.ToTable(\"AspNetUserClaims\", (string)null);", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityUserLogin<int>\", b =>", "                {", "                    b.Property<string>(\"LoginProvider\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"ProviderKey\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"ProviderDisplayName\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<int>(\"UserId\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.HasKey(\"LoginProvider\", \"ProviderKey\");", "", "                    b.HasIndex(\"UserId\");", "", "                    b.ToTable(\"AspNetUserLogins\", (string)null);", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityUserRole<int>\", b =>", "                {", "                    b.Property<int>(\"UserId\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<int>(\"RoleId\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.HasKey(\"UserId\", \"RoleId\");", "", "                    b.HasIndex(\"RoleId\");", "", "                    b.ToTable(\"AspNetUserRoles\", (string)null);", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityUserToken<int>\", b =>", "                {", "                    b.Property<int>(\"UserId\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<string>(\"LoginProvider\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"Name\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"Value\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.HasKey(\"UserId\", \"LoginProvider\", \"Name\");", "", "                    b.ToTable(\"AspNetUserTokens\", (string)null);", "                });", "", "            modelBuilder.Entity(\"AuthorAuthor\", b =>", "                {", "                    b.HasOne(\"ChirpCore.Domain.Author\", null)", "                        .WithMany()", "                        .HasForeignKey(\"AuthorId\")", "                        .OnDelete(DeleteBehavior.Cascade)", "                        .IsRequired();", "", "                    b.HasOne(\"ChirpCore.Domain.Author\", null)", "                        .WithMany()", "                        .HasForeignKey(\"FollowsId\")", "                        .OnDelete(DeleteBehavior.Cascade)", "                        .IsRequired();", "                });", "", "            modelBuilder.Entity(\"ChirpCore.Domain.Cheep\", b =>", "                {", "                    b.HasOne(\"ChirpCore.Domain.Author\", \"Author\")", "                        .WithMany(\"Cheeps\")", "                        .HasForeignKey(\"Id\")", "                        .OnDelete(DeleteBehavior.Cascade)", "                        .IsRequired();", "", "                    b.Navigation(\"Author\");", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityRoleClaim<int>\", b =>", "                {", "                    b.HasOne(\"Microsoft.AspNetCore.Identity.IdentityRole<int>\", null)", "                        .WithMany()", "                        .HasForeignKey(\"RoleId\")", "                        .OnDelete(DeleteBehavior.Cascade)", "                        .IsRequired();", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityUserClaim<int>\", b =>", "                {", "                    b.HasOne(\"ChirpCore.Domain.Author\", null)", "                        .WithMany()", "                        .HasForeignKey(\"UserId\")", "                        .OnDelete(DeleteBehavior.Cascade)", "                        .IsRequired();", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityUserLogin<int>\", b =>", "                {", "                    b.HasOne(\"ChirpCore.Domain.Author\", null)", "                        .WithMany()", "                        .HasForeignKey(\"UserId\")", "                        .OnDelete(DeleteBehavior.Cascade)", "                        .IsRequired();", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityUserRole<int>\", b =>", "                {", "                    b.HasOne(\"Microsoft.AspNetCore.Identity.IdentityRole<int>\", null)", "                        .WithMany()", "                        .HasForeignKey(\"RoleId\")", "                        .OnDelete(DeleteBehavior.Cascade)", "                        .IsRequired();", "", "                    b.HasOne(\"ChirpCore.Domain.Author\", null)", "                        .WithMany()", "                        .HasForeignKey(\"UserId\")", "                        .OnDelete(DeleteBehavior.Cascade)", "                        .IsRequired();", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityUserToken<int>\", b =>", "                {", "                    b.HasOne(\"ChirpCore.Domain.Author\", null)", "                        .WithMany()", "                        .HasForeignKey(\"UserId\")", "                        .OnDelete(DeleteBehavior.Cascade)", "                        .IsRequired();", "                });", "", "            modelBuilder.Entity(\"ChirpCore.Domain.Author\", b =>", "                {", "                    b.Navigation(\"Cheeps\");", "                });", "#pragma warning restore 612, 618", "        }", "    }", "}"], "file_path": "src/ChirpInfrastructure/Migrations/20241205124342_FollowMigration.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP26/Chirp/commit/aa73519486dfddd293662278aef0044d2c15dbc5", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 42, "n_files_impacted": 3, "longest_chunk": ["", "    public async Task<(byte[] FileData, string ContentType, string FileName)> DownloadAuthorInfo(Author author)", "    {", "        var name = author.UserName;", "        var email = author.Email;", "        ", "        // var followingList = await GetFollowingAuthorsAsync(name);", "        // var userCheeps = await cheepRepository.ReadCheeps(-1, 0, );", "        ", "        // Create the textfile", "        var content = new StringBuilder();", "        content.AppendLine($\"{name}'s information:\");", "        content.AppendLine($\"-----------------------\");", "        content.AppendLine($\"Name: {name}\");", "        content.AppendLine($\"Email: {email}\");", "        ", "        content.AppendLine(\"Following:\");", "        if (author.FollowingList != null && author.FollowingList.Count != 0)", "        {", "            foreach (var following in author.FollowingList)", "            {", "                content.AppendLine($\"- {following.UserName}\");", "            }", "        }", "        else content.AppendLine(\"- No following\");", "        ", "        content.AppendLine(\"Cheeps:\");", "        if (author.Cheeps != null && author.Cheeps.Count != 0)", "        {", "            foreach (var cheep in author.Cheeps)", "            {", "                content.AppendLine($\"- \\\"{cheep.Text}\\\" ({cheep.TimeStamp})\");", "            }", "        }", "        else content.AppendLine(\"- No Cheeps posted yet\");", "", "        // Convert content into bytes and return file", "        var fileBytes = Encoding.UTF8.GetBytes(content.ToString());", "        const string contentType = \"text/plain\";", "        var fileName = $\"{name}_Chirp_data.txt\";", "        return (fileBytes, contentType, fileName);", "    }"], "file_path": "src/Infrastructure/CheepService.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP1/Chirp/commit/f46414a2cf2310f68f1764241d29c0d895f72260", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 62, "n_files_impacted": 1, "longest_chunk": ["// Made by ChatGPT", "const { graphql } = require(\"@octokit/graphql\");", "", "const moveIssue = async (token, issueId, columnName) => {", "  const graphqlWithAuth = graphql.defaults({", "    headers: {", "      authorization: `token ${token}`,", "    },", "  });", "", "  // Query your project to get column IDs", "  const projectData = await graphqlWithAuth(`", "    query {", "      user(login: \"ITU-BDSA2024-GROUP1\") {  // Replace with your org or user login", "        projectNext(number: 1) {  // Replace with your project number", "          id", "          fields(first: 20) {", "            nodes {", "              id", "              name", "            }", "          }", "        }", "      }", "    }", "  `);", "", "  const columnField = projectData.user.projectNext.fields.nodes.find(", "    (field) => field.name === \"Status\"", "  );", "  ", "  if (!columnField) {", "    throw new Error(\"Column 'Status' not found in project.\");", "  }", "", "  const fieldId = columnField.id;", "", "  // Move the issue to the desired column (e.g., \"In Progress\")", "  const result = await graphqlWithAuth(`", "    mutation {", "      updateProjectNextItemField(input: {", "        projectId: \"${projectData.user.projectNext.id}\",", "        itemId: \"${issueId}\",", "        fieldId: \"${fieldId}\",", "        value: \"${columnName}\"", "      }) {", "        projectNextItem {", "          id", "        }", "      }", "    }", "  `);", "", "  console.log(`Issue moved to '${columnName}' column successfully.`);", "};", "", "// Grab the token, issue ID, and column name from the command line args", "const [_, __, token, issueId, columnName] = process.argv;", "", "moveIssue(token, issueId, columnName)", "  .then(() => console.log(\"Move completed!\"))", "  .catch((error) => console.error(`Error moving issue: ${error.message}`));"], "file_path": ".github/workflows/scripts/move-issue.js"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP4/Chirp/commit/0aa0ddd6db1997f29a997b3a5b9e5ac20198f071", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 90, "n_files_impacted": 1, "longest_chunk": ["        [Test]", "        public async Task End_To_End_Test()", "        {", "            // Arrange", "            var username = Faker.Name.First();", "            var email = Faker.Internet.Email();", "            var password = $\"{Faker.Name.First()}!{Faker.Name.Last()}{Faker.RandomNumber.Next()}\";", "            var message = Faker.Lorem.Sentence();", "            await Page.GotoAsync(_factory.GetBaseAddress());", "", "            // Act", "            // Register", "            await Page.GetByRole(AriaRole.Link, new() { Name = \"register\", Exact = true }).ClickAsync();", "            await Page.GetByPlaceholder(\"name@example.com\").ClickAsync();", "            await Page.GetByPlaceholder(\"name@example.com\").FillAsync(email);", "            await Page.GetByPlaceholder(\"name\", new() { Exact = true }).ClickAsync();", "            await Page.GetByPlaceholder(\"name\", new() { Exact = true }).FillAsync(username);", "            await Page.GetByLabel(\"Password\", new() { Exact = true }).ClickAsync();", "            await Page.GetByLabel(\"Password\", new() { Exact = true }).FillAsync(password);", "            await Page.GetByLabel(\"Confirm Password\").ClickAsync();", "            await Page.GetByLabel(\"Confirm Password\").FillAsync(password);", "            await Page.GetByRole(AriaRole.Button, new() { Name = \"Register\" }).ClickAsync();", "", "            // Assert", "            // Should be logged in", "            await Expect(Page.GetByRole(AriaRole.Heading, new() { Name = $\"What's on your mind {username}?\" })).ToBeVisibleAsync();", "", "            // Act", "            // Post Cheep", "            await Page.Locator(\"#Message\").ClickAsync();", "            await Page.Locator(\"#Message\").FillAsync(message);", "            await Page.GetByRole(AriaRole.Button, new() { Name = \"Share\" }).ClickAsync();", "", "            // Assert", "            // Cheep should be visible", "            var postLocator = Page.Locator(\"li\").Filter(new() { HasText = $\"{username}\" }).First;", "            await Expect(postLocator).ToHaveTextAsync(new Regex($\".*{message}.*\"));", "", "            // Act", "            // Follow", "            var posterToFollow = Page.Locator(\"li\").Filter(new() { HasText = \"Follow 0\" }).GetByRole(AriaRole.Button).Nth(1);", "            var posterToFollowUsername = await posterToFollow.TextContentAsync();", "            await posterToFollow.ClickAsync();", "", "            Console.WriteLine($\"Text: {posterToFollowUsername}\");", "", "            // Assert", "            // Should be following", "            await Expect(Page.GetByRole(AriaRole.Button, new() { Name = \"Unfollow\" })).ToBeVisibleAsync();", "            await Expect(Page).ToHaveURLAsync(new Regex($\".*{posterToFollowUsername}\"));", "", "            // Act", "            // Unfollow", "", "            await Page.GetByRole(AriaRole.Button, new() { Name = \"Unfollow\" }).ClickAsync();", "", "            // Assert", "            // Should be unfollowed", "            await Expect(Page.GetByRole(AriaRole.Button, new() { Name = \"Follow\" })).ToBeVisibleAsync();", "", "            // Act", "            // Like a post", "            await Page.GetByRole(AriaRole.Link, new() { Name = \"public timeline\" }).ClickAsync();", "            var postToLike = Page.Locator(\"li\").Filter(new() { HasText = \"\u2661\" }).First;", "            var likeButton = postToLike.GetByRole(AriaRole.Button, new() { Name = \"\u2661\" });", "            await likeButton.ClickAsync();", "", "            // Assert", "            // Should be liked", "            await Expect(Page.GetByRole(AriaRole.Button, new() { Name = \"\u2665\" })).ToBeVisibleAsync();", "            await Expect(postToLike).ToHaveTextAsync(new Regex(@\".*1.*\"));", "", "            // Act", "            // Unlike a post", "            await Page.GetByRole(AriaRole.Button, new() { Name = \"\u2665\" }).ClickAsync();", "", "            // Assert", "            // Should be unliked", "            await Expect(postToLike.GetByRole(AriaRole.Button, new() { Name = \"\u2661\" })).ToBeVisibleAsync();", "            await Expect(postToLike).ToHaveTextAsync(new Regex(@\".*0.*\"));", "", "            // Act  ", "            // Logout", "", "            await Page.GetByRole(AriaRole.Link, new() { Name = $\"logout [{username}]\" }).ClickAsync();", "", "            // Assert", "            // Should be logged out", "            await Expect(Page.GetByRole(AriaRole.Link, new() { Name = \"login\" })).ToBeVisibleAsync();", "        }"], "file_path": "test/PlaywrightTests/UITests.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP1/Chirp/commit/e257bbce2cfc9aecd44ff968ce4ba40f285db4b0", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 60, "n_files_impacted": 16, "longest_chunk": ["    public static async Task<AuthorDTO[]> SetUpTestAuthorDB(IAuthorRepository authorRepository, SqliteConnection connection)", "    {", "        using (var command = new SqliteCommand(\"DELETE FROM authors;\", connection))", "        {", "            command.ExecuteNonQuery();", "        }", "", "        AuthorDTO[] authors = new AuthorDTO[4];", "        for (int i = 0; i < authors.Length; i++)", "        {", "            authors[i] = new AuthorDTO", "            {", "                Id = i+1,", "                Name = $\"Test{i+1}\",", "                Email = $\"Test{i+1}@Tester.com\"", "            };", "            authors[i].Id = await authorRepository.AddAuthorAsync(authors[i]);", "        }", "", "        return authors;", "", "    }", "    public static async Task<CheepDTO[]> SetUpTestCheepDB(ICheepRepository cheepRepository, SqliteConnection connection, AuthorDTO[] authors)", "    {", "        using (var command = new SqliteCommand(\"DELETE FROM cheeps;\", connection))", "        {", "            command.ExecuteNonQuery();", "        }", "", "        DateTime timeStamp = DateTime.Now;", "        long timeStampLong = timeStamp.Ticks;", "        CheepDTO[] cheeps = new CheepDTO[160];", "        for (int i = 0; i < cheeps.Length; i++)", "        {", "            timeStampLong += 10000000;", "            timeStamp = new DateTime(timeStampLong);", "            cheeps[i] = new CheepDTO", "            {", "                Id = i + 1,", "                Name = authors[i % authors.Length].Name,", "                Message = $\"Text{i + 1}\",", "                TimeStamp = timeStamp.ToString(\"yyyy\\\\-MM\\\\-dd HH\\\\:mm\\\\:ss\"),", "                AuthorId = authors[i % authors.Length].Id", "            };", "            cheeps[i].Id = await cheepRepository.AddCheepAsync(cheeps[i]);", "        }", "", "        Array.Reverse(cheeps);", "        return cheeps;", "", "    }", "", "", "    public static void AssertCheep(CheepDTO expected, CheepDTO actual)", "    {", "        Assert.Equal(expected.Id, actual.Id);", "        Assert.Equal(expected.Name, actual.Name);", "        Assert.Equal(expected.TimeStamp, actual.TimeStamp);", "        Assert.Equal(expected.AuthorId, actual.AuthorId);", "        Assert.Equal(expected.Message, actual.Message);"], "file_path": "test/Chirp.RazorTest/CheepServiceUnitTest.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP26/Chirp/commit/87956ca6553dc4a695f5b490c96dcfd779aabcad", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 12, "n_files_impacted": 2, "longest_chunk": ["                ", "                var info = await _signInManager.GetExternalLoginInfoAsync(); //get information about the external login(github)", "                 ", "            if (info != null)//check if any external login info is retrieved", "        {", "            // Check for email claim in external login", "            var emailClaim = info.Principal.FindFirst(claim => claim.Type == System.Security.Claims.ClaimTypes.Email)?.Value;", "            if (emailClaim != null)", "            {", "                Input.Email = emailClaim; // Automatically set the email if found", "            }", "        }"], "file_path": "src/Chirp.Web/Areas/Identity/Pages/Account/Register.cshtml.cs"}
{"Link_to_commit": "https://github.com/ITU-DevOps2025-GROUP-A/itu-minitwit/commit/b50359b49f1e204b54944bdbc970619c39c67762", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 73, "n_files_impacted": 1, "longest_chunk": ["", "    [Fact]", "    public async Task GetFilteredMessages_returnsFilteredMessages()", "    {", "        var context = fixture.GetDbContext();", "        var user = new User { Username = \"Man\", Email = \"Man@Man.com\", PwHash = \"23456\" };", "    ", "        var msg = new Message { AuthorId = user.UserId, Text = \"Hello from Man\", PubDate = (int)DateTimeOffset.Now.ToUnixTimeSeconds() };", "        var msg2 = new Message { AuthorId = user.UserId, Text = \"Hello again from Man\", PubDate = (int)DateTimeOffset.Now.ToUnixTimeSeconds() };", "", "        await context.Users.AddAsync(user);", "        await context.Messages.AddAsync(msg);", "        await context.Messages.AddAsync(msg2);", "", "        await context.SaveChangesAsync();", "", "        var response = await client.GetAsync(\"/msgs/Man\");", "        var json = await response.Content.ReadAsStringAsync();", "        ", "", "        if (response.StatusCode == HttpStatusCode.OK)", "        {", "            var messagesResponse = JsonConvert.DeserializeObject<MessagesResponse>(json);", "            var messages = messagesResponse.Messages; // Access the messages", "", "            foreach (var message in messages)", "            {", "                message.User.Should().Be(user.Username);", "            }", "        }", "    }", "    ", "    ", "    [Fact]", "    public async Task GetFilteredMessagesFromNonExistentUser_returnsErrorResponse()", "    {", "        var context = fixture.GetDbContext();", "        ", "        var response = await client.GetAsync(\"/msgs/MysteryMan\");", "        ", "        Assert.Equal(HttpStatusCode.NotFound, response.StatusCode);", "        ", "            ", "    }", "    ", "    ", "    [Fact]", "    public async Task GetEmptyFilteredMessages_returnsErrorResponse()", "    {", "        var context = fixture.GetDbContext();", "        var user = new User { Username = \"Man\", Email = \"Man@Man.com\", PwHash = \"23456\" };", "", "        await context.Users.AddAsync(user);", "        await context.SaveChangesAsync();", "", "        var response = await client.GetAsync(\"/msgs/Man\");", "        ", "        Assert.Equal(HttpStatusCode.NoContent, response.StatusCode);", "    }", "    ", "    ", "    public class MessageDto", "    {", "        public string Text { get; set; }", "        public int PubDate { get; set; }", "        public string User { get; set; }", "    }", "    ", "    public class MessagesResponse", "    {", "        public string Status { get; set; }", "        public List<MessageDto> Messages { get; set; }", "    }"], "file_path": "itu-minitwit/minitwit.test/API.Tests.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP4/Chirp/commit/4b3fc98e64cf651007ccba9dc30ae27813f1fd93", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 83, "n_files_impacted": 10, "longest_chunk": ["\ufeff// <auto-generated />\r", "using System;\r", "using Chirp.Razor.CheepRepository;\r", "using Microsoft.EntityFrameworkCore;\r", "using Microsoft.EntityFrameworkCore.Infrastructure;\r", "using Microsoft.EntityFrameworkCore.Migrations;\r", "using Microsoft.EntityFrameworkCore.Storage.ValueConversion;\r", "\r", "#nullable disable\r", "\r", "namespace Chirp.Razor.Migrations\r", "{\r", "    [DbContext(typeof(ChirpDBContext))]\r", "    [Migration(\"20241009084314_IntialAdd\")]\r", "    partial class IntialAdd\r", "    {\r", "        /// <inheritdoc />\r", "        protected override void BuildTargetModel(ModelBuilder modelBuilder)\r", "        {\r", "#pragma warning disable 612, 618\r", "            modelBuilder.HasAnnotation(\"ProductVersion\", \"8.0.8\");\r", "\r", "            modelBuilder.Entity(\"Chirp.Razor.CheepRepository.Author\", b =>\r", "                {\r", "                    b.Property<int>(\"AuthorId\")\r", "                        .ValueGeneratedOnAdd()\r", "                        .HasColumnType(\"INTEGER\");\r", "\r", "                    b.Property<string>(\"Email\")\r", "                        .IsRequired()\r", "                        .HasColumnType(\"TEXT\");\r", "\r", "                    b.Property<string>(\"Name\")\r", "                        .IsRequired()\r", "                        .HasColumnType(\"TEXT\");\r", "\r", "                    b.HasKey(\"AuthorId\");\r", "\r", "                    b.ToTable(\"Authors\");\r", "                });\r", "\r", "            modelBuilder.Entity(\"Chirp.Razor.CheepRepository.Cheep\", b =>\r", "                {\r", "                    b.Property<int>(\"CheepId\")\r", "                        .ValueGeneratedOnAdd()\r", "                        .HasColumnType(\"INTEGER\");\r", "\r", "                    b.Property<int>(\"AuthorId\")\r", "                        .HasColumnType(\"INTEGER\");\r", "\r", "                    b.Property<string>(\"Text\")\r", "                        .IsRequired()\r", "                        .HasColumnType(\"TEXT\");\r", "\r", "                    b.Property<DateTime>(\"TimeStamp\")\r", "                        .HasColumnType(\"TEXT\");\r", "\r", "                    b.HasKey(\"CheepId\");\r", "\r", "                    b.HasIndex(\"AuthorId\");\r", "\r", "                    b.ToTable(\"Cheeps\");\r", "                });\r", "\r", "            modelBuilder.Entity(\"Chirp.Razor.CheepRepository.Cheep\", b =>\r", "                {\r", "                    b.HasOne(\"Chirp.Razor.CheepRepository.Author\", \"Author\")\r", "                        .WithMany(\"Cheeps\")\r", "                        .HasForeignKey(\"AuthorId\")\r", "                        .OnDelete(DeleteBehavior.Cascade)\r", "                        .IsRequired();\r", "\r", "                    b.Navigation(\"Author\");\r", "                });\r", "\r", "            modelBuilder.Entity(\"Chirp.Razor.CheepRepository.Author\", b =>\r", "                {\r", "                    b.Navigation(\"Cheeps\");\r", "                });\r", "#pragma warning restore 612, 618\r", "        }\r", "    }\r", "}\r"], "file_path": "src/Chirp.Razor/Migrations/20241009084314_IntialAdd.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP16/Chirp/commit/6f66057057ccd55546b601ef9235bab29aebd007", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 40, "n_files_impacted": 2, "longest_chunk": ["        ", "        //This updates the users (authors) email, which also makes sure that the cheeps have the NewEmail", "        user.Email = Input.NewEmail;", "        user.UserName = Input.NewEmail;", "        var updateResult = await _userManager.UpdateAsync(user);", "        if (!updateResult.Succeeded)", "        {", "            foreach (var error in updateResult.Errors)", "            {", "                ModelState.AddModelError(string.Empty, error.Description);", "            }", "            StatusMessage = \"Unexpected error when trying to update email.\";", "            return RedirectToPage();", "        }", "        ", "        /*", "        // Update email in the user manager with verification", "        var userId = await _userManager.GetUserIdAsync(user);", "        var code = await _userManager.GenerateChangeEmailTokenAsync(user, Input.NewEmail);", "        code = WebEncoders.Base64UrlEncode(Encoding.UTF8.GetBytes(code));", "        var callbackUrl = Url.Page(", "            \"/Account/ConfirmEmailChange\",", "            pageHandler: null,", "            values: new { area = \"Identity\", userId = userId, email = Input.NewEmail, code = code },", "            protocol: Request.Scheme);", "        await _emailSender.SendEmailAsync(", "            Input.NewEmail,", "            \"Confirm your email\",", "            $\"Please confirm your account by <a href='{HtmlEncoder.Default.Encode(callbackUrl)}'>clicking here</a>.\");", "            */", "", "        StatusMessage = \"Confirmation link to change email sent. Please check your email.\";", "        return RedirectToPage();", "        ", "    }", "", "    StatusMessage = \"Your email is unchanged.\";", "    return RedirectToPage();", "}", ""], "file_path": "src/Chirp.Web/Areas/Identity/Pages/Account/Manage/Email.cshtml.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP10/Chirp/commit/1145016af436242fe583415dcc4e92d675acee59", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 31, "n_files_impacted": 2, "longest_chunk": ["    ", "    private void WaitUntilServerIsAvailable(string url)", "    {", "        var uri = new Uri(url);", "        using var client = new TcpClient();", "", "        var maxAttempts = 5;", "        var attempt = 0;", "", "        while (attempt < maxAttempts)", "        {", "            try", "            {", "                client.Connect(uri.Host, uri.Port);", "                if (client.Connected)", "                {", "                    break;", "                }", "            }", "            catch", "            {", "                attempt++;", "                Thread.Sleep(1000);", "            }", "        }", "", "        if (attempt == maxAttempts)", "        {", "            throw new Exception(\"Server did not start in time.\");", "        }", "    }"], "file_path": "Chirp/test/PlaywrightTests/CustomWebApplicationFactory.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP16/Chirp/commit/b6cf51365e1533024b183a1b01d6e6702d5e1ee4", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 47, "n_files_impacted": 4, "longest_chunk": ["\ufeff@page", "@model EmailModel", "@{", "    ViewData[\"Title\"] = \"Manage Email\";", "    ViewData[\"ActivePage\"] = ManageNavPages.Email;", "}", "", "<h3>@ViewData[\"Title\"]</h3>", "<partial name=\"_StatusMessage\" for=\"StatusMessage\" />", "<div class=\"row\">", "    <div class=\"col-md-6\">", "        <form id=\"email-form\" method=\"post\">", "            <div asp-validation-summary=\"All\" class=\"text-danger\" role=\"alert\"></div>", "            @if (Model.IsEmailConfirmed)", "            {", "                <div class=\"form-floating mb-3 input-group\">", "                    <div>", "                        <label asp-for=\"Email\" class=\"form-label\"></label>", "                    </div>", "                    <input asp-for=\"Email\" class=\"form-control\" placeholder=\"Please enter your email.\" disabled/>", "                    <span class=\"h-100 input-group-text text-success font-weight-bold\">\u2713</span>", "                    <div style=\"height: 20px;\"></div>", "                    <label class=\"form-label\">Change Email:</label>", "                </div>", "            }", "            else", "            {", "                <div class=\"form-floating mb-3\">", "                    <input asp-for=\"Email\" class=\"form-control\" placeholder=\"Please enter your email.\" disabled />", "                    <label asp-for=\"Email\" class=\"form-label\"></label>", "                    <button id=\"email-verification\" type=\"submit\" asp-page-handler=\"SendVerificationEmail\" class=\"btn btn-link\">Send verification email</button>", "                </div>", "            }", "            <div class=\"form-floating mb-3\">", "                <input asp-for=\"Input.NewEmail\" class=\"form-control\" autocomplete=\"email\" aria-required=\"true\" placeholder=\"Please enter new email\" value=\"\" />", "                <label asp-for=\"Input.NewEmail\" class=\"form-label\"></label>", "                <span asp-validation-for=\"Input.NewEmail\" class=\"text-danger\"></span>", "            </div>", "", "            <button id=\"change-email-button\" type=\"submit\" asp-page-handler=\"ChangeEmail\" class=\"w-100 btn btn-lg btn-primary\">Change email</button>", "        </form>", "    </div>", "</div>", "", "@section Scripts {", "    <partial name=\"_ValidationScriptsPartial\" />", "}"], "file_path": "src/Chirp.Web/Areas/Identity/Pages/Account/Manage/Email.cshtml.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP16/Chirp/commit/05d156e0dbd769f0282c50dacded7f8303aa57f3", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 385, "n_files_impacted": 8, "longest_chunk": ["\ufeff// <auto-generated />", "using System;", "using Chirp.Infrastructure;", "using Microsoft.EntityFrameworkCore;", "using Microsoft.EntityFrameworkCore.Infrastructure;", "using Microsoft.EntityFrameworkCore.Migrations;", "using Microsoft.EntityFrameworkCore.Storage.ValueConversion;", "", "#nullable disable", "", "namespace Chirp.Infrastructure.Migrations", "{", "    [DbContext(typeof(CheepDBContext))]", "    [Migration(\"20241204154344_NewMigration\")]", "    partial class NewMigration", "    {", "        /// <inheritdoc />", "        protected override void BuildTargetModel(ModelBuilder modelBuilder)", "        {", "#pragma warning disable 612, 618", "            modelBuilder.HasAnnotation(\"ProductVersion\", \"8.0.0\");", "", "            modelBuilder.Entity(\"AuthorCheep\", b =>", "                {", "                    b.Property<int>(\"LikedByAuthorsId\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<int>(\"LikedCheepsCheepId\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.HasKey(\"LikedByAuthorsId\", \"LikedCheepsCheepId\");", "", "                    b.HasIndex(\"LikedCheepsCheepId\");", "", "                    b.ToTable(\"AuthorLikedCheeps\", (string)null);", "                });", "", "            modelBuilder.Entity(\"AuthorFollows\", b =>", "                {", "                    b.Property<int>(\"FollowedId\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<int>(\"FollowerId\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.HasKey(\"FollowedId\", \"FollowerId\");", "", "                    b.HasIndex(\"FollowerId\");", "", "                    b.ToTable(\"AuthorFollows\");", "                });", "", "            modelBuilder.Entity(\"Chirp.Core.Author\", b =>", "                {", "                    b.Property<int>(\"Id\")", "                        .ValueGeneratedOnAdd()", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<int>(\"AccessFailedCount\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<int>(\"AuthorId\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<string>(\"ConcurrencyStamp\")", "                        .IsConcurrencyToken()", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"Email\")", "                        .HasMaxLength(256)", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<bool>(\"EmailConfirmed\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<bool>(\"LockoutEnabled\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<DateTimeOffset?>(\"LockoutEnd\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"Name\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"NormalizedEmail\")", "                        .HasMaxLength(256)", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"NormalizedUserName\")", "                        .HasMaxLength(256)", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"PasswordHash\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"PhoneNumber\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<bool>(\"PhoneNumberConfirmed\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<string>(\"SecurityStamp\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<bool>(\"TwoFactorEnabled\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<string>(\"UserName\")", "                        .HasMaxLength(256)", "                        .HasColumnType(\"TEXT\");", "", "                    b.HasKey(\"Id\");", "", "                    b.HasIndex(\"Email\")", "                        .IsUnique();", "", "                    b.HasIndex(\"Name\")", "                        .IsUnique();", "", "                    b.HasIndex(\"NormalizedEmail\")", "                        .HasDatabaseName(\"EmailIndex\");", "", "                    b.HasIndex(\"NormalizedUserName\")", "                        .IsUnique()", "                        .HasDatabaseName(\"UserNameIndex\");", "", "                    b.ToTable(\"AspNetUsers\", (string)null);", "                });", "", "            modelBuilder.Entity(\"Chirp.Core.Cheep\", b =>", "                {", "                    b.Property<int>(\"CheepId\")", "                        .ValueGeneratedOnAdd()", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<int>(\"AuthorId\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<int?>(\"Likes\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<string>(\"Text\")", "                        .HasMaxLength(160)", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<DateTime>(\"TimeStamp\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.HasKey(\"CheepId\");", "", "                    b.HasIndex(\"AuthorId\");", "", "                    b.ToTable(\"Cheeps\");", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityRole<int>\", b =>", "                {", "                    b.Property<int>(\"Id\")", "                        .ValueGeneratedOnAdd()", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<string>(\"ConcurrencyStamp\")", "                        .IsConcurrencyToken()", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"Name\")", "                        .HasMaxLength(256)", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"NormalizedName\")", "                        .HasMaxLength(256)", "                        .HasColumnType(\"TEXT\");", "", "                    b.HasKey(\"Id\");", "", "                    b.HasIndex(\"NormalizedName\")", "                        .IsUnique()", "                        .HasDatabaseName(\"RoleNameIndex\");", "", "                    b.ToTable(\"AspNetRoles\", (string)null);", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityRoleClaim<int>\", b =>", "                {", "                    b.Property<int>(\"Id\")", "                        .ValueGeneratedOnAdd()", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<string>(\"ClaimType\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"ClaimValue\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<int>(\"RoleId\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.HasKey(\"Id\");", "", "                    b.HasIndex(\"RoleId\");", "", "                    b.ToTable(\"AspNetRoleClaims\", (string)null);", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityUserClaim<int>\", b =>", "                {", "                    b.Property<int>(\"Id\")", "                        .ValueGeneratedOnAdd()", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<string>(\"ClaimType\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"ClaimValue\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<int>(\"UserId\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.HasKey(\"Id\");", "", "                    b.HasIndex(\"UserId\");", "", "                    b.ToTable(\"AspNetUserClaims\", (string)null);", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityUserLogin<int>\", b =>", "                {", "                    b.Property<string>(\"LoginProvider\")", "                        .HasMaxLength(128)", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"ProviderKey\")", "                        .HasMaxLength(128)", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"ProviderDisplayName\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<int>(\"UserId\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.HasKey(\"LoginProvider\", \"ProviderKey\");", "", "                    b.HasIndex(\"UserId\");", "", "                    b.ToTable(\"AspNetUserLogins\", (string)null);", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityUserRole<int>\", b =>", "                {", "                    b.Property<int>(\"UserId\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<int>(\"RoleId\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.HasKey(\"UserId\", \"RoleId\");", "", "                    b.HasIndex(\"RoleId\");", "", "                    b.ToTable(\"AspNetUserRoles\", (string)null);", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityUserToken<int>\", b =>", "                {", "                    b.Property<int>(\"UserId\")", "                        .HasColumnType(\"INTEGER\");", "", "                    b.Property<string>(\"LoginProvider\")", "                        .HasMaxLength(128)", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"Name\")", "                        .HasMaxLength(128)", "                        .HasColumnType(\"TEXT\");", "", "                    b.Property<string>(\"Value\")", "                        .HasColumnType(\"TEXT\");", "", "                    b.HasKey(\"UserId\", \"LoginProvider\", \"Name\");", "", "                    b.ToTable(\"AspNetUserTokens\", (string)null);", "                });", "", "            modelBuilder.Entity(\"AuthorCheep\", b =>", "                {", "                    b.HasOne(\"Chirp.Core.Author\", null)", "                        .WithMany()", "                        .HasForeignKey(\"LikedByAuthorsId\")", "                        .OnDelete(DeleteBehavior.Cascade)", "                        .IsRequired();", "", "                    b.HasOne(\"Chirp.Core.Cheep\", null)", "                        .WithMany()", "                        .HasForeignKey(\"LikedCheepsCheepId\")", "                        .OnDelete(DeleteBehavior.Cascade)", "                        .IsRequired();", "                });", "", "            modelBuilder.Entity(\"AuthorFollows\", b =>", "                {", "                    b.HasOne(\"Chirp.Core.Author\", null)", "                        .WithMany()", "                        .HasForeignKey(\"FollowedId\")", "                        .OnDelete(DeleteBehavior.Restrict)", "                        .IsRequired();", "", "                    b.HasOne(\"Chirp.Core.Author\", null)", "                        .WithMany()", "                        .HasForeignKey(\"FollowerId\")", "                        .OnDelete(DeleteBehavior.Cascade)", "                        .IsRequired();", "                });", "", "            modelBuilder.Entity(\"Chirp.Core.Cheep\", b =>", "                {", "                    b.HasOne(\"Chirp.Core.Author\", \"Author\")", "                        .WithMany(\"Cheeps\")", "                        .HasForeignKey(\"AuthorId\")", "                        .OnDelete(DeleteBehavior.Cascade)", "                        .IsRequired();", "", "                    b.Navigation(\"Author\");", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityRoleClaim<int>\", b =>", "                {", "                    b.HasOne(\"Microsoft.AspNetCore.Identity.IdentityRole<int>\", null)", "                        .WithMany()", "                        .HasForeignKey(\"RoleId\")", "                        .OnDelete(DeleteBehavior.Cascade)", "                        .IsRequired();", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityUserClaim<int>\", b =>", "                {", "                    b.HasOne(\"Chirp.Core.Author\", null)", "                        .WithMany()", "                        .HasForeignKey(\"UserId\")", "                        .OnDelete(DeleteBehavior.Cascade)", "                        .IsRequired();", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityUserLogin<int>\", b =>", "                {", "                    b.HasOne(\"Chirp.Core.Author\", null)", "                        .WithMany()", "                        .HasForeignKey(\"UserId\")", "                        .OnDelete(DeleteBehavior.Cascade)", "                        .IsRequired();", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityUserRole<int>\", b =>", "                {", "                    b.HasOne(\"Microsoft.AspNetCore.Identity.IdentityRole<int>\", null)", "                        .WithMany()", "                        .HasForeignKey(\"RoleId\")", "                        .OnDelete(DeleteBehavior.Cascade)", "                        .IsRequired();", "", "                    b.HasOne(\"Chirp.Core.Author\", null)", "                        .WithMany()", "                        .HasForeignKey(\"UserId\")", "                        .OnDelete(DeleteBehavior.Cascade)", "                        .IsRequired();", "                });", "", "            modelBuilder.Entity(\"Microsoft.AspNetCore.Identity.IdentityUserToken<int>\", b =>", "                {", "                    b.HasOne(\"Chirp.Core.Author\", null)", "                        .WithMany()", "                        .HasForeignKey(\"UserId\")", "                        .OnDelete(DeleteBehavior.Cascade)", "                        .IsRequired();", "                });", "", "            modelBuilder.Entity(\"Chirp.Core.Author\", b =>", "                {", "                    b.Navigation(\"Cheeps\");", "                });", "#pragma warning restore 612, 618", "        }", "    }", "}"], "file_path": "src/Chirp.Infrastructure/Migrations/20241204154344_NewMigration.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP16/Chirp/commit/9fb8996bde01194076a6fdb54302aade28599c29", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 54, "n_files_impacted": 11, "longest_chunk": ["    public async Task<ActionResult> OnPostLike(string authorDto, string text, string timeStamp, int? likes)", "    {", "        // Find the author that's logged in", "        var authorName = User.FindFirst(\"Name\")?.Value;", "        if (string.IsNullOrEmpty(authorName))", "        {", "            throw new ArgumentException(\"Author name cannot be null or empty.\");", "        }", "", "        var author = await AuthorRepository.FindAuthorWithName(authorName);", "        var cheep = await CheepRepository.FindCheep(text,timeStamp, authorDto);", "        ", "        // Adds the cheep to the author's list of liked cheeps", "        await CheepRepository.LikeCheep(cheep, author);", "        ", "        likedCheeps = await AuthorRepository.getLikedCheeps(author.AuthorId);", "        ", "        return RedirectToPage();", "    }", "", "    ", "    public async Task<ActionResult> OnPostUnLike(string authorDto, string text, string timeStamp, int? likes)", "    {", "        // Find the author that's logged in", "        var authorName = User.FindFirst(\"Name\")?.Value;", "        if (string.IsNullOrEmpty(authorName))", "        {", "            throw new ArgumentException(\"Author name cannot be null or empty.\");", "        }", "", "        var author = await AuthorRepository.FindAuthorWithName(authorName);", "        var cheep = await CheepRepository.FindCheep(text,timeStamp,authorDto);", "        ", "        await CheepRepository.UnLikeCheep(cheep, author);", "        ", "        likedCheeps = await AuthorRepository.getLikedCheeps(author.AuthorId);", "        ", "        return RedirectToPage();", "    }", "", "    public async Task<bool> DoesUserLikeCheep(string authorDto, string text, string timeStamp)", "    {", "        var authorName = User.FindFirst(\"Name\")?.Value;", "        if (string.IsNullOrEmpty(authorName))", "        {", "            throw new ArgumentException(\"Author name cannot be null or empty.\");", "        }", "        ", "        var author = await AuthorRepository.FindAuthorWithLikes(authorName);", "        var cheep = await CheepRepository.FindCheep(text,timeStamp,authorDto);", "        ", "        return await  CheepRepository.DoesUserLikeCheep(cheep, author);", "    }", "    "], "file_path": "src/Chirp.Web/Pages/UserTimeline.cshtml.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP16/Chirp/commit/65217ec7d53f5e054c9be452b3f41b426d9c1514", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 81, "n_files_impacted": 8, "longest_chunk": ["using Chirp.Core;", "using Microsoft.AspNetCore.Mvc;", "using Microsoft.AspNetCore.Mvc.RazorPages;", "using System.Collections.Generic;", "using System.Linq;", "using System.Security.Claims;", "using System.Threading.Tasks;", "using Chirp.Infrastructure;", "using Microsoft.AspNetCore.Identity;", "", "namespace Chirp.Web.Pages", "{", "    public class SearchResultsModel : PageModel", "    {", "        private readonly IAuthorRepository _authorRepository;", "        public readonly SignInManager<Author> _signInManager;", "        public List<Author> followedAuthors { get; set; } = new List<Author>();", "        string SearchText { get; set; }", "        ", "", "", "        public SearchResultsModel(IAuthorRepository authorRepository, SignInManager<Author> signInManager)", "        {", "            _authorRepository = authorRepository;", "            _signInManager = signInManager;", "        }", "", "        [BindProperty(SupportsGet = true)]", "        public string SearchWord { get; set; }", "", "        public List<Author> Authors { get; set; } = new List<Author>();", "", "        public async Task OnGet()", "        {", "            if (!string.IsNullOrEmpty(SearchWord))", "            {", "                // Fetch authors filtered by the search word", "                Authors = await _authorRepository.SearchAuthorsAsync(SearchWord);", "                SearchText = SearchWord;", "                Console.WriteLine(\"THIS IS SEARCHTEXT \" + SearchText);", "            }", "        }", "        ", "        public async Task<ActionResult> OnPostFollow(string followAuthorName)", "        {", "            //Finds the author thats logged in", "            var authorName = User.FindFirst(ClaimTypes.Name)?.Value;", "            var author = await _authorRepository.FindAuthorWithEmail(authorName);", "        ", "            //Finds the author that the logged in author wants to follow", "            var followAuthor = await _authorRepository.FindAuthorWithName(followAuthorName);", "        ", "            await _authorRepository.FollowUserAsync(author.AuthorId, followAuthor.AuthorId);", "        ", "            //updates the current author's list of followed authors", "            followedAuthors = await _authorRepository.getFollowing(author.AuthorId);", "", "            return new RedirectToPageResult(\"/SearchResults\", new { SearchWord = SearchText });", "        }", "        ", "        ", "        public async Task<ActionResult> OnPostUnfollow(string followAuthorName)", "        {", "            //Finds the author thats logged in", "            var authorName = User.FindFirst(ClaimTypes.Name)?.Value;", "            var author = await _authorRepository.FindAuthorWithEmail(authorName);", "        ", "            //Finds the author that the logged in author wants to follow", "            var followAuthor = await _authorRepository.FindAuthorWithName(followAuthorName);", "        ", "            await _authorRepository.UnFollowUserAsync(author.AuthorId, followAuthor.AuthorId);", "        ", "            //updates the current author's list of followed authors", "            followedAuthors = await _authorRepository.getFollowing(author.AuthorId);", "        ", "            Console.WriteLine(\"Number of followed authors\" + followedAuthors.Count);", "", "            return RedirectToPage(\"/SearchResults\", \"jacq\");", "        }", "    }", "}"], "file_path": "src/Chirp.Web/Pages/SearchResult.cshtml.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA23-GROUP18/Chirp/commit/025d8fdb4f3526dca9dfddbbd315fa0ee73283dc", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 42, "n_files_impacted": 8, "longest_chunk": ["    public async Task SetDarkMode(string name, bool isDarkMode)", "    {", "        var author = await _authorDb.Authors.FirstOrDefaultAsync(a => a.Name == name);", "        if (author == null)", "        {", "            throw new ArgumentException($\"Author {name} does not exist\");", "        }", "        author.IsDarkMode = isDarkMode;", "        await _authorDb.SaveChangesAsync();", "    }", "    ", "    public async Task<bool> IsDarkMode(string name)", "    {", "        var author = await _authorDb.Authors.FirstOrDefaultAsync(a => a.Name == name);", "        if (author == null)", "        {", "            throw new ArgumentException($\"Author {name} does not exist\");", "        }", "        return author.IsDarkMode;", "    }", "    ", "    public async Task SetFontSizeScale(string name, int fontSizeScale)", "    {", "        var author = await _authorDb.Authors.FirstOrDefaultAsync(a => a.Name == name);", "        if (author == null)", "        {", "            throw new ArgumentException($\"Author {name} does not exist\");", "        }", "        author.FontSizeScale = fontSizeScale;", "        await _authorDb.SaveChangesAsync();", "    }", "    ", "    public async Task<int> GetFontSizeScale(string name)", "    {", "        var author = await _authorDb.Authors.FirstOrDefaultAsync(a => a.Name == name);", "        if (author == null)", "        {", "            throw new ArgumentException($\"Author {name} does not exist\");", "        }", "        return author.FontSizeScale;", "    }", "    "], "file_path": "src/Chirp.Infrastructure/Repositories/AuthorRepository.cs"}
{"Link_to_commit": "https://github.com/Only-Smiles/DevOps-2025/commit/78a1bcd0ab1e62da0eaf23a43d905e3580b88b16", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 5, "n_files_impacted": 31, "longest_chunk": ["BASE_URL = 'http://localhost:4567'", "API_URL = f\"{BASE_URL}/api\"", "BASE_DIR = dirname(abspath(__file__))", "DATABASE = join(BASE_DIR, \"tmp\", \"mock.db\")", "SCHEMA = join(BASE_DIR, \"tmp\", \"schema.sql\")"], "file_path": "src/test/conftest.py"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP30/Chirp/commit/c475a82b6d188a031fbea115713294f74931bced", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 6, "n_files_impacted": 9, "longest_chunk": ["\t\tif (LoggedInAuthorUsername == null || AuthorToFollowUsername == null) {", "\t\t\tthrow new ArgumentNullException(\"Usernames null\");", "\t\t}", "\t\tAuthor LoggedInAuthor = GetAuthorFromUsername(LoggedInAuthorUsername);", "\t\tAuthor AuthorToFollow = GetAuthorFromUsername(AuthorToFollowUsername);", "\t\tif (LoggedInAuthor.Follows.Contains(AuthorToFollow))"], "file_path": "src/ChirpInfrastructure/AuthorRepository.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP4/Chirp/commit/2bacfc6badb3026db2cbc42949f4f01bb39fab84", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 10, "n_files_impacted": 7, "longest_chunk": ["        {\r", "            Name = authors[0].Name,\r", "            Email = authors[0].Email\r", "        };\r", "    }\r", "\r", "    public void CreateAuthor(string name, string email)\r", "    {\r", "        _repository.CreateAuthor(name, email);\r", "       \r"], "file_path": "src/Chirp.Infrastructure/CheepService.cs"}
{"Link_to_commit": "https://github.com/DevOps-GroupF/itu-minitwit-devops/commit/b9944a9b016b1f92c49c2d50ed3095dbd49bea87", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 159, "n_files_impacted": 5, "longest_chunk": ["using Microsoft.AspNetCore.Mvc;", "using Microsoft.AspNetCore.Mvc.RazorPages;", "using Microsoft.EntityFrameworkCore;", "using MiniTwit.Data;", "using MiniTwit.Models;", "", "namespace MiniTwit.Pages;", "", "public class UserTimelineModel : PageModel", "{", "    private readonly ILogger<IndexModel> _logger;", "    private readonly MiniTwitContext _context;", "", "    private readonly int PER_PAGE = 30;", "", "    public UserTimelineModel(ILogger<IndexModel> logger, MiniTwitContext context)", "    {", "        _logger = logger;", "        _context = context;", "    }", "", "    public async Task<IActionResult> OnGet(string username)", "    {", "        var tempData = TempData[\"message\"];", "        if (tempData != null)", "        {", "            ViewData[\"message\"] = tempData.ToString();", "        }", "", "        User pageUser = await _context.Users.FirstOrDefaultAsync(u => u.UserName == username);", "", "        if (pageUser == null)", "        {", "            return new NotFoundResult();", "        }", "", "        if (await Utility.ValidUserIsLoggedIn(HttpContext, _context))", "        {", "            int loggedInUserIdFromSesssion;", "            User loggedInUser;", "", "            loggedInUserIdFromSesssion = Utility.GetUserIdFromHttpSession(HttpContext);", "", "            loggedInUser = await Models.User.GetUserFromUserIdAsync(", "                loggedInUserIdFromSesssion,", "                _context", "            );", "", "            ViewData[\"user\"] = loggedInUser.Id;", "", "            bool followed = _context.Followers.Any(f =>", "                f.WhoId == loggedInUser.Id && f.WhomId == pageUser.Id", "            );", "            ViewData[\"followed\"] = followed;", "        }", "", "        var messagesWithUsers = await _context", "            .Twits.Where(t => t.AuthorId == pageUser.Id)", "            .OrderByDescending(t => t.PubDate)", "            .Take(PER_PAGE)", "            .Join(", "                _context.Users,", "                message => message.AuthorId,", "                user => user.Id,", "                (message, user) =>", "                    new TwitViewModel", "                    {", "                        AuthorUsername = user.UserName,", "                        Text = message.Text,", "                        PubDate = message.PubDate,", "                        GravatarString = Utility.GetGravatar(user.Email, 48)", "                    }", "            )", "            .ToListAsync();", "", "        ViewData[\"twits\"] = messagesWithUsers;", "        ViewData[\"timelineof\"] = pageUser.Id;", "", "        return Page();", "    }", "", "    public async Task<IActionResult> OnGetFollow(string username)", "    {", "        User whomUser;", "", "        whomUser = await _context.Users.FirstOrDefaultAsync(u => u.UserName == username);", "", "        if (whomUser == null)", "        {", "            return new NotFoundResult();", "        }", "", "        bool validUserIsLoggedIn = await Utility.ValidUserIsLoggedIn(HttpContext, _context);", "", "        if (!validUserIsLoggedIn)", "        {", "            return new UnauthorizedResult();", "        }", "", "        int loggedInUserIdFromSesssion;", "        User loggedInUser;", "", "        loggedInUserIdFromSesssion = Utility.GetUserIdFromHttpSession(HttpContext);", "", "        loggedInUser = await Models.User.GetUserFromUserIdAsync(", "            loggedInUserIdFromSesssion,", "            _context", "        );", "", "        if (!await Follower.DoesFollowerExistAsync(loggedInUser.Id, whomUser.Id, _context))", "        {", "            string sqlQuery = $\"INSERT INTO Follower VALUES ({loggedInUser.Id}, {whomUser.Id})\";", "            await _context.Database.ExecuteSqlRawAsync(sqlQuery);", "        }", "", "        TempData[\"message\"] = $\"You are now following \\\"{whomUser.UserName}\\\"\";", "", "        return await OnGet(username);", "    }", "", "    public async Task<IActionResult> OnGetUnfollow(string username)", "    {", "        User whomUser;", "", "        whomUser = await _context.Users.FirstOrDefaultAsync(u => u.UserName == username);", "", "        if (whomUser == null)", "        {", "            return new NotFoundResult();", "        }", "", "        if (!await Utility.ValidUserIsLoggedIn(HttpContext, _context))", "        {", "            return new UnauthorizedResult();", "        }", "", "        int loggedInUserIdFromSesssion;", "        User loggedInUser;", "", "        loggedInUserIdFromSesssion = Utility.GetUserIdFromHttpSession(HttpContext);", "", "        loggedInUser = await Models.User.GetUserFromUserIdAsync(", "            loggedInUserIdFromSesssion,", "            _context", "        );", "", "        if (await Follower.DoesFollowerExistAsync(loggedInUser.Id, whomUser.Id, _context))", "        {", "            string sqlQuery =", "                $\"DELETE FROM Follower WHERE who_id={loggedInUser.Id} AND whom_id={whomUser.Id}\";", "", "            await _context.Database.ExecuteSqlRawAsync(sqlQuery);", "        }", "", "        TempData[\"message\"] = $\"You are no longer following \\\"{whomUser.UserName}\\\"\";", "", "        return await OnGet(username);", "    }", "}"], "file_path": "MiniTwit/Utility.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP22/Chirp/commit/79e72915526807c1929b4da13d75f31e83945504", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 40, "n_files_impacted": 1, "longest_chunk": ["    private readonly WebApplicationFactory<Program> _factory;", "    private readonly HttpClient _client;", "", "    public TestGetHttpClient(WebApplicationFactory<Program> factory)", "    {", "        _factory = factory;", "        _client = factory.CreateClient();", "    }", "", "    [Fact]", "    public async void TimeLineTest()", "    {", "        var response = await _client.GetAsync(\"/\");", "        response.EnsureSuccessStatusCode();", "", "        var publicTL = await response.Content.ReadAsStringAsync();", "        Assert.Contains(\"Public Timeline\", publicTL);", "    }", "", "    [Fact]", "    public async void AuthorTest()", "    {", "        var response = await _client.GetAsync(\"/Helge\");", "        response.EnsureSuccessStatusCode();", "", "        var helgeCheep = await response.Content.ReadAsStringAsync();", "        Assert.Contains(\"Hello, BDSA students!\", helgeCheep);", "    }", "", "    [Fact]", "    public async void PrivateTimeLineTest()", "    {", "        var response = await _client.GetAsync(\"/Adrian\");", "        response.EnsureSuccessStatusCode();", "", "        var responseString = await response.Content.ReadAsStringAsync();", "        Assert.Contains(\"Hej, velkommen til kurset\", responseString);", "        Assert.Contains(\"Adrian\", responseString);", "    }", "}"], "file_path": "test/Chirp.Tests/IntergrationTests.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP16/Chirp/commit/ed5e39a00e677a10bf987d355227b78f899b61d2", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 12, "n_files_impacted": 6, "longest_chunk": ["            var a1 = new Author() { AuthorId = 1, Id = 1, Name = \"Roger Histand\", Email = \"Roger+Histand@hotmail.com\", Cheeps = new List<Cheep>() };", "            var a2 = new Author() { AuthorId = 2, Id = 2, Name = \"Luanna Muro\", Email = \"Luanna-Muro@ku.dk\", Cheeps = new List<Cheep>() };", "            var a3 = new Author() { AuthorId = 3, Id = 3, Name = \"Wendell Ballan\", Email = \"Wendell-Ballan@gmail.com\", Cheeps = new List<Cheep>() };", "            var a4 = new Author() { AuthorId = 4, Id = 4, Name = \"Nathan Sirmon\", Email = \"Nathan+Sirmon@dtu.dk\", Cheeps = new List<Cheep>() };", "            var a5 = new Author() { AuthorId = 5, Id = 5, Name = \"Quintin Sitts\", Email = \"Quintin+Sitts@itu.dk\", Cheeps = new List<Cheep>() };", "            var a6 = new Author() { AuthorId = 6, Id = 6, Name = \"Mellie Yost\", Email = \"Mellie+Yost@ku.dk\", Cheeps = new List<Cheep>() };", "            var a7 = new Author() { AuthorId = 7, Id = 7, Name = \"Malcolm Janski\", Email = \"Malcolm-Janski@gmail.com\", Cheeps = new List<Cheep>() };", "            var a8 = new Author() { AuthorId = 8, Id = 8, Name = \"Octavio Wagganer\", Email = \"Octavio.Wagganer@dtu.dk\", Cheeps = new List<Cheep>() };", "            var a9 = new Author() { AuthorId = 9, Id = 9, Name = \"Johnnie Calixto\", Email = \"Johnnie+Calixto@itu.dk\", Cheeps = new List<Cheep>() };", "            var a10 = new Author() { AuthorId = 10, Id = 10, Name = \"Jacqualine Gilcoine\", Email = \"Jacqualine.Gilcoine@gmail.com\", Cheeps = new List<Cheep>() };", "            var a11 = new Author() { AuthorId = 11, Id = 11, Name = \"Helge\", Email = \"ropf@itu.dk\", Cheeps = new List<Cheep>() };", "            var a12 = new Author() { AuthorId = 12, Id = 12, Name = \"Adrian\", Email = \"adho@itu.dk\", Cheeps = new List<Cheep>() };"], "file_path": "src/Chirp.Web/DbInitializer.cs"}
{"Link_to_commit": "https://github.com/DevOps-GroupF/itu-minitwit-devops/commit/b6c3471f3b889b10c9678b821a90b1083098fbf3", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 122, "n_files_impacted": 9, "longest_chunk": ["using Microsoft.AspNetCore.Mvc;", "using MiniTwitInfra.Models.DataModels;", "using MiniTwitInfra.Data;", "using Microsoft.Extensions.Caching.Memory;", "using Newtonsoft.Json;", "using Microsoft.EntityFrameworkCore;", "", "using System.ComponentModel.DataAnnotations;", "", "", "", "namespace MiniTwitAPI.Controllers;", "", "[Route(\"/fllws\")]", "[ApiController]", "public class FollowController : ControllerBase", "{", "    private readonly MiniTwitContext _context;", "    private readonly IMemoryCache _memoryCache;", "    public string cacheKey = \"latest\";", "", "", "    public FollowController(MiniTwitContext context, IMemoryCache memoryCache)", "    {", "        _context = context;", "        _memoryCache = memoryCache;", "    }", "", "    /// <summary>", "    /// Used to get a given no. of follow for the a given user", "    /// </summary>", "    /// <param name=\"no\"></param>", "    /// <param name=\"latest\"></param>", "    /// <returns></returns>", "    [HttpGet]", "    public async Task<ActionResult<IEnumerable<int>>> GetFollow(string username, int no, int latest)", "    {", "        _memoryCache.Set(cacheKey, latest.ToString());", "", "        User user;", "        try", "        {", "            user = _context.Users.FirstOrDefault(u => u.UserName == username);", "        }", "        catch (ArgumentException e)", "        {", "            throw new ArgumentException(e.Message);", "        }", "", "        var followingIds = _context", "        .Followers.Where(f => f.WhoId == user.Id)", "        .Select(f => f.WhomId)", "        .ToList();", "", "        Response.ContentType = \"application/json\";", "        return followingIds;", "", "    }", "", "    [HttpPost]", "    public async Task<ActionResult<string>> FollowAction(string username, int latest)", "    {", "        _memoryCache.Set(cacheKey, latest.ToString());", "", "        string body;", "        using (StreamReader stream = new StreamReader(HttpContext.Request.Body))", "        {", "            body = await stream.ReadToEndAsync();", "        }", "", "        Dictionary<string, string> dataDic = JsonConvert.DeserializeObject<Dictionary<string, string>>(body);", "", "        Response.ContentType = \"application/json\";", "", "        if (dataDic.ContainsKey(\"follow\"))", "        {", "            // followed person ", "            User user;", "            User whom;", "            try", "            {", "                user = _context.Users.FirstOrDefault(u => u.UserName == username);", "                whom = _context.Users.FirstOrDefault(u => u.UserName == dataDic[\"follow\"]);", "            }", "            catch (ArgumentException e)", "            {", "                throw new ArgumentException(e.Message);", "            }", "", "            string sqlQuery = $\"INSERT INTO Follower VALUES ({user.Id}, {whom.Id})\";", "            await _context.Database.ExecuteSqlRawAsync(sqlQuery);", "", "            return \"successful followed person\";", "", "        }", "        else if (dataDic.ContainsKey(\"unfollow\"))", "        {", "            // unfollow person", "            User user;", "            User whom;", "            try", "            {", "                user = _context.Users.FirstOrDefault(u => u.UserName == username);", "                whom = _context.Users.FirstOrDefault(u => u.UserName == dataDic[\"unfollow\"]);", "            }", "            catch (ArgumentException e)", "            {", "                throw new ArgumentException(e.Message);", "            }", "", "            string sqlQuery =", "            $\"DELETE FROM Follower WHERE who_id={user.Id} AND whom_id={whom.Id}\";", "            await _context.Database.ExecuteSqlRawAsync(sqlQuery);", "            return \"successful Unfollowed person\";", "        }", "", "        Response.ContentType = \"application/json\";", "        return \"Error Eccour\";", "    }", "", "}", ""], "file_path": "MiniTwitAPI/Controllers/MsgsController.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP2/Chirp/commit/b0b2002a9e66cc051f1233118a6630d4ff77a595", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 24, "n_files_impacted": 2, "longest_chunk": ["    ", "    [Fact]", "    public void CsvToCheepInConsole() //tror den er e2e", "    {", "        // Arrange", "        string path = \"../../../../../data/CsvParseTest.csv\";", "", "        // Act", "        var cheeps = CSVParser.Parse<Cheep>(path);", "        using (var consoleOutput = new StringWriter())", "        {", "            Console.SetOut(consoleOutput);", "", "", "            // Act", "            UserInterface.PrintCheeps(cheeps);", "            var outputLines = consoleOutput.ToString().Trim().Split(Environment.NewLine); //source: https://stackoverflow.com/a/22878533 .newLine sikrer at det virker b\u00e5de p\u00e5 mac og windows", "", "", "            // Assert", "            Assert.Equal(\"ageh @ 01/08/23 14:09:20: SIIIIIUUUUUUUU!\", outputLines[0].Trim());", "            Assert.Equal(\"nitn @ 02/08/23 14:19:38: Recently engaged\", outputLines[1].Trim());", "        }", "    }"], "file_path": "test/Chirp.CLI.Tests/Chirp.CLI.IntegrationTests.cs"}
{"Link_to_commit": "https://github.com/DevOps-2024-group-p/maxitwit/commit/28ce9108ada32882e6c34973131766ef27fd7276", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 100, "n_files_impacted": 2, "longest_chunk": ["const sqlite3 = require('sqlite3').verbose();", "", "class UserService {", "    constructor() {", "        this.db = new sqlite3.Database('./db/minitwit.db', sqlite3.OPEN_READWRITE, (err) => {", "            if (err) {", "                console.error(err.message);", "            } else {", "                console.log('Added db connection from user service');", "            }", "        });", "    }", "", "    async addMessage(userId, messageContent, currentDate) {", "        const flagged = 0;", "        const sql = `INSERT INTO message (author_id, text, pub_date, flagged) VALUES (?, ?, ?, ?)`;", "        return new Promise((resolve, reject) => {", "            this.db.run(sql, [userId, messageContent, currentDate, flagged], (err) => {", "                if (err) {", "                    reject(err);", "                } else {", "                    resolve();", "                }", "            });", "        });", "    }", "", "    async getMessagesByUserId(id) {", "        const sql = `SELECT message.text, message.pub_date, message.flagged, user.username, user.email ", "                    FROM message", "                    JOIN user ON message.author_id = user.user_id", "                    WHERE message.flagged != 1 AND message.author_id = ?", "                    ORDER BY message.pub_date DESC", "                    LIMIT 50`;", "        return new Promise((resolve, reject) => {", "            this.db.all(sql, [id], (err, messages) => {", "                if (err) {", "                    reject(err);", "                } else {", "                    resolve(messages);", "                }", "            });", "        });", "    }", "", "    async getMessagesFromUserAndFollowedUsers(userId) {", "        const sql = `SELECT message.text, message.pub_date, message.flagged, user.username, user.email ", "                    FROM message", "                    JOIN user ON message.author_id = user.user_id", "                    JOIN follower ON user.user_id = follower.who_id", "                    WHERE message.flagged != 1 AND (follower.whom_id = message.author_id OR message.author_id = ?)", "                    ORDER BY message.pub_date DESC", "                    LIMIT 50`;", "        return new Promise((resolve, reject) => {", "            this.db.all(sql, [userId], (err, messages) => {", "                if (err) {", "                    reject(err);", "                } else {", "                    resolve(messages);", "                }", "            });", "        });", "    }", "", "    async getPublicTimelineMessages() {", "        const sql = `SELECT message.text, message.pub_date, message.flagged, user.username, user.email ", "                    FROM message", "                    JOIN user ON message.author_id = user.user_id", "                    WHERE message.flagged != 1", "                    ORDER BY message.pub_date DESC", "                    LIMIT 50`;", "        return new Promise((resolve, reject) => {", "            this.db.all(sql, [], (err, messages) => {", "                if (err) {", "                    reject(err);", "                } else {", "                    resolve(messages);", "                }", "            });", "        });", "    }", "", "    async getUserIdByUsername(username) {", "        const sql = `SELECT user_id FROM user ", "                    JOIN message m ", "                    ON m.author_id = user.user_id ", "                    WHERE user.username = ?`;", "        return new Promise((resolve, reject) => {", "            this.db.get(sql, [username], (err, row) => {", "                if (err) {", "                    reject(err);", "                } else {", "                    resolve(row ? row.user_id : null);", "                }", "            });", "        });", "    }", "}", "", "module.exports = UserService;"], "file_path": "services/userService.js"}
{"Link_to_commit": "https://github.com/DevOps-Ben11/minitwit/commit/3c52bb3d12ebab6fb55e07e454d29c758887f860", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 51, "n_files_impacted": 2, "longest_chunk": ["", "func NewServer() Server {", "\tdb, err := gorm.Open(sqlite.Open(\"../tmp/minitwit.db\"), &gorm.Config{})", "", "\tif err != nil {", "\t\tlog.Fatalln(\"Could not open Database\", err)", "\t}", "", "\ts := Server{", "\t\tr:  mux.NewRouter(),", "\t\tdb: db,", "\t}", "", "\treturn s", "}", "", "func (s *Server) StartServer() {", "\tlog.Println(\"Starting server on port\", port)", "\tlog.Fatal(http.ListenAndServe(port, s.r))", "}", "", "func (s *Server) InitRoutes() error {", "\trepo := repository.CreateRepository(s.db)", "\trH := handler.CreateRegisterHandler(repo)", "\tlH := handler.CreateLoginHandler(repo)", "", "\ts.r.Handle(\"/register\", mw.Auth(http.HandlerFunc(rH.RegisterHandler)))", "\ts.r.Handle(\"/login\", mw.Auth(http.HandlerFunc(lH.LoginHandler)))", "", "\ts.r.PathPrefix(\"/static/\").Handler(http.StripPrefix(\"/static/\", http.FileServer(http.Dir(\"web/static\"))))", "", "\t// s.Get(\"/latest\", s.LatestHandler)", "\t// s.Post(\"/sim/register\", s.RegisterSimHandler)", "", "\t// s.Get(\"/msgs/{username}\", s.GetUserMsgsHandler)", "\t// s.Post(\"/msgs/{username}\", s.PostUserMsgsHandler)", "\t// s.Get(\"/msgs\", s.MsgsHandler)", "\t// s.Get(\"/fllws/{username}\", s.GetUserFollowsHandler)", "\t// s.Post(\"/fllws/{username}\", s.PostUserFollowsHandler)", "", "\treturn nil", "}", "", "func (s *Server) InitDB() error {", "\terr := s.db.AutoMigrate(", "\t\t&model.User{},", "\t\t&model.Follower{},", "\t\t&model.Message{},", "\t)", "\treturn err", "}"], "file_path": "backend/repository/repository.go"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP4/Chirp/commit/7382b2a918c2569c709f208774711fa8e416fc8a", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 238, "n_files_impacted": 7, "longest_chunk": ["// Licensed to the .NET Foundation under one or more agreements.", "// The .NET Foundation licenses this file to you under the MIT license.", "#nullable disable", "", "using System;", "using System.ComponentModel.DataAnnotations;", "using System.Security.Claims;", "using System.Text;", "using System.Text.Encodings.Web;", "using System.Threading;", "using System.Threading.Tasks;", "using Microsoft.AspNetCore.Authorization;", "using Microsoft.Extensions.Options;", "using Microsoft.AspNetCore.Identity;", "using Microsoft.AspNetCore.Identity.UI.Services;", "using Microsoft.AspNetCore.Mvc;", "using Microsoft.AspNetCore.Mvc.RazorPages;", "using Microsoft.AspNetCore.WebUtilities;", "using Microsoft.Extensions.Logging;", "using Chirp.Infrastructure;", "", "namespace Chirp.Web.Areas.Identity.Pages.Account", "{", "    [AllowAnonymous]", "    public class ExternalLoginModel : PageModel", "    {", "        private readonly SignInManager<ChirpUser> _signInManager;", "        private readonly UserManager<ChirpUser> _userManager;", "        private readonly IUserStore<ChirpUser> _userStore;", "        private readonly IUserEmailStore<ChirpUser> _emailStore;", "        private readonly IEmailSender _emailSender;", "        private readonly ILogger<ExternalLoginModel> _logger;", "", "        public ExternalLoginModel(", "            SignInManager<ChirpUser> signInManager,", "            UserManager<ChirpUser> userManager,", "            IUserStore<ChirpUser> userStore,", "            ILogger<ExternalLoginModel> logger,", "            IEmailSender emailSender)", "        {", "            _signInManager = signInManager;", "            _userManager = userManager;", "            _userStore = userStore;", "            _emailStore = GetEmailStore();", "            _logger = logger;", "            _emailSender = emailSender;", "        }", "", "        /// <summary>", "        ///     This API supports the ASP.NET Core Identity default UI infrastructure and is not intended to be used", "        ///     directly from your code. This API may change or be removed in future releases.", "        /// </summary>", "        [BindProperty]", "        public InputModel Input { get; set; }", "", "        /// <summary>", "        ///     This API supports the ASP.NET Core Identity default UI infrastructure and is not intended to be used", "        ///     directly from your code. This API may change or be removed in future releases.", "        /// </summary>", "        public string ProviderDisplayName { get; set; }", "", "        /// <summary>", "        ///     This API supports the ASP.NET Core Identity default UI infrastructure and is not intended to be used", "        ///     directly from your code. This API may change or be removed in future releases.", "        /// </summary>", "        public string ReturnUrl { get; set; }", "", "        /// <summary>", "        ///     This API supports the ASP.NET Core Identity default UI infrastructure and is not intended to be used", "        ///     directly from your code. This API may change or be removed in future releases.", "        /// </summary>", "        [TempData]", "        public string ErrorMessage { get; set; }", "", "        /// <summary>", "        ///     This API supports the ASP.NET Core Identity default UI infrastructure and is not intended to be used", "        ///     directly from your code. This API may change or be removed in future releases.", "        /// </summary>", "        public class InputModel", "        {", "            /// <summary>", "            ///     This API supports the ASP.NET Core Identity default UI infrastructure and is not intended to be used", "            ///     directly from your code. This API may change or be removed in future releases.", "            /// </summary>", "            [Required]", "            [EmailAddress]", "            public string Email { get; set; }", "        }", "", "        public IActionResult OnGet() => RedirectToPage(\"./Login\");", "", "        public IActionResult OnPost(string provider, string returnUrl = null)", "        {", "            // Request a redirect to the external login provider.", "            var redirectUrl = Url.Page(\"./ExternalLogin\", pageHandler: \"Callback\", values: new { returnUrl });", "            var properties = _signInManager.ConfigureExternalAuthenticationProperties(provider, redirectUrl);", "", "            return new ChallengeResult(provider, properties);", "        }", "", "        public async Task<IActionResult> OnGetCallbackAsync(string returnUrl = null, string remoteError = null)", "        {", "            returnUrl = returnUrl ?? Url.Content(\"~/\");", "            if (remoteError != null)", "            {", "                ErrorMessage = $\"Error from external provider: {remoteError}\";", "                return RedirectToPage(\"./Login\", new { ReturnUrl = returnUrl });", "            }", "", "            var info = await _signInManager.GetExternalLoginInfoAsync();", "            if (info == null)", "            {", "                ErrorMessage = \"Error loading external login information.\";", "                return RedirectToPage(\"./Login\", new { ReturnUrl = returnUrl });", "            }", "", "            // Attempt to sign in the user with the external login info", "            var result = await _signInManager.ExternalLoginSignInAsync(info.LoginProvider, info.ProviderKey, isPersistent: false, bypassTwoFactor: true);", "            if (result.Succeeded)", "            {", "                _logger.LogInformation(\"{Name} logged in with {LoginProvider} provider.\", info.Principal.Identity.Name, info.LoginProvider);", "                return LocalRedirect(returnUrl);", "            }", "            if (result.IsLockedOut)", "            {", "                return RedirectToPage(\"./Lockout\");", "            }", "", "            // If user doesn't exist, create the user automatically and log them in", "            var email = info.Principal.FindFirstValue(ClaimTypes.Email);", "            var userName = info.Principal.FindFirstValue(ClaimTypes.Name);", "", "            if (email == null)", "            {", "                ErrorMessage = \"Email not provided by external provider.\";", "                return RedirectToPage(\"./Login\", new { ReturnUrl = returnUrl });", "            }", "", "            var user = new ChirpUser", "            {", "                UserName = userName,", "                Email = email", "            };", "", "            var createResult = await _userManager.CreateAsync(user);", "            if (createResult.Succeeded)", "            {", "                var addLoginResult = await _userManager.AddLoginAsync(user, info);", "                if (addLoginResult.Succeeded)", "                {", "                    _logger.LogInformation(\"User created and logged in with {LoginProvider} provider.\", info.LoginProvider);", "", "                    await _signInManager.SignInAsync(user, isPersistent: false, info.LoginProvider);", "                    return LocalRedirect(returnUrl);", "                }", "            }", "", "            foreach (var error in createResult.Errors)", "            {", "                ModelState.AddModelError(string.Empty, error.Description);", "            }", "", "            // If creation or login failed, redirect to login with an error", "            return RedirectToPage(\"./Login\", new { ReturnUrl = returnUrl });", "        }", "        public async Task<IActionResult> OnPostConfirmationAsync(string returnUrl = null)", "        {", "            returnUrl = returnUrl ?? Url.Content(\"~/\");", "            // Get the information about the user from the external login provider", "            var info = await _signInManager.GetExternalLoginInfoAsync();", "            if (info == null)", "            {", "                ErrorMessage = \"Error loading external login information during confirmation.\";", "                return RedirectToPage(\"./Login\", new { ReturnUrl = returnUrl });", "            }", "", "            if (ModelState.IsValid)", "            {", "                var user = CreateUser();", "", "                // Use the email provided by the external login provider directly", "                var userEmail = info.Principal.FindFirstValue(ClaimTypes.Email);", "                var userName = info.Principal.FindFirstValue(ClaimTypes.Name);", "", "                Console.WriteLine($\"{userName} {userEmail}\");", "", "                await _userStore.SetUserNameAsync(user, userName, CancellationToken.None);", "                await _emailStore.SetEmailAsync(user, Input.Email, CancellationToken.None);", "", "                var result = await _userManager.CreateAsync(user);", "                if (result.Succeeded)", "                {", "                    result = await _userManager.AddLoginAsync(user, info);", "                    if (result.Succeeded)", "                    {", "                        _logger.LogInformation(\"User created an account using {Name} provider.\", info.LoginProvider);", "", "                        // Skip email confirmation and sign the user in directly", "                        await _signInManager.SignInAsync(user, isPersistent: false, info.LoginProvider);", "                        return LocalRedirect(returnUrl);", "                    }", "                }", "                foreach (var error in result.Errors)", "                {", "                    ModelState.AddModelError(string.Empty, error.Description);", "                }", "            }", "", "            ProviderDisplayName = info.ProviderDisplayName;", "            ReturnUrl = returnUrl;", "            return Page();", "        }", "", "", "        private ChirpUser CreateUser()", "        {", "            try", "            {", "                return Activator.CreateInstance<ChirpUser>();", "            }", "            catch", "            {", "                throw new InvalidOperationException($\"Can't create an instance of '{nameof(ChirpUser)}'. \" +", "                    $\"Ensure that '{nameof(ChirpUser)}' is not an abstract class and has a parameterless constructor, or alternatively \" +", "                    $\"override the external login page in /Areas/Identity/Pages/Account/ExternalLogin.cshtml\");", "            }", "        }", "", "        private IUserEmailStore<ChirpUser> GetEmailStore()", "        {", "            if (!_userManager.SupportsUserEmail)", "            {", "                throw new NotSupportedException(\"The default UI requires a user store with email support.\");", "            }", "            return (IUserEmailStore<ChirpUser>)_userStore;", "        }", "    }", "}"], "file_path": "src/Chirp.Web/Areas/Identity/Pages/Account/Manage/ManageNavPages.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP7/Chirp/commit/69b6620a705cbe5d62a1fc104ae373717c0ba4ac", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 13, "n_files_impacted": 5, "longest_chunk": ["\ufeffusing Microsoft.AspNetCore.Mvc;", "using Microsoft.AspNetCore.Mvc.RazorPages;", "using Chirp.Web.Pages;", "", "namespace Chirp.Web.Pages.Shared;", "", "public class SubmitCheepComponent : ViewComponent", "{", "    public void OnGet()", "    {", "        ", "    }", "}"], "file_path": "src/Chirp.Web/Pages/Shared/SubmitCheepComponent.cshtml.cs"}
{"Link_to_commit": "https://github.com/devops-group-l/chirp/commit/aa2e354fe831f4ad683fd50190268b20efdbf2d8", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 39, "n_files_impacted": 3, "longest_chunk": ["        private string _filePath;", "", "        public CsvDatabase(string filePath)", "        {", "            _filePath = filePath;", "        }", "", "        public void AddCheep(Cheep cheep)", "        {", "            using (var sw = new StreamWriter(_filePath, append: true))", "            using (var csv = new CsvWriter(sw, CultureInfo.InvariantCulture))", "            {", "                csv.WriteRecord(cheep);", "                sw.WriteLine();", "            }", "        }", "", "        public List<String> GetCheeps()", "        {", "            List<string> cheepsList = new List<string>();", "", "            using (StreamReader sr = new StreamReader(_filePath))", "            using (var csv = new CsvReader(sr, CultureInfo.InvariantCulture))", "            {", "                while (csv.Read())", "                {", "                    var cheep = csv.GetRecord<Cheep>();", "                    cheepsList.Add($\"{cheep.Author} @ {TimeStampConversion(cheep.Timestamp)}: {cheep.Message}\");", "                }", "            }", "", "            return cheepsList;", "        }", "        static string TimeStampConversion(long unix)", "        {", "            DateTimeOffset dto = DateTimeOffset.FromUnixTimeSeconds(unix);", "            string Date = dto.ToString(\"dd/MM/yyyy HH:mm:ss\");", "            return Date;", "        }"], "file_path": "src/Chirp.CSVDB/CSVDatabase.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA23-GROUP11/Chirp/commit/30cdc35c1c7e1c9d181adb5e1c5e48247147ee49", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 24, "n_files_impacted": 2, "longest_chunk": ["        public static void Main(string[] args)", "        {", "            try", "            {", "                switch (args[0])", "                {", "                    case \"read\":", "                        Read();", "                        break;", "", "                    case \"cheep\":", "                        CheepWrite(args.Skip(1).ToArray());", "                        break;", "", "                    default:", "                        Console.WriteLine(\"Error: Invalid command.\");", "                        break;", "                }", "            }", "            catch (IndexOutOfRangeException e)", "            {", "                Console.WriteLine(\"Error: \" + e.Message);", "                Console.WriteLine(\"It appears that you did not specify a command.\");", "                Console.WriteLine(\"* Try: read or cheep\");"], "file_path": "src/Chirp.CLI.Client/Program.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA23-GROUP9/Chirp/commit/0fdf9e862246dde4bb3a8fbff35b528cc6aa13b2", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 5, "n_files_impacted": 2, "longest_chunk": ["    /// <summary>", "    /// Initializes a new instance of public timeline.", "    /// </summary>", "    /// <param name=\"cheepRepo\"></param>", "    /// <param name=\"authorRepo\"></param>"], "file_path": "src/Chirp.Web/Pages/Public.cshtml.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA23-GROUP13/Chirp/commit/f0b819ef7ef701b50bc1bd20142fabab4875f9fe", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 22, "n_files_impacted": 2, "longest_chunk": ["                IDatabase<Cheep> db = new CSVDatabase<Cheep>();", "                //Read cheeps", "                if (options.CheepCount != null)", "                {", "", "                    var cheeps = db.Read(options.CheepCount.Value);", "                    UserInterface.PrintCheeps(cheeps);", "                }", "", "                //Cheep a cheep", "                if (!string.IsNullOrWhiteSpace(options.CheepMessage))", "                {", "", "                    string Author = Environment.UserName;", "                    string Message = options.CheepMessage;", "                    long Timestamp = DateTimeOffset.UtcNow.ToUnixTimeSeconds();", "", "                    db.Store(new Cheep(Author, Message, Timestamp));", "", "                    UserInterface.PrintMessage($\"Cheeped a cheep! The cheep is: {options.CheepMessage}\");", "                }", ""], "file_path": "src/Chirp.CLI/Program.cs"}
{"Link_to_commit": "https://github.com/DevOps-GroupF/itu-minitwit-devops/commit/0e2219c96a5eccc07b43fe36e01b49467cf116a0", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 13, "n_files_impacted": 2, "longest_chunk": ["                        // Save changes to the database", "                        await _context.SaveChangesAsync();", "                        return Ok($\"You are now following \\\"{whom.UserName}\\\"\");", "                    }", "                    else", "                    {", "                        return BadRequest(\"You are already not following the user\");", "                    }", "                }", "                else", "                {", "                    return NotFound(\"Follower user not found\");", "                }"], "file_path": "MiniTwit/Areas/Api/Controllers/FollowerController.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA23-GROUP6/Chirp/commit/dca50ec4a9fba84d225cb4995c87544fe8a1cc8b", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 8, "n_files_impacted": 4, "longest_chunk": ["", "        totalCheeps = Cheeps.Count;", "        cheepsPerPage = 32;", "", "        if (page == 0) ", "        {", "            page = 1;", "        }"], "file_path": "src/Chirp.WebService/Pages/Public.cshtml.cs"}
{"Link_to_commit": "https://github.com/HULKs/hulk/commit/678469667703b0095f7d3d8523c3be24eec12acf", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 51, "n_files_impacted": 9, "longest_chunk": ["from stable_baselines3.common.monitor import Monitor", "from stable_baselines3.common.utils import get_device", "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder", "from wandb.integration.sb3 import WandbCallback", "", "if get_device() != torch.device(\"cpu\"):", "    NVIDIA_ICD_CONFIG_PATH = \"/usr/share/glvnd/egl_vendor.d/10_nvidia.json\"", "    if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):", "        with open(NVIDIA_ICD_CONFIG_PATH, \"w\") as f:", "            _ = f.write(\"\"\"{", "                                \"file_format_version\" : \"1.0.0\",", "                                \"ICD\" : {", "                                    \"library_path\" : \"libEGL_nvidia.so.0\"", "                                }", "                            }\"\"\")", "", "    # Configure MuJoCo to use the EGL rendering backend (requires GPU)", "    os.environ[\"MUJOCO_GL\"] = \"egl\"", "", "", "# taken from https://gymnasium.farama.org/main/_modules/gymnasium/wrappers/record_video/", "def capped_cubic_video_schedule(episode_id: int) -> bool:", "    \"\"\"The default episode trigger.", "", "    This function will trigger recordings at the episode indices 0, 1, 8, 27, ..., :math:`k^3`, ..., 729, 1000, 2000, 3000, ...", "", "    Args:", "        episode_id: The episode number", "", "    Returns:", "        If to apply a video schedule number", "    \"\"\"", "    if episode_id < 10000:", "        return int(round(episode_id ** (1.0 / 3))) ** 3 == episode_id", "    else:", "        return episode_id % 10000 == 0", "", "", "gym.register(", "    id=\"NaoStandup-v1\",", "    entry_point=\"nao_env:NaoStandup\",", "    max_episode_steps=2500,", ")", "", "config = {", "    \"policy_type\": \"MlpPolicy\",", "    \"total_timesteps\": 1000000,", "    \"env_name\": \"NaoStandup-v1\",", "    \"render_mode\": \"rgb_array\",", "}", ""], "file_path": "tools/machine-learning/mujoco/standup.py"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP4/Chirp/commit/f7805880706a75106eb93ee348a4d52f7bd82c80", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 11, "n_files_impacted": 26, "longest_chunk": ["is_global = true", "build_property.TargetFramework = net7.0", "build_property.TargetPlatformMinVersion = ", "build_property.UsingMicrosoftNETSdkWeb = ", "build_property.ProjectTypeGuids = ", "build_property.InvariantGlobalization = ", "build_property.PlatformNeutralAssembly = ", "build_property.EnforceExtendedAnalyzerRules = ", "build_property._SupportedPlatformList = Linux,macOS,Windows", "build_property.RootNamespace = Chrip.CLI", "build_property.ProjectDir = /Users/bergurdavidsen/Downloads/Chirp/Chirp.CLI/"], "file_path": "Chirp.CLI/obj/Debug/net7.0/Chrip.CLI.GlobalUsings.g.cs"}
{"Link_to_commit": "https://github.com/richardTowers/boat-physics-sim/commit/ea57808da39605265c3cd74f61e95403a41075b4", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 62, "n_files_impacted": 4, "longest_chunk": ["// Credit: ChatGPT - https://chat.openai.com/share/a1af86f5-0449-4215-9bab-61b70ea4de84", "", "function doPolygonsIntersect(polygon1, polygon2) {", "  function getAxes(polygon) {", "    const axes = [];", "    const points = polygon.length;", "", "    for (let i = 0; i < points; i++) {", "      const p1 = polygon[i];", "      const p2 = polygon[(i + 1) % points];", "      const edge = { x: p2.x - p1.x, y: p2.y - p1.y };", "      const normal = { x: -edge.y, y: edge.x };", "      axes.push(normal);", "    }", "", "    return axes;", "  }", "", "  function project(polygon, axis) {", "    const points = polygon.length;", "    let min = Infinity;", "    let max = -Infinity;", "", "    for (let i = 0; i < points; i++) {", "      const dotProduct = polygon[i].x * axis.x + polygon[i].y * axis.y;", "      if (dotProduct < min) min = dotProduct;", "      if (dotProduct > max) max = dotProduct;", "    }", "", "    return { min, max };", "  }", "", "  function overlap(projection1, projection2) {", "    return (", "      projection1.min <= projection2.max && projection1.max >= projection2.min", "    );", "  }", "", "  const axes1 = getAxes(polygon1);", "  const axes2 = getAxes(polygon2);", "", "  for (const axis of [...axes1, ...axes2]) {", "    const projection1 = project(polygon1, axis);", "    const projection2 = project(polygon2, axis);", "", "    if (!overlap(projection1, projection2)) {", "      return false; // No collision, early exit", "    }", "  }", "", "  return true; // Collided on all axes, there is a collision", "}", "", "export default function isBoatColliding(boat, walls) {", "  for (const wall of walls) {", "    if (doPolygonsIntersect(boat, wall)) {", "      return wall; // Collision detected", "    }", "  }", "", "  return false; // No collision", "}"], "file_path": "src/overworld.js"}
{"Link_to_commit": "https://github.com/ITU-BDSA23-GROUP11/Chirp/commit/c28152cc0d5a7efa609503e92ab5ac5ac6a954ce", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 8, "n_files_impacted": 4, "longest_chunk": ["", "    public List<Cheep> GetCheepsFromAuthorNameForPage(string authorName, int pageNumber)", "    {", "        return GetCheepsFromAuthorNameWithAuthors(authorName)", "            .Skip((pageNumber - 1) * 32)", "            .Take(32)//Refactor", "            .ToList();", "    }"], "file_path": "src/Chirp.DBService/Repositories/CheepRepository.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP30/Chirp/commit/1914a6522456c836fb24957dc7f24682421d6193", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 7, "n_files_impacted": 2, "longest_chunk": ["string wwwrootPath = Path.Combine(Directory.GetCurrentDirectory(), \"UserFacade\");", "app.UseStaticFiles(new StaticFileOptions", "{", "    FileProvider = new PhysicalFileProvider(wwwrootPath),", "    RequestPath = \"/wwwroot\"", "});", "//app.UseStaticFiles();"], "file_path": "src/Program.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA23-GROUP11/Chirp/commit/d0118784bc601ae651500ca17919d77ac4a37f2a", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 3, "longest_chunk": ["", "    [Fact]", "    public async void FrontPageContains32Cheeps()", "    {", "        //Arrange & Act", "        var rsp = await usableClient.GetAsync(\"/\");", "        string htmlContent = await rsp.Content.ReadAsStringAsync();", "", "        //Parse the htmlContent to a HTMLDocument", "        HtmlDocument doc = new HtmlDocument();", "        doc.LoadHtml(htmlContent);", "", "        int amountOfListItems = doc.DocumentNode.SelectNodes(\"//li\").Count();", "", "        Assert.Equal(32, amountOfListItems);", "    }"], "file_path": "test/Chirp.WebService.Tests/PublicTimeline/PublicTimelineIntegrationTest.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA23-GROUP11/Chirp/commit/be1347dfea2c33cd26b6e0c0f5f67c963d8cbc77", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 11, "n_files_impacted": 9, "longest_chunk": ["    static string filePath = @\"../../data/Chirp.CLI/chirp_cli_db.csv\";", "    IDatabaseRepository db = new CsvDatabase(filePath);", "    static string userName = Environment.UserName;", "    static long timestamp = DateTimeOffset.UtcNow.ToUnixTimeSeconds();", "    static int count = 0;", "    ", "    public record Cheep(string Author, string Message, long Timestamp);", "        ", "    static void Main(string[] args)", "    {", "        try"], "file_path": "src/Chirp.CLI.Client/Program.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA23-GROUP11/Chirp/commit/d49d87e78649fd7a6faf80a9e5efb74de604d68b", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 11, "n_files_impacted": 9, "longest_chunk": ["    static string filePath = @\"../../data/Chirp.CLI/chirp_cli_db.csv\";", "    IDatabaseRepository db = new CsvDatabase(filePath);", "    static string userName = Environment.UserName;", "    static long timestamp = DateTimeOffset.UtcNow.ToUnixTimeSeconds();", "    static int count = 0;", "    ", "    public record Cheep(string Author, string Message, long Timestamp);", "        ", "    static void Main(string[] args)", "    {", "        try"], "file_path": "src/Chirp.CLI.Client/Program.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA23-GROUP27/Chirp/commit/9f807aa38ed1bab0eea80c4ab9689134297e3199", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 16, "n_files_impacted": 4, "longest_chunk": ["        Cheeps = _cheepRepository.GetCheepsFromPage(CurrentPage);", "", "        // Set follow status for each cheep author", "        if (User.Identity?.IsAuthenticated == true)", "        {", "            foreach (var cheep in Cheeps)", "            {", "                var authorName = cheep.AuthorName;", "                var isFollowing = _followerRepository", "                    .GetFollowersFromAuthor(authorName)", "                    .Any(follower => follower.Name == User.Identity.Name);", "", "                FollowStatus[authorName] = isFollowing;", "            }", "        }", ""], "file_path": "src/Chirp.Web/Pages/Public.cshtml.cs"}
{"Link_to_commit": "https://github.com/andy19910102/Spinning-Donut-in-Terminal-by-ChatGTP-4/commit/58e7dcdc7bf260124c791b4100f655be99fd5a0b", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 152, "n_files_impacted": 5, "longest_chunk": ["import numpy as np", "import time", "import json", "", "# Create a class for the donut", "class SpinningDonut:", "    \"\"\"", "    A class representing a spinning 3D donut.", "    \"\"\"", "", "    # Initialize the donut", "    def __init__(self, screen_size=40, theta_spacing=0.07, phi_spacing=0.02, delay=0.01):", "        \"\"\"", "        Initialize the donut with certain parameters.", "", "        Parameters:", "        screen_size (int): The size of the screen.", "        theta_spacing (float): The spacing between thetas.", "        phi_spacing (float): The spacing between phis.", "        delay (float): The delay between frames.", "        \"\"\"", "        # Set the variables", "        # screen_size: The size of the screen", "        self.screen_size = screen_size", "        # theta_spacing: The spacing between thetas", "        self.theta_spacing = theta_spacing", "        # phi_spacing: The spacing between phis", "        self.phi_spacing = phi_spacing", "        # illumination: The illumination of the donut", "        self.illumination = np.fromiter(\".,-~:;=!*#$@\", dtype=\"<U1\")", "        self.A = 1", "        self.B = 1", "        self.R1 = 1", "        self.R2 = 2", "        self.K2 = 5", "        self.K1 = self.screen_size * self.K2 * 3 / (8 * (self.R1 + self.R2))", "        # delay: The delay between frames", "        self.delay = delay", "", "    # Get the frame to render", "    def get_render_frame(self):", "        \"\"\"", "        Calculate the frame to render.", "", "        Returns:", "        output (np.array): The frame to render.", "        \"\"\"", "        # Get the cos and sin of A and B", "        cos_A = np.cos(self.A)", "        sin_A = np.sin(self.A)", "        cos_B = np.cos(self.B)", "        sin_B = np.sin(self.B)", "        # Create the output and zbuffer", "        output = np.full((self.screen_size, self.screen_size), \" \")", "        # zbuffer: The zbuffer of the donut", "        zbuffer = np.zeros((self.screen_size, self.screen_size))", "        # Get the cos and sin of phi and theta", "        cos_phi = np.cos(phi := np.arange(0, 2 * np.pi, self.phi_spacing))", "        # sin_phi: The sin of phi", "        sin_phi = np.sin(phi)", "        # Get the cos and sin of theta", "        cos_theta = np.cos(theta := np.arange(0, 2 * np.pi, self.theta_spacing))", "        # sin_theta: The sin of theta", "        sin_theta = np.sin(theta)", "        # Get the circle x and y", "        circle_x = self.R2 + self.R1 * cos_theta", "        # circle_y: The y of the circle", "        circle_y = self.R1 * sin_theta", "", "        # Get the x, y, and z", "        x = (np.outer(cos_B * cos_phi + sin_A * sin_B * sin_phi, circle_x) - circle_y * cos_A * sin_B).T", "        y = (np.outer(sin_B * cos_phi - sin_A * cos_B * sin_phi, circle_x) + circle_y * cos_A * cos_B).T", "        z = ((self.K2 + cos_A * np.outer(sin_phi, circle_x)) + circle_y * sin_A).T", "        # ooz: The reciprocal of z", "        ooz = np.reciprocal(z)", "        # xp: The x position", "        xp = (self.screen_size / 2 + self.K1 * ooz * x).astype(int)", "        # yp: The y position", "        yp = (self.screen_size / 2 - self.K1 * ooz * y).astype(int)", "        L1 = (((np.outer(cos_phi, cos_theta) * sin_B) - cos_A * np.outer(sin_phi, cos_theta)) - sin_A * sin_theta)", "        L2 = cos_B * (cos_A * sin_theta - np.outer(sin_phi, cos_theta * sin_A))", "        L = np.around(((L1 + L2) * 8)).astype(int).T", "        mask_L = L >= 0", "        # chars: The characters to use", "        chars = self.illumination[L]", "", "        # Render the frame", "        for i in range(90):", "            # mask: The mask", "            mask = mask_L[i] & (ooz[i] > zbuffer[xp[i], yp[i]])", "            # zbuffer: The zbuffer", "            zbuffer[xp[i], yp[i]] = np.where(mask, ooz[i], zbuffer[xp[i], yp[i]])", "            # output: The output", "            output[xp[i], yp[i]] = np.where(mask, chars[i], output[xp[i], yp[i]])", "", "        return output", "", "    # Render the frame", "    def render(self, array):", "        \"\"\"", "        Render the frame on the console.", "", "        Parameters:", "        array (np.array): The frame to render.", "        \"\"\"", "        # Print the array", "        print(*[\" \".join(row) for row in array], sep=\"\\n\")", "        # Sleep", "        time.sleep(self.delay)", "", "    def save_frames_to_json(self, filename):", "        \"\"\"", "        Save the frames to a JSON file.", "", "        Parameters:", "        filename (str): The name of the file to save.", "        \"\"\"", "        frames = []", "        for _ in range(self.screen_size * self.screen_size):", "            self.A += self.theta_spacing", "            self.B += self.phi_spacing", "            frame = self.get_render_frame()", "            # Convert the entire frame to a single string with \"\\n\" as line separators", "            frame_string = \"\\n\".join(\"\".join(row) for row in frame)", "            frames.append(frame_string)", "", "        # Save the frames to a JSON file", "        with open(filename, 'w') as f:", "            json.dump(frames, f)", "", "    # Run the donut", "    def run(self):", "        \"\"\"", "        Run the donut animation. This method loops indefinitely.", "        \"\"\"", "        # Run the donut", "        for _ in range(self.screen_size * self.screen_size):", "            # Increment A and B", "            self.A += self.theta_spacing", "            self.B += self.phi_spacing", "            # Clear the screen", "            print(\"\\x1b[H\")", "            self.render(self.get_render_frame())", "           ", "# If the file is run directly:", "if __name__ == \"__main__\":", "    # Create the donut", "    donut = SpinningDonut()", "    # Run the donut", "    # donut.run()", "    donut.save_frames_to_json(\"donut.json\")", "    print(\"done\")"], "file_path": "donut.py"}
{"Link_to_commit": "https://github.com/ITU-BDSA23-GROUP11/Chirp/commit/497f10c0e41c08c2be510e83a35aaeb40b61396f", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 65, "n_files_impacted": 1, "longest_chunk": ["\ufeffusing System.Security.Claims;", "using System.Text.Encodings.Web;", "using Chirp.Infrastructure.Contexts;", "using Microsoft.AspNetCore.Authentication;", "using Microsoft.AspNetCore.Mvc.Testing;", "using Microsoft.EntityFrameworkCore;", "using Microsoft.Extensions.Options;", "namespace Chirp.WebService.Tests.E2ETests;", "", "public class WebApplicationFactoryWithAuth<TProgram> : WebApplicationFactory<TProgram> where TProgram : class", "{", "    protected override void ConfigureWebHost(IWebHostBuilder builder)", "    {", "        builder.ConfigureServices(s =>", "        {", "            //Remove the default DBContext configuration", "            var descriptor = s.SingleOrDefault(", "                d => d.ServiceType ==", "                     typeof(DbContextOptions<ChirpDbContext>));", "", "            if (descriptor != null)", "            {", "                s.Remove(descriptor);", "            }", "            ", "            //Create an in-memory DB instance", "            s.AddDbContext<ChirpDbContext>(options =>", "            {", "                options.UseInMemoryDatabase(\"MemoryDB\");", "            });", "            ", "            s.AddAuthentication(defaultScheme: \"E2EScheme\")", "                .AddScheme<AuthenticationSchemeOptions, MockAuth>(", "                    \"E2EScheme\",options => {});", "        });", "", "        builder.UseEnvironment(\"Development\");", "    }", "}", "", "public class MockAuth : AuthenticationHandler<AuthenticationSchemeOptions>", "{", "    public MockAuth(IOptionsMonitor<AuthenticationSchemeOptions> options,", "        ILoggerFactory logger, UrlEncoder encoder, ISystemClock clock)", "        : base(options, logger, encoder, clock)", "    {", "    }", "", "    protected override Task<AuthenticateResult> HandleAuthenticateAsync()", "    {", "        var claims = new[] { ", "            new Claim(ClaimTypes.Name, \"PlaywrightTester\"), ", "            new Claim(ClaimTypes.Email, \"bdsagrup11@gmail.com\"), ", "            new Claim(ClaimTypes.GivenName, \"E2E\"), ", "            new Claim(ClaimTypes.Surname, \"User\") ", "        };", "        var identity = new ClaimsIdentity(claims, \"E2ETest\");", "        var principal = new ClaimsPrincipal(identity);", "        var ticket = new AuthenticationTicket(principal, \"E2EScheme\");", "", "        var result = AuthenticateResult.Success(ticket);", "", "        return Task.FromResult(result);", "    }", "}"], "file_path": "test/Chirp.WebService.Tests/E2ETests/WebApplicationFactoryWithAuth.cs"}
{"Link_to_commit": "https://github.com/ITU-BDSA2024-GROUP16/Chirp/commit/1f35d1420d3534e8174b6ef923e5cd57e0f19f1f", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 21, "n_files_impacted": 2, "longest_chunk": ["                // User doesn't exist; create a new user", "                user = CreateUser();", "                await _userStore.SetUserNameAsync(user, email, CancellationToken.None);", "                await _emailStore.SetEmailAsync(user, email, CancellationToken.None);", "", "                // Set user properties from external provider", "                user.Name = info.Principal.Identity.Name ?? \"Unknown\";", "                user.AuthorId = await _userManager.Users.CountAsync() + 1;", "", "                var createUserResult = await _userManager.CreateAsync(user);", "                if (createUserResult.Succeeded)", "                {", "                    await _userManager.AddClaimAsync(user, new Claim(\"Name\", user.Name));", "                    var addLoginResult = await _userManager.AddLoginAsync(user, info);", "                    if (!addLoginResult.Succeeded)", "                    {", "                        ErrorMessage = \"Failed to add external login for new user.\";", "                        return RedirectToPage(\"./Login\", new { ReturnUrl = returnUrl });", "                    }", "                }", "                else"], "file_path": "src/Chirp.Web/Areas/Identity/Pages/Account/ExternalLogin.cshtml.cs"}
{"Link_to_commit": "https://github.com/devops-group-l/chirp/commit/497f10c0e41c08c2be510e83a35aaeb40b61396f", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 65, "n_files_impacted": 1, "longest_chunk": ["\ufeffusing System.Security.Claims;", "using System.Text.Encodings.Web;", "using Chirp.Infrastructure.Contexts;", "using Microsoft.AspNetCore.Authentication;", "using Microsoft.AspNetCore.Mvc.Testing;", "using Microsoft.EntityFrameworkCore;", "using Microsoft.Extensions.Options;", "namespace Chirp.WebService.Tests.E2ETests;", "", "public class WebApplicationFactoryWithAuth<TProgram> : WebApplicationFactory<TProgram> where TProgram : class", "{", "    protected override void ConfigureWebHost(IWebHostBuilder builder)", "    {", "        builder.ConfigureServices(s =>", "        {", "            //Remove the default DBContext configuration", "            var descriptor = s.SingleOrDefault(", "                d => d.ServiceType ==", "                     typeof(DbContextOptions<ChirpDbContext>));", "", "            if (descriptor != null)", "            {", "                s.Remove(descriptor);", "            }", "            ", "            //Create an in-memory DB instance", "            s.AddDbContext<ChirpDbContext>(options =>", "            {", "                options.UseInMemoryDatabase(\"MemoryDB\");", "            });", "            ", "            s.AddAuthentication(defaultScheme: \"E2EScheme\")", "                .AddScheme<AuthenticationSchemeOptions, MockAuth>(", "                    \"E2EScheme\",options => {});", "        });", "", "        builder.UseEnvironment(\"Development\");", "    }", "}", "", "public class MockAuth : AuthenticationHandler<AuthenticationSchemeOptions>", "{", "    public MockAuth(IOptionsMonitor<AuthenticationSchemeOptions> options,", "        ILoggerFactory logger, UrlEncoder encoder, ISystemClock clock)", "        : base(options, logger, encoder, clock)", "    {", "    }", "", "    protected override Task<AuthenticateResult> HandleAuthenticateAsync()", "    {", "        var claims = new[] { ", "            new Claim(ClaimTypes.Name, \"PlaywrightTester\"), ", "            new Claim(ClaimTypes.Email, \"bdsagrup11@gmail.com\"), ", "            new Claim(ClaimTypes.GivenName, \"E2E\"), ", "            new Claim(ClaimTypes.Surname, \"User\") ", "        };", "        var identity = new ClaimsIdentity(claims, \"E2ETest\");", "        var principal = new ClaimsPrincipal(identity);", "        var ticket = new AuthenticationTicket(principal, \"E2EScheme\");", "", "        var result = AuthenticateResult.Success(ticket);", "", "        return Task.FromResult(result);", "    }", "}"], "file_path": "test/Chirp.WebService.Tests/E2ETests/WebApplicationFactoryWithAuth.cs"}
{"Link_to_commit": "https://github.com/michael-borck/hands-on-ai/commit/b9aa320028e42c748712590a7f71d46292d4a428", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 32, "n_files_impacted": 17, "longest_chunk": ["import os", "import json", "from pathlib import Path", "import importlib.resources", "", "", "def load_fallbacks():", "    \"\"\"", "    Load fallback personality messages from user, local, or default locations.", "", "    Priority:", "    1. ~/.chatcraft/fallbacks.json", "    2. ./chatcraft/data/fallbacks.local.json", "    3. packaged fallback (chatcraft/data/fallbacks.json)", "", "    Returns:", "        dict: fallback personality responses", "    \"\"\"", "    user_file = Path.home() / \".chatcraft\" / \"fallbacks.json\"", "    local_file = Path(\"chatcraft/data/fallbacks.local.json\")", "", "    if user_file.exists():", "        with user_file.open(\"r\", encoding=\"utf-8\") as f:", "            return json.load(f)", "", "    elif local_file.exists():", "        with local_file.open(\"r\", encoding=\"utf-8\") as f:", "            return json.load(f)", "", "    else:", "        with importlib.resources.open_text(\"chatcraft.data\", \"fallbacks.json\") as f:", "            return json.load(f)"], "file_path": "tools/load_fallbacks.py"}
{"Link_to_commit": "https://github.com/moshimeow/mercury_steamvr_driver/commit/0eeccd9a5a1abe5b74c5140dc4fa982163116e39", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 114, "n_files_impacted": 4, "longest_chunk": ["#include <iostream>", "#include <string>", "#include <winsock2.h>", "#include <windows.h>", "", "#pragma comment(lib, \"ws2_32.lib\")", "", "int main()", "{", "    // Initialize Winsock", "    WSADATA wsaData;", "    int iResult = WSAStartup(MAKEWORD(2,2), &wsaData);", "    if (iResult != 0) {", "        std::cerr << \"WSAStartup failed: \" << iResult << std::endl;", "        return 1;", "    }", "", "    // Create a socket for the subprocess to connect to", "    SOCKET listenSocket = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);", "    if (listenSocket == INVALID_SOCKET) {", "        std::cerr << \"Error creating socket: \" << WSAGetLastError() << std::endl;", "        WSACleanup();", "        return 1;", "    }", "", "    // Bind the socket to any available address and port 0 to let the operating system choose a free port", "    sockaddr_in listenAddr;", "    listenAddr.sin_family = AF_INET;", "    listenAddr.sin_addr.s_addr = htonl(INADDR_ANY);", "    listenAddr.sin_port = htons(0);", "    iResult = bind(listenSocket, (sockaddr*)&listenAddr, sizeof(listenAddr));", "    if (iResult == SOCKET_ERROR) {", "        std::cerr << \"Error binding socket: \" << WSAGetLastError() << std::endl;", "        closesocket(listenSocket);", "        WSACleanup();", "        return 1;", "    }", "", "    // Get the local address and port of the socket", "    sockaddr_in localAddr;", "    int localAddrLen = sizeof(localAddr);", "    iResult = getsockname(listenSocket, (sockaddr*)&localAddr, &localAddrLen);", "    if (iResult == SOCKET_ERROR) {", "        std::cerr << \"Error getting socket name: \" << WSAGetLastError() << std::endl;", "        closesocket(listenSocket);", "        WSACleanup();", "        return 1;", "    }", "", "    // Start the subprocess with the local address and port as arguments", "    STARTUPINFO startupInfo;", "    PROCESS_INFORMATION processInfo;", "    ZeroMemory(&startupInfo, sizeof(startupInfo));", "    ZeroMemory(&processInfo, sizeof(processInfo));", "    startupInfo.cb = sizeof(startupInfo);", "    std::string commandLine = \"C:\\\\dev\\\\mercury_steamvr_driver\\\\build\\\\attic\\\\cs_test\\\\cs_test_subprocess.exe \" + std::to_string(ntohs(localAddr.sin_port));", "    std::wstring wideCommandLine(commandLine.begin(), commandLine.end());", "    if (!CreateProcess(NULL, commandLine.data(), NULL, NULL, FALSE, 0, NULL, NULL, &startupInfo, &processInfo)) {", "        std::cerr << \"Error creating subprocess: \" << GetLastError() << std::endl;", "        closesocket(listenSocket);", "        WSACleanup();", "        return 1;", "    }", "", "    // Listen for the subprocess to connect", "    iResult = listen(listenSocket, SOMAXCONN);", "    if (iResult == SOCKET_ERROR) {", "        std::cerr << \"Error listening for connection: \" << WSAGetLastError() << std::endl;", "        closesocket(listenSocket);", "        WSACleanup();", "        return 1;", "    }", "", "    // Accept the connection", "    SOCKET clientSocket = accept(listenSocket, NULL, NULL);", "    if (clientSocket == INVALID_SOCKET) {", "        std::cerr << \"Error accepting connection: \" << WSAGetLastError() << std::endl;", "        closesocket(listenSocket);", "        WSACleanup();", "        return 1;", "    }", "", "    // Receive data from the subprocess", "    char recvBuffer[1024];", "    iResult = recv(clientSocket, recvBuffer, sizeof(recvBuffer), 0);", "    if (iResult == SOCKET_ERROR) {", "        std::cerr << \"Error receiving data: \" << WSAGetLastError() << std::endl;", "        closesocket(clientSocket);", "        closesocket(listenSocket);", "        WSACleanup();", "        return 1;", "    }", "    recvBuffer[iResult] = '\\0';", "    std::cout << \"Received data from subprocess: \" << recvBuffer << std::endl;", "", "    // Send data to the subprocess", "    const char* sendBuffer = \"Hello, subprocess!\";", "    iResult = send(clientSocket, sendBuffer, strlen(sendBuffer), 0);", "    if (iResult == SOCKET_ERROR) {", "        std::cerr << \"Error sending data: \" << WSAGetLastError() << std::endl;", "        closesocket(clientSocket);", "        closesocket(listenSocket);", "        WSACleanup();", "        return 1;", "    }", "", "    // Close the sockets and cleanup Winsock", "    closesocket(clientSocket);", "    closesocket(listenSocket);", "    WSACleanup();", "", "    return 0;", "}", ""], "file_path": "attic/cs_test/cs_test_subprocess.cpp"}
{"Link_to_commit": "https://github.com/ITU-BDSA23-GROUP27/Chirp/commit/b6e237239b10c62b29ef3ab45d23d76c03187df2", "n-gram matched": "co-authored-by: chatgpt", "n_lines_longer_change": 7, "n_files_impacted": 1, "longest_chunk": ["            //! ChatGPT's parser -  Parse the page parameter", "            var pageValues = Request.Query[\"page\"].ToString();", "            if (int.TryParse(pageValues, out int parsedPage) && parsedPage > 0)", "            {", "                CurrentPage = parsedPage;", "            }", ""], "file_path": "src/Chirp.Web/Pages/AboutMe.cshtml.cs"}
{"Link_to_commit": "https://github.com/abnormalhare/Elemental-On-Cards/commit/69cd694d7ba6fd73c40e7af52a1fce0695363d7a", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 95, "n_files_impacted": 2, "longest_chunk": ["package main", "", "import (", "\t\"log\"", "\t\"os\"", "\t\"os/exec\"", "\t\"path/filepath\"", ")", "", "func main() {", "\t// Define the repository URL and the temporary directory for cloning", "\trepoURL := \"https://github.com/abnormalhare/elemental-on-cards.git\"", "\ttempDir := filepath.Join(os.TempDir(), \"discord-bot-game\")", "", "\t// Remove the temporary directory if it exists", "\tif _, err := os.Stat(tempDir); err == nil {", "\t\tif err := os.RemoveAll(tempDir); err != nil {", "\t\t\tlog.Fatalf(\"Failed to remove temporary directory: %v\", err)", "\t\t}", "\t}", "", "\t// Clone the repository", "\tlog.Println(\"Cloning repository...\")", "\tcmd := exec.Command(\"git\", \"clone\", repoURL, tempDir)", "\tcmd.Stdout = os.Stdout", "\tcmd.Stderr = os.Stderr", "\tif err := cmd.Run(); err != nil {", "\t\tlog.Fatalf(\"Failed to clone repository: %v\", err)", "\t}", "", "\t// Copy files from the cloned repository to the current directory", "\tlog.Println(\"Updating files...\")", "\terr := filepath.Walk(tempDir, func(path string, info os.FileInfo, err error) error {", "\t\tif err != nil {", "\t\t\treturn err", "\t\t}", "", "\t\t// Skip the root directory", "\t\tif path == tempDir {", "\t\t\treturn nil", "\t\t}", "", "\t\t// Determine the relative path and destination path", "\t\trelPath, err := filepath.Rel(tempDir, path)", "\t\tif err != nil {", "\t\t\treturn err", "\t\t}", "\t\tdestPath := filepath.Join(\".\", relPath)", "", "\t\t// If it's a directory, create it", "\t\tif info.IsDir() {", "\t\t\tif err := os.MkdirAll(destPath, os.ModePerm); err != nil {", "\t\t\t\treturn err", "\t\t\t}", "\t\t} else {", "\t\t\t// If it's a file, copy it", "\t\t\tsrcFile, err := os.Open(path)", "\t\t\tif err != nil {", "\t\t\t\treturn err", "\t\t\t}", "\t\t\tdefer srcFile.Close()", "", "\t\t\tdestFile, err := os.Create(destPath)", "\t\t\tif err != nil {", "\t\t\t\treturn err", "\t\t\t}", "\t\t\tdefer destFile.Close()", "", "\t\t\tif _, err := destFile.ReadFrom(srcFile); err != nil {", "\t\t\t\treturn err", "\t\t\t}", "\t\t}", "\t\treturn nil", "\t})", "\tif err != nil {", "\t\tlog.Fatalf(\"Failed to update files: %v\", err)", "\t}", "", "\t// Clean up the temporary directory", "\tlog.Println(\"Cleaning up...\")", "\tif err := os.RemoveAll(tempDir); err != nil {", "\t\tlog.Fatalf(\"Failed to remove temporary directory: %v\", err)", "\t}", "", "\t// Restart the bot", "\tlog.Println(\"Restarting the bot...\")", "\tbotCmd := exec.Command(\"python\", \"eoc.py\")", "\tbotCmd.Stdout = os.Stdout", "\tbotCmd.Stderr = os.Stderr", "\tif err := botCmd.Start(); err != nil {", "\t\tlog.Fatalf(\"Failed to restart the bot: %v\", err)", "\t}", "", "\tlog.Println(\"Update completed successfully.\")", "}"], "file_path": "update.go"}
{"Link_to_commit": "https://github.com/Lwalters7/ReactConfusion/commit/472ec9a01abf00eb7a705f4ca611e6b5695cc0bf", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 72, "n_files_impacted": 11, "longest_chunk": ["import React, { Component } from 'react';", "import {", "  Button, Modal, ModalHeader, ModalBody,", "  Form, FormGroup, Input, Label", "} from 'reactstrap';", "", "class CommentForm extends Component {", "  constructor(props) {", "    super(props);", "", "    this.state = {", "      isModalOpen: false", "    };", "", "    this.toggleModal = this.toggleModal.bind(this);", "    this.handleSubmit = this.handleSubmit.bind(this);", "  }", "", "  toggleModal() {", "    this.setState({ isModalOpen: !this.state.isModalOpen });", "  }", "", "  handleSubmit(event) {", "    event.preventDefault();", "    this.toggleModal();", "    this.props.addComment(", "      this.props.dishId,", "      this.rating.value,", "      this.author.value,", "      this.comment.value", "    );", "  }", "", "  render() {", "    return (", "      <div>", "        <Button outline onClick={this.toggleModal}>", "          <span className=\"fa fa-pencil fa-lg\"></span> Submit Comment", "        </Button>", "", "        <Modal isOpen={this.state.isModalOpen} toggle={this.toggleModal}>", "          <ModalHeader toggle={this.toggleModal}>Submit Comment</ModalHeader>", "          <ModalBody>", "            <Form onSubmit={this.handleSubmit}>", "              <FormGroup>", "                <Label htmlFor=\"rating\">Rating</Label>", "                <Input type=\"select\" id=\"rating\" innerRef={(input) => this.rating = input}>", "                  <option>1</option>", "                  <option>2</option>", "                  <option>3</option>", "                  <option>4</option>", "                  <option>5</option>", "                </Input>", "              </FormGroup>", "              <FormGroup>", "                <Label htmlFor=\"author\">Your Name</Label>", "                <Input type=\"text\" id=\"author\" innerRef={(input) => this.author = input} />", "              </FormGroup>", "              <FormGroup>", "                <Label htmlFor=\"comment\">Comment</Label>", "                <Input type=\"textarea\" id=\"comment\" rows=\"6\" innerRef={(input) => this.comment = input} />", "              </FormGroup>", "              <Button type=\"submit\" value=\"submit\" color=\"primary\">Submit</Button>", "            </Form>", "          </ModalBody>", "        </Modal>", "      </div>", "    );", "  }", "}", "", "export default CommentForm;"], "file_path": "src/components/DishdetailComponent.js"}
{"Link_to_commit": "https://github.com/ArienR/dotnet-movie-rec-app/commit/80585b20b950ab9e0d3a1050a3b80150a14ca24e", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 59, "n_files_impacted": 2, "longest_chunk": ["    ", "    public async Task EvaluateHoldoutAsync(float testFraction = 0.2f)", "    {", "        // 1) Fetch & filter", "        var allRatings = await _db.Ratings", "            .Select(r => new RatingInput {", "                UserName = r.UserName,", "                MovieId  = r.MovieId,", "                Score    = r.Score", "            })", "            .ToListAsync();", "        var popularMovieIds = allRatings", "            .GroupBy(r => r.MovieId)", "            .Where(g => g.Count() >= 5)", "            .Select(g => g.Key)", "            .ToHashSet();", "        var filtered = allRatings", "            .Where(r => popularMovieIds.Contains(r.MovieId))", "            .ToList();", "", "        // 2) Apply the same weighting\u2010by\u2010duplication", "        var weighted = new List<RatingInput>();", "        foreach (var r in filtered)", "        {", "            weighted.Add(r);", "            if (r.Score <= 2 || r.Score >= 9)", "                weighted.Add(r);", "        }", "", "        // 3) Split train/test", "        var dataView = _mlContext.Data.LoadFromEnumerable(weighted);", "        var split    = _mlContext.Data.TrainTestSplit(dataView, testFraction: testFraction);", "        var trainSet = split.TrainSet;", "        var testSet  = split.TestSet;", "", "        // 4) Train & evaluate", "        var pipeline = _mlContext.Transforms", "            .Conversion.MapValueToKey(\"userKey\", nameof(RatingInput.UserName))", "            .Append(_mlContext.Transforms", "                .Conversion.MapValueToKey(\"movieKey\", nameof(RatingInput.MovieId)))", "            .Append(_mlContext.Recommendation()", "                .Trainers.MatrixFactorization(new MatrixFactorizationTrainer.Options", "                {", "                    MatrixColumnIndexColumnName = \"userKey\",", "                    MatrixRowIndexColumnName    = \"movieKey\",", "                    LabelColumnName             = \"Label\",", "                    NumberOfIterations          = 30,", "                    ApproximationRank           = 150,", "                    Lambda                      = 0.1", "                }));", "        var model   = pipeline.Fit(trainSet);", "        var preds   = model.Transform(testSet);", "        var metrics = _mlContext.Regression.Evaluate(preds, \"Label\", \"Score\");", "", "        Console.WriteLine($\"=== Hold-out (weighted+filtered) {1-testFraction:P0}/{testFraction:P0} ===\");", "        Console.WriteLine($\"  RMSE = {metrics.RootMeanSquaredError:F3}\");", "        Console.WriteLine($\"  MAE  = {metrics.MeanAbsoluteError:F3}\");", "        Console.WriteLine($\"  R\u00b2   = {metrics.RSquared:F3}\");", "    }"], "file_path": "MovieRecApp.Server/Services/RecommendationService.cs"}
{"Link_to_commit": "https://github.com/zomblic/WeatherMeNow/commit/66491290287eb3875109c8f702ab7e8e08debff0", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 21, "n_files_impacted": 2, "longest_chunk": ["  async getWeatherForCity(city: string): Promise<Weather[]> {", "    try {", "      this.cityName = city;", "", "      console.log(`\ud83d\udccd Fetching weather data for: ${city}`);", "", "      // Step 1: Get coordinates for the city", "      const coordinates = await this.fetchAndDestructureLocationData();", "", "      // Step 2: Fetch weather data using coordinates", "      const weatherData = await this.fetchWeatherData(coordinates);", "", "      // Step 3: Parse current weather", "      const currentWeather = this.parseCurrentWeather(weatherData);", "", "      // Step 4: Build the 5-day forecast", "      return this.buildForecastArray(currentWeather, weatherData.list);", "    } catch (error: any) {", "      console.error(`\u274c Error retrieving weather for city (${city}):`, error.message);", "      throw error;", "    }"], "file_path": "server/src/service/weatherService.ts"}
{"Link_to_commit": "https://github.com/juliancolling/Road_Accidents_B-ham_Area/commit/2ac34cf5666779eaa5a85c6470f90ca9a435c0ff", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 40, "n_files_impacted": 1, "longest_chunk": ["monthly_fig.update_traces(textposition='top center')  # Adjust label positioning", "", "col2.subheader(\"Monthly Accident Trend\")", "col2.plotly_chart(monthly_fig)", "", "# Weekly Trend", "st.subheader(\"Weekly Accident Trend\")", "weekday_counts = final_filtered_df['Weekday'].value_counts().sort_index()", "weekday_fig = px.bar(", "    x=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],", "    y=weekday_counts.values,", "    labels={'x': 'Day of the Week', 'y': 'Number of Accidents'}", ")", "st.plotly_chart(weekday_fig)", "", "# Urban/Rural & Vehicle Breakdown", "col3, col4 = st.columns(2)", "urban_rural_counts = final_filtered_df['Urban_or_Rural_Area'].value_counts()", "urban_rural_fig = px.bar(x=urban_rural_counts.index, y=urban_rural_counts.values, text=urban_rural_counts.values,", "                          labels={'x': 'Location Type', 'y': 'Number of Accidents'})", "col3.subheader(\"Urban vs. Rural Accidents\")", "col3.plotly_chart(urban_rural_fig)", "", "vehicle_category_counts = final_filtered_df['Vehicle_Category'].value_counts()", "vehicle_category_fig = px.bar(x=vehicle_category_counts.index, y=vehicle_category_counts.values, text=vehicle_category_counts.values,", "                              labels={'x': 'Vehicle Type', 'y': 'Number of Accidents'})", "col4.subheader(\"Vehicle Type Breakdown\")", "col4.plotly_chart(vehicle_category_fig)", "", "# Light Conditions vs Severity & Road Surface Conditions vs Severity", "col5, col6 = st.columns(2)", "light_severity_fig = px.histogram(final_filtered_df, x='Light_Conditions', color='Accident_Severity', barmode='group',", "                                  labels={'Light_Conditions': 'Lighting', 'Accident_Severity': 'Severity'})", "col5.subheader(\"Light Conditions vs. Severity\")", "col5.plotly_chart(light_severity_fig)", "", "road_surface_fig = px.histogram(final_filtered_df, x='Road_Surface_Conditions', color='Accident_Severity', barmode='group',", "                                labels={'Road_Surface_Conditions': 'Road Surface', 'Accident_Severity': 'Severity'})", "col6.subheader(\"Road Surface Conditions vs. Severity\")", "col6.plotly_chart(road_surface_fig)"], "file_path": "Dashboard/streamlit/district_dashboard.py"}
{"Link_to_commit": "https://github.com/linusriddler/InfotechCenter/commit/4c9e247e1b628fe213748bb360ee51f4caf81f8e", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 15, "n_files_impacted": 1, "longest_chunk": ["    messages = {", "        \"Empty\": \"****WARNING - YOU ARE OUT OF GAS****\\nCalling AAA...\",", "        \"Low\": f\"Your gas tank is extremely low, checking GPS for the closest gas station...\\n\"", "               f\"The closest gas station is {gas_stations()} which is {miles_to_gas_station['Low']} miles away.\",", "        \"Quarter Tank\": f\"Your gas tank is at a Quarter Tank, checking GPS for the closest gas station...\\n\"", "                        f\"The closest gas station is {gas_stations()} which is {miles_to_gas_station['Quarter Tank']} miles away.\",", "        \"Half Tank\": \"Your gas tank is Half Full, plenty to get to your destination!\",", "        \"Three Quarter Tank\": \"Your gas tank is Three Quarters Full!\",", "        \"Full Tank\": \"Your gas tank is FULL, Vroom Vroom!\"", "    }", "", "    print(messages[gas_level])", "", "print(\"\\n****************************************\\n\")", "print(\"Gasoline Branch - Developer Linus Riddle\\n\")"], "file_path": "BlackWidow.py"}
{"Link_to_commit": "https://github.com/joonhoswe/NodeBB-LLM-Translation/commit/b25d019770921b8d55a3db92337afe136bf84dfc", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 122, "n_files_impacted": 2, "longest_chunk": ["# def test_japanese():", "#     is_english, translated_content = translate_content(\"\u3053\u308c\u306f\u65e5\u672c\u8a9e\u306e\u30e1\u30c3\u30bb\u30fc\u30b8\u3067\u3059\")", "#     assert is_english == False", "#     assert translated_content == \"This is a Japanese message\"", "", "# def test_detect_chinese():", "#     is_english, translated_content = translate_content(\"\u8fd9\u662f\u4e2d\u6587\")", "#     assert is_english == False", "#     assert translated_content == \"This is Chinese\"", "", "# Evaluation dataset", "translation_eval_set = [", "    {", "        \"post\": \"Hier ist dein erstes Beispiel.\",", "        \"expected_answer\": \"Here is your first example.\"", "    },", "    {", "        \"post\": \"\u00bfD\u00f3nde est\u00e1 la biblioteca?\",", "        \"expected_answer\": \"Where is the library?\"", "    },", "    {", "        \"post\": \"Je t\u2019aime beaucoup.\",", "        \"expected_answer\": \"I love you very much.\"", "    },", "    {", "        \"post\": \"\u4eca\u65e5\u306f\u5929\u6c17\u304c\u3044\u3044\u3067\u3059\u306d\u3002\",", "        \"expected_answer\": \"The weather is nice today, isn't it?\"", "    },", "    {", "        \"post\": \"Ciao, come stai?\",", "        \"expected_answer\": \"Hi, how are you?\"", "    },", "    {", "        \"post\": \"\ub098\ub294 \ud55c\uad6d\uc5b4\ub97c \ubc30\uc6b0\uace0 \uc788\uc5b4\uc694.\",", "        \"expected_answer\": \"I am learning Korean.\"", "    },", "    {", "        \"post\": \"\u0421\u043f\u0430\u0441\u0438\u0431\u043e \u0437\u0430 \u043f\u043e\u043c\u043e\u0449\u044c!\",", "        \"expected_answer\": \"Thank you for the help!\"", "    },", "    {", "        \"post\": \"Buongiorno, signore.\",", "        \"expected_answer\": \"Good morning, sir.\"", "    },", "    {", "        \"post\": \"J'ai besoin d'aide.\",", "        \"expected_answer\": \"I need help.\"", "    },", "    {", "        \"post\": \"\u8fd9\u662f\u4ec0\u4e48\u4e1c\u897f\uff1f\",", "        \"expected_answer\": \"What is this thing?\"", "    },", "]", "", "@pytest.mark.parametrize(\"test_case\", translation_eval_set)", "def test_translation(test_case):", "    post = test_case[\"post\"]", "    expected_answer = test_case[\"expected_answer\"]", "", "    # Call the function to test", "    is_english, translated_content = translate_content(post)", "", "    # Assert that the translation matches the expected answer", "    assert is_english == False  # Assuming all posts are non-English", "    assert translated_content == expected_answer", "", "", "", "# Language detection evaluation dataset", "language_detection_eval_set = [", "    {", "        \"post\": \"Hier ist dein erstes Beispiel.\",", "        \"expected_answer\": \"German\"", "    },", "    {", "        \"post\": \"\u00bfD\u00f3nde est\u00e1 la biblioteca?\",", "        \"expected_answer\": \"Spanish\"", "    },", "    {", "        \"post\": \"Je t\u2019aime beaucoup.\",", "        \"expected_answer\": \"French\"", "    },", "    {", "        \"post\": \"\u4eca\u65e5\u306f\u5929\u6c17\u304c\u3044\u3044\u3067\u3059\u306d\u3002\",", "        \"expected_answer\": \"Japanese\"", "    },", "    {", "        \"post\": \"Ciao, come stai?\",", "        \"expected_answer\": \"Italian\"", "    },", "    {", "        \"post\": \"\ub098\ub294 \ud55c\uad6d\uc5b4\ub97c \ubc30\uc6b0\uace0 \uc788\uc5b4\uc694.\",", "        \"expected_answer\": \"Korean\"", "    },", "    {", "        \"post\": \"\u0421\u043f\u0430\u0441\u0438\u0431\u043e \u0437\u0430 \u043f\u043e\u043c\u043e\u0449\u044c!\",", "        \"expected_answer\": \"Russian\"", "    },", "    {", "        \"post\": \"Buongiorno, signore.\",", "        \"expected_answer\": \"Italian\"", "    },", "    {", "        \"post\": \"J'ai besoin d'aide.\",", "        \"expected_answer\": \"French\"", "    },", "    {", "        \"post\": \"\u8fd9\u662f\u4ec0\u4e48\u4e1c\u897f\uff1f\",", "        \"expected_answer\": \"Chinese\"", "    },", "]", "", "@pytest.mark.parametrize(\"test_case\", language_detection_eval_set)", "def test_detect_language(test_case):", "    post = test_case[\"post\"]", "    expected_answer = test_case[\"expected_answer\"]", "", "    # Call the function to test", "    detected_language = detect_language(post)", "", "    # Assert that the detected language matches the expected answer", "    assert detected_language == expected_answer"], "file_path": "test/unit/test_translator.py"}
{"Link_to_commit": "https://github.com/mig-ael/DMOJproblems/commit/94a9d8eee05c1bd7a08ccddee131f574bf7b10de", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 11, "n_files_impacted": 1, "longest_chunk": ["# https://dmoj.ca/problem/aac2p0", "", "n = int(input())", "", "if n%2 ==0:", "    x=n-2", "else:", "    x=n-1", "", "a = (n+x)//2", "print(a)"], "file_path": "An Animal Contest 2 P0 - Koala Matchmaking.py"}
{"Link_to_commit": "https://github.com/Mythronn/mythronn.github.io/commit/e3e513a2dd1637e12765c1f8d6e50a9c560910f0", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 34, "n_files_impacted": 2, "longest_chunk": ["}", "", "", "function parseSpellText(spell) {", "  // Extracting relevant properties from the text field", "  const regexPatterns = {", "    freq: /<p3>Freq:<\\/p3>\\s*([^<]*)/,", "    school: /<p3>School:<\\/p3>\\s*([^<]*)/,", "    range: /<p3>Range:<\\/p3>\\s*([^<]*)/,", "    materials: /<p3>Materials:<\\/p3>\\s*([^<]*)/,", "    effect: /<p3>Effect:<\\/p3>\\s*([^<]*)/,", "    limitations: /<p3>Limitations:<\\/p3>\\s*([^<]*)/,", "    notes: /<p3>Notes:<\\/p3>\\s*([^<]*)/", "  };", "", "  let extractedData = {};", "", "  for (let key in regexPatterns) {", "    let match = spell.text.match(regexPatterns[key]);", "    extractedData[key] = match ? match[1].trim() : \"\";", "  }", "", "  // Creating the cleaned spell object", "  return {", "    ...spell,", "    freq: extractedData.freq,", "    school: extractedData.school,", "    range: extractedData.range,", "    materials: extractedData.materials,", "    effect: extractedData.effect,", "    limitations: extractedData.limitations,", "    notes: extractedData.notes,", "    text: \"\" // Clear out the original text field", "  };"], "file_path": "sappyspellbook/quizzes/script.js"}
{"Link_to_commit": "https://github.com/sucrammal/MyPortfolio/commit/aa19592669e31f9651bdd838e2b53f5cfe4bd0d2", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 104, "n_files_impacted": 3, "longest_chunk": ["  const [isMobileMenuOpen, setIsMobileMenuOpen] = useState(false);", "", "  useEffect(() => {", "    const loadFont = async () => {", "      await document.fonts.load('1rem \"Inter\"');", "      document.body.classList.add('font-inter');", "    };", "    ", "    const link = document.createElement('link');", "    link.href = 'https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap';", "    link.rel = 'stylesheet';", "    document.head.appendChild(link);", "    ", "    loadFont();", "  }, []);", "", "  return (", "    <header className=\"w-full bg-gradient-to-r from-purple-500/10 to-purple-600/5 backdrop-blur-sm border-b border-purple-100/20\">", "      <div className=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">", "        <nav className=\"flex items-center justify-between h-16\">", "          {/* Logo/Name */}", "          <div className=\"flex-shrink-0\">", "            <Link ", "              to=\"/\" ", "              className=\"text-2xl sm:text-3xl md:text-4xl lg:text-5xl font-semibold bg-gradient-to-r from-purple-600 to-purple-800 bg-clip-text text-transparent hover:opacity-80 transition-opacity font-inter\"", "            >", "              Marcus Lam", "            </Link>", "          </div>", "", "          {/* Desktop Navigation Links */}", "          <div className=\"hidden sm:flex sm:space-x-8\">", "            <Link", "              to=\"/projects\"", "              className=\"group relative px-3 py-2 text-base md:text-lg font-medium text-gray-700 transition-colors font-inter\"", "            >", "              <span className=\"relative\">", "                Projects", "                <span className=\"absolute bottom-[-24px] left-0 h-[2px] w-full origin-left scale-x-0 transform bg-purple-600 transition-transform duration-300 ease-out group-hover:scale-x-100\"></span>", "              </span>", "            </Link>", "            <Link", "              to=\"/news\"", "              className=\"group relative px-3 py-2 text-base md:text-lg font-medium text-gray-700 transition-colors font-inter\"", "            >", "              <span className=\"relative\">", "                News", "                <span className=\"absolute bottom-[-24px] left-0 h-[2px] w-full origin-left scale-x-0 transform bg-purple-600 transition-transform duration-300 ease-out group-hover:scale-x-100\"></span>", "              </span>", "            </Link>", "          </div>", "", "          {/* Mobile Menu Button */}", "          <div className=\"sm:hidden\">", "            <button", "              type=\"button\"", "              className=\"text-gray-700 hover:text-purple-600 p-2 rounded-md transition-colors\"", "              aria-label=\"Toggle menu\"", "              onClick={() => setIsMobileMenuOpen(!isMobileMenuOpen)}", "            >", "              <svg", "                className=\"h-6 w-6\"", "                fill=\"none\"", "                viewBox=\"0 0 24 24\"", "                stroke=\"currentColor\"", "              >", "                <path", "                  strokeLinecap=\"round\"", "                  strokeLinejoin=\"round\"", "                  strokeWidth={2}", "                  d={isMobileMenuOpen ? \"M6 18L18 6M6 6l12 12\" : \"M4 6h16M4 12h16M4 18h16\"}", "                />", "              </svg>", "            </button>", "          </div>", "        </nav>", "", "        {/* Mobile Menu */}", "        {isMobileMenuOpen && (", "          <div className=\"sm:hidden pb-4\">", "            <div className=\"flex flex-col space-y-4\">", "              <Link", "                to=\"/projects\"", "                className=\"px-3 py-2 text-base font-medium text-gray-700 hover:text-purple-600 transition-colors\"", "                onClick={() => setIsMobileMenuOpen(false)}", "              >", "                Projects", "              </Link>", "              <Link", "                to=\"/news\"", "                className=\"px-3 py-2 text-base font-medium text-gray-700 hover:text-purple-600 transition-colors\"", "                onClick={() => setIsMobileMenuOpen(false)}", "              >", "                News", "              </Link>", "            </div>", "          </div>", "        )}", "      </div>", "    </header>", "  );", "};", "", "export default Header;"], "file_path": "src/components/HomePage.js"}
{"Link_to_commit": "https://github.com/jocoso/Chess/commit/c554a76277d159f4cf937309bc724ae4c8920003", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 41, "n_files_impacted": 1, "longest_chunk": ["\tPiece(Piece&& other) noexcept :", "\t\t_attrs_map(std::move(other._attrs_map)),", "\t\t_current_loc(std::move(other._current_loc)),", "\t\t_name(std::move(other._name)),", "\t\t_symbol(other._symbol)", "\t{}", "\t", "\tPiece& operator=(Piece&& other) noexcept {", "\t\tif(this != &other) {", "\t\t\t_attrs_map = std::move(other._attrs_map);", "\t\t\t_current_loc = std::move(other._current_loc);", "\t\t\t_name = std::move(other._name);", "\t\t\t_symbol = other._symbol;", "\t\t}", "\t\treturn *this;", "\t}", "\t", "\tPiece(const Piece& other)", "\t: _attrs_map(other._attrs_map),", "\t_name(other._name),", "\t_symbol(other._symbol) {", "\t\tif(other._current_loc) ", "\t\t\t_current_loc = std::make_unique<cc::BoardCoord>(*other._current_loc);", "\t\telse _current_loc = nullptr;", "\t}", "\t", "\t// Copy assignment (deep copy)", "    Piece& operator=(const Piece& other) {", "        if (this != &other) {", "            _attrs_map = other._attrs_map;", "            _name = other._name;", "            _symbol = other._symbol;", "            if (other._current_loc) {", "                _current_loc = std::make_unique<cc::BoardCoord>(*other._current_loc);", "            } else {", "                _current_loc.reset();", "            }", "        }", "        return *this;", "    }", ""], "file_path": "main.cpp"}
{"Link_to_commit": "https://github.com/mathchou/mathchou.github.io/commit/527ee82b7d93657ebe61509fe31a448f539d3b7c", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 156, "n_files_impacted": 6, "longest_chunk": ["<!DOCTYPE html>", "<html lang=\"en\">", "<head>", "    <meta charset=\"UTF-8\">", "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">", "    <title>RSA Key Pair Generator</title>", "    <style>", "        body {", "            font-family: Arial, sans-serif;", "            padding: 20px;", "        }", "        #output {", "            margin-top: 20px;", "        }", "        textarea {", "            width: 100%;", "            height: 200px;", "            margin-top: 10px;", "            font-family: monospace;", "            white-space: pre-wrap;", "        }", "        button {", "            padding: 10px;", "            font-size: 16px;", "            cursor: pointer;", "            background-color: #4CAF50;", "            color: white;", "            border: none;", "            border-radius: 5px;", "        }", "        button:hover {", "            background-color: #45a049;", "        }", "        #zipLink {", "            display: none;", "        }", "    </style>", "    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/jszip/3.10.1/jszip.min.js\"></script>", "</head>", "<body>", "    <h1>RSA Key Pair Generator</h1>", "    <p>Click the button below to generate a 2048-bit RSA key pair and download them, along with the key components in decimal format as a ZIP file.</p>", "    <button id=\"generateBtn\">Generate RSA Key Pair</button>", "    <div id=\"output\">", "        <h3>Download Link:</h3>", "        <a id=\"zipLink\" href=\"#\" download=\"rsa_key_pair.zip\">Download RSA Key Pair and Components (ZIP)</a>", "        <h3>Key Components (in Decimal):</h3>", "        <pre id=\"keyComponents\"></pre>", "    </div>", "", "    <script>", "        async function generateRSAKeyPair() {", "            try {", "                // Generate RSA key pair with 2048-bit modulus", "                const keyPair = await window.crypto.subtle.generateKey(", "                    {", "                        name: \"RSA-OAEP\",", "                        modulusLength: 2048, // 2048-bit modulus", "                        publicExponent: new Uint8Array([1, 0, 1]), // 65537 in hexadecimal", "                        hash: { name: \"SHA-256\" },", "                    },", "                    true,", "                    [\"encrypt\", \"decrypt\"]", "                );", "", "                // Export public key in SPKI format (PEM)", "                const publicKeySpki = await window.crypto.subtle.exportKey(\"spki\", keyPair.publicKey);", "                const publicKeyPem = arrayBufferToPem(publicKeySpki, \"PUBLIC KEY\");", "", "                // Export private key in PKCS8 format (PEM)", "                const privateKeyPkcs8 = await window.crypto.subtle.exportKey(\"pkcs8\", keyPair.privateKey);", "                const privateKeyPem = arrayBufferToPem(privateKeyPkcs8, \"PRIVATE KEY\");", "", "                // Extract key components from private key in JWK format", "                const privateKeyJson = await window.crypto.subtle.exportKey(\"jwk\", keyPair.privateKey);", "                const publicKeyJson = await window.crypto.subtle.exportKey(\"jwk\", keyPair.publicKey);", "", "                // Extract modulus, public exponent, private exponent, p, q", "                const n = publicKeyJson.n;  // Modulus (n)", "                const e = publicKeyJson.e;  // Public exponent (e)", "                const d = privateKeyJson.d; // Private exponent (d)", "                const p = privateKeyJson.p; // Prime factor (p)", "                const q = privateKeyJson.q; // Prime factor (q)", "", "                // Convert from Base64URL to Decimal", "                const nDecimal = base64urlToDecimal(n);", "                const eDecimal = base64urlToDecimal(e);", "                const dDecimal = base64urlToDecimal(d);", "                const pDecimal = base64urlToDecimal(p);", "                const qDecimal = base64urlToDecimal(q);", "", "                // Display extracted key components in decimal", "                document.getElementById('keyComponents').textContent = `", "Modulus (N): ${nDecimal}", "Public Exponent (e): ${eDecimal}", "Private Exponent (d): ${dDecimal}", "Prime Factor (p): ${pDecimal}", "Prime Factor (q): ${qDecimal}", "                `;", "", "                // Create ZIP file", "                const zip = new JSZip();", "                zip.file(\"public_key.pem\", publicKeyPem);", "                zip.file(\"private_key.pem\", privateKeyPem);", "                zip.file(\"rsa_key_components.txt\", `", "Modulus (N): ${nDecimal}", "Public Exponent (e): ${eDecimal}", "Private Exponent (d): ${dDecimal}", "Prime Factor (p): ${pDecimal}", "Prime Factor (q): ${qDecimal}", "                `);", "", "                // Generate the ZIP file and create a download link", "                const zipBlob = await zip.generateAsync({ type: \"blob\" });", "                const zipLink = document.getElementById('zipLink');", "                const zipUrl = URL.createObjectURL(zipBlob);", "                zipLink.href = zipUrl;", "                zipLink.style.display = 'block';  // Show the download link", "            } catch (error) {", "                console.error(\"Error generating key pair:\", error);", "                alert(\"Error generating RSA keys.\");", "            }", "        }", "", "        // Convert ArrayBuffer to PEM format", "        function arrayBufferToPem(buffer, type) {", "            const base64 = arrayBufferToBase64(buffer);", "            return `-----BEGIN ${type}-----\\n${base64}\\n-----END ${type}-----`;", "        }", "", "        // Convert ArrayBuffer to Base64 encoded string", "        function arrayBufferToBase64(buffer) {", "            const binary = String.fromCharCode.apply(null, new Uint8Array(buffer));", "            return window.btoa(binary);", "        }", "", "        // Convert Base64URL string to Decimal string", "        function base64urlToDecimal(base64urlStr) {", "            // Replace Base64URL specific characters with standard Base64", "            const base64Str = base64urlStr.replace(/-/g, '+').replace(/_/g, '/');", "            const binaryString = window.atob(base64Str);", "            let decimalValue = BigInt(0);", "", "            for (let i = 0; i < binaryString.length; i++) {", "                const byteValue = binaryString.charCodeAt(i);", "                decimalValue = (decimalValue << 8n) + BigInt(byteValue);", "            }", "", "            return decimalValue.toString();", "        }", "", "        // Add event listener to button", "        document.getElementById('generateBtn').addEventListener('click', generateRSAKeyPair);", "    </script>", "</body>", "</html>"], "file_path": "pkcs8-mytry.js"}
{"Link_to_commit": "https://github.com/Jordrn4l/Group-1/commit/47e13a43f8c3fc79c17e4ff34bd4cf8f49b297a1", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 97, "n_files_impacted": 1, "longest_chunk": ["// Old undocumented code", "void placeRandomPiece(int gameBoard[BOARD_HEIGHT][BOARD_WIDTH], bool& startOfGame)", "{", "\tsrand(time(0));", "", "\tif(startOfGame == true)", "\t{", "\t\tfor(int i = 0; i < 2; i++)", "\t\t{", "\t\t\tint row = rand() % 4;", "\t\t\tint column = rand() % 4;", "", "\t\t\tif(gameBoard[row][column] == EMPTY)", "\t\t\t{", "\t\t\t\tgameBoard[row][column] = 1;", "\t\t\t\tcontinue;", "\t\t\t}", "\t\t\telse", "\t\t\t{", "\t\t\t\tcontinue;", "\t\t\t}", "\t\t}", "\t\tstartOfGame = false;", "\t}", "\telse", "\t{", "\t\tdo", "\t\t{", "\t\t\tint row = rand() % 4;", "\t\t\tint column = rand() % 4;", "", "\t\t\tif(gameBoard[row][column] == EMPTY)", "\t\t\t{", "\t\t\t\tgameBoard[row][column] = 1;", "\t\t\t\tbreak;", "\t\t\t}", "\t\t\telse", "\t\t\t{", "\t\t\t\tcontinue;", "\t\t\t}", "\t\t} while(true);", "\t}", "", "\t", "}", "", "", "// New documented code", "void placeRandomPiece(int gameBoard[BOARD_HEIGHT][BOARD_WIDTH], bool& startOfGame)", "{", "    // Seed the random number generator with the current time to ensure different outputs each run", "    srand(time(0));", "", "    // Check if it's the start of the game", "    if(startOfGame == true)", "    {", "        // At the beginning of the game, place two pieces randomly on the board", "        for(int i = 0; i < 2; i++)", "        {", "            int row = rand() % 4;    // Generate a random row index (0 to 3)", "            int column = rand() % 4; // Generate a random column index (0 to 3)", "", "            // Check if the chosen position is empty", "            if(gameBoard[row][column] == EMPTY)", "            {", "                gameBoard[row][column] = 1; // Place a new tile with the value 1", "                continue; // Continue to the next piece placement", "            }", "            else", "            {", "                continue; // If the position is occupied, try again in the next loop iteration", "            }", "        }", "        // After placing the initial two pieces, set startOfGame to false", "        startOfGame = false;", "    }", "    else", "    {", "        // During normal gameplay, place a single piece in a random empty spot", "        do", "        {", "            int row = rand() % 4;    // Generate a random row index", "            int column = rand() % 4; // Generate a random column index", "", "            // Check if the chosen position is empty", "            if(gameBoard[row][column] == EMPTY)", "            {", "                gameBoard[row][column] = 1; // Place a new tile with the value 1", "                break; // Exit the loop once a piece is placed successfully", "            }", "            else", "            {", "                continue; // If the position is occupied, retry until a valid position is found", "            }", "        } while(true); // Infinite loop until a valid placement is made", "    }", "}"], "file_path": "gen_ai/task2.2/Jaydan_docstring_update.cpp"}
{"Link_to_commit": "https://github.com/kalagotla/lptlib/commit/06000fb09a0263ae746fce058e5b1d4110a454a2", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 22, "n_files_impacted": 1, "longest_chunk": ["            final_data = None", "", "        # Flatten the local data for sending", "        sendbuf = local_data.ravel()", "", "        # Determine the MPI datatype corresponding to the numpy dtype.", "        # This works for common types (e.g. 'd' for float64, 'i' for int32, etc.).", "        mpi_dtype = MPI._typedict[local_data.dtype.char]", "", "        # On root, prepare counts and displacements for Gatherv.", "        if rank == 0:", "            counts = [r * local_cols for r in all_rows]", "            displacements = [sum(counts[:i]) for i in range(len(counts))]", "        else:", "            counts = None", "            displacements = None", "", "        # Use Gatherv to gather the flattened arrays into final_data (also flattened).", "        comm.Gatherv(sendbuf, [final_data, counts, displacements, mpi_dtype], root=0)", "", "        comm.Barrier()  # Synchronize processes", "        return final_data if rank == 0 else None"], "file_path": "src/lptlib/io/dataio.py"}
{"Link_to_commit": "https://github.com/Kraoshin/holbertonschool-chatgpt-introduction/commit/948a2efaf0ac6edd0035e8105c64bd643734c839", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 72, "n_files_impacted": 1, "longest_chunk": ["def print_board(board):", "    for row in board:", "        print(\" | \".join(row))", "        print(\"-\" * 5)", "", "def check_winner(board):", "    # Check rows", "    for row in board:", "        if row.count(row[0]) == len(row) and row[0] != \" \":", "            return True", "", "    # Check columns", "    for col in range(len(board[0])):", "        if board[0][col] == board[1][col] == board[2][col] and board[0][col] != \" \":", "            return True", "", "    # Check diagonals", "    if board[0][0] == board[1][1] == board[2][2] and board[0][0] != \" \":", "        return True", "", "    if board[0][2] == board[1][1] == board[2][0] and board[0][2] != \" \":", "        return True", "", "    return False", "", "def tic_tac_toe():", "    board = [[\" \"]*3 for _ in range(3)]", "    player = \"X\"", "    move_count = 0  # To track the number of moves", "", "    while move_count < 9 and not check_winner(board):  # Max 9 moves", "        print_board(board)", "", "        # Get valid user input", "        while True:", "            try:", "                row = int(input(f\"Enter row (0, 1, or 2) for player {player}: \"))", "                col = int(input(f\"Enter column (0, 1, or 2) for player {player}: \"))", "                if 0 <= row < 3 and 0 <= col < 3:  # Ensure valid range for row and column", "                    if board[row][col] == \" \":", "                        break  # If the spot is empty, exit the loop", "                    else:", "                        print(\"That spot is already taken! Try again.\")", "                else:", "                    print(\"Invalid input! Row and column must be between 0 and 2.\")", "            except ValueError:", "                print(\"Invalid input! Please enter integer values for row and column.\")", "", "        # Make the move", "        board[row][col] = player", "        move_count += 1", "", "        # Switch player", "        if player == \"X\":", "            player = \"O\"", "        else:", "            player = \"X\"", "", "    print_board(board)", "", "    # After the loop ends, check for the winner", "    if check_winner(board):", "        # The player who made the last move is the winner", "        if player == \"X\":", "            print(\"Player O wins!\")", "        else:", "            print(\"Player X wins!\")", "    else:", "        print(\"It's a draw!\")", "", "tic_tac_toe()", ""], "file_path": "debugging/tic.py"}
{"Link_to_commit": "https://github.com/daryusg/LeaveManagementSystem_vsc_9/commit/14038d2adb74fa2ab6c7af84df3a43e9b9598c64", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 44, "n_files_impacted": 243, "longest_chunk": ["", "    //-------------------------------------------------------------------------------------------------", "    //-------------------------------------------------------------------------------------------------", "    //-------------------------------------------------------------------------------------------------", "", "    public class DateValidationAttribute : ValidationAttribute //27/04/25 from chstGPT", "    {", "        private readonly int _minimumDate;", "        private readonly int _maximumDate; //mine", "        private readonly string _prefix; //mine", "        private readonly string _suffix;", "", "        public DateValidationAttribute(int minimumDate, int maximumDate, string prefix = \"Employee\", string suffix = \" years old\")", "        {", "            //note: prefix & suffix jic i need to expand this in the future. 27/04/25", "            _minimumDate = minimumDate;", "            _maximumDate = maximumDate; //mine", "            _prefix = prefix; //mine", "            this._suffix = suffix;", "            ErrorMessage = $\"{_prefix} must be between {_minimumDate} and {_maximumDate}{_suffix}.\"; //mine", "        }", "", "        protected override ValidationResult IsValid(object value, ValidationContext validationContext)", "        {", "            if ((value is DateOnly) || (value is DateTime))", "            {", "                DateOnly dob = value is DateTime ? DateOnly.FromDateTime((DateTime)value) : (DateOnly)value;", "", "                var today = DateTime.Today;", "                var age = today.Year - dob.Year;", "                if (dob > DateOnly.FromDateTime(today.AddYears(-age))) age--;", "", "                if ((age < _minimumDate) || (age > _maximumDate)) //mine", "                {", "                    return new ValidationResult(ErrorMessage);", "                }", "", "                return ValidationResult.Success!;", "            }", "", "            return new ValidationResult(\"Invalid date format.\");", "        }", "    }", ""], "file_path": "LeaveManagementSystem.Common/Functions.cs"}
{"Link_to_commit": "https://github.com/Levi-Spellmeyer/Financial-Application/commit/fbe09e591750438c7c42349bcc59e2d6008f3490", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 38, "n_files_impacted": 3, "longest_chunk": ["import aiosmtpd", "from aiosmtpd.handlers import Message", "from email.parser import Parser", "", "class EmailHandler(Message):", "    def __init__(self):", "        super().__init__()", "", "    async def handle_DATA(self, server, session, envelope):", "        # Parse the email message content", "        message = Parser().parsestr(envelope.content.decode())", "        subject = message.get(\"Subject\", \"No Subject\")", "        sender = message.get(\"From\", \"Unknown Sender\")", "        recipient = message.get(\"To\", \"Unknown Recipient\")", "        body = envelope.content.decode()", "", "        # Create or append to a .txt file", "        with open(\"emails_received.txt\", \"a\") as file:", "            file.write(f\"--- New Email ---\\n\")", "            file.write(f\"From: {sender}\\n\")", "            file.write(f\"To: {recipient}\\n\")", "            file.write(f\"Subject: {subject}\\n\")", "            file.write(f\"Body:\\n{body}\\n\")", "            file.write(\"-\" * 40 + \"\\n\")", "", "        print(f\"\u2705 Email saved to emails_received.txt\")", "        return \"250 Message accepted for delivery\"", "", "# Set up the SMTP server", "async def run_server():", "    handler = EmailHandler()", "    server = aiosmtpd.controller.Controller(handler, hostname='localhost', port=1025)", "    server.start()", "    print(\"\ud83d\ude80 Local SMTP server running on localhost:1025\")", "", "if __name__ == \"__main__\":", "    import asyncio", "    asyncio.run(run_server())"], "file_path": "Project-Files/smtp_server.py"}
{"Link_to_commit": "https://github.com/BigDaveCoding/snoot-city/commit/de215a2a241b7cf56d07dc45022f88dcd742f3f2", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 54, "n_files_impacted": 1, "longest_chunk": ["", "", "const getAuthToken = async () => {", "    const response = await fetch(\"https://api.petfinder.com/v2/oauth2/token\", {", "        method: \"POST\",", "        headers: {", "            \"Content-Type\": \"application/x-www-form-urlencoded\",", "        },", "        body: new URLSearchParams({", "            grant_type: \"client_credentials\",", "            client_id: import.meta.env.VITE_PETFINDER_CLIENT_ID,", "            client_secret: import.meta.env.VITE_PETFINDER_CLIENT_SECRET,", "        }),", "    });", "", "    const data = await response.json();", "    return data.access_token;", "};", "", "async function GetSighthounds() { // Default location = UK", "    const token = await getAuthToken();", "", "    const sighthoundBreeds = [", "        \"Greyhound\",", "        \"Whippet\",", "        \"Saluki\",", "        \"Afghan Hound\",", "        \"Borzoi\",", "        \"Ibizan Hound\",", "        \"Italian Greyhound\",", "        \"Scottish Deerhound\",", "        \"Sloughi\",", "    ];", "", "    const response = await fetch(", "        `https://api.petfinder.com/v2/animals?type=dog&breed=${sighthoundBreeds.join(\",\")}`,", "        {", "            method: \"GET\",", "            headers: {", "                Authorization: `Bearer ${token}`,", "                \"Content-Type\": \"application/json\",", "            },", "        }", "    );", "", "    const data = await response.json();", "    console.log(data)", "    console.log(data.animals)", "    return data.animals; // Return only the list of animals", "}", "", "// GetSighthounds();", "", "export default GetSighthounds"], "file_path": "snoot-city/src/utilities/petfinder-Api.js"}
{"Link_to_commit": "https://github.com/Geerteind/AI-for-MIA/commit/b95d6dbc7652cb44a4bab4f0bd5d738eb7eaf49e", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 44, "n_files_impacted": 1, "longest_chunk": ["import os\r", "import random\r", "import matplotlib.pyplot as plt\r", "from PIL import Image\r", "\r", "def visualize_Images(directory, nr_samples=5):\r", "    classes = {\"0\": \"Without Metastases\", \"1\": \"With Metastases\"}     #train and validation map exists of a '0' and a '1' map\r", "    samples = {}\t\r", "    for Group in classes.keys():\r", "        samples[Group] = get_sample_images(directory, Group, nr_samples)\r", "    print(samples)\r", "\r", "\r", "    fig, axes = plt.subplots(2, nr_samples, figsize=(15, 6))          #create figure\r", "    fig.suptitle(\"Comparison of tissue with or without metastases\", fontsize=14)\r", "\r", "\r", "    for i, Group in enumerate(classes.keys()):\r", "        for j, img_name in enumerate(samples[Group]):\r", "            img_path = os.path.join(directory, Group, img_name)\r", "            img = Image.open(img_path)\r", "\r", "            axes[i, j].imshow(img)\r", "            axes[i, j].axis(\"off\")\r", "            if j == 0:  # Label only the first image in each row\r", "                axes[i, j].set_title(classes[Group], fontsize=12)\r", "\r", "    plt.tight_layout()\r", "    plt.show()\r", "\r", "\r", "#Function to get the sample images from each class\r", "def get_sample_images(directory, Classnr, num_samples=5):\r", "    Class_path = os.path.join(directory, Classnr)\r", "    image_files = [f for f in os.listdir(Class_path) if f.endswith((\".jpg\"))]\r", "    return random.sample(image_files, min(num_samples, len(image_files)))               #take random samples from data\r", "\r", "\r", "# Run for training dataset\r", "visualize_Images(\"train\")\r", "\r", "\r", "# Run for validation dataset\r", "visualize_Images(\"valid\")\r"], "file_path": "Code_exercise1,4.py"}
{"Link_to_commit": "https://github.com/OliverR-C/InfoTechCenter1.0/commit/f513c1f1dc038dc6a29f450649b530c1bb13835e", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 62, "n_files_impacted": 1, "longest_chunk": ["# Reset display cursor after running a test", "print(TextColors.RESET, end=\"\")", "", "# Initialize counters for the boot process", "x = 0  # Counter for booting iterations", "ellipsis = 0  # Counter for the ellipsis effect", "", "timetosleep = 4  # variable to set the time library to 4 seconds when called", "time.sleep(timetosleep)  # calling the time to sleep library with the variable time", "", "# Loop to simulate the system booting process", "while x != 20:", "    x += 1  # Increment the boot counter", "    # Create a booting message with an ellipsis effect and cyan color", "    message = f\"{TextColors.CYAN}InfoTech Center System Booting\" + \".\" * ellipsis + f\"{TextColors.RESET}\"", "    ellipsis += 1  # Increment the ellipsis counter", "    sys.stdout.write(\"\\r\" + message)  # Overwrite the current line with the message", "    time.sleep(0.2)  # Pause for half a second", "", "    # Reset ellipsis counter after reaching 4 dots", "    if ellipsis == 4:", "        ellipsis = 0", "", "Certainly! Here\u2019s the code without the comments:", "", "python", "Copy code", "import time", "import sys", "", "class TextColors:", "    RESET = \"\\033[0m\"", "    RED = \"\\033[91m\"", "    GREEN = \"\\033[92m\"", "    YELLOW = \"\\033[93m\"", "    CYAN = \"\\033[96m\"", "", "def log(message, level=\"INFO\", color=TextColors.RESET):", "    print(f\"{color}[{level}] {message}{TextColors.RESET}\")", "", "LOG_LEVELS = {", "    \"INFO\": (TextColors.GREEN, \"INFO\"),", "    \"WARNING\": (TextColors.YELLOW, \"WARNING\"),", "    \"ERROR\": (TextColors.RED, \"ERROR\"),", "}", "", "def log_message(level, message):", "    color, level_str = LOG_LEVELS.get(level, (TextColors.RESET, \"INFO\"))", "    log(message, level=level_str, color=color)", "", "for i in range(5):", "    if i == 0:", "        log_message(\"INFO\", \"Process started\")", "    elif i == 3:", "        log_message(\"WARNING\", \"CPU usage is high\")", "    elif i == 4:", "        log_message(\"ERROR\", \"Process failed\")", "    else:", "        log_message(\"INFO\", f\"Running iteration {i}\")", "", "print(TextColors.RESET, end=\"\")", ""], "file_path": "CastleBravo.py"}
{"Link_to_commit": "https://github.com/pallavpriyam/Block_SmashKart/commit/d3260b90d484bd82a1c0b96b2b726829525fe330", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 24, "n_files_impacted": 7, "longest_chunk": ["<!DOCTYPE html>\r", "<html lang=\"en\">\r", "<head>\r", "    <meta charset=\"UTF-8\">\r", "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r", "    <title>Blocking Timer</title>\r", "    <style>\r", "        body {\r", "            font-family: Arial, sans-serif;\r", "            padding: 20px;\r", "            width: 200px;\r", "        }\r", "        .time-remaining {\r", "            font-size: 18px;\r", "            color: #333;\r", "        }\r", "    </style>\r", "</head>\r", "<body>\r", "    <h2>Time Remaining:</h2>\r", "    <div class=\"time-remaining\" id=\"timeRemaining\">Loading...</div>\r", "    <script src=\"popup.js\"></script>\r", "</body>\r", "</html>\r"], "file_path": "popup.js"}
{"Link_to_commit": "https://github.com/unobatbayar/problem-solving/commit/922b866dd4b90364b37d16cd78088462fe76c608", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 30, "n_files_impacted": 1, "longest_chunk": ["# 1371. Find the Longest Substring Containing Vowels in Even Counts", "", "class Solution:", "    def findTheLongestSubstring(self, s: str) -> int:", "        bitmask = 0", "        max_length = 0", "        seen = {0: -1} # bitmask 0 at index -1", "", "        for i in range(len(s)):", "            c = s[i]", "            if c == 'a':", "                bitmask ^= 1 << 0  # Flip the 0th bit for 'a'", "            elif c == 'e':", "                bitmask ^= 1 << 1  # Flip the 1st bit for 'e'", "            elif c == 'i':", "                bitmask ^= 1 << 2  # Flip the 2nd bit for 'i'", "            elif c == 'o':", "                bitmask ^= 1 << 3  # Flip the 3rd bit for 'o'", "            elif c == 'u':", "                bitmask ^= 1 << 4  # Flip the 4th bit for 'u'", "            ", "            # Check if the current bitmask has been seen before", "            if bitmask in seen:", "                # Calculate the length of the valid substring", "                max_length = max(max_length, i - seen[bitmask])", "                #print(s[i:seen[bitmask]])", "            else:", "                # If it's the first time we see this bitmask, store the index", "                seen[bitmask] = i", "        return max_length"], "file_path": "LeetCode/1371.py"}
{"Link_to_commit": "https://github.com/akhunters/code-stream-frontend/commit/4f8c89cbdc7056bbc0051559e52433cb2320c24e", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 40, "n_files_impacted": 1, "longest_chunk": ["/**", " * @file instrumentation.ts", " *", " * \ud83c\udfad **Welcome to the Instrumentation Zone!** \ud83c\udfad", " *", " * This file is currently **as empty as a production database on day one** \u2013 but fear not!", " * It's the **designated home** for all things related to **performance monitoring, logging, and tracing**", " * in our Next.js App Router project.", " *", " * ## \ud83c\udfaf Purpose:", " * - Next.js offers an **Instrumentation API** that lets us **peek under the hood**", " *   and understand how our app is behaving.", " * - This file will (eventually) be responsible for **tracking, measuring, and logging**", " *   critical performance and error metrics.", " * - Think of it as **a fitness tracker for your Next.js app** \u2013 but instead of counting steps,", " *   it counts **request times, rendering delays, and unexpected \"uh-ohs.\"**", " *", " * ## \ud83d\udd25 What Can We Do With This?", " * - **Monitor API requests** \ud83d\udd75\ufe0f\u200d\u2642\ufe0f \u2013 Ever wondered why your API response takes ages? We\u2019ll find out!", " * - **Trace execution paths** \ud83d\uddfa\ufe0f \u2013 See where your requests are coming from and how they flow.", " * - **Collect error logs** \ud83d\udea8 \u2013 Because \"works on my machine\" isn\u2019t a logging strategy.", " * - **Profile React Server Components** \ud83c\udfcb\ufe0f \u2013 Lift those rendering weights, optimize performance!", " * - **Track middleware performance** \u23f3 \u2013 Why did my middleware take longer than my coffee break?", " * - **Integrate with external monitoring tools** \ud83d\udce1 \u2013 Datadog, OpenTelemetry, Sentry, or whatever", " *   fancy logging tool your ops team loves.", " *", " * ## \ud83d\udcda Useful References (for those who like to read before coding \ud83d\ude43):", " * - \ud83d\udcd6 Next.js Instrumentation API: https://nextjs.org/docs/app/building-your-application/optimizing/instrumentation", " * - \ud83d\udd75\ufe0f OpenTelemetry (Tracing in Next.js): https://opentelemetry.io/", " * - \ud83d\udea8 Sentry (Error Monitoring for Next.js): https://docs.sentry.io/platforms/javascript/guides/nextjs/", " * - \ud83d\udce1 Datadog (Monitoring Next.js Apps): https://docs.datadoghq.com/tracing/setup_overview/setup/nodejs/", " *", " * ## \ud83e\udd14 What's Next?", " * - \ud83d\udccc **TODO:** Actually write some instrumentation logic.", " * - \ud83c\udfa4 **TODO:** Figure out why this file still isn't doing anything useful.", " * - \ud83d\udd27 **TODO:** Add a coffee consumption tracker? (Okay, maybe not.)", " *", " * Until then, this file remains a **peaceful void**. \ud83c\udf0c", " */", ""], "file_path": "src/intrumentation.ts"}
{"Link_to_commit": "https://github.com/uce002/JavaScript_Projects/commit/24cb4bb2498c7fc0e5d59363bbfd6e87bcf48e7f", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 19, "n_files_impacted": 1, "longest_chunk": ["function pyramid(pattern, rows, boo) {", "    let result = \"\";", "", "    if (!boo) { // Normal Pyramid", "        for (let i = 1; i <= rows; i++) {", "            let spaces = \" \".repeat(rows - i);", "            let py = spaces + pattern.repeat(2 * i - 1);", "            result += (i === 1 ? \"\" : \"\\n\") + py;", "        }", "    } else { // Inverted Pyramid", "        for (let i = rows; i >= 1; i--) {", "            let spaces = \" \".repeat(rows - i);", "            let py = spaces + pattern.repeat(2 * i - 1);", "            result += (i === rows ? \"\" : \"\\n\") + py;", "        }", "    }", "", "    return \"\\n\" + result + \"\\n\";", "}"], "file_path": "9_pyramidGenerator.js"}
{"Link_to_commit": "https://github.com/GauravSK2001/NBI-SRT-Astro/commit/c7603f9ae0b6218a7b769a8b6ff63c412803ee9e", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 148, "n_files_impacted": 5, "longest_chunk": ["from Tracking import *", "from Controls import *", "", "def print_help():", "    \"\"\"", "    Print usage instructions for the interactive commands.", "    \"\"\"", "    print(", "        \"\"\"", "    Available Commands:", "    ------------------", "    help or h", "        Show this help message.", "", "    t <L> <B>", "        Track a target at galactic coordinates L, B continuously.", "        Example: t 10 10", "", "    s <L> <B>", "        Slew immediately to the specified galactic coordinates (L, B).", "        Example: s 10 15", "", "    s <Az> <El> azel", "        Slew immediately to specified horizontal coords (Az, El) in degrees.", "        Example: s 180 45 azel", "", "    r", "        Restart the rotator.", "", "    off or exit or q", "        Terminate the program and exit.", "    \"\"\"", "    )", "", "", "def main():", "    print(\"Welcome to the Interactive Telescope Terminal\")", "", "    # Instantiate the hardware control", "    try:", "        control = Rot2Prog()", "        print(\"Rot2Prog control initialized.\")", "    except Exception as e:", "        print(f\"Error initializing Rot2Prog: {e}\")", "        control = None", "", "    # Instantiate the high-level source tracking, passing in the control", "    rotor = source_tracking(control=control)", "", "    # Main command loop", "    while True:", "        cmd = input(\"\\nEnter command (type 'help' for options): \").strip().lower()", "", "        if not cmd:", "            continue", "", "        if cmd in [\"help\", \"h\"]:", "            print_help()", "            continue", "", "        if cmd in [\"off\", \"exit\", \"quit\", \"q\", 'shutdown','off']:", "            print(\"\\n ...Shutting down... \\n\")", "            # The rotator will be moved to stow mode.Need to add this ", "            break", "", "        if cmd == \"r\":", "            if control is not None:", "                try:", "                    control.Restart()", "                except Exception as e:", "                    print(f\"Error restarting rotator: {e}\")", "            else:", "                print(\"No rotator control available to restart.\")", "            continue", "", "        # ----------------------------------------------------------", "        # T <L> <B> => continuous tracking", "        # This starts or continues the 5-second update cycle", "        # ----------------------------------------------------------", "        if cmd.startswith(\"t \"):", "            parts = cmd.split()", "            if len(parts) != 3:", "                print(\"Error: Usage: t <L> <B> (e.g., t 10 10)\")", "                continue", "            try:", "                l_val = float(parts[1])", "                b_val = float(parts[2])", "            except ValueError:", "                print(\"Invalid numeric values for L, B.\")", "                continue", "", "            # Set the rotor's current galactic coords", "            rotor.current_lb = SkyCoord(l=l_val*u.deg, b=b_val*u.deg, frame='galactic')", "            print(f\"\\nTarget galactic coordinates set to: L={l_val:.2f}\u00b0, B={b_val:.2f}\u00b0.\\n\")", "            # Start or continue the monitoring loop", "            rotor._monitor_pointing(update_time=5)", "            # If user hits Ctrl+C, we'll return here", "            continue", "", "        # ----------------------------------------------------------", "        # Slew => s ...", "        #   s <L> <B>  or  s <Az> <El> azel", "        # ----------------------------------------------------------", "        if cmd.startswith(\"s \"):", "            parts = cmd.split()", "", "            # 3 parts => presumably s <L> <B> (galactic)", "            if len(parts) == 3:", "                try:", "                    L = float(parts[1])", "                    B = float(parts[2])", "                except ValueError:", "                    print(\"Invalid numeric values for L, B.\")", "                    continue", "", "                current_time, az, el = rotor.tracking_galactic_coordinates(L, B)", "                # Attempt immediate slew", "                try:", "                    rotor.set_pointing(az, el)", "                    rotor.current_azel = SkyCoord(alt=el*u.deg, az=az*u.deg, frame='altaz')", "                    rotor.current_lb = SkyCoord(l=L*u.deg, b=B*u.deg, frame='galactic')", "                    rotor.telescope_pointing = rotor.current_azel", "                    print(f\"Slewed to galactic L={L:.2f}\u00b0, B={B:.2f}\u00b0 => \"", "                        f\"Az={round(az)}\u00b0, El={round(el)}\u00b0\")", "                except Exception as e:", "                    print(f\"Error in galactic slew: {e}\")", "", "            # 4 parts => s <Az> <El> azel", "            elif len(parts) == 4 and parts[-1] == \"azel\":", "                try:", "                    az = float(parts[1])", "                    el = float(parts[2])", "                except ValueError:", "                    print(\"Invalid numeric values for Az, El.\")", "                    continue", "", "                # Attempt to slew to these horizontal coordinates", "                try:", "                    rotor.set_pointing(az, el)", "                    rotor.current_azel = SkyCoord(alt=el*u.deg, az=az*u.deg, frame='altaz')", "                    rotor.telescope_pointing = rotor.current_azel", "                    rotor.current_lb = None  # We don't know the galactic coords here", "                    print(f\"Slewed to horizontal Az={round(az)}\u00b0, El={round(el)}\u00b0\")", "                except Exception as e:", "                    print(f\"Error in az/el slew: {e}\")", "            else:", "                print(\"Invalid usage. Try:\\n  s <L> <B>\\n  s <Az> <El> azel\")", "            continue"], "file_path": "source_tracking/terminal_controls.py"}
{"Link_to_commit": "https://github.com/lmorgan2398/color-palette/commit/291836e2086cff3afec648c99ee2067283ed0ffd", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 42, "n_files_impacted": 3, "longest_chunk": ["", "// Create function to create new palette board", "", "let newButton = document.querySelector('.new-button');", "newButton.addEventListener('click', () => {", "  localStorage.setItem(`currentName`, ``);", "  localStorage.setItem(`currentColor1`, `null`);", "  localStorage.setItem(`currentColor2`, `null`);", "  localStorage.setItem(`currentColor3`, `null`);", "  localStorage.setItem(`currentColor4`, `null`);", "  localStorage.setItem(`currentColor5`, `null`);", "  localStorage.setItem(`currentColor6`, `null`);", "  localStorage.setItem(`currentColor7`, `null`);", "  currentColor1 = localStorage.getItem('currentColor1');", "  currentColor2 = localStorage.getItem('currentColor2');", "  currentColor3 = localStorage.getItem('currentColor3');", "  currentColor4 = localStorage.getItem('currentColor4');", "  currentColor5 = localStorage.getItem('currentColor5');", "  currentColor6 = localStorage.getItem('currentColor6');", "  currentColor7 = localStorage.getItem('currentColor7');", "  for(let i = 1; i < 8; i++) {", "    console.log(localStorage.getItem(`currentColor${i}`));", "    if ((localStorage.getItem(`currentColor${i}`)) === 'null') {", "      document.querySelector(`.column${i} label`).style.backgroundColor = 'lightgray';", "      for(let j = 1; j < 10; j++) {", "        document.querySelector(`.column${i} .shade${j}`).style.backgroundColor = 'lightgray';", "        document.querySelector(`.column${i} .shade${j}`).textContent = null;", "      }", "      continue;", "    } else {", "      console.log('working');", "      let hue = hexToHue(localStorage.getItem(`currentColor${i}`));", "      shadeHue(hue, `${i}`);", "      applyTrueColor((localStorage.getItem(`currentColor${i}`)), `${i}`);", "    }", "  }", "  nameBox.value = null;", "  while(savedNameBox.firstChild) {", "    savedNameBox.removeChild(savedNameBox.firstChild);", "  };", "  savedNameBox.style.zIndex = 0;", "})"], "file_path": "script.js"}
{"Link_to_commit": "https://github.com/lalitm2005/DSA/commit/a4948b38779f912edeeaec1591f418d553552f34", "n-gram matched": "used chatgpt to", "n_lines_longer_change": 14, "n_files_impacted": 1, "longest_chunk": ["", "class Node{", "        public:", "        int data;", "        Node* next;", "        // defined a class node", "        // defining a constructor", "        Node(int val){", "            data=val;", "            next=nullptr;", "        }", "        ", "    };", "class linkedlist{"], "file_path": "linkedlist.cpp"}
{"Link_to_commit": "https://github.com/samchon/openapi/commit/024322731b9d1230e37b620328cfadda81c7a6a5", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 5, "n_files_impacted": 4, "longest_chunk": [" * function for correcting AI agent's mistakes, and this is the reason why", " * `@samchon/openapi` recommends not to use the", " * [`mcp_servers`](https://openai.github.io/openai-agents-python/mcp/#using-mcp-servers)", " * property of LLM API directly, but to use the function calling feature", " * instead."], "file_path": "src/structures/IMcpLlmApplication.ts"}
{"Link_to_commit": "https://github.com/cirglo/dfs/commit/d59640921f7afa02c2c4abb4fc212962c0d4f108", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 225, "n_files_impacted": 69, "longest_chunk": ["package name", "", "import (", "\t\"context\"", "\t\"errors\"", "\t\"fmt\"", "\t\"github.com/cirglo.com/dfs/pkg/proto\"", "\t\"github.com/sirupsen/logrus\"", "\t\"math/rand\"", "\t\"slices\"", "\t\"sync\"", "\t\"time\"", ")", "", "type HealingOpts struct {", "\tLogger            *logrus.Logger", "\tNumReplicas       uint", "\tFileService       FileService", "\tNodeExpiration    time.Duration", "\tConnectionFactory proto.ConnectionFactory", "}", "", "func (o *HealingOpts) Validate() error {", "\tif o.Logger == nil {", "\t\treturn fmt.Errorf(\"logger is required\")", "\t}", "", "\tif o.NumReplicas >= 255 {", "\t\treturn fmt.Errorf(\"number of replicas must be less than 256\")", "\t}", "", "\tif o.NumReplicas == 0 {", "\t\treturn fmt.Errorf(\"num replicas is required\")", "\t}", "", "\tif o.FileService == nil {", "\t\treturn fmt.Errorf(\"fileService is required\")", "\t}", "", "\tif o.ConnectionFactory == nil {", "\t\treturn fmt.Errorf(\"connection factory is required\")", "\t}", "", "\treturn nil", "}", "", "type HealingService interface {", "\tNotifyNodeAlive(host string, at time.Time)", "\tHeal(since time.Time) error", "}", "", "type healingService struct {", "\tOpts  HealingOpts", "\tNodes map[string]time.Time", "\tLock  sync.RWMutex", "}", "", "var _ HealingService = &healingService{}", "", "func NewHealingService(opts HealingOpts) (HealingService, error) {", "\terr := opts.Validate()", "\tif err != nil {", "\t\treturn nil, fmt.Errorf(\"invalid options: %w\", err)", "\t}", "", "\treturn &healingService{", "\t\tOpts:  opts,", "\t\tNodes: map[string]time.Time{},", "\t\tLock:  sync.RWMutex{},", "\t}, nil", "}", "", "func (s *healingService) NotifyNodeAlive(host string, at time.Time) {", "\ts.Lock.Lock()", "\tdefer s.Lock.Unlock()", "", "\ts.Nodes[host] = at", "}", "", "func (s *healingService) Heal(since time.Time) error {", "\tremovedHosts := s.removeExpiredNodes(since)", "\tvar allErrors []error", "\tfor _, host := range removedHosts {", "\t\ts.Opts.Logger.WithField(\"host\", host).Info(\"Removing expired node\")", "\t\terr := s.Opts.FileService.NodeRemoved(host)", "\t\tallErrors = append(allErrors, err)", "\t}", "", "\tblockInfos, err := s.Opts.FileService.GetAllBlockInfos()", "\tif err != nil {", "\t\treturn fmt.Errorf(\"could not get block infos: %w\", err)", "\t}", "", "\tcurrentLocations := map[string][]string{}", "", "\tfor _, blockInfo := range blockInfos {", "\t\tid := blockInfo.ID", "\t\tcurrentLocations[id] = []string{}", "", "\t\tfor _, location := range blockInfo.Locations {", "\t\t\thost := location.Host", "\t\t\tcurrentLocations[id] = append(currentLocations[id], host)", "\t\t}", "\t}", "", "\tfor id := range currentLocations {", "\t\tslices.Sort(currentLocations[id])", "\t}", "\tfor _, blockInfo := range blockInfos {", "\t\ts.checkBlock(blockInfo, currentLocations[blockInfo.ID])", "\t}", "", "\treturn errors.Join(allErrors...)", "}", "", "func (s *healingService) removeExpiredNodes(since time.Time) []string {", "\ts.Lock.Lock()", "\tdefer s.Lock.Unlock()", "", "\tvar toRemove []string", "", "\tfor host, at := range s.Nodes {", "\t\texpiration := at.Add(s.Opts.NodeExpiration)", "\t\tif expiration.Before(since) {", "\t\t\ttoRemove = append(toRemove, host)", "\t\t}", "\t}", "", "\tfor _, host := range toRemove {", "\t\ts.Opts.Logger.WithField(\"host\", host).Info(\"node is dead\")", "\t\tdelete(s.Nodes, host)", "\t}", "", "\treturn toRemove", "}", "", "func (s *healingService) checkBlock(blockInfo BlockInfo, currentLocations []string) {", "\ts.Lock.RLock()", "\tdefer s.Lock.RUnlock()", "", "\tneededCount := int(s.Opts.NumReplicas) - len(blockInfo.Locations)", "", "\tif neededCount > 0 {", "\t\ts.Opts.Logger.WithFields(logrus.Fields{", "\t\t\t\"block-id\":                  blockInfo.ID,", "\t\t\t\"mandatory-replicas-count\":  s.Opts.NumReplicas,", "\t\t\t\"replicas-count\":            len(blockInfo.Locations),", "\t\t\t\"needed-new-replicas-count\": neededCount,", "\t\t}).Info(\"Block needs more replicas\")", "\t\tdestinations, found := s.findDestinations(currentLocations, neededCount)", "\t\tif found {", "\t\t\tfor _, destination := range destinations {", "\t\t\t\tif len(currentLocations) == 0 {", "\t\t\t\t\ts.Opts.Logger.WithField(\"block-id\", blockInfo.ID).Warn(\"No current locations available to select a source for block replication\")", "\t\t\t\t\tcontinue", "\t\t\t\t}", "\t\t\t\tsource := currentLocations[rand.Intn(len(currentLocations))]", "\t\t\t\tgo s.copyBlock(blockInfo.ID, source, destination)", "\t\t\t}", "\t\t}", "\t}", "}", "", "func (s *healingService) findDestinations(currentLocations []string, count int) ([]string, bool) {", "\tvar candidates []string", "", "\tfor location := range s.Nodes {", "\t\t_, found := slices.BinarySearch(currentLocations, location)", "\t\tif !found {", "\t\t\tcandidates = append(candidates, location)", "\t\t}", "\t}", "", "\tif len(candidates) < count {", "\t\treturn nil, false", "\t}", "", "\tshuffle(candidates)", "", "\treturn candidates[:count], true", "}", "", "func (s *healingService) copyBlock(blockId string, source string, dest string) {", "\tconnection, err := s.Opts.ConnectionFactory(source)", "\tif err != nil {", "\t\ts.Opts.Logger.WithError(err).WithField(\"host\", dest).Error(\"could not create connection\")", "\t\treturn", "\t}", "\tdefer connection.Close()", "", "\tclient := proto.NewNodeClient(connection)", "", "\ts.Opts.Logger.WithFields(logrus.Fields{", "\t\t\"source\":      source,", "\t\t\"destination\": dest,", "\t\t\"block-id\":    blockId,", "\t}).Info(\"Copying block\")", "\t_, err = client.CopyBlock(context.Background(), &proto.CopyBlockRequest{", "\t\tId:          blockId,", "\t\tDestination: dest,", "\t})", "\tif err != nil {", "\t\ts.Opts.Logger.", "\t\t\tWithError(err).", "\t\t\tWithFields(logrus.Fields{", "\t\t\t\t\"block-id\":    blockId,", "\t\t\t\t\"source\":      source,", "\t\t\t\t\"destination\": dest,", "\t\t\t}).", "\t\t\tError(\"unable to copy block\")", "\t} else {", "\t\ts.Opts.Logger.WithFields(logrus.Fields{", "\t\t\t\"source\":      source,", "\t\t\t\"destination\": dest,", "\t\t\t\"block-id\":    blockId,", "\t\t}).Info(\"block copied\")", "\t}", "}", "", "func shuffle(slice []string) {", "\tfor i := range slice {", "\t\tj := rand.Intn(len(slice))", "\t\tslice[i], slice[j] = slice[j], slice[i]", "\t}", "}"], "file_path": "pkg/name/notificationserver.go"}
{"Link_to_commit": "https://github.com/wildenthal/ardanlabs-service/commit/61254a0cb8fe10127eb52ccfdc0feaf0d7f92168", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 28, "n_files_impacted": 5, "longest_chunk": ["package middleware", "", "import (", "\t\"log/slog\"", "\t\"net/http\"", ")", "", "type middleware struct {", "\tlogger *slog.Logger", "}", "", "func New(logger *slog.Logger) *middleware {", "\treturn &middleware{", "\t\tlogger: logger,", "\t}", "}", "", "func (m *middleware) HTTPMiddleware(next http.Handler) http.Handler {", "\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {", "\t\tdefer func() {", "\t\t\tif err := recover(); err != nil {", "\t\t\t\tm.logger.ErrorContext(r.Context(), \"Recovered from panic\", \"error\", err)", "\t\t\t\thttp.Error(w, \"Internal Server Error\", http.StatusInternalServerError)", "\t\t\t}", "\t\t}()", "\t\tnext.ServeHTTP(w, r)", "\t})", "}"], "file_path": "pkg/logging/logging.go"}
{"Link_to_commit": "https://github.com/CERTUNLP/ngen/commit/31420d96aa1915dfb6590d98c0f171816c23fbd1", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 27, "n_files_impacted": 8, "longest_chunk": ["", "# These variables are used to provide visualization on the admin interface", "# and are not used in the code", "ENVIRON_CONFIG = {", "    \"DEBUG\": DEBUG,", "    \"ALLOWED_HOSTS\": ALLOWED_HOSTS,", "    \"DATABASE_DEFAULT_NAME\": DATABASES[\"default\"][\"NAME\"],", "    \"DATABASE_DEFAULT_HOST\": DATABASES[\"default\"][\"HOST\"],", "    \"LANGUAGE_CODE\": LANGUAGE_CODE,", "    \"CELERY_BROKER_URL\": CELERY_BROKER_URL,", "    \"CELERY_RESULT_BACKEND\": CELERY_RESULT_BACKEND,", "    \"EMAIL_HOST\": EMAIL_HOST,", "    \"EMAIL_PORT\": EMAIL_PORT,", "    \"CONSTANCE_REDIS_CONNECTION\": CONSTANCE_REDIS_CONNECTION,", "    \"CONSTANCE_REDIS_CACHE_TIMEOUT\": CONSTANCE_REDIS_CACHE_TIMEOUT,", "    \"DEBUG_INTERNAL_IPS\": DEBUG_INTERNAL_IPS,", "    \"CORS_ALLOWED_ORIGINS\": frontend_urls,", "    \"CORS_ALLOW_ALL_ORIGINS\": CORS_ALLOW_ALL_ORIGINS,", "    \"CSRF_TRUSTED_ORIGINS\": CSRF_TRUSTED_ORIGINS,", "    \"CORS_ALLOW_CREDENTIALS\": CORS_ALLOW_CREDENTIALS,", "    \"ELASTIC_ENABLED\": ELASTIC_ENABLED,", "    \"ELASTIC_HOST_PORT\": (", "        ELASTICSEARCH_DSL[\"default\"][\"hosts\"] if ELASTIC_ENABLED else \"\"", "    ),", "    \"JWT_ACCESS_TOKEN_LIFETIME\": SIMPLE_JWT[\"ACCESS_TOKEN_LIFETIME\"],", "    \"JWT_REFRESH_TOKEN_LIFETIME\": SIMPLE_JWT[\"REFRESH_TOKEN_LIFETIME\"],", "}"], "file_path": "project/settings.py"}
{"Link_to_commit": "https://github.com/tyrm/mcp-dbmem/commit/2aace11e9118ee88e34be79cb4384542fd3c5af6", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 49, "n_files_impacted": 38, "longest_chunk": ["func deriveBunDBMyOptions(cfg ClientConfig) (string, error) {", "\t// these are all optional, the bun adapter figures out defaults", "\tport := cfg.Port", "\taddress := cfg.Address", "\tusername := cfg.User", "\tpassword := cfg.Password", "", "\t// validate database", "\tdatabase := cfg.Database", "\tif database == \"\" {", "\t\treturn \"\", errors.New(\"no database set\")", "\t}", "", "\ttlsConfig, err := makeTLSConfig(cfg)", "\tif err != nil {", "\t\tzap.L().Error(\"Error creating TLS config\", zap.Error(err))", "\t\treturn \"\", fmt.Errorf(\"could not create tls config: %w\", err)", "\t}", "", "\tmysqlOptions := \"\"", "\tif username != \"\" {", "\t\tmysqlOptions += username", "\t\tif password != \"\" {", "\t\t\tmysqlOptions += \":\" + password", "\t\t}", "\t\tmysqlOptions += \"@\"", "\t}", "\tif address != \"\" {", "\t\tmysqlOptions += \"tcp(\" + address", "\t\tif port > 0 {", "\t\t\tmysqlOptions += \":\" + strconv.Itoa(int(port))", "\t\t}", "\t\tmysqlOptions += \")\"", "\t}", "\tmysqlOptions += \"/\" + database", "", "\t// options", "\tif tlsConfig != nil {", "\t\tif err := mysql.RegisterTLSConfig(\"bun\", tlsConfig); err != nil {", "\t\t\treturn \"\", fmt.Errorf(\"could not register tls config: %w\", err)", "\t\t}", "", "\t\tmysqlOptions += \"?tls=bun\"", "\t}", "", "\treturn mysqlOptions, nil", "}", "", "func deriveBunDBPGOptions(cfg ClientConfig) (*pgx.ConnConfig, error) {"], "file_path": "internal/db/bun/client.go"}
{"Link_to_commit": "https://github.com/Daldek/IMGWTools/commit/0d17f4861bac35f930cb6d051b4de0be544062e9", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 60, "n_files_impacted": 3, "longest_chunk": ["import tkinter as tk", "from tkinter import messagebox", "from tkinter import ttk  # Importowanie ttk dla Combobox", "from imgw_api import PMAXTPAPI", "", "", "def fetch_data():", "    # Pobieranie warto\u015bci z Combobox i mapowanie na odpowiednie skr\u00f3ty", "    method = method_combobox.get()", "    lon = lon_entry.get()", "    lat = lat_entry.get()", "    output_location = output_location_entry.get()", "", "    if not method or not lon or not lat:", "        messagebox.showerror(\"B\u0142\u0105d\", \"Wszystkie pola musz\u0105 by\u0107 wype\u0142nione!\")", "        return", "", "    try:", "        api = PMAXTPAPI(", "            method=method,", "            # data_type=data_type,", "            lon=lon,", "            lat=lat,", "        )", "        api.get_data()", "        api.save_json_to_file(output_location=output_location)", "        messagebox.showinfo(\"Sukces\", \"Dane zosta\u0142y pobrane i zapisane do pliku JSON.\")", "        root.destroy()  # Zamyka aplikacj\u0119", "    except Exception as e:", "        messagebox.showerror(\"B\u0142\u0105d\", f\"Wyst\u0105pi\u0142 b\u0142\u0105d podczas pobierania danych: {e}\")", "", "", "# Tworzenie g\u0142\u00f3wnego okna aplikacji", "root = tk.Tk()", "root.title(\"PMAXTP API - Pobieranie danych\")", "", "# Etykiety i pola tekstowe dla parametr\u00f3w", "tk.Label(root, text=\"Metoda:\").grid(row=0, column=0, padx=10, pady=5)", "method_combobox = ttk.Combobox(root, values=[\"POT\", \"AMP\"])  # Lista rozwijana", "method_combobox.grid(row=0, column=1, padx=10, pady=5)", "method_combobox.set(\"POT\")  # Ustawienie domy\u015blnej warto\u015bci", "", "tk.Label(root, text=\"D\u0142ugo\u015b\u0107 geograficzna:\").grid(row=1, column=0, padx=10, pady=5)", "lon_entry = tk.Entry(root)", "lon_entry.grid(row=1, column=1, padx=10, pady=5)", "", "tk.Label(root, text=\"Szeroko\u015b\u0107 geograficzna:\").grid(row=2, column=0, padx=10, pady=5)", "lat_entry = tk.Entry(root)", "lat_entry.grid(row=2, column=1, padx=10, pady=5)", "", "tk.Label(root, text=\"Folder docelowy:\").grid(row=3, column=0, padx=10, pady=5)", "output_location_entry = tk.Entry(root)", "output_location_entry.grid(row=3, column=1, padx=10, pady=5)", "", "# Przycisk do pobierania danych", "fetch_button = tk.Button(root, text=\"Pobierz dane\", command=fetch_data)", "fetch_button.grid(row=4, column=0, columnspan=2, pady=10)", "", "# Uruchomienie p\u0119tli g\u0142\u00f3wnej aplikacji", "root.mainloop()"], "file_path": "code/pmaxtp_gui.py"}
{"Link_to_commit": "https://github.com/brianduc/library-management-be/commit/b2e08b21aa495c6c7909474153da0bb309ca82c7", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 158, "n_files_impacted": 38, "longest_chunk": ["const BorrowRecord = require(\"../models/borrow-record\");", "const BorrowRequest = require(\"../models/borrow-request\");", "const userService = require(\"../services/user.service\");", "const Book = require(\"../models/book\");", "", "const transporter = require(\"../../utils/mailer\")", "async function getAll() {", "  return await BorrowRequest.find()", "    .populate(\"user_id\", \"full_name email _id\")", "    .populate(\"book_id\", \"title author\");", "}", "", "async function getByUserId(userId) {", "  const userExist = await userService.getUserById(userId);", "  if (!userExist) return null;", "", "  return await BorrowRequest.find({ user_id: userId }).populate(", "    \"book_id\",", "    \"title author\"", "  );", "}", "", "async function create(data) {", "  const userExist = await userService.getUserById(data.user_id);", "  if (!userExist) {", "    return {", "      error: true,", "      message: \"User not found\",", "      statusCode: 404,", "    };", "  }", "", "  const bookExist = await Book.findById(data.book_id);", "  if (!bookExist) {", "    return {", "      error: true,", "      message: \"Book not found\",", "      statusCode: 404,", "    };", "  }", "", "  const isBorrowing = await BorrowRecord.findOne({", "    user_id: data.user_id,", "    book_id: data.book_id,", "    is_returned: false,", "  });", "", "  if (isBorrowing) {", "    return {", "      error: true,", "      message: \"This book is currently borrowed and not returned yet.\",", "      statusCode: 400,", "    };", "  }", "", "  const pendingRequest = await BorrowRequest.findOne({", "    user_id: data.user_id,", "    book_id: data.book_id,", "    status: \"pending\",", "  });", "", "  if (pendingRequest) {", "    return {", "      error: true,", "      message: \"Borrow request is already pending. Please wait...\",", "      statusCode: 400,", "    };", "  }", "", "  return (await BorrowRequest.create(data)).toObject();", "}", "", "async function updateStatus(id, status) {", "  const updateData = { status };", "  const data = await BorrowRequest.findById(id)", "    .populate(\"user_id\", \"full_name email\")", "    .populate(\"book_id\", \"title\");", "", "  if (!data) return null;", "", "  if (data.status === \"approved\") {", "    return {", "      error: true,", "      message: \"Borrow request is already approved\",", "      statusCode: 400,", "    };", "  }", "", "  if (status === \"approved\") {", "    const book = await Book.findById(data.book_id);", "    if (!book) {", "      return {", "        error: true,", "        message: \"Book not found\",", "        statusCode: 404,", "      };", "    }", "", "    if (book.quantity_available <= 0) {", "      return {", "        error: true,", "        message: \"Book is currently not available for borrowing\",", "        statusCode: 400,", "      };", "    }", "", "    book.quantity_available -= 1;", "    if (book.quantity_available === 0) {", "      book.status = \"out_of_stock\";", "    }", "", "    await book.save();", "    updateData.approved_date = new Date();", "  }", "", "  if (status === \"rejected\") {", "    updateData.rejected_date = new Date();", "", "    // G\u1eedi email t\u1eeb ch\u1ed1i", "    await sendRejectionEmail(", "      data.user_id.email,", "      data.user_id.full_name,", "      data.book_id.title", "    );", "  }", "", "  return await BorrowRequest.findByIdAndUpdate(id, updateData, { new: true })", "    .populate(\"user_id\", \"full_name email _id\")", "    .populate(\"book_id\", \"title author\");", "}", "", "async function sendRejectionEmail(email, fullName, bookTitle) {", "  console.log(\"sendRejectionEmail\", email, fullName, bookTitle)", "", "", "  const mailOptions = {", "    from: '\"Library System\" <your.email@gmail.com>',", "    to: email,", "    subject: \"Y\u00eau c\u1ea7u m\u01b0\u1ee3n s\u00e1ch \u0111\u00e3 b\u1ecb t\u1eeb ch\u1ed1i\",", "    html: `<p>Ch\u00e0o ${fullName},</p>", "           <p>R\u1ea5t ti\u1ebfc, y\u00eau c\u1ea7u m\u01b0\u1ee3n s\u00e1ch <strong>${bookTitle}</strong> c\u1ee7a b\u1ea1n \u0111\u00e3 b\u1ecb t\u1eeb ch\u1ed1i.</p>", "           <p>Vui l\u00f2ng li\u00ean h\u1ec7 v\u1edbi th\u01b0 vi\u1ec7n \u0111\u1ec3 bi\u1ebft th\u00eam chi ti\u1ebft.</p>", "           <p>Tr\u00e2n tr\u1ecdng,<br/>H\u1ec7 th\u1ed1ng th\u01b0 vi\u1ec7n</p>`,", "  };", "  try {", "    await transporter.sendMail(mailOptions);", "    console.log(\"Rejection email sent successfully\");", "  } catch (error) {", "    console.error(\"Error sending rejection email:\", error);", "  }", "}", "", "module.exports = {", "  getAll,", "  getByUserId,", "  create,", "  updateStatus,", "};"], "file_path": "services/category.service.js"}
{"Link_to_commit": "https://github.com/codeceptjs/CodeceptJS/commit/256e52567e37baf7b1726b080f4b3683d9df5df9", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 6, "n_files_impacted": 1, "longest_chunk": ["      try {", "        output.print(output.styles.debug(JSON.stringify(val, null, 2)))", "      } catch (err) {", "        output.print(output.styles.error(' ERROR '), 'Failed to stringify result:', err.message)", "        output.print(output.styles.error(' RAW VALUE '), String(val))", "      }"], "file_path": "lib/pause.js"}
{"Link_to_commit": "https://github.com/plugveg/stock-merch-ugs/commit/7e119366d6b0cbc7f415c0b0bb7f663f9dd22e96", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 90, "n_files_impacted": 18, "longest_chunk": ["import { renderHook } from \"@testing-library/react\";", "import { useCurrentUser } from \"../useCurrentUser\";", "import {", "  describe,", "  expect,", "  it,", "  beforeEach,", "  afterEach,", "  vi,", "  type Mock,", "} from \"vitest\";", "import { useConvexAuth, useQuery } from \"convex/react\";", "", "// Mock the convex/react module", "vi.mock(\"convex/react\", async () => {", "  const actual = await vi.importActual(\"convex/react\");", "  return {", "    ...actual,", "    useConvexAuth: vi.fn(),", "    useQuery: vi.fn(),", "  };", "});", "", "const mockUseConvexAuth = useConvexAuth as Mock;", "const mockUseQuery = useQuery as Mock;", "", "describe(\"useCurrentUser hook\", () => {", "  beforeEach(() => {", "    // By default, user is not loading nor authenticated, and query returns null", "    mockUseConvexAuth.mockReturnValue({", "      isLoading: false,", "      isAuthenticated: false,", "    });", "    mockUseQuery.mockReturnValue(null);", "  });", "", "  afterEach(() => {", "    vi.clearAllMocks();", "  });", "", "  it(\"returns loading true while auth is loading\", () => {", "    mockUseConvexAuth.mockReturnValue({", "      isLoading: true,", "      isAuthenticated: false,", "    });", "    const { result } = renderHook(() => useCurrentUser());", "    expect(result.current.isLoading).toBe(true);", "    expect(result.current.isAuthenticated).toBe(false);", "    expect(result.current.userInConvex).toBeNull();", "  });", "", "  it(\"returns not loading and not authenticated when auth done but no user\", () => {", "    // auth done, not authenticated (or no user)", "    mockUseConvexAuth.mockReturnValue({", "      isLoading: false,", "      isAuthenticated: false,", "    });", "    mockUseQuery.mockReturnValue(null);", "    const { result } = renderHook(() => useCurrentUser());", "    expect(result.current.isLoading).toBe(false);", "    expect(result.current.isAuthenticated).toBe(false);", "    expect(result.current.userInConvex).toBeNull();", "  });", "", "  it(\"returns loading true when authenticated but user query still null\", () => {", "    mockUseConvexAuth.mockReturnValue({", "      isLoading: false,", "      isAuthenticated: true,", "    });", "    mockUseQuery.mockReturnValue(null);", "    const { result } = renderHook(() => useCurrentUser());", "    // isUserQueryLoading = true => overall isLoading true", "    expect(result.current.isLoading).toBe(true);", "    expect(result.current.isAuthenticated).toBe(false);", "    expect(result.current.userInConvex).toBeNull();", "  });", "", "  it(\"returns loaded and authenticated when user is returned\", () => {", "    const fakeUser = { id: \"1\", name: \"Alice\" };", "    mockUseConvexAuth.mockReturnValue({", "      isLoading: false,", "      isAuthenticated: true,", "    });", "    mockUseQuery.mockReturnValue(fakeUser);", "    const { result } = renderHook(() => useCurrentUser());", "    expect(result.current.isLoading).toBe(false);", "    expect(result.current.isAuthenticated).toBe(true);", "    expect(result.current.userInConvex).toBe(fakeUser);", "  });", "});"], "file_path": "src/hooks/useCurrentUser.ts"}
{"Link_to_commit": "https://github.com/artsy/cohesion/commit/280ac28bcd7c9342b3d01150a8d927ff5789c887", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 23, "n_files_impacted": 2, "longest_chunk": ["import { ActionType } from \".\"", "", "/**", " * When the dark mode option is updated", " *", " * This schema describes events sent to Segment when a user updates the dark mode option in their account settings.", " *", " *  @example", " *  ```", " *  {", " *    action: \"darkModeOptionUpdated\",", " *    context_module: \"accountSettings\",", " *    context_screen_owner_type: \"accountDarkMode\",", " *    dark_mode_option: \"system\"", " *  }", " * ```", " */", "export interface DarkModeOptionUpdated {", "  action: ActionType.darkModeOptionUpdated", "  context_module: string", "  context_screen_owner_type: string", "  dark_mode_option: string", "}"], "file_path": "src/Schema/Events/Settings.ts"}
{"Link_to_commit": "https://github.com/getgreenspark/sdks/commit/4c6606a04b1ea042d6c2f1a8aa64fe8734ce4ea1", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 16, "n_files_impacted": 1, "longest_chunk": ["", "  if (document.readyState === 'loading') {", "    return new Promise((resolve) => {", "      document.addEventListener('DOMContentLoaded', () => {", "        setup().then(resolve)", "      }, { once: true })", "    })", "  }", "", "  try {", "    await loadScript(widgetUrl)", "    window.dispatchEvent(new Event('greenspark-setup'))", "  } catch (error) {", "    console.error('Greenspark Widget - Failed to load script:', error)", "    setTimeout(() => setup(), 1000)", "  }"], "file_path": "widgets-shopify/src/index.ts"}
{"Link_to_commit": "https://github.com/kornia/kornia/commit/fc0ac03d1a3f523785d3b4166b4358f6bec7ac73", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 6, "n_files_impacted": 1, "longest_chunk": ["        device = image_rgb.device", "        dtype = image_rgb.dtype", "        # Move quantization tables to the same device and dtype as input", "        # and store it in the local variables created in init", "        quantization_table_y = self.quantization_table_y.to(device, dtype)", "        quantization_table_c = self.quantization_table_c.to(device, dtype)"], "file_path": "kornia/enhance/jpeg.py"}
{"Link_to_commit": "https://github.com/K20shores/cudaviz/commit/323f3430a799b37b076e1c99ed2fc8ac04ed900f", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 74, "n_files_impacted": 10, "longest_chunk": ["#include <cudaviz/Diffusion>", "#include <cudaviz/kernels.hpp>", "", "#include \"check_error.hpp\"", "", "#include <vector>", "#include <iostream>", "#include <string>", "#include <stdexcept>", "#include <math.h>", "", "#include <cuda_runtime.h>", "", "namespace cudaviz", "{", "  void set_initial_conditions(std::vector<float> &h_old, int nx, int ny, float central_temperature, float spread) {", "    for(int y = 0; y < ny; ++y) {", "      for(int x = 0; x < nx; ++x) {", "        float X = x - nx / 2;", "        float Y = y - ny / 2;", "        h_old[y*nx + x] = central_temperature * exp(-(X * X + Y * Y) / spread);", "      }", "    }", "  }", "", "  std::vector<std::vector<std::vector<float>>> naive_diffusion(int nx, int ny, int nt, float D, float central_temperature, float spread)", "  {", "    std::size_t sz = nx * ny * sizeof(float);", "    std::vector<float> h_old = std::vector<float>(nx * ny, 0.0f);", "", "    set_initial_conditions(h_old, nx, ny, central_temperature, spread);", "", "    float *d_old;", "    float *d_new;", "", "    CUDA_CHECK(cudaMalloc(&d_old, sz));", "    CUDA_CHECK(cudaMalloc(&d_new, sz));", "", "    CUDA_CHECK(cudaMemcpy(d_old, h_old.data(), sz, cudaMemcpyHostToDevice));", "", "    std::vector<std::vector<std::vector<float>>> grid3D(nt, std::vector<std::vector<float>>(nx, std::vector<float>(ny, 0)));", "    for(int y = 0; y < ny; ++y) {", "      for(int x = 0; x < nx; ++x) {", "        grid3D[0][y][x] = h_old[y*nx + x];", "      }", "    }", "", "    // h = dx = dy ", "    float h = 1.0f;", "    float dt = 0.1f;", "    float alpha = dt * D / (h * h);", "    for (int t = 1; t < nt; ++t)", "    {", "      int num_substeps = static_cast<int>(1.0f / dt);", "      for(int substep = 0; substep < num_substeps; ++substep) {", "        float current_time = substep * dt;", "        naiive_diffusion_iteration(d_old, d_new, nx, ny, alpha);", "        std::swap(d_old, d_new);", "      }", "", "      CUDA_CHECK(cudaMemcpy(h_old.data(), d_old, sz, cudaMemcpyDeviceToHost));", "      for(int y = 0; y < ny; ++y) {", "        for(int x = 0; x < nx; ++x) {", "          grid3D[t][y][x] = h_old[y*nx + x];", "        }", "      }", "    }", "", "    CUDA_CHECK(cudaFree(d_old));", "    CUDA_CHECK(cudaFree(d_new));", "", "    return grid3D;", "  }", "}"], "file_path": "src/diffusion.cpp"}
{"Link_to_commit": "https://github.com/MyCarrier-DevOps/goLibMyCarrier/commit/ffbd5dea3120e70b80b98dc755faf0bb33361186", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 141, "n_files_impacted": 20, "longest_chunk": ["package kafka", "", "import (", "\t\"crypto/tls\"", "\t\"fmt\"", "", "\t\"strconv\"", "", "\t\"github.com/segmentio/kafka-go\"", "\t\"github.com/segmentio/kafka-go/sasl/scram\"", "\t\"github.com/spf13/viper\"", ")", "", "// KafkaConfig represents the configuration for Kafka.", "type KafkaConfig struct {", "\tAddress   string `mapstructure:\"address\"`", "\tTopic     string `mapstructure:\"topic\"`", "\tUsername  string `mapstructure:\"username\"`", "\tPassword  string `mapstructure:\"password\"`", "\tGroupID   string `mapstructure:\"groupid\"`", "\tPartition string `mapstructure:\"partition\"`", "}", "", "// LoadConfig loads the configuration from environment variables using Viper.", "func LoadConfig() (*KafkaConfig, error) {", "\t// Bind environment variables", "\tviper.BindEnv(\"address\", \"KAFKA_ADDRESS\")", "\tviper.BindEnv(\"topic\", \"KAFKA_TOPIC\")", "\tviper.BindEnv(\"username\", \"KAFKA_USERNAME\")", "\tviper.BindEnv(\"password\", \"KAFKA_PASSWORD\")", "\tviper.BindEnv(\"groupid\", \"KAFKA_GROUPID\")", "\tviper.BindEnv(\"partition\", \"KAFKA_PARTITION\")", "", "\t// Read environment variables", "\tviper.AutomaticEnv()", "", "\tvar kafkaConfig KafkaConfig", "", "\t// Unmarshal environment variables into the Config struct", "\tif err := viper.Unmarshal(&kafkaConfig); err != nil {", "\t\treturn nil, fmt.Errorf(\"unable to decode into struct, %v\", err)", "\t}", "", "\t// Validate the configuration", "\tif err := validateConfig(&kafkaConfig); err != nil {", "\t\treturn nil, err", "\t}", "", "\treturn &kafkaConfig, nil", "}", "", "// validateConfig validates the loaded configuration.", "func validateConfig(kafkaConfig *KafkaConfig) error {", "\tif kafkaConfig.Address == \"\" {", "\t\treturn fmt.Errorf(\"kafka address is required\")", "\t}", "\tif kafkaConfig.Topic == \"\" {", "\t\treturn fmt.Errorf(\"kafka topic is required\")", "\t}", "\tif kafkaConfig.Username == \"\" {", "\t\treturn fmt.Errorf(\"kafka username is required\")", "\t}", "\tif kafkaConfig.Password == \"\" {", "\t\treturn fmt.Errorf(\"kafka password is required\")", "\t}", "\tif kafkaConfig.GroupID == \"\" {", "\t\tkafkaConfig.GroupID = \"default-group\"", "\t}", "\tif kafkaConfig.Partition == \"\" {", "\t\tkafkaConfig.Partition = \"0\"", "\t} else {", "\t\tif _, err := strconv.Atoi(kafkaConfig.Partition); err != nil {", "\t\t\treturn fmt.Errorf(\"kafka partition must be a valid numeric value\")", "\t\t}", "\t}", "\treturn nil", "}", "", "func InitializeKafkaReader(kafkacfg *KafkaConfig) (*kafka.Reader, error) {", "\tmechanism, mech_err := scram.Mechanism(scram.SHA512, kafkacfg.Username, kafkacfg.Password)", "\tif mech_err != nil {", "\t\treturn nil, fmt.Errorf(\"error creating sasl mechanism: %v\", mech_err)", "\t}", "", "\tdialer := &kafka.Dialer{", "\t\tSASLMechanism: mechanism,", "\t\tTLS: &tls.Config{", "\t\t\tInsecureSkipVerify: true,", "\t\t},", "\t}", "", "\t// Create a new Kafka reader", "\treaderConfig := kafka.ReaderConfig{", "\t\tBrokers:     []string{kafkacfg.Address},", "\t\tGroupID:     kafkacfg.GroupID,", "\t\tMinBytes:    1,    // 1 Byte", "\t\tMaxBytes:    10e6, // 10MB", "\t\tStartOffset: kafka.FirstOffset,", "\t\tDialer:      dialer,", "\t\tMaxAttempts: 5,", "\t}", "", "\t// Set Partition based on GroupID presence", "\tif kafkacfg.GroupID == \"\" {", "\t\tpartition, err := strconv.Atoi(kafkacfg.Partition)", "\t\tif err != nil {", "\t\t\treturn nil, fmt.Errorf(\"invalid partition value: %v\", err)", "\t\t}", "\t\treaderConfig.Partition = partition", "\t}", "", "\treader := kafka.NewReader(readerConfig)", "\treturn reader, nil", "}", "", "// InitializeKafkaWriter initializes a Kafka writer with the provided configuration.", "func InitializeKafkaWriter(kafkacfg *KafkaConfig) (*kafka.Writer, error) {", "\t// Initialize Kafka writer", "\tmechanism, err := scram.Mechanism(scram.SHA512, kafkacfg.Username, kafkacfg.Password)", "\tif err != nil {", "\t\treturn nil, fmt.Errorf(\"error creating SASL mechanism: %v\", err)", "\t}", "", "\tdialer := &kafka.Dialer{", "\t\tSASLMechanism: mechanism,", "\t\tTLS: &tls.Config{", "\t\t\tInsecureSkipVerify: true,", "\t\t},", "\t}", "", "\twriter := kafka.NewWriter(kafka.WriterConfig{", "\t\tBrokers:     []string{kafkacfg.Address},", "\t\tTopic:       kafkacfg.Topic,", "\t\tDialer:      dialer,", "\t\tBalancer:    &kafka.LeastBytes{},", "\t\tAsync:       true,", "\t\tMaxAttempts: 5,", "\t})", "", "\treturn writer, nil", "}"], "file_path": "kafka/kafka_test.go"}
{"Link_to_commit": "https://github.com/mattermost/mattermost-plugin-msteams/commit/aca73a206af42a3b9e2417a0373c2bb5e1b77d29", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 13, "n_files_impacted": 2, "longest_chunk": ["\tauthor, err := a.p.API.GetUser(post.UserId)", "\tif err != nil {", "\t\ta.p.API.LogError(\"Failed to get author\", \"user_id\", post.UserId, \"error\", err.Error())", "\t\thttp.Error(w, \"failed to get author\", http.StatusInternalServerError)", "\t\treturn", "\t}", "", "\tchannel, err := a.p.API.GetChannel(post.ChannelId)", "\tif err != nil {", "\t\tlogrus.Errorf(\"failed to get channel for channel ID %s: %v\", post.ChannelId, err)", "\t\thttp.Error(w, fmt.Sprintf(\"failed to get channel: %v\", err), http.StatusInternalServerError)", "\t}", ""], "file_path": "server/iframe.go"}
{"Link_to_commit": "https://github.com/fearless-lark/mlops-sandbox/commit/e1649c55808f536cad5e2e0e3d67b0cf8eb0ddb9", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 82, "n_files_impacted": 8, "longest_chunk": ["'''Utility functions for downloading files and checking their existence.'''", "import os", "import pickle", "import requests", "import pandas as pd", "", "", "def download_files(file_paths, urls):", "    '''", "    Downloads files from the given list of URLs if not already present locally.", "", "    Args:", "        file_paths (list of str): Local paths where files should be stored.", "        urls (list of str): URLs to fetch the files from if not present locally.", "", "    Returns:", "        None", "    '''", "    if len(file_paths) != len(urls):", "        raise ValueError('file_paths and urls must have the same length')", "", "    for file_path, url in zip(file_paths, urls):", "        os.makedirs(os.path.dirname(file_path), exist_ok=True)", "", "        if not os.path.exists(file_path):", "            print(f'{file_path} not found. Fetching from {url}...')", "            response = requests.get(url, timeout=10)", "            if response.status_code == 200:", "                with open(file_path, 'wb') as f:", "                    f.write(response.content)", "                print(f'File downloaded and saved to {file_path}.')", "            else:", "                print(f'Failed to fetch the file from {url}. HTTP Status Code: {response.status_code}')", "        else:", "            print(f'File found locally at {file_path}.')", "", "", "def read_dataframe(filename):", "    '''", "    Reads a CSV or Parquet file into a pandas DataFrame, processes the data,", "    and returns the DataFrame.", "", "    Args:", "        filename (str): Path to the CSV or Parquet file.", "", "    Returns:", "        pd.DataFrame: Processed DataFrame with duration in minutes and categorical columns as strings.", "    '''", "    if filename.endswith('.csv'):", "        df = pd.read_csv(filename)", "", "        df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)", "        df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)", "    elif filename.endswith('.parquet'):", "        df = pd.read_parquet(filename)", "", "    df['duration'] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime", "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)", "", "    df = df[(df.duration >= 1) & (df.duration <= 60)]", "", "    categorical = ['PULocationID', 'DOLocationID']", "    df[categorical] = df[categorical].astype(str)", "", "    return df", "", "", "def save_model(obj, path):", "    '''", "    Saves the given object to a file using pickle, ensuring the directory exists.", "", "    Args:", "        obj (Any): Object to serialize (e.g., a tuple like (dv, lr)).", "        path (str): Path to the .bin file to save the object.", "    '''", "    # Ensure the directory exists", "    os.makedirs(os.path.dirname(path), exist_ok=True)", "", "    # Save the object", "    with open(path, 'wb') as f_out:", "        pickle.dump(obj, f_out)", "    print(f'Model saved to {path}')"], "file_path": "utils.py"}
{"Link_to_commit": "https://github.com/plugveg/stock-merch-ugs/commit/2eb36298aa1efe5d2d7b25eb2bd3afb495e38b98", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 441, "n_files_impacted": 6, "longest_chunk": ["# Help for the Convex functions directory", "", "## Functions for userProducts", "", "### Creating a Many-to-Many Relationship Between Users and Products", "", "Here's how you can define this association table:", "", "```typescript", "import { defineSchema, defineTable } from \"convex/server\";", "import { v } from \"convex/values\";", "", "export default defineSchema({", "  // Your existing users and products tables would be here", "", "  // Junction table for the many-to-many relationship", "  userProducts: defineTable({", "    ownerUserId: v.id(\"users\"), // Foreign key to users table", "    productId: v.id(\"products\"), // Foreign key to products table", "    // You can add additional fields related to this relationship if needed", "  })", "    .index(\"by_owner\", [\"ownerUserId\"]) // Index to find all products for a user", "    .index(\"by_product\", [\"productId\"]) // Index to find all users for a product", "    .index(\"by_owner_and_product\", [\"ownerUserId\", \"productId\"]), // Index for checking if a specific relationship exists", "});", "```", "", "This schema follows the best practices for modeling many-to-many relationships in Convex as described in [Relationship Structures: Let's Talk About Schemas](https://stack.convex.dev/relationship-structures-let-s-talk-about-schemas#many-to-many).", "", "### Using the Many-to-Many Relationship", "", "With this structure, you can:", "", "1. Find all products owned by a user:", "", "```typescript", "const userProducts = await ctx.db", "  .query(\"userProducts\")", "  .withIndex(\"by_owner\", (q) => q.eq(\"ownerUserId\", userId))", "  .collect();", "const productIds = userProducts.map((up) => up.productId);", "const products = await Promise.all(productIds.map((id) => ctx.db.get(id)));", "```", "", "2. Find all users who own a specific product:", "", "```typescript", "const userProducts = await ctx.db", "  .query(\"userProducts\")", "  .withIndex(\"by_product\", (q) => q.eq(\"productId\", productId))", "  .collect();", "const userIds = userProducts.map((up) => up.ownerUserId);", "const users = await Promise.all(userIds.map((id) => ctx.db.get(id)));", "```", "", "3. Check if a user owns a specific product:", "", "```typescript", "const relationship = await ctx.db", "  .query(\"userProducts\")", "  .withIndex(\"by_owner_and_product\", (q) =>", "    q.eq(\"ownerUserId\", userId).eq(\"productId\", productId),", "  )", "  .first();", "const userOwnsProduct = relationship !== null;", "```", "", "You can also use the relationship helpers from the Convex helpers library to simplify these operations, as described in [Database Relationship Helpers](https://stack.convex.dev/functional-relationships-helpers#many-to-many).", "", "---", "", "## Functions for Transactions, Products, Buyers, and Sellers", "", "Here are the functions for creating, retrieving, and managing these relationships:", "", "```typescript", "import { mutation, query } from \"./_generated/server\";", "import { v } from \"convex/values\";", "import { Id } from \"./_generated/dataModel\";", "import {", "  getAll,", "  getManyFrom,", "  getManyVia,", "} from \"convex-helpers/server/relationships\";", "", "// --- Transaction-Product Relationships ---", "", "// Create a relationship between a transaction and a product", "export const linkTransactionToProduct = mutation({", "  args: {", "    transactionId: v.id(\"transactions\"),", "    productId: v.id(\"products\"),", "  },", "  handler: async (ctx, args) => {", "    // Verify that both the transaction and product exist", "    const transaction = await ctx.db.get(args.transactionId);", "    const product = await ctx.db.get(args.productId);", "", "    if (!transaction || !product) {", "      throw new Error(\"Transaction or product not found\");", "    }", "", "    // Create the relationship", "    return await ctx.db.insert(\"transactionProducts\", {", "      transactionId: args.transactionId,", "      productId: args.productId,", "    });", "  },", "});", "", "// Get all products for a transaction", "export const getProductsForTransaction = query({", "  args: {", "    transactionId: v.id(\"transactions\"),", "  },", "  handler: async (ctx, args) => {", "    // Use the getManyVia helper to get products via the junction table", "    return await getManyVia(", "      ctx.db,", "      \"transactionProducts\",", "      \"productId\",", "      \"transactionId\",", "      args.transactionId,", "    );", "  },", "});", "", "// Get all transactions for a product", "export const getTransactionsForProduct = query({", "  args: {", "    productId: v.id(\"products\"),", "  },", "  handler: async (ctx, args) => {", "    // Use the getManyVia helper to get transactions via the junction table", "    return await getManyVia(", "      ctx.db,", "      \"transactionProducts\",", "      \"transactionId\",", "      \"productId\",", "      args.productId,", "    );", "  },", "});", "", "// --- Buyer-Transaction Relationships ---", "", "// Link a buyer to a transaction", "export const linkBuyerToTransaction = mutation({", "  args: {", "    transactionId: v.id(\"transactions\"),", "    buyerUserId: v.id(\"users\"),", "  },", "  handler: async (ctx, args) => {", "    // Verify that both the transaction and user exist", "    const transaction = await ctx.db.get(args.transactionId);", "    const buyer = await ctx.db.get(args.buyerUserId);", "", "    if (!transaction || !buyer) {", "      throw new Error(\"Transaction or buyer not found\");", "    }", "", "    // Create the relationship", "    return await ctx.db.insert(\"userBuyerTransactions\", {", "      transactionId: args.transactionId,", "      buyerUserId: args.buyerUserId,", "    });", "  },", "});", "", "// Get all transactions for a buyer", "export const getTransactionsForBuyer = query({", "  args: {", "    buyerUserId: v.id(\"users\"),", "  },", "  handler: async (ctx, args) => {", "    // Use the getManyVia helper to get transactions via the junction table", "    return await getManyVia(", "      ctx.db,", "      \"userBuyerTransactions\",", "      \"transactionId\",", "      \"buyerUserId\",", "      args.buyerUserId,", "    );", "  },", "});", "", "// Get the buyer for a transaction", "export const getBuyerForTransaction = query({", "  args: {", "    transactionId: v.id(\"transactions\"),", "  },", "  handler: async (ctx, args) => {", "    const buyerRelation = await ctx.db", "      .query(\"userBuyerTransactions\")", "      .withIndex(\"by_transaction\", (q) =>", "        q.eq(\"transactionId\", args.transactionId),", "      )", "      .first();", "", "    if (!buyerRelation) {", "      return null;", "    }", "", "    return await ctx.db.get(buyerRelation.buyerUserId);", "  },", "});", "", "// --- Seller-Transaction Relationships ---", "", "// Link a seller to a transaction", "export const linkSellerToTransaction = mutation({", "  args: {", "    transactionId: v.id(\"transactions\"),", "    sellerUserId: v.id(\"users\"),", "  },", "  handler: async (ctx, args) => {", "    // Verify that both the transaction and user exist", "    const transaction = await ctx.db.get(args.transactionId);", "    const seller = await ctx.db.get(args.sellerUserId);", "", "    if (!transaction || !seller) {", "      throw new Error(\"Transaction or seller not found\");", "    }", "", "    // Create the relationship", "    return await ctx.db.insert(\"userSellerTransactions\", {", "      transactionId: args.transactionId,", "      sellerUserId: args.sellerUserId,", "    });", "  },", "});", "", "// Get all transactions for a seller", "export const getTransactionsForSeller = query({", "  args: {", "    sellerUserId: v.id(\"users\"),", "  },", "  handler: async (ctx, args) => {", "    // Use the getManyVia helper to get transactions via the junction table", "    return await getManyVia(", "      ctx.db,", "      \"userSellerTransactions\",", "      \"transactionId\",", "      \"sellerUserId\",", "      args.sellerUserId,", "    );", "  },", "});", "", "// Get the seller for a transaction", "export const getSellerForTransaction = query({", "  args: {", "    transactionId: v.id(\"transactions\"),", "  },", "  handler: async (ctx, args) => {", "    const sellerRelation = await ctx.db", "      .query(\"userSellerTransactions\")", "      .withIndex(\"by_transaction\", (q) =>", "        q.eq(\"transactionId\", args.transactionId),", "      )", "      .first();", "", "    if (!sellerRelation) {", "      return null;", "    }", "", "    return await ctx.db.get(sellerRelation.sellerUserId);", "  },", "});", "", "// --- Complete Transaction Creation ---", "", "// Create a complete transaction with products, buyer, and seller", "export const createCompleteTransaction = mutation({", "  args: {", "    transactionName: v.string(),", "    quantity: v.number(),", "    soldPrice: v.number(),", "    soldDate: v.number(),", "    soldLocation: v.string(),", "    productId: v.id(\"products\"),", "    buyerUserId: v.id(\"users\"),", "    sellerUserId: v.id(\"users\"),", "  },", "  handler: async (ctx, args) => {", "    // Verify that the product, buyer, and seller exist", "    const product = await ctx.db.get(args.productId);", "    const buyer = await ctx.db.get(args.buyerUserId);", "    const seller = await ctx.db.get(args.sellerUserId);", "", "    if (!product || !buyer || !seller) {", "      throw new Error(\"Product, buyer, or seller not found\");", "    }", "", "    // Create the transaction", "    const transactionId = await ctx.db.insert(\"transactions\", {", "      transactionName: args.transactionName,", "      quantity: args.quantity,", "      soldPrice: args.soldPrice,", "      soldDate: args.soldDate,", "      soldLocation: args.soldLocation,", "    });", "", "    // Create the relationships", "    await ctx.db.insert(\"transactionProducts\", {", "      transactionId,", "      productId: args.productId,", "    });", "", "    await ctx.db.insert(\"userBuyerTransactions\", {", "      transactionId,", "      buyerUserId: args.buyerUserId,", "    });", "", "    await ctx.db.insert(\"userSellerTransactions\", {", "      transactionId,", "      sellerUserId: args.sellerUserId,", "    });", "", "    return transactionId;", "  },", "});", "", "// Get complete transaction details", "export const getCompleteTransaction = query({", "  args: {", "    transactionId: v.id(\"transactions\"),", "  },", "  handler: async (ctx, args) => {", "    const transaction = await ctx.db.get(args.transactionId);", "", "    if (!transaction) {", "      return null;", "    }", "", "    // Get related products", "    const products = await getManyVia(", "      ctx.db,", "      \"transactionProducts\",", "      \"productId\",", "      \"transactionId\",", "      args.transactionId,", "    );", "", "    // Get buyer", "    const buyerRelation = await ctx.db", "      .query(\"userBuyerTransactions\")", "      .withIndex(\"by_transaction\", (q) =>", "        q.eq(\"transactionId\", args.transactionId),", "      )", "      .first();", "", "    const buyer = buyerRelation", "      ? await ctx.db.get(buyerRelation.buyerUserId)", "      : null;", "", "    // Get seller", "    const sellerRelation = await ctx.db", "      .query(\"userSellerTransactions\")", "      .withIndex(\"by_transaction\", (q) =>", "        q.eq(\"transactionId\", args.transactionId),", "      )", "      .first();", "", "    const seller = sellerRelation", "      ? await ctx.db.get(sellerRelation.sellerUserId)", "      : null;", "", "    return {", "      ...transaction,", "      products,", "      buyer,", "      seller,", "    };", "  },", "});", "```", "", "These functions follow the patterns described in [Database Relationship Helpers](https://stack.convex.dev/functional-relationships-helpers) and [Relationship Structures: Let's Talk About Schemas](https://stack.convex.dev/relationship-structures-let-s-talk-about-schemas#many-to-many).", "", "This approach leverages Convex's transaction system to ensure data integrity while providing a clean API for working with your relationships.", "", "---", "", "## Using Indexes in Convex Queries for Transactions, Products, Buyers, and Sellers", "", "### Can I use `withIndex` in my get functions?", "", "Yes, you can definitely use `withIndex` in your get functions to improve query performance. In fact, it's a best practice to use indexes when querying your Convex database, especially as your tables grow larger.", "", "Let's modify some of the get functions from the previous examples to use indexes more explicitly:", "", "```typescript", "// Get all products for a transaction", "export const getProductsForTransaction = query({", "  args: {", "    transactionId: v.id(\"transactions\"),", "  },", "  handler: async (ctx, args) => {", "    // Using withIndex explicitly", "    const relationships = await ctx.db", "      .query(\"transactionProducts\")", "      .withIndex(\"by_transaction\", (q) =>", "        q.eq(\"transactionId\", args.transactionId),", "      )", "      .collect();", "", "    // Get all the products", "    return await Promise.all(", "      relationships.map((rel) => ctx.db.get(rel.productId)),", "    );", "  },", "});", "", "// Get the buyer for a transaction", "export const getBuyerForTransaction = query({", "  args: {", "    transactionId: v.id(\"transactions\"),", "  },", "  handler: async (ctx, args) => {", "    const buyerRelation = await ctx.db", "      .query(\"userBuyerTransactions\")", "      .withIndex(\"by_transaction\", (q) =>", "        q.eq(\"transactionId\", args.transactionId),", "      )", "      .unique();", "", "    if (!buyerRelation) {", "      return null;", "    }", "", "    return await ctx.db.get(buyerRelation.buyerUserId);", "  },", "});", "```", "", "Using `withIndex` is more efficient than using `.filter()` because it allows Convex to use the index data structure to quickly find the relevant documents rather than scanning the entire table [Queries that scale](https://stack.convex.dev/queries-that-scale#1-fetching-exactly-what-you-need-with-indexes).", "", "The performance of a query using an index is based on how many documents are in the index range, not the total size of the table. This makes indexed queries much more scalable as your database grows [Introduction to Indexes and Query Performance](https://docs.convex.dev/database/reading-data/indexes/indexes-and-query-perf#conclusions).", "", "Remember that when using compound indexes (indexes on multiple fields), you must reference the fields in the same order they appear in the index definition, starting with the first field [Introduction to Indexes and Query Performance](https://docs.convex.dev/database/reading-data/indexes/indexes-and-query-perf#indexing-multiple-fields)."], "file_path": "convex/schema.ts"}
{"Link_to_commit": "https://github.com/grumpycatyo-collab/max-plamadeala.com/commit/339e8f5dd60218128d57a50c25d77124dc7ae8da", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 9, "n_files_impacted": 8, "longest_chunk": ["const projectsCollection = defineCollection({", "  loader: glob({ pattern: \"**/*.md\", base: \"./content/projects\" }),", "  schema: z.object({", "    title: z.string(),", "    link: z.string(),", "    slug: z.string(),", "  }),", "});", ""], "file_path": "src/content.config.ts"}
{"Link_to_commit": "https://github.com/gabrigode/daily-rem-js/commit/3d233ec29861aaf1fc894213dcbca539a8122d23", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 24, "n_files_impacted": 20, "longest_chunk": ["class DanbooruService {", "  static getPost = async () => {", "    try {", "      const apiUrl = process.env.DANBOORU_API_URL;", "      if (!apiUrl) {", "        throw new Error('DANBOORU_API_URL is not set or is empty');", "      }", "      const response = await fetch(apiUrl);", "", "      const data = await response.json();", "", "      if (!response.ok) {", "        throw new Error('Failed to fetch data from Danbooru');", "      }", "", "      return data;", "    } catch (err) {", "      console.error('Error fetching posts:', err);", "      return null;", "    }", "  };", "}", "", "export default DanbooruService;"], "file_path": "src/services/twitter.ts"}
{"Link_to_commit": "https://github.com/grafana/grafana-image-renderer/commit/7d7f79bcccded10b3c6fc2da96bf8a34b892909b", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 17, "n_files_impacted": 3, "longest_chunk": ["      const headers = req.headers();", "      const url = req.url();", "      const method = req.method();", "      const referer = headers['referer'] ?? '';", "", "      try {", "        const urlHostname = new URL(url).hostname;", "        const refererHostname = referer ? new URL(referer).hostname : '';", "        const shouldAddHeaders = req.isNavigationRequest() || urlHostname === refererHostname;", "        this.log.debug('Comparing referer and URL hostnames', 'method', method, 'shouldAddHeaders', shouldAddHeaders, 'url', url, 'referer', referer);", "", "        if (shouldAddHeaders) {", "          headers['traceparent'] = optionsHeaders['traceparent'] ?? '';", "          headers['tracestate'] = optionsHeaders['tracestate'] ?? '';", "        }", "      } catch (error) {", "        this.log.debug('Failed to add tracing headers', 'url', url, 'referer', referer, 'error', error.message);"], "file_path": "src/browser/browser.ts"}
{"Link_to_commit": "https://github.com/moller2866/simple-fortune-cookie/commit/e33625a5d72763ddebb969f63a23ae3c80decce9", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 32, "n_files_impacted": 3, "longest_chunk": ["name: CI Pipeline", "", "on:", "  pull_request:", "    branches:", "      - main", "  push:", "    branches:", "      - main", "", "jobs:", "  build-and-test:", "    runs-on: ubuntu-latest", "", "    steps:", "    - name: Checkout code", "      uses: actions/checkout@v3", "", "    - name: Set up Go", "      uses: actions/setup-go@v3", "      with:", "        go-version: 1.21", "", "    - name: Build", "      run: |", "        cd frontend", "        go build", "", "    - name: Run tests", "      run: |", "        cd frontend", "        go test ./..."], "file_path": "backend/main.go"}
{"Link_to_commit": "https://github.com/yannis-mlgrn/MyCyberMonitor/commit/f0327664dd6da55af57783bd72fd80020ffd22d7", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 7, "n_files_impacted": 6, "longest_chunk": ["from pydantic import BaseModel", "", "", "class CVE(BaseModel):", "    id: str", "    description: str", "    link: str"], "file_path": "backend/app/utils/cveScraping.py"}
{"Link_to_commit": "https://github.com/interTwin-eu/itwinai/commit/7006ac8595ca0557f717391979b6c09406f228a7", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 6, "n_files_impacted": 3, "longest_chunk": ["    pre_exec_cmd: Annotated[", "        str | None,", "        typer.Option(", "            \"--pre-exec-cmd\", help=\"The pre-execution command to use for the python script.\"", "        ),", "    ] = None,"], "file_path": "src/itwinai/cli.py"}
{"Link_to_commit": "https://github.com/EltonTML/testing-aiops/commit/1337d225de94522b949f5ae833f90cbfb0490696", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 27, "n_files_impacted": 1, "longest_chunk": ["# models.py", "from sqlalchemy import Column, Integer, String, create_engine", "from sqlalchemy.ext.declarative import declarative_base", "from sqlalchemy.orm import sessionmaker", "", "# Example credentials for PostgreSQL (can switch to SQLite if preferred)", "import os", "", "DB_USER = os.getenv('DB_USER', 'default_user')", "DB_PASS = os.getenv('DB_PASS', 'default_password')", "DB_HOST = os.getenv('DB_HOST', 'localhost')", "DB_PORT = os.getenv('DB_PORT', '5432')", "DB_NAME = os.getenv('DB_NAME', 'testdb')", "DATABASE_URL = f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"", "# For SQLite instead: DATABASE_URL = \"sqlite:///posts.db\"", "", "Base = declarative_base()", "", "class Post(Base):", "    __tablename__ = 'posts'", "    id = Column(Integer, primary_key=True)", "    title = Column(String)", "    body = Column(String)", "", "# Set up the engine and session", "engine = create_engine(DATABASE_URL)", "SessionLocal = sessionmaker(bind=engine)"], "file_path": "models.py"}
{"Link_to_commit": "https://github.com/EltonTML/testing-aiops/commit/bd4affcf762db4768e719c48d2c16d05b1e59d13", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 10, "n_files_impacted": 1, "longest_chunk": ["        return []", "", "def save_posts_to_db(posts):", "    Base.metadata.create_all(bind=engine)", "    with SessionLocal() as session:", "        for post_data in posts:", "            post = Post(id=post_data['id'], title=post_data['title'], body=post_data['body'])", "            session.merge(post)  # merge to avoid duplicates on re-run", "        session.commit()", "    print(\"Posts saved to database.\")"], "file_path": "main.py"}
{"Link_to_commit": "https://github.com/AvaProtocol/EigenLayer-AVS/commit/eddf4405206e8a36fd716576ab283eb903924cca", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 14, "n_files_impacted": 11, "longest_chunk": ["\t// Execute the task logic", "\t_, runErr := x.RunTask(task, queueData)", "", "\tif runErr == nil {", "\t\t// Task logic executed successfully. Clean up the TaskTriggerKey for this async execution.", "\t\tif queueData != nil && queueData.ExecutionID != \"\" { // Assumes `ExecutionID` is always set for queued jobs. Verify this assumption if the logic changes.", "\t\t\ttriggerKeyToClean := TaskTriggerKey(task, queueData.ExecutionID)", "\t\t\tif delErr := x.db.Delete(triggerKeyToClean); delErr != nil {", "\t\t\t\tx.logger.Error(\"Perform: Failed to delete TaskTriggerKey after successful async execution\",", "\t\t\t\t\t\"key\", string(triggerKeyToClean), \"task_id\", task.Id, \"execution_id\", queueData.ExecutionID, \"error\", delErr)", "\t\t\t} else {", "\t\t\t\t// Successfully deleted, no need for a verbose log here unless for specific debug scenarios", "\t\t\t\t// x.logger.Info(\"Perform: Successfully deleted TaskTriggerKey after async execution\",", "\t\t\t\t// \t\"key\", string(triggerKeyToClean), \"task_id\", task.Id, \"execution_id\", queueData.ExecutionID)"], "file_path": "core/taskengine/executor.go"}
{"Link_to_commit": "https://github.com/Schnitzels-tue/AttacKit/commit/2cac56cd4ef878a002bd28b7a98ff409818832e9", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 58, "n_files_impacted": 15, "longest_chunk": ["#include \"network_scout/sniffing.h\"", "#include \"PcapLiveDeviceList.h\"", "#include \"RawPacket.h\"", "#include \"common/pcap_to_common.h\"", "", "namespace {", "constexpr int MAX_CAPTURE_TIMEOUT = 10;", "", "/**", " * Tracks the progress of packet capture", " */", "struct PacketTracker {", "    int packetsToCapture; // how many more packets to capture.", "    std::vector<ATK::Common::PacketInfo> *packetInfoList;", "};", "", "/**", " * Caputure N packets", " *", " */", "bool onPacketArrives(pcpp::RawPacket *packet, pcpp::PcapLiveDevice * /*dev*/,", "                     void *cookie) {", "", "    auto *packetTracker = static_cast<PacketTracker *>(cookie);", "    pcpp::Packet parsedPacket(packet);", "", "    packetTracker->packetInfoList->emplace_back(", "        ATK::Common::toPacketInfo(parsedPacket));", "    packetTracker->packetsToCapture--;", "", "    // return false means we don't want to stop capturing after this", "    // callback", "    return packetTracker->packetsToCapture <= 0;", "}", "} // namespace", "", "std::vector<ATK::Common::PacketInfo>", "ATK::Scout::sniffPackets(const std::string &deviceIpOrName, int numPackets) {", "    std::vector<ATK::Common::PacketInfo> packetInfoList;", "    packetInfoList.reserve(numPackets);", "", "    pcpp::PcapLiveDevice *device =", "        pcpp::PcapLiveDeviceList::getInstance().getPcapLiveDeviceByIpOrName(", "            deviceIpOrName);", "", "    if (device == nullptr || !device->open()) {", "        throw std::invalid_argument(\"Unable to open device: \" + deviceIpOrName);", "    }", "", "    PacketTracker tracker{.packetsToCapture = numPackets,", "                          .packetInfoList = &packetInfoList};", "    device->startCaptureBlockingMode(onPacketArrives, &tracker,", "                                     MAX_CAPTURE_TIMEOUT);", "", "    device->close();", "", "    return packetInfoList;", "}"], "file_path": "core/src/network_scout/sniffing.cpp"}
{"Link_to_commit": "https://github.com/Jacobbrewer1/web/commit/95c900aba729e6ff70a5b58d59ed9c1b6687481e", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 78, "n_files_impacted": 5, "longest_chunk": ["", "func TestChecker_ErrorGracePeriod(t *testing.T) {", "\tt.Parallel()", "", "\tt.Run(\"within grace period\", func(t *testing.T) {", "\t\tt.Parallel()", "", "\t\tc, err := NewChecker(WithCheckerErrorGracePeriod(5 * time.Second))", "\t\trequire.NoError(t, err)", "\t\trequire.NotNil(t, c)", "", "\t\tcheck := NewCheck(\"test_check\", func(_ context.Context) error {", "\t\t\treturn errors.New(\"test error\")", "\t\t})", "", "\t\terr = c.AddCheck(check)", "\t\trequire.NoError(t, err)", "", "\t\t// Simulate a failure", "\t\tc.firstFailInCycle.Store(time.Now().UTC().Add(-2 * time.Second))", "", "\t\tres := c.Check(context.Background())", "\t\trequire.Equal(t, StatusUp, res.Status)", "\t})", "", "\tt.Run(\"outside grace period\", func(t *testing.T) {", "\t\tt.Parallel()", "", "\t\tc, err := NewChecker(WithCheckerErrorGracePeriod(5 * time.Second))", "\t\trequire.NoError(t, err)", "\t\trequire.NotNil(t, c)", "", "\t\tcheck := NewCheck(\"test_check\", func(_ context.Context) error {", "\t\t\treturn errors.New(\"test error\")", "\t\t})", "", "\t\terr = c.AddCheck(check)", "\t\trequire.NoError(t, err)", "", "\t\t// Simulate a failure", "\t\tc.firstFailInCycle.Store(time.Now().UTC().Add(-10 * time.Second))", "", "\t\tres := c.Check(context.Background())", "\t\trequire.Equal(t, StatusDown, res.Status)", "\t})", "", "\tt.Run(\"multiple async checks\", func(t *testing.T) {", "\t\tt.Parallel()", "", "\t\tc, err := NewChecker(WithCheckerErrorGracePeriod(5 * time.Second))", "\t\trequire.NoError(t, err)", "\t\trequire.NotNil(t, c)", "", "\t\tcheck1 := NewCheck(\"test_check_1\", func(_ context.Context) error {", "\t\t\treturn errors.New(\"test error\")", "\t\t})", "", "\t\tcheck2 := NewCheck(\"test_check_2\", func(_ context.Context) error {", "\t\t\treturn errors.New(\"test error 2\")", "\t\t})", "", "\t\tcheck3 := NewCheck(\"test_check_3\", func(_ context.Context) error {", "\t\t\treturn errors.New(\"test error 3\")", "\t\t})", "", "\t\terr = c.AddCheck(check1)", "\t\trequire.NoError(t, err)", "", "\t\terr = c.AddCheck(check2)", "\t\trequire.NoError(t, err)", "", "\t\terr = c.AddCheck(check3)", "\t\trequire.NoError(t, err)", "", "\t\tres := c.Check(context.Background())", "\t\trequire.Equal(t, StatusUp, res.Status)", "\t})", "}"], "file_path": "options.go"}
{"Link_to_commit": "https://github.com/FEREorg/ferelight/commit/f80868879b9d77d5d1dfecb377ace21299982fc9", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 11, "n_files_impacted": 3, "longest_chunk": ["```", "", "## Development", "", "### Releasing New Versions", "", "To release a new version to PyPI:", "", "1. Update the version number in `setup.py`", "2. Create a new GitHub release or tag with a version number (e.g., `v1.0.1`)", "3. The GitHub Actions workflow will automatically build and publish the package to PyPI"], "file_path": "setup.py"}
{"Link_to_commit": "https://github.com/adamsilverstein/mathml-block/commit/8351985b200f0b3f7b577917c50ab0e6c2c642e3", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 19, "n_files_impacted": 7, "longest_chunk": ["\t\tplugins: [", "\t\t\tnew WebpackBar(", "\t\t\t\t{", "\t\t\t\t\tname: 'Plugin Entry Points',", "\t\t\t\t\tcolor: '#B6CD58',", "\t\t\t\t}", "\t\t\t),", "\t\t\tnew ESLintPlugin({", "\t\t\t\tfailOnError: true,", "\t\t\t\textensions: ['js', 'jsx'],", "\t\t\t}),", "\t\t],", "\t\tperformance: {", "\t\t\thints: 'warning',", "\t\t},", "\t\toptimization: {", "\t\t\tminimize: true,", "\t\t},", "\t\ttarget: ['web', 'es5'],"], "file_path": "webpack.config.js"}
{"Link_to_commit": "https://github.com/m0-foundation/solana-m/commit/492c87ca6ed67ed687a1309d9667d8d4da891a54", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 15, "n_files_impacted": 3, "longest_chunk": ["    try {", "      const { lastValidBlockHeight, blockhash } = await connection.getLatestBlockhash();", "", "      await connection.confirmTransaction(", "        {", "          blockhash: blockhash,", "          lastValidBlockHeight: lastValidBlockHeight,", "          signature: sig,", "        },", "        'confirmed',", "      );", "    } catch (error) {", "      const errorMessage = error instanceof Error ? error.message : JSON.stringify(error);", "      throw new Error(`Failed to confirm transaction: ${sig}. Error details: ${errorMessage}`);", "    }"], "file_path": "dashboard/src/services/rpc.ts"}
{"Link_to_commit": "https://github.com/ProfiFlow/backend/commit/2555513197c30f36f53212ac13c5330fd2fff08b", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 393, "n_files_impacted": 59, "longest_chunk": ["import logging", "from datetime import datetime", "from typing import List, Optional", "", "from sqlalchemy.ext.asyncio import AsyncSession", "", "from app.database.repositories.report import ReportRepository, TeamReportRepository", "from app.database.repositories.user import UserRepository", "from app.database.user import User", "from app.schemas.report import SprintStats", "from app.schemas.sprint_report import SprintReport", "from app.schemas.team_report import (", "    EmployeeSprintStats,", "    MetricWithComparison,", "    TeamSprintReport,", ")", "from app.schemas.yandex_tracker import Task", "from app.services.yandex_gpt_service import YandexGPTMLService", "from app.services.yandex_tracker import YandexTrackerService", "", "log = logging.getLogger(__name__)", "", "", "class ReportService:", "    \"\"\"", "    Service for generating various types of reports.", "    Relies on injected tracker and ML services.", "    \"\"\"", "", "    def __init__(", "        self,", "        db: AsyncSession,", "        yandex_tracker_service: YandexTrackerService,", "        yandex_gpt_service: YandexGPTMLService,", "        user_repo: UserRepository,", "        report_repo: ReportRepository,", "        team_report_repo: TeamReportRepository,", "    ):", "        self.db = db", "        self.yandex_tracker_service = yandex_tracker_service", "        self.yandex_gpt_service = yandex_gpt_service", "        self.user_repo = user_repo", "        self.report_repo = report_repo", "        self.team_report_repo = team_report_repo", "", "    async def generate_sprint_report(", "        self,", "        user: User,", "        sprint_id: int,", "        current_user_id: int,", "    ) -> SprintReport:", "        \"\"\"", "        Generate a comprehensive sprint report for an employee.", "        Uses provided service instances.", "        \"\"\"", "", "        sprint = await self.yandex_tracker_service.get_sprint(", "            sprint_id, current_user_id", "        )", "        if not sprint:", "            raise ValueError(f\"Sprint with ID {sprint_id} not found.\")", "", "        tracker_info = await self.user_repo.get_user_current_tracker(user.id)", "        if not tracker_info:", "            raise ValueError(\"\u041d\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c tracker_id \u0434\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\")", "        tracker, _ = tracker_info", "        tracker_id = tracker.id", "", "        tasks = await self.yandex_tracker_service.get_sprint_tasks(", "            sprint_id, current_user_id, user.login", "        )", "        sprint_stats = await self._process_tasks(tasks, current_user_id)", "", "        existing_report = await self.report_repo.get_sprint_report_by_id(", "            user.id, tracker_id, sprint_id", "        )", "        if existing_report:", "            prev_report = await self.report_repo.get_previous_sprint_report(", "                user.id, tracker_id, existing_report.sprint_start_date", "            )", "            prev_stats = None", "            if prev_report:", "                prev_stats = SprintStats(", "                    total_story_points=prev_report.story_points_closed,", "                    total_tasks=prev_report.tasks_completed,", "                    deadlines_missed=prev_report.deadlines_missed,", "                    average_completion_time=prev_report.average_task_completion_time,", "                )", "            return SprintReport(", "                user_id=existing_report.user_id,", "                employee_name=user.display_name,", "                sprint_name=existing_report.sprint_name,", "                sprint_start_date=existing_report.sprint_start_date,", "                sprint_end_date=existing_report.sprint_end_date,", "                story_points_closed=self._create_metric_comparison(", "                    existing_report.story_points_closed,", "                    prev_stats.total_story_points if prev_stats else None,", "                ),", "                tasks_completed=self._create_metric_comparison(", "                    existing_report.tasks_completed,", "                    prev_stats.total_tasks if prev_stats else None,", "                ),", "                deadlines_missed=self._create_metric_comparison(", "                    existing_report.deadlines_missed,", "                    prev_stats.deadlines_missed if prev_stats else None,", "                ),", "                average_task_completion_time=self._create_metric_comparison(", "                    existing_report.average_task_completion_time,", "                    prev_stats.average_completion_time if prev_stats else None,", "                ),", "                activity_analysis=existing_report.activity_analysis,", "                recommendations=existing_report.recommendations,", "            )", "", "        prev_report = await self.report_repo.get_previous_sprint_report(", "            user.id, tracker_id, sprint.start_date", "        )", "        prev_stats = None", "        if prev_report:", "            prev_stats = SprintStats(", "                total_story_points=prev_report.story_points_closed,", "                total_tasks=prev_report.tasks_completed,", "                deadlines_missed=prev_report.deadlines_missed,", "                average_completion_time=prev_report.average_task_completion_time,", "            )", "        story_points_closed = self._create_metric_comparison(", "            sprint_stats.total_story_points,", "            prev_stats.total_story_points if prev_stats else None,", "        )", "        tasks_completed = self._create_metric_comparison(", "            sprint_stats.total_tasks, prev_stats.total_tasks if prev_stats else None", "        )", "        deadlines_missed = self._create_metric_comparison(", "            sprint_stats.deadlines_missed,", "            prev_stats.deadlines_missed if prev_stats else None,", "        )", "        average_task_completion_time = self._create_metric_comparison(", "            sprint_stats.average_completion_time,", "            prev_stats.average_completion_time if prev_stats else None,", "        )", "", "        try:", "            activity_analysis = await self.yandex_gpt_service.analyze_employee_activity(", "                tasks, sprint_stats", "            )", "            recommendations = (", "                await self.yandex_gpt_service.generate_employee_recommendations(", "                    sprint_stats", "                )", "            )", "        except Exception as e:", "            log.error(f\"LLM error: {e}\")", "            raise", "", "        await self.report_repo.save_or_update_sprint_report(", "            user_id=user.id,", "            tracker_id=tracker_id,", "            sprint_id=sprint_id,", "            sprint_name=sprint.name,", "            sprint_start_date=sprint.start_date,", "            sprint_end_date=sprint.end_date,", "            story_points_closed=story_points_closed,", "            tasks_completed=tasks_completed,", "            deadlines_missed=deadlines_missed,", "            average_task_completion_time=average_task_completion_time,", "            activity_analysis=activity_analysis,", "            recommendations=recommendations,", "        )", "        return SprintReport(", "            user_id=user.id,", "            employee_name=user.display_name,", "            sprint_name=sprint.name,", "            sprint_start_date=sprint.start_date,", "            sprint_end_date=sprint.end_date,", "            story_points_closed=story_points_closed,", "            tasks_completed=tasks_completed,", "            deadlines_missed=deadlines_missed,", "            average_task_completion_time=average_task_completion_time,", "            activity_analysis=activity_analysis,", "            recommendations=recommendations,", "        )", "", "    def _calculate_percent_change(self, current: float, previous: float) -> float:", "        \"\"\"", "        Calculate percent change between current and previous values.", "        Positive percentage means improvement, negative means decline.", "        \"\"\"", "        if previous == 0:", "            return 100.0 if current > 0 else 0.0", "", "        return ((current - previous) / previous) * 100", "", "    def _create_metric_comparison(", "        self, current: float, previous: Optional[float] = None", "    ) -> MetricWithComparison:", "        \"\"\"", "        Create a metric with comparison between current and previous sprint.", "        \"\"\"", "        metric = MetricWithComparison(current=current)", "", "        if previous is not None:", "            metric.previous = previous", "            metric.change_percent = self._calculate_percent_change(current, previous)", "", "        return metric", "", "    async def generate_team_sprint_report(", "        self,", "        current_user_id: int,", "        sprint_id: int,", "    ) -> TeamSprintReport:", "        \"\"\"", "        Generate a comprehensive sprint report for the entire team.", "        Uses provided service instances.", "        \"\"\"", "", "        # \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0442\u0435\u043a\u0443\u0449\u0438\u0439 \u0442\u0440\u0435\u043a\u0435\u0440 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f", "        tracker_info = await self.user_repo.get_user_current_tracker(current_user_id)", "        if not tracker_info:", "            raise ValueError(\"\u041d\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c tracker_id \u0434\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\")", "        tracker, role = tracker_info", "", "        if role != \"manager\":", "            raise ValueError(\"\u0423 \u0432\u0430\u0441 \u043d\u0435\u0442 \u0434\u043e\u0441\u0442\u0443\u043f\u0430 \u043a \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u043a\u043e\u043c\u0430\u043d\u0434\u043d\u044b\u0445 \u043e\u0442\u0447\u0435\u0442\u043e\u0432\")", "", "        tracker_id = tracker.id", "", "        existing_team_report = await self.team_report_repo.get_team_sprint_report_by_id(", "            tracker_id, sprint_id", "        )", "        if existing_team_report:", "            employee_stats_final = [", "                EmployeeSprintStats(**emp)", "                for emp in existing_team_report.employee_stats", "            ]", "            return TeamSprintReport(", "                sprint_id=existing_team_report.sprint_id,", "                sprint_start_date=existing_team_report.sprint_start_date,", "                sprint_end_date=existing_team_report.sprint_end_date,", "                employee_stats=employee_stats_final,", "            )", "", "        users = await self.user_repo.get_users_for_tracker(tracker_id)", "", "        sprint = await self.yandex_tracker_service.get_sprint(", "            sprint_id, current_user_id", "        )", "        if not sprint:", "            raise ValueError(f\"Sprint with ID {sprint_id} not found.\")", "", "        prev_sprint = None", "        all_sprints = await self.yandex_tracker_service.get_sprints(current_user_id)", "        sprints_objs = all_sprints", "        sprints_sorted = sorted(sprints_objs, key=lambda s: s.start_date)", "        for idx, s in enumerate(sprints_sorted):", "            if s.id == sprint_id and idx > 0:", "                prev_sprint = sprints_sorted[idx - 1]", "                break", "", "        employee_stats = []", "        prev_employee_stats = []", "        for user in users:", "            sprint_report = await self.generate_sprint_report(", "                user, sprint_id, current_user_id", "            )", "            story_points_closed = sprint_report.story_points_closed", "            tasks_completed = sprint_report.tasks_completed", "            deadlines_missed = sprint_report.deadlines_missed", "            average_task_completion_time = sprint_report.average_task_completion_time", "            employee_stats.append(", "                {", "                    \"employee_id\": str(user.id),", "                    \"employee_name\": user.display_name,", "                    \"story_points_closed\": story_points_closed.model_dump(),", "                    \"tasks_completed\": tasks_completed.model_dump(),", "                    \"deadlines_missed\": deadlines_missed.model_dump(),", "                    \"average_task_completion_time\": average_task_completion_time.model_dump(),", "                }", "            )", "", "            log.debug(f\"Employee stats: {employee_stats}\")", "            prev_stats_report = None", "            if prev_sprint:", "                prev_stats_report = await self.report_repo.get_sprint_report_by_id(", "                    user.id, tracker_id, prev_sprint.id", "                )", "            if prev_stats_report:", "                prev_employee_stats.append(", "                    {", "                        \"employee_id\": str(user.id),", "                        \"employee_name\": user.display_name,", "                        \"story_points_closed\": {", "                            \"current\": prev_stats_report.story_points_closed", "                        },", "                        \"tasks_completed\": {", "                            \"current\": prev_stats_report.tasks_completed", "                        },", "                        \"deadlines_missed\": {", "                            \"current\": prev_stats_report.deadlines_missed", "                        },", "                        \"average_task_completion_time\": {", "                            \"current\": prev_stats_report.average_task_completion_time", "                        },", "                    }", "                )", "", "        try:", "            llm_result = await self.yandex_gpt_service.rate_team_performance(", "                employee_stats=employee_stats,", "                prev_employee_stats=(", "                    prev_employee_stats if prev_employee_stats else None", "                ),", "            )", "            rating_map = {str(r[\"employee_id\"]): r for r in llm_result}", "        except Exception as e:", "            log.error(f\"LLM error (team report): {e}\")", "            raise e", "", "        employee_stats_final = []", "        for emp in employee_stats:", "            rid = emp[\"employee_id\"]", "            rating = rating_map.get(rid, {}).get(\"rating\", 3)", "            rating_explanation = rating_map.get(rid, {}).get(", "                \"rating_explanation\", \"\u041e\u0448\u0438\u0431\u043a\u0430 AI \u043f\u0440\u0438 \u043e\u0446\u0435\u043d\u043a\u0435 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438\"", "            )", "            employee_stats_final.append(", "                EmployeeSprintStats(", "                    employee_id=emp[\"employee_id\"],", "                    employee_name=emp[\"employee_name\"],", "                    story_points_closed=MetricWithComparison(", "                        **emp[\"story_points_closed\"]", "                    ),", "                    tasks_completed=MetricWithComparison(**emp[\"tasks_completed\"]),", "                    deadlines_missed=MetricWithComparison(**emp[\"deadlines_missed\"]),", "                    average_task_completion_time=MetricWithComparison(", "                        **emp[\"average_task_completion_time\"]", "                    ),", "                    rating=rating,", "                    rating_explanation=rating_explanation,", "                )", "            )", "", "        await self.team_report_repo.save_or_update_team_sprint_report(", "            tracker_id=tracker_id,", "            sprint_id=sprint_id,", "            sprint_start_date=sprint.start_date,", "            sprint_end_date=sprint.end_date,", "            employee_stats=[emp.model_dump() for emp in employee_stats_final],", "        )", "        return TeamSprintReport(", "            sprint_id=sprint_id,", "            sprint_start_date=sprint.start_date,", "            sprint_end_date=sprint.end_date,", "            employee_stats=employee_stats_final,", "        )", "", "    async def _process_tasks(", "        self, tasks: List[Task], current_user_id: int", "    ) -> SprintStats:", "        \"\"\"", "        Process tasks to extract relevant statistics.", "        \"\"\"", "        total_story_points = 0", "        total_tasks = 0", "        deadlines_missed = 0", "        total_completion_time = 0.0", "", "        for task in tasks:", "            total_story_points += task.story_points if task.story_points else 0", "            total_tasks += 1", "", "            if task.deadline and (", "                task.status.key == \"done\"", "                and task.deadline < task.resolved_at", "                or task.deadline < datetime.utcnow().date()", "            ):", "                deadlines_missed += 1", "", "            if task.status.key == \"done\":", "                total_completion_time += (", "                    await self.yandex_tracker_service.get_issue_logged_time(", "                        task.id, current_user_id", "                    )", "                )", "", "        return SprintStats(", "            total_story_points=total_story_points,", "            total_tasks=total_tasks,", "            deadlines_missed=deadlines_missed,", "            average_completion_time=(", "                total_completion_time / total_tasks if total_tasks > 0 else 0", "            ),", "        )"], "file_path": "app/services/token_manager.py"}
{"Link_to_commit": "https://github.com/gorgias/gorgias-webflow/commit/d1b40c8bcd2b3d646324a548944216afeccad2e7", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 105, "n_files_impacted": 3, "longest_chunk": ["$('.edit-email').on('click', function(e) {", "    e.preventDefault(); // Prevent the link's default behavior", "    // Find the sibling input element with the \"form_input\" class.", "    let $input = $(this).siblings('input.form_input');", "    // Remove the \"is-ghost\" class and then focus the input.", "    $input.removeClass('is-ghost').focus();", "    // Hide the clicked \".edit-email\" element by setting its opacity to 0 and disabling pointer events.", "    $(this).css({'opacity': '0', 'pointer-events': 'none'});", "  });", "  ", "  // Mirror email input", "  $(document).ready(function () {", "    const $source = $('[data-el=\"mirror-email\"]');", "    const $target = $('[data-el=\"mirror-email-target\"]');", "  ", "    if (!$source.length || !$target.length) return;", "  ", "    // Mirror function", "    const mirrorEmail = () => {", "      $target.val($source.val());", "    };", "  ", "    // Initial sync (in case of pre-filled)", "    mirrorEmail();", "  ", "    // Sync on input and other possible events", "    $source.on('input change blur', mirrorEmail);", "  });", "  ", "  ", "  ", "  function waitForSuperformAndInit(retries = 10, delay = 300) {", "    if (!window.SuperformAPI || typeof window.SuperformAPI.push !== 'function') {", "      if (retries > 0) {", "        return setTimeout(() => waitForSuperformAndInit(retries - 1, delay), delay);", "      } else {", "        console.log(\"SuperformAPI never initialized.\");", "        return;", "      }", "    }", "  ", "    window.SuperformAPI.push(({ allForms }) => {", "      console.log(\"Full Superform objects:\", allForms);", "  ", "      const myForm = allForms[0];", "  ", "      if (!myForm || typeof myForm.onStepChange !== 'function') {", "        console.log(\"No usable Superform instance found.\");", "        return;", "      }", "  ", "      const formStepData = {};", "  ", "      myForm.onStepChange((params) => {", "        const { stepCount, data } = params;", "        Object.assign(formStepData, data);", "  ", "        console.log(\"All stored data so far:\", formStepData);", "  ", "        // Try to populate HubSpot form", "        populateHubspotForm(formStepData);", "      });", "    });", "  }", "  ", "  function populateHubspotForm(formData) {", "    const hsForm = document.querySelector(\"#hsForm_c0b510e0-b9e8-49bf-a54c-872a45c50040\");", "    if (!hsForm) {", "      console.log(\"HubSpot form not found yet.\");", "      return;", "    }", "  ", "    const fieldMapping = {", "      email: 'email',", "      Website: 'company_domain',", "      'ecommerce-platform': 'demo_ecommerce_platform',", "      conversations: 'demo_tickets_volume',", "      'annual-sales-range': 'demo_annual_sales_range',", "      '0-1_number_of_agents': 'number_of_agents',", "      '0-1_demo_current_helpdesk': 'demo_current_helpdesk',", "      '0-1_demo_utm_source': 'demo_utm_source',", "      '0-1_demo_utm_medium': 'demo_utm_medium',", "      '0-1_demo_utm_campaign': 'demo_utm_campaign',", "      '0-1_demo_utm_term': 'demo_utm_term',", "      '0-1_demo_timezone': 'demo_timezone',", "      '0-1_demo_lead_product_interest': 'demo_lead_product_interest'", "    };", "  ", "    for (const [sourceKey, targetName] of Object.entries(fieldMapping)) {", "      const value = formData[sourceKey];", "      if (!value) continue;", "  ", "      const input = hsForm.querySelector(`[name=\"${targetName}\"]`);", "      if (input) {", "        input.value = value;", "        input.dispatchEvent(new Event('input', { bubbles: true }));", "        input.dispatchEvent(new Event('change', { bubbles: true }));", "      } else {", "        console.log(`HubSpot field \"${targetName}\" not found in form.`);", "      }", "    }", "  }", "  ", "  // Kick things off", "  waitForSuperformAndInit();"], "file_path": "src/js/demo/demo-worker.js"}
{"Link_to_commit": "https://github.com/grumpycatyo-collab/max-plamadeala.com/commit/cd32aae4edad0a221749001e89a912252017f434", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 33, "n_files_impacted": 5, "longest_chunk": ["---", "interface Props {", "  id?: string;", "  mapping?: string;", "}", "", "const { ", "  id = \"comments\",", "  mapping = \"pathname\"", "} = Astro.props;", "", "const initialTheme = \"preferred_color_scheme\"; ", "---", "", "<div id={id} class=\"giscus-container\">", "    <script src=\"https://giscus.app/client.js\"", "        data-repo=\"grumpycatyo-collab/max-plamadeala.com\"", "        data-repo-id=\"R_kgDOOhpI6g\"", "        data-category=\"General\"", "        data-category-id=\"DIC_kwDOOhpI6s4CptT6\"", "        data-mapping=\"pathname\"", "        data-strict=\"0\"", "        data-reactions-enabled=\"1\"", "        data-emit-metadata=\"0\"", "        data-input-position=\"bottom\"", "        data-theme={initialTheme}", "        data-lang=\"en\"", "        data-loading=\"lazy\"", "        crossorigin=\"anonymous\"", "        async>", "    </script>", "</div>", ""], "file_path": "src/content.config.ts"}
{"Link_to_commit": "https://github.com/gofiber/fiber/commit/bfef962d868c8783624b40d16bcb6c5a62936850", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 5, "n_files_impacted": 1, "longest_chunk": ["\t\t// checking the found parameterStartChar is a cluster", "\t\ti := nextParamPosition + 1", "\t\tfor i < len(pattern) {", "\t\t\tif findNextNonEscapedCharsetPosition(pattern[i:i+1], parameterStartChars) != 0 {", "\t\t\t\t// It was a single parameter start char or end of cluster"], "file_path": "path.go"}
{"Link_to_commit": "https://github.com/TosiDrop/vm-sdk/commit/90cfa1d80d6a1c6f4a3c4809e638d5a6d5ad84d7", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 59, "n_files_impacted": 34, "longest_chunk": ["}", "", "export interface GetSettings {", "  withdrawal_fee: number;", "  tokens_fee: number;", "  epoch: number;", "  switching_epoch: boolean;", "  frontend_version: string;", "  backend_version: string;", "  min_balance: number;", "  confirmations_required: number;", "  max_assets_in_request: number;", "}", "", "export interface GetEstimateFees {", "  withdrawal_fee: string;", "  tokens_fee: number;", "  fee: number;", "  deposit: number;", "}", "", "export interface GetCustomRequest {", "  request_id: string;", "  deposit: number;", "  overhead_fee: number;", "  withdrawal_address: string;", "  is_whitelisted: boolean;", "}", "", "export interface GetTokenRequest {", "  token_id: string;", "  logo: string;", "  ticker: string;", "  name: string;", "  balance: string;", "  decimals: string;", "}", "", "export interface GetDeliveredRewards {", "  id: string;", "  staking_address: string;", "  epoch: string;", "  token: string;", "  amount: string;", "  withdrawal_request: string;", "  expiry_return_pool_id: string | null;", "  expiry: string;", "  return_policy: string;", "  delivered_on: string;", "}", "", "export interface GetPendingTxCount {", "  pending_tx_count: number;", "}", "", "export interface GetTx {", "  tx: string;", "  slot: string;", "  info: string;"], "file_path": "src/types/apiResponse.ts"}
{"Link_to_commit": "https://github.com/amp-labs/react/commit/2e3e5df48e0eef8d3bafb00ef453f8f07a2b26ed", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 18, "n_files_impacted": 3, "longest_chunk": ["import { OauthConnectOperationRequest } from \"@generated/api/src\";", "import { useMutation, useQueryClient } from \"@tanstack/react-query\";", "import { useAPI } from \"services/api\";", "", "export const useCreateOauthConnectionMutation = () => {", "  const getAPI = useAPI();", "  const queryClient = useQueryClient();", "  return useMutation({", "    mutationKey: [\"createOauthConnection\"],", "    mutationFn: async (request: OauthConnectOperationRequest) => {", "      const api = await getAPI();", "      return api.oAuthApi.oauthConnect(request);", "    },", "    onSuccess: () => {", "      queryClient.invalidateQueries({ queryKey: [\"connections\"] });", "    },", "  });", "};"], "file_path": "src/hooks/mutation/useCreateOauthConnectionMutation.ts"}
{"Link_to_commit": "https://github.com/Schnitzels-tue/AttacKit/commit/e088fc7e8e829bed4d302a1f49bf2350a701f35e", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 8, "n_files_impacted": 1, "longest_chunk": ["    // determine whether or not to handle packet", "    if ((requestEthLayer->getSourceMac() == device->getMacAddress() ||", "         requestEthLayer->getSourceMac() ==", "             allOutArpPoisoningCookie->attackerMacAddress) ||", "        requestArpLayer->getTargetIpAddr() != device->getIPv4Address()) {", "        return;", "    }", ""], "file_path": "core/src/arp_poisoning/all_out.cpp"}
{"Link_to_commit": "https://github.com/neo-project/neo/commit/58ec2dd3a38ac673c4d12c35d9609257286f8c56", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 25, "n_files_impacted": 1, "longest_chunk": ["            Transactions = DeserializeTransactions(ref reader, ushort.MaxValue, Header.MerkleRoot);", "        }", "", "        private static Transaction[] DeserializeTransactions(ref MemoryReader reader, int maxCount, UInt256 merkleRoot)", "        {", "            var count = (int)reader.ReadVarInt((ulong)maxCount);", "            var hashes = new UInt256[count];", "            var txs = new Transaction[count];", "", "            if (count > 0)", "            {", "                var hashset = new HashSet<UInt256>();", "                for (var i = 0; i < count; i++)", "                {", "                    var tx = reader.ReadSerializable<Transaction>();", "                    if (!hashset.Add(tx.Hash))", "                        throw new FormatException();", "                    txs[i] = tx;", "                    hashes[i] = tx.Hash;", "                }", "            }", "", "            if (MerkleTree.ComputeRoot(hashes) != merkleRoot)", "                throw new FormatException(\"The computed Merkle root does not match the expected value.\");", "            return txs;"], "file_path": "src/Neo/Network/P2P/Payloads/Block.cs"}
{"Link_to_commit": "https://github.com/neo-project/neo/commit/cedb65c32b43f04adfca5c3865fce147d28493d5", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 39, "n_files_impacted": 4, "longest_chunk": ["", "            murmur128.Reset();", "            murmur128.Append(\"hello \"u8.ToArray());", "            murmur128.Append(\"world\"u8.ToArray());", "            buffer = murmur128.GetCurrentHash();", "            Assert.AreEqual(\"e0a0632d4f51302c55e3b3e48d28795d\", buffer.ToHexString());", "", "            murmur128.Reset();", "            murmur128.Append(\"hello worldhello world\"u8.ToArray());", "            buffer = murmur128.GetCurrentHash();", "            Assert.AreEqual(\"76f870485d4e69f8302d4b3fad28fd39\", buffer.ToHexString());", "", "            murmur128.Reset();", "            murmur128.Append(\"hello world\"u8.ToArray());", "            murmur128.Append(\"hello world\"u8.ToArray());", "            buffer = murmur128.GetCurrentHash();", "            Assert.AreEqual(\"76f870485d4e69f8302d4b3fad28fd39\", buffer.ToHexString());", "", "            murmur128.Reset();", "            murmur128.Append(\"hello worldhello \"u8.ToArray());", "            murmur128.Append(\"world\"u8.ToArray());", "            buffer = murmur128.GetCurrentHash();", "            Assert.AreEqual(\"76f870485d4e69f8302d4b3fad28fd39\", buffer.ToHexString());", "        }", "", "        [TestMethod]", "        public void TestAppend()", "        {", "            var random = new Random();", "            var buffer = new byte[random.Next(1, 2048)];", "            random.NextBytes(buffer);", "            for (int i = 0; i < 32; i++)", "            {", "                int split = random.Next(1, buffer.Length - 1);", "                var murmur128 = new Murmur128(123u);", "                murmur128.Append(buffer.AsSpan(0, split));", "                murmur128.Append(buffer.AsSpan(split));", "                Assert.AreEqual(murmur128.GetCurrentHash().ToHexString(), buffer.Murmur128(123u).ToHexString());", "            }"], "file_path": "tests/Neo.UnitTests/Cryptography/UT_Murmur128.cs"}
{"Link_to_commit": "https://github.com/formbricks/formbricks/commit/409f5b17913a254027b5f56a8999709b425e2cc1", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 80, "n_files_impacted": 14, "longest_chunk": ["import \"@testing-library/jest-dom/vitest\";", "import { cleanup } from \"@testing-library/react\";", "import { afterEach, describe, expect, test } from \"vitest\";", "import { TProject } from \"@formbricks/types/project\";", "import { TXMTemplate } from \"@formbricks/types/templates\";", "import { replacePresetPlaceholders } from \"./utils\";", "", "// Mock data", "const mockProject: TProject = {", "  id: \"project1\",", "  createdAt: new Date(),", "  updatedAt: new Date(),", "  name: \"Test Project\",", "  organizationId: \"org1\",", "  styling: {", "    allowStyleOverwrite: true,", "    brandColor: { light: \"#FFFFFF\" },", "  },", "  recontactDays: 30,", "  inAppSurveyBranding: true,", "  linkSurveyBranding: true,", "  config: {", "    channel: \"link\" as const,", "    industry: \"eCommerce\" as \"eCommerce\" | \"saas\" | \"other\" | null,", "  },", "  placement: \"bottomRight\",", "  clickOutsideClose: true,", "  darkOverlay: false,", "  environments: [],", "  languages: [],", "  logo: null,", "};", "const mockTemplate: TXMTemplate = {", "  name: \"$[projectName] Survey\",", "  questions: [", "    {", "      id: \"q1\",", "      inputType: \"text\",", "      type: \"email\" as any,", "      headline: { default: \"$[projectName] Question\" },", "      required: false,", "      charLimit: { enabled: true, min: 400, max: 1000 },", "    },", "  ],", "  endings: [", "    {", "      id: \"e1\",", "      type: \"endScreen\",", "      headline: { default: \"Thank you for completing the survey!\" },", "    },", "  ],", "  styling: {", "    brandColor: { light: \"#0000FF\" },", "    questionColor: { light: \"#00FF00\" },", "    inputColor: { light: \"#FF0000\" },", "  },", "};", "", "describe(\"replacePresetPlaceholders\", () => {", "  afterEach(() => {", "    cleanup();", "  });", "", "  test(\"replaces projectName placeholder in template name\", () => {", "    const result = replacePresetPlaceholders(mockTemplate, mockProject);", "    expect(result.name).toBe(\"Test Project Survey\");", "  });", "", "  test(\"replaces projectName placeholder in question headline\", () => {", "    const result = replacePresetPlaceholders(mockTemplate, mockProject);", "    expect(result.questions[0].headline.default).toBe(\"Test Project Question\");", "  });", "", "  test(\"returns a new object without mutating the original template\", () => {", "    const originalTemplate = structuredClone(mockTemplate);", "    const result = replacePresetPlaceholders(mockTemplate, mockProject);", "    expect(result).not.toBe(mockTemplate);", "    expect(mockTemplate).toEqual(originalTemplate);", "  });", "});"], "file_path": "apps/web/app/(app)/(onboarding)/environments/[environmentId]/xm-templates/lib/xm-templates.test.ts"}
{"Link_to_commit": "https://github.com/test-time-training/ttt-video-dit/commit/ce9d0f208fd3661c148888c8df8569b203ebc651", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 11, "n_files_impacted": 3, "longest_chunk": ["        try:", "            # Expected dict for finetuned checkpoints", "            state_dict = {MODEL_STATE_DICT_KEY: get_model_state_dict(model)}", "            dcp.load(state_dict=state_dict, checkpoint_id=job_config.checkpoint.init_state_dir)  # type: ignore", "            state_dict = state_dict[MODEL_STATE_DICT_KEY]", "        except RuntimeError:", "            # Expected dict for newly converted checkpoints.", "            state_dict = get_model_state_dict(model)", "            dcp.load(state_dict=state_dict, checkpoint_id=job_config.checkpoint.init_state_dir)  # type: ignore", "", "        set_model_state_dict(model, model_state_dict=state_dict, options=StateDictOptions(strict=True))"], "file_path": "ttt/models/cogvideo/sampler.py"}
{"Link_to_commit": "https://github.com/r-Techsupport/TechSupportBot/commit/6bfcfbeb4cde67805ecd619a2ee1616af47d61e2", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 5, "n_files_impacted": 4, "longest_chunk": ["        try:", "            await message.add_reaction(emoji)", "        except discord.NotFound:", "            # Message was deleted, ignore and stop executing", "            return"], "file_path": "techsupport_bot/core/auxiliary.py"}
{"Link_to_commit": "https://github.com/0xcpu/saidia/commit/d4a8884b83f7540bf5e24e52eb6bd1524c1a9e1a", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 14, "n_files_impacted": 11, "longest_chunk": ["#!/bin/bash", "", "echo \"Building Saidia Extension...\"", "", "mkdir -p dist", "mkdir -p images", "", "echo \"Copying files to dist directory...\"", "cp manifest.json dist/", "cp popup.html dist/", "cp *.js dist/", "cp -r images dist/", "", "echo \"Build complete. Files are in the 'dist' directory.\""], "file_path": "chrome/content.js"}
{"Link_to_commit": "https://github.com/portapack-mayhem/mayhem-firmware/commit/584af02dba2323cd096f1fc2e56d06b8cd96aba3", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 12, "n_files_impacted": 2, "longest_chunk": ["    waterfall_widget.on_touch_select = [this](int32_t x, int32_t y) {", "        if (y > screen_height - screen_height * 0.1) return;  // prevent ghost touch", "", "        frequency_scale.focus();  // focus on frequency scale to show cursor", "", "        if (sampling_rate) {", "            // screen x to frequency scale x, NB we need two widgets align", "            int32_t cursor_position = x - (screen_width / 2);", "            frequency_scale.set_cursor_position(cursor_position);", "        }", "    };", ""], "file_path": "firmware/application/ui/ui_spectrum.cpp"}
{"Link_to_commit": "https://github.com/Thxssio/VSSS-Firmware/commit/da81b0d9efcafd29ed478e9a0f3b1d0e9a1cfa45", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 42, "n_files_impacted": 83, "longest_chunk": ["#include \"NRF24_Diagnostics.h\"", "#include \"nrf24l01p.h\"", "#include <stdio.h>", "#include <string.h>", "", "", "static UART_HandleTypeDef *debug_uart = NULL;", "extern SPI_HandleTypeDef hspi1;", "", "", "void NRF24_Diagnostics_Init(UART_HandleTypeDef *uart) {", "    debug_uart = uart;", "}", "", "void NRF24_UpdateStats(NRF24_Stats *stats) {", "    uint8_t obs = 0;", "    HAL_SPI_TransmitReceive(&hspi1, (uint8_t[]){R_REGISTER | OBSERVE_TX}, &obs, 1, HAL_MAX_DELAY);", "    HAL_SPI_Receive(&hspi1, &obs, 1, HAL_MAX_DELAY);", "", "    stats->lost_packets = (obs >> 4) & 0x0F;", "    stats->retransmissions = obs & 0x0F;", "    stats->link_quality = NRF24_CalculateLinkQuality(stats->lost_packets, stats->retransmissions);", "}", "", "float NRF24_CalculateLinkQuality(uint8_t lost, uint8_t retries) {", "    // F\u00f3rmula simplificada \u2014 pode ajustar para refletir melhor sua aplica\u00e7\u00e3o", "    float loss_factor = (float)lost / 15.0f;", "    float retry_factor = (float)retries / 15.0f;", "", "    float quality = 1.0f - (0.6f * loss_factor + 0.4f * retry_factor);", "    if (quality < 0.0f) quality = 0.0f;", "    return quality;", "}", "", "void NRF24_PrintStats(const NRF24_Stats *stats) {", "    if (debug_uart == NULL) return;", "", "    char buffer[64];", "    snprintf(buffer, sizeof(buffer), \"Lost: %d, Retries: %d, Quality: %.2f\\r\\n\",", "             stats->lost_packets, stats->retransmissions, stats->link_quality);", "    HAL_UART_Transmit(debug_uart, (uint8_t*)buffer, strlen(buffer), 100);", "}"], "file_path": "Core/Src/VSSS.c"}
{"Link_to_commit": "https://github.com/k-nuth/consensus/commit/ee0ffee1777a368c170e9f7bf7e223b1a9045534", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 97, "n_files_impacted": 8, "longest_chunk": ["// Copyright (c) 2016-2023 Knuth Project developers.", "// Distributed under the MIT software license, see the accompanying", "// file COPYING or http://www.opensource.org/licenses/mit-license.php.", "", "#include <kth/consensus/conversions.hpp>", "", "#include \"script/script_flags.h\"", "", "namespace kth::consensus {", "", "// This mapping decouples the consensus API from the satoshi implementation", "// files. We prefer to keep our copies of consensus files isomorphic.", "// This function is not published (but non-static for testability).", "unsigned int verify_flags_to_script_flags(unsigned int flags) {", "    unsigned int script_flags = SCRIPT_VERIFY_NONE;", "", "    if ((flags & verify_flags_p2sh) != 0) {", "        script_flags |= SCRIPT_VERIFY_P2SH;", "    }", "", "    if ((flags & verify_flags_strictenc) != 0) {", "        script_flags |= SCRIPT_VERIFY_STRICTENC;", "    }", "", "    if ((flags & verify_flags_dersig) != 0) {", "        script_flags |= SCRIPT_VERIFY_DERSIG;", "    }", "", "    if ((flags & verify_flags_low_s) != 0) {", "        script_flags |= SCRIPT_VERIFY_LOW_S;", "    }", "", "    if ((flags & verify_flags_sigpushonly) != 0) {", "        script_flags |= SCRIPT_VERIFY_SIGPUSHONLY;", "    }", "", "    if ((flags & verify_flags_minimaldata) != 0) {", "        script_flags |= SCRIPT_VERIFY_MINIMALDATA;", "    }", "", "    if ((flags & verify_flags_discourage_upgradable_nops) != 0) {", "        script_flags |= SCRIPT_VERIFY_DISCOURAGE_UPGRADABLE_NOPS;", "    }", "", "    if ((flags & verify_flags_cleanstack) != 0) {", "        script_flags |= SCRIPT_VERIFY_CLEANSTACK;", "    }", "", "    if ((flags & verify_flags_checklocktimeverify) != 0) {", "        script_flags |= SCRIPT_VERIFY_CHECKLOCKTIMEVERIFY;", "    }", "", "    if ((flags & verify_flags_checksequenceverify) != 0) {", "        script_flags |= SCRIPT_VERIFY_CHECKSEQUENCEVERIFY;", "    }", "", "    if ((flags & verify_flags_null_fail) != 0) {", "        script_flags |= SCRIPT_VERIFY_NULLFAIL;", "    }", "", "    // Removed", "    // if ((flags & verify_flags_compressed_pubkeytype) != 0)", "    //     script_flags |= SCRIPT_VERIFY_COMPRESSED_PUBKEYTYPE;", "", "    if ((flags & verify_flags_enable_sighash_forkid) != 0) {", "        script_flags |= SCRIPT_ENABLE_SIGHASH_FORKID;", "    }", "", "    if ((flags & verify_flags_disallow_segwit_recovery) != 0)", "        script_flags |= SCRIPT_DISALLOW_SEGWIT_RECOVERY;", "", "    if ((flags & verify_flags_enable_schnorr_multisig) != 0)", "        script_flags |= SCRIPT_ENABLE_SCHNORR_MULTISIG;", "", "    if ((flags & verify_flags_input_sigchecks) != 0)", "        script_flags |= SCRIPT_VERIFY_INPUT_SIGCHECKS;", "", "    if ((flags & verify_flags_enforce_sigchecks) != 0)", "        script_flags |= SCRIPT_ENFORCE_SIGCHECKS;", "", "    if ((flags & verify_flags_64_bit_integers) != 0)", "        script_flags |= SCRIPT_64_BIT_INTEGERS;", "", "    if ((flags & verify_flags_native_introspection) != 0)", "        script_flags |= SCRIPT_NATIVE_INTROSPECTION;", "", "    if ((flags & verify_flags_enable_p2sh_32) != 0)", "        script_flags |= SCRIPT_ENABLE_P2SH_32;", "", "    if ((flags & verify_flags_enable_tokens) != 0)", "        script_flags |= SCRIPT_ENABLE_TOKENS;", "", "    return script_flags;", "}", "", "", "} // namespace kth::consensus"], "file_path": "src/consensus/conversions.cpp"}
{"Link_to_commit": "https://github.com/henrygraesberg/tdt4100/commit/8c0e8a087a5f21250beeb1a627c75a7a4c259e01", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 36, "n_files_impacted": 5, "longest_chunk": ["  /**", "   * Sorts the list of expenses by time (timestamp).", "   * This uses the natural ordering of Expense objects, which compare by timestamp.", "   *", "   * @param ascending If true, sorts from oldest to newest. If false, sorts from newest to oldest.", "   */", "  public void sortExpensesByTime(boolean ascending) {", "    Collections.sort(expenses);", "", "    if(!ascending) Collections.reverse(expenses);", "  }", "", "  /**", "   * Sorts the list of expenses by the associated person.", "   * Uses ExpenseComparatorPerson to sort expenses by last name then first name of the person.", "   *", "   * @param ascending If true, sorts in ascending alphabetical order. If false, sorts in descending order.", "   */", "  public void sortExpensesByPerson(boolean ascending) {", "    expenses.sort(new ExpenseComparatorPerson());", "", "    if(!ascending) Collections.reverse(expenses);", "  }", "", "  /**", "   * Sorts the list of expenses by status.", "   * Uses ExpenseComparatorStatus to sort expenses in the order: PENDING, REJECTED, PAID.", "   *", "   * @param ascending If true, sorts in the default status order. If false, reverses the order.", "   */", "  public void sortExpensesByStatus(boolean ascending) {", "    expenses.sort(new ExpenseComparatorStatus());", "", "    if(!ascending) Collections.reverse(expenses);", "  }", ""], "file_path": "project/src/main/java/ExpenseForm/ExpenseFormController.java"}
{"Link_to_commit": "https://github.com/microsoft/teams-agent-accelerator-templates/commit/ca8086d66fd0e94242b9e93f37fb376965a76e12", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 429, "n_files_impacted": 45, "longest_chunk": ["\ufeffusing AdaptiveCards;", "using Microsoft.Bot.Schema;", "using Newtonsoft.Json.Linq;", "using DexAgent.GitHubModels;", "", "namespace DexAgent", "{", "    /// <summary>", "    /// Creates the adaptive cards for the GitHub pull requests.", "    /// </summary>", "    public class GitHubCards", "    {", "        /// <summary>", "        /// Creates the adaptive card for the \"ListPRs\" plugin", "        /// </summary>", "        /// <param name=\"title\">The title of the card</param>", "        /// <param name=\"pullRequests\">The list of pull requests</param>", "        /// <param name=\"allLabels\">All the labels for filtering</param>", "        /// <param name=\"allAssignees\">All the assignees for filtering</param>", "        /// <param name=\"allAuthors\">All the authors for filtering</param>", "        /// <returns></returns>", "        public static AdaptiveCard CreateListPRsAdaptiveCard(string title, IList<GitHubPR> pullRequests, HashSet<string> allLabels, HashSet<string> allAssignees, HashSet<string> allAuthors)", "        {", "            var card = new AdaptiveCard(new AdaptiveSchemaVersion(1, 5))", "            {", "                Body = new List<AdaptiveElement>", "                {", "                    new AdaptiveTextBlock", "                    {", "                        Text = $\"\ud83d\udcc4 {title}\",", "                        Weight = AdaptiveTextWeight.Bolder,", "                        Size = AdaptiveTextSize.Large,", "                        Color = AdaptiveTextColor.Accent,", "                    },", "                }", "            };", "", "            var prListContainer = new AdaptiveContainer", "            {", "                Id = \"prContainer\",", "                Items = new List<AdaptiveElement>()", "            };", "", "            if (pullRequests == null || pullRequests.Count == 0)", "            {", "                prListContainer.Items.Add(new AdaptiveTextBlock", "                {", "                    Text = \"No pull requests found \ud83d\udeab\",", "                    Wrap = true,", "                });", "            }", "            else", "            {", "                foreach (var pr in pullRequests)", "                {", "                    var prItemContainer = CreatePRItemContainer(pr);", "                    prListContainer.Items.Add(prItemContainer);", "                }", "            }", "", "            card.Body.Add(prListContainer);", "", "            if (pullRequests!.Count > 0)", "            {", "                var filters = new AdaptiveContainer", "                {", "                    Items = new List<AdaptiveElement>", "                    {", "                        new AdaptiveTextBlock", "                        {", "                            Text = \"\ud83d\udd0d Filters\",", "                            Weight = AdaptiveTextWeight.Bolder", "                        },", "                        new AdaptiveChoiceSetInput", "                        {", "                            Id = \"labelFilter\",", "                            Style = AdaptiveChoiceInputStyle.Compact,", "                            IsMultiSelect = true,", "                            Label = \"Labels\",", "                            Choices = allLabels.Select(label => new AdaptiveChoice", "                            {", "                                Title = label,", "                                Value = label", "                            }).ToList()", "                        },", "                        new AdaptiveChoiceSetInput", "                        {", "                            Id = \"assigneeFilter\",", "                            Style = AdaptiveChoiceInputStyle.Compact,", "                            IsMultiSelect = true,", "                            Label = \"Assignees\",", "                            Choices = allAssignees.Select(assignee => new AdaptiveChoice", "                            {", "                                Title = assignee,", "                                Value = assignee", "                            }).ToList()", "                        },", "                        new AdaptiveChoiceSetInput", "                        {", "                            Id = \"authorFilter\",", "                            Style = AdaptiveChoiceInputStyle.Compact,", "                            IsMultiSelect = true,", "                            Label = \"Authors\",", "                            Choices = allAuthors.Select(author => new AdaptiveChoice", "                            {", "                                Title = author,", "                                Value = author", "                            }).ToList()", "                        },", "                        new AdaptiveActionSet", "                        {", "                            Actions = new List<AdaptiveAction>", "                            {", "                                new AdaptiveSubmitAction", "                                {", "                                    Title = \"Apply Filters\",", "                                    Data = new Dictionary<string, object>", "                                    {", "                                        { \"verb\", \"githubFilters\" },", "                                        { \"pullRequests\", pullRequests }", "                                    }", "                                }", "                            }", "                        }", "                    }", "                };", "                card.Body.Add(filters);", "            }", "            return card;", "        }", "", "        /// <summary>", "        /// Creates the adaptive card for the \"FilterPRs\" plugin", "        /// </summary>", "        /// <param name=\"title\">Title of the card</param>", "        /// <param name=\"pullRequests\">The list of pull requests</param>", "        /// <param name=\"selectedLabels\">The labels used to filter</param>", "        /// <param name=\"selectedAssignees\">The assignees used to filter</param>", "        /// <param name=\"selectedAuthors\">The authors used to filter</param>", "        /// <returns></returns>", "        public static AdaptiveCard CreateFilterPRsAdaptiveCard(string title, IList<GitHubPR> pullRequests, string[] selectedLabels, string[] selectedAssignees, string[] selectedAuthors)", "        {", "            var card = new AdaptiveCard(new AdaptiveSchemaVersion(1, 5))", "            {", "                Body = new List<AdaptiveElement>", "                {", "                    new AdaptiveTextBlock", "                    {", "                        Text = title,", "                        Weight = AdaptiveTextWeight.Bolder,", "                        Size = AdaptiveTextSize.Large", "                    },", "                }", "            };", "", "            var prListContainer = new AdaptiveContainer", "            {", "                Id = \"prContainer\",", "                Items = new List<AdaptiveElement>()", "            };", "", "            if (pullRequests == null || pullRequests.Count == 0)", "            {", "                prListContainer.Items.Add(new AdaptiveTextBlock", "                {", "                    Text = \"No pull requests found\",", "                    Wrap = true", "                });", "            }", "            else", "            {", "                foreach (var pr in pullRequests)", "                {", "                    var prItemContainer = CreatePRItemContainer(pr);", "                    prListContainer.Items.Add(prItemContainer);", "                }", "            }", "", "            card.Body.Add(prListContainer);", "", "            var combinedFilters = selectedLabels.Concat(selectedAssignees).Concat(selectedAuthors).ToArray();", "", "            if (combinedFilters.Length > 0)", "            {", "                var filterContainer = new AdaptiveContainer", "                {", "                    Items = new List<AdaptiveElement>", "                    {", "                        new AdaptiveTextBlock", "                        {", "                            Text = \"Fiters applied:\",", "                            Weight = AdaptiveTextWeight.Bolder,", "                            Size = AdaptiveTextSize.Medium,", "                        }", "                    }", "                };", "", "                foreach (var filter in combinedFilters)", "                {", "                    filterContainer.Items.Add(new AdaptiveTextBlock", "                    {", "                        Text = filter,", "                        Color = AdaptiveTextColor.Accent,", "                        Weight = AdaptiveTextWeight.Bolder,", "                        Size = AdaptiveTextSize.Small,", "                        Wrap = true,", "                        Spacing = AdaptiveSpacing.Small", "                    });", "                }", "", "                card.Body.Add(filterContainer);", "            }", "", "            return card;", "        }", "", "        private static AdaptiveContainer CreatePRItemContainer(GitHubPR pr)", "        {", "            var prItemContainer = new AdaptiveContainer", "            {", "                Spacing = AdaptiveSpacing.Medium,", "                Style = AdaptiveContainerStyle.Accent,", "                Items = new List<AdaptiveElement>", "                {", "                    new AdaptiveColumnSet", "                    {", "                        Columns = new List<AdaptiveColumn>", "                        {", "                            new AdaptiveColumn", "                            {", "                                Width = AdaptiveColumnWidth.Stretch,", "                                Items = new List<AdaptiveElement>", "                                {", "                                    new AdaptiveTextBlock", "                                    {", "                                        Text = $\"#{pr.Number}: {pr.Title}\",", "                                        Weight = AdaptiveTextWeight.Bolder,", "                                        Wrap = true,", "                                        Size = AdaptiveTextSize.Small,", "                                        Color = AdaptiveTextColor.Accent", "                                    }", "                                }", "                            }", "                        }", "                    },", "                    new AdaptiveTextBlock", "                    {", "                        Text = $\"**Author**: {pr.User.Login ?? \"Unknown\"}\",", "                        IsSubtle = true,", "                        Spacing = AdaptiveSpacing.None", "                    },", "                    new AdaptiveTextBlock", "                    {", "                        Text = $\"**Created**: {pr.CreatedAt:MMM dd, yyyy}\",", "                        IsSubtle = true,", "                        Spacing = AdaptiveSpacing.None", "                    },", "                    new AdaptiveTextBlock", "                    {", "                        Text = $\"**Status**: {(pr.State == \"open\" ? \"\ud83d\udfe2 Open\" : \"\ud83d\udd34 Closed\")}\",", "                        IsSubtle = true,", "                        Spacing = AdaptiveSpacing.None", "                    }", "                }", "            };", "", "            if (pr.Labels != null && pr.Labels.Count > 0)", "            {", "                var labelTexts = pr.Labels.Select(l => l.Name).ToList();", "                prItemContainer.Items.Add(new AdaptiveTextBlock", "                {", "                    Text = $\"**Labels**: {string.Join(\", \", labelTexts)}\",", "                    IsSubtle = true,", "                    Spacing = AdaptiveSpacing.None,", "                    Wrap = true,", "                    Italic = true,", "                });", "            }", "", "            prItemContainer.Items.Add(new AdaptiveActionSet", "            {", "                Actions = new List<AdaptiveAction>", "                        {", "                            new AdaptiveToggleVisibilityAction", "                            {", "                                Title = \"Show/Hide Description\",", "                                TargetElements = new List<AdaptiveTargetElement>", "                                {", "                                    new AdaptiveTargetElement", "                                    {", "                                        ElementId = $\"description-{pr.Number}\"", "                                    }", "                                }", "                            }", "                        }", "            });", "", "            prItemContainer.Items.Add(", "                    new AdaptiveTextBlock", "                    {", "                        Id = $\"description-{pr.Number}\",", "                        Text = $\"Description: {pr.Body}\",", "                        Wrap = true,", "                        IsSubtle = true,", "                        Spacing = AdaptiveSpacing.None,", "                        IsVisible = false", "                    });", "", "            if (!string.IsNullOrEmpty(pr.HtmlUrl))", "            {", "                prItemContainer.Items.Add(new AdaptiveActionSet", "                {", "                    Actions = new List<AdaptiveAction>", "                    {", "                        new AdaptiveOpenUrlAction", "                        {", "                            Title = \"View on GitHub\",", "                            Url = new Uri(pr.HtmlUrl)", "                        }", "                    }", "                });", "            }", "", "            return prItemContainer;", "        }", "", "        public static Attachment CreatePullRequestCard(JObject payload)", "        {", "            var pullRequest = payload[\"pull_request\"];", "            string assignee = pullRequest[\"assignee\"] != null ? pullRequest[\"assignee\"][\"login\"].ToString() : \"Unknown User\";", "            string prTitle = pullRequest[\"title\"].ToString();", "            string prUrl = pullRequest[\"html_url\"].ToString();", "            int prNumber = pullRequest[\"number\"].Value<int>();", "", "            var card = new AdaptiveCard(new AdaptiveSchemaVersion(1, 2))", "            {", "                Body = new List<AdaptiveElement>", "                {", "                    new AdaptiveTextBlock", "                    {", "                        Text = $\"\ud83d\udc64 Assignee Request for PR #{prNumber}\",", "                        Weight = AdaptiveTextWeight.Bolder,", "                        Size = AdaptiveTextSize.Large,", "                        Color = AdaptiveTextColor.Accent", "                    },", "                    new AdaptiveTextBlock", "                    {", "                        Text = $\"{prTitle}\",", "                        Wrap = true,", "                        Size = AdaptiveTextSize.Medium,", "                        Weight = AdaptiveTextWeight.Bolder,", "                    },", "                    new AdaptiveTextBlock", "                    {", "                        Text = $\"{assignee} has been assigned this pull request.\",", "                        Wrap = true,", "                        Size = AdaptiveTextSize.Medium,", "                        Spacing = AdaptiveSpacing.Medium", "                    }", "                },", "                Actions = new List<AdaptiveAction>", "                {", "                    new AdaptiveOpenUrlAction", "                    {", "                        Title = \"View on GitHub\",", "                        Url = new Uri(prUrl)", "                    }", "                }", "            };", "", "            return new Attachment", "            {", "                ContentType = AdaptiveCard.ContentType,", "                Content = card", "            };", "        }", "", "        public static Attachment CreatePullRequestStateCard(JObject payload)", "        {", "            var pullRequest = payload[\"pull_request\"];", "            string action = payload[\"action\"].ToString();", "            string prTitle = pullRequest[\"title\"].ToString();", "            string prUrl = pullRequest[\"html_url\"].ToString();", "            int prNumber = pullRequest[\"number\"].Value<int>();", "", "            var card = new AdaptiveCard(new AdaptiveSchemaVersion(1, 2))", "            {", "                Body = new List<AdaptiveElement>", "                {", "                     new AdaptiveTextBlock", "                            {", "                                Text = $\"\ud83d\udd14 Status Update for PR #{prNumber}\",", "                                Weight = AdaptiveTextWeight.Bolder,", "                                Size = AdaptiveTextSize.Large,", "                                Color = AdaptiveTextColor.Accent", "                            },", "                            new AdaptiveTextBlock", "                            {", "                                Text = $\"{prTitle}\",", "                                Wrap = true,", "                                Size = AdaptiveTextSize.Medium,", "                                Weight = AdaptiveTextWeight.Bolder,", "                            },", "                            new AdaptiveTextBlock", "                            {", "                                Text = $\"PR is now {action}\",", "                                Wrap = true,", "                                Size = AdaptiveTextSize.Medium,", "                                Spacing = AdaptiveSpacing.Medium", "                            }", "                },", "                Actions = new List<AdaptiveAction>", "                {", "                    new AdaptiveOpenUrlAction", "                    {", "                        Title = \"View on GitHub\",", "                        Url = new Uri(prUrl)", "                    }", "                }", "            };", "", "            return new Attachment", "            {", "                ContentType = AdaptiveCard.ContentType,", "                Content = card", "            };", "        }", "    }", "}"], "file_path": "dotnet/dex-agent/GitHubModels/GitHubFilterActivity.cs"}
{"Link_to_commit": "https://github.com/HyperloopUPV-H8/ST-LIB/commit/ff13d0671b3c6a0d20cbcba32e48ee695b207641", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 28, "n_files_impacted": 13, "longest_chunk": ["#include \"HALAL/Models/MAC/MAC.hpp\"", "", "#ifdef HAL_ETH_MODULE_ENABLED", "", "MAC::MAC(string address) : string_address(address) {", "    stringstream sstream(address);", "    for (u8_t& byte : this->address) {", "        string temp;", "        getline(sstream, temp, ':');", "        byte = stoi(temp, nullptr, 16);", "    }", "}", "", "MAC::MAC(u8_t addr[6])", "    : address{addr[0], addr[1], addr[2], addr[3], addr[4], addr[5]},", "      string_address([&]() {", "          stringstream sstream;", "          for (int i = 0; i < 6; ++i) {", "              if (i > 0) sstream << \":\";", "              sstream << std::hex << std::setw(2) << std::setfill('0')", "                      << static_cast<int>(addr[i]);", "          }", "          return sstream.str();", "      }()) {}", "", "MAC::MAC() : MAC({0, 0, 0, 0, 0, 0}) {}", "", "#endif"], "file_path": "Src/HALAL/Services/Communication/Ethernet/Ethernet.cpp"}
{"Link_to_commit": "https://github.com/Frodigo/garage/commit/78c3958fd8a93e790eee9bfef3db7bb8c4f30fdf", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 227, "n_files_impacted": 21, "longest_chunk": ["import os\r", "import requests\r", "import json\r", "from .prompt import Prompt\r", "\r", "class BaseSummarizer:\r", "    \"\"\"Base class for all summarizers\"\"\"\r", "    \r", "    def __init__(self):\r", "        self.prompt = Prompt()\r", "    \r", "    def summarize(self, text, metadata=None):\r", "        \"\"\"Summarize the given text\"\"\"\r", "        raise NotImplementedError(\"Subclasses must implement this method\")\r", "\r", "\r", "class ClaudeSummarizer(BaseSummarizer):\r", "    \"\"\"Summarizer that uses Anthropic's Claude API\"\"\"\r", "    \r", "    def __init__(self, api_key=None):\r", "        super().__init__()\r", "        self.api_key = api_key or os.environ.get(\"ANTHROPIC_API_KEY\")\r", "        if not self.api_key:\r", "            raise ValueError(\"Claude API key is required\")\r", "    \r", "    def summarize(self, text, metadata=None):\r", "        \"\"\"Summarize text using Claude API\"\"\"\r", "        if not text:\r", "            return \"\"\r", "        \r", "        try:\r", "            headers = {\r", "                \"x-api-key\": self.api_key,\r", "                \"Content-Type\": \"application/json\",\r", "                \"anthropic-version\": \"2023-06-01\"\r", "            }\r", "            \r", "            # Prepare prompt\r", "            prompt = self.prompt.format(text, metadata)\r", "            \r", "            data = {\r", "                \"model\": \"claude-3-haiku-20240307\",\r", "                \"max_tokens\": 1000,\r", "                \"messages\": [\r", "                    {\"role\": \"user\", \"content\": prompt}\r", "                ]\r", "            }\r", "            \r", "            response = requests.post(\r", "                \"https://api.anthropic.com/v1/messages\",\r", "                headers=headers,\r", "                data=json.dumps(data)\r", "            )\r", "            \r", "            if response.status_code != 200:\r", "                print(f\"Error from Claude API: {response.status_code}\")\r", "                print(response.text)\r", "                return \"\"\r", "            \r", "            response_data = response.json()\r", "            return response_data[\"content\"][0][\"text\"]\r", "            \r", "        except Exception as e:\r", "            print(f\"Error summarizing with Claude: {e}\")\r", "            return \"\"\r", "\r", "\r", "class ChatGPTSummarizer(BaseSummarizer):\r", "    \"\"\"Summarizer that uses OpenAI's ChatGPT API\"\"\"\r", "    \r", "    def __init__(self, api_key=None):\r", "        super().__init__()\r", "        self.api_key = api_key or os.environ.get(\"OPENAI_API_KEY\")\r", "        if not self.api_key:\r", "            raise ValueError(\"OpenAI API key is required\")\r", "    \r", "    def summarize(self, text, metadata=None):\r", "        \"\"\"Summarize text using OpenAI API\"\"\"\r", "        if not text:\r", "            return \"\"\r", "        \r", "        try:\r", "            headers = {\r", "                \"Authorization\": f\"Bearer {self.api_key}\",\r", "                \"Content-Type\": \"application/json\"\r", "            }\r", "            \r", "            # Prepare prompt\r", "            prompt = self.prompt.format(text, metadata)\r", "            \r", "            data = {\r", "                \"model\": \"gpt-3.5-turbo\",\r", "                \"messages\": [\r", "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes newsletter content.\"},\r", "                    {\"role\": \"user\", \"content\": prompt}\r", "                ],\r", "                \"max_tokens\": 1000\r", "            }\r", "            \r", "            response = requests.post(\r", "                \"https://api.openai.com/v1/chat/completions\",\r", "                headers=headers,\r", "                data=json.dumps(data)\r", "            )\r", "            \r", "            if response.status_code != 200:\r", "                print(f\"Error from OpenAI API: {response.status_code}\")\r", "                print(response.text)\r", "                return \"\"\r", "            \r", "            response_data = response.json()\r", "            return response_data[\"choices\"][0][\"message\"][\"content\"]\r", "            \r", "        except Exception as e:\r", "            print(f\"Error summarizing with ChatGPT: {e}\")\r", "            return \"\"\r", "\r", "\r", "class OllamaSummarizer(BaseSummarizer):\r", "    \"\"\"Summarizer that uses a local Ollama instance\"\"\"\r", "    \r", "    def __init__(self, model=\"mistral\", base_url=\"http://localhost:11434\"):\r", "        super().__init__()\r", "        self.model = model\r", "        self.base_url = base_url\r", "    \r", "    def summarize(self, text, metadata=None):\r", "        \"\"\"Summarize text using Ollama API\"\"\"\r", "        if not text:\r", "            return \"\"\r", "        \r", "        try:\r", "            headers = {\"Content-Type\": \"application/json\"}\r", "            \r", "            # Prepare prompt\r", "            prompt = self.prompt.format(text, metadata)\r", "            \r", "            data = {\r", "                \"model\": self.model,\r", "                \"prompt\": prompt,\r", "                \"stream\": False\r", "            }\r", "            \r", "            response = requests.post(\r", "                f\"{self.base_url}/api/generate\",\r", "                headers=headers,\r", "                data=json.dumps(data)\r", "            )\r", "            \r", "            if response.status_code != 200:\r", "                print(f\"Error from Ollama API: {response.status_code}\")\r", "                print(response.text)\r", "                return \"\"\r", "            \r", "            response_data = response.json()\r", "            return response_data[\"response\"]\r", "            \r", "        except Exception as e:\r", "            print(f\"Error summarizing with Ollama: {e}\")\r", "            return \"\"\r", "\r", "\r", "# Example usage\r", "def test_summarizer():\r", "    # Test text\r", "    text = \"\"\"\r", "    Welcome to our weekly tech newsletter!\r", "    \r", "    # Latest Updates\r", "    \r", "    ## AI Advances\r", "    OpenAI has released GPT-5 with significant improvements in reasoning and multimodal capabilities.\r", "    The new model can process images, audio, and text simultaneously with higher accuracy than previous versions.\r", "    \r", "    ## Industry News\r", "    Apple announced their new M3 Pro chips that offer 40% better performance with lower power consumption.\r", "    Google Cloud introduced new serverless database options for enterprise customers.\r", "    \r", "    # Tips & Tutorials\r", "    Learn how to optimize your React applications with our step-by-step guide.\r", "    \r", "    # Upcoming Events\r", "    Join us for the annual Developer Conference on May 15-17 in San Francisco.\r", "    \"\"\"\r", "    \r", "    metadata = {\r", "        'from': 'Tech Weekly <news@techweekly.com>',\r", "        'subject': 'This Week in Tech - Issue #42',\r", "        'date': 'Mon, 1 Apr 2025 09:30:00 -0700'\r", "    }\r", "    \r", "    # Try to use Claude if API key exists\r", "    api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\r", "    if api_key:\r", "        print(\"Testing Claude summarizer...\")\r", "        summarizer = ClaudeSummarizer(api_key)\r", "        summary = summarizer.summarize(text, metadata)\r", "        print(summary)\r", "    else:\r", "        print(\"No Claude API key found, skipping Claude test\")\r", "    \r", "    # Try to use ChatGPT if API key exists\r", "    api_key = os.environ.get(\"OPENAI_API_KEY\")\r", "    if api_key:\r", "        print(\"\\nTesting ChatGPT summarizer...\")\r", "        summarizer = ChatGPTSummarizer(api_key)\r", "        summary = summarizer.summarize(text, metadata)\r", "        print(summary)\r", "    else:\r", "        print(\"No OpenAI API key found, skipping ChatGPT test\")\r", "    \r", "    # Try to use Ollama if it's running locally\r", "    try:\r", "        response = requests.get(\"http://localhost:11434/api/tags\")\r", "        if response.status_code == 200:\r", "            print(\"\\nTesting Ollama summarizer...\")\r", "            summarizer = OllamaSummarizer()\r", "            summary = summarizer.summarize(text, metadata)\r", "            print(summary)\r", "        else:\r", "            print(\"Ollama not running locally, skipping Ollama test\")\r", "    except:\r", "        print(\"Ollama not running locally, skipping Ollama test\")\r", "\r", "\r", "if __name__ == \"__main__\":\r", "    test_summarizer()\r"], "file_path": "tools/NitroDigest/summary_writer.py"}
{"Link_to_commit": "https://github.com/SEEDYK/my-dify/commit/ef188564f30ab4feafe1dda015642cf2af800305", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 233, "n_files_impacted": 2, "longest_chunk": ["", "/**", " * Preprocesses mermaid code to fix common syntax issues", " */", "export function preprocessMermaidCode(code: string): string {", "  if (!code || typeof code !== 'string')", "    return ''", "", "  // First check if this is a gantt chart", "  if (code.trim().startsWith('gantt')) {", "    // For gantt charts, we need to ensure each task is on its own line", "    // Split the code into lines and process each line separately", "    const lines = code.split('\\n').map(line => line.trim())", "    return lines.join('\\n')", "  }", "", "  return code", "    // Replace English colons with Chinese colons in section nodes to avoid parsing issues", "    .replace(/section\\s+([^:]+):/g, (match, sectionName) => `section ${sectionName}\uff1a`)", "    // Fix common syntax issues", "    .replace(/fifopacket/g, 'rect')", "    // Ensure graph has direction", "    .replace(/^graph\\s+((?:TB|BT|RL|LR)*)/, (match, direction) => {", "      return direction ? match : 'graph TD'", "    })", "    // Clean up empty lines and extra spaces", "    .trim()", "}", "", "/**", " * Prepares mermaid code based on selected style", " */", "export function prepareMermaidCode(code: string, style: 'classic' | 'handDrawn'): string {", "  let finalCode = preprocessMermaidCode(code)", "", "  // Special handling for gantt charts", "  if (finalCode.trim().startsWith('gantt')) {", "    // For gantt charts, preserve the structure exactly as is", "    return finalCode", "  }", "", "  if (style === 'handDrawn') {", "    finalCode = finalCode", "      // Remove style definitions that interfere with hand-drawn style", "      .replace(/style\\s+[^\\n]+/g, '')", "      .replace(/linkStyle\\s+[^\\n]+/g, '')", "      .replace(/^flowchart/, 'graph')", "      // Remove any styles that might interfere with hand-drawn style", "      .replace(/class=\"[^\"]*\"/g, '')", "      .replace(/fill=\"[^\"]*\"/g, '')", "      .replace(/stroke=\"[^\"]*\"/g, '')", "", "    // Ensure hand-drawn style charts always start with graph", "    if (!finalCode.startsWith('graph') && !finalCode.startsWith('flowchart'))", "      finalCode = `graph TD\\n${finalCode}`", "  }", "", "  return finalCode", "}", "", "/**", " * Converts SVG to base64 string for image rendering", " */", "export function svgToBase64(svgGraph: string): Promise<string> {", "  if (!svgGraph)", "    return Promise.resolve('')", "", "  try {", "    // Ensure SVG has correct XML declaration", "    if (!svgGraph.includes('<?xml'))", "      svgGraph = `<?xml version=\"1.0\" encoding=\"UTF-8\"?>${svgGraph}`", "", "    const blob = new Blob([new TextEncoder().encode(svgGraph)], { type: 'image/svg+xml;charset=utf-8' })", "    return new Promise((resolve, reject) => {", "      const reader = new FileReader()", "      reader.onloadend = () => resolve(reader.result as string)", "      reader.onerror = reject", "      reader.readAsDataURL(blob)", "    })", "  }", "  catch (error) {", "    console.error('Error converting SVG to base64:', error)", "    return Promise.resolve('')", "  }", "}", "", "/**", " * Processes SVG for theme styling", " */", "export function processSvgForTheme(", "  svg: string,", "  isDark: boolean,", "  isHandDrawn: boolean,", "  themes: {", "    light: any", "    dark: any", "  },", "): string {", "  let processedSvg = svg", "", "  if (isDark) {", "    processedSvg = processedSvg", "      .replace(/style=\"fill: ?#000000\"/g, 'style=\"fill: #e2e8f0\"')", "      .replace(/style=\"stroke: ?#000000\"/g, 'style=\"stroke: #94a3b8\"')", "      .replace(/<rect [^>]*fill=\"#ffffff\"/g, '<rect $& fill=\"#1e293b\"')", "", "    if (isHandDrawn) {", "      processedSvg = processedSvg", "        .replace(/fill=\"#[a-fA-F0-9]{6}\"/g, `fill=\"${themes.dark.nodeColors[0].bg}\"`)", "        .replace(/stroke=\"#[a-fA-F0-9]{6}\"/g, `stroke=\"${themes.dark.connectionColor}\"`)", "        .replace(/stroke-width=\"1\"/g, 'stroke-width=\"1.5\"')", "    }", "    else {", "      let i = 0", "      themes.dark.nodeColors.forEach(() => {", "        const regex = /fill=\"#[a-fA-F0-9]{6}\"[^>]*class=\"node-[^\"]*\"/g", "        processedSvg = processedSvg.replace(regex, (match: string) => {", "          const colorIndex = i % themes.dark.nodeColors.length", "          i++", "          return match.replace(/fill=\"#[a-fA-F0-9]{6}\"/, `fill=\"${themes.dark.nodeColors[colorIndex].bg}\"`)", "        })", "      })", "", "      processedSvg = processedSvg", "        .replace(/<path [^>]*stroke=\"#[a-fA-F0-9]{6}\"/g,", "          `<path stroke=\"${themes.dark.connectionColor}\" stroke-width=\"1.5\"`)", "        .replace(/<(line|polyline) [^>]*stroke=\"#[a-fA-F0-9]{6}\"/g,", "          `<$1 stroke=\"${themes.dark.connectionColor}\" stroke-width=\"1.5\"`)", "    }", "  }", "  else {", "    if (isHandDrawn) {", "      processedSvg = processedSvg", "        .replace(/fill=\"#[a-fA-F0-9]{6}\"/g, `fill=\"${themes.light.nodeColors[0].bg}\"`)", "        .replace(/stroke=\"#[a-fA-F0-9]{6}\"/g, `stroke=\"${themes.light.connectionColor}\"`)", "        .replace(/stroke-width=\"1\"/g, 'stroke-width=\"1.5\"')", "    }", "    else {", "      themes.light.nodeColors.forEach(() => {", "        const regex = /fill=\"#[a-fA-F0-9]{6}\"[^>]*class=\"node-[^\"]*\"/g", "        let i = 0", "        processedSvg = processedSvg.replace(regex, (match: string) => {", "          const colorIndex = i % themes.light.nodeColors.length", "          i++", "          return match.replace(/fill=\"#[a-fA-F0-9]{6}\"/, `fill=\"${themes.light.nodeColors[colorIndex].bg}\"`)", "        })", "      })", "", "      processedSvg = processedSvg", "        .replace(/<path [^>]*stroke=\"#[a-fA-F0-9]{6}\"/g,", "          `<path stroke=\"${themes.light.connectionColor}\"`)", "        .replace(/<(line|polyline) [^>]*stroke=\"#[a-fA-F0-9]{6}\"/g,", "          `<$1 stroke=\"${themes.light.connectionColor}\"`)", "    }", "  }", "", "  return processedSvg", "}", "", "/**", " * Checks if mermaid code is complete and valid", " */", "export function isMermaidCodeComplete(code: string): boolean {", "  if (!code || code.trim().length === 0)", "    return false", "", "  try {", "    const trimmedCode = code.trim()", "", "    // Special handling for gantt charts", "    if (trimmedCode.startsWith('gantt')) {", "      // For gantt charts, check if it has at least a title and one task", "      const lines = trimmedCode.split('\\n').filter(line => line.trim().length > 0)", "      return lines.length >= 3", "    }", "", "    // Check for basic syntax structure", "    const hasValidStart = /^(graph|flowchart|sequenceDiagram|classDiagram|classDef|class|stateDiagram|gantt|pie|er|journey|requirementDiagram)/.test(trimmedCode)", "", "    // Check for balanced brackets and parentheses", "    const isBalanced = (() => {", "      const stack = []", "      const pairs = { '{': '}', '[': ']', '(': ')' }", "", "      for (const char of trimmedCode) {", "        if (char in pairs) {", "          stack.push(char)", "        }", "        else if (Object.values(pairs).includes(char)) {", "          const last = stack.pop()", "          if (pairs[last as keyof typeof pairs] !== char)", "            return false", "        }", "      }", "", "      return stack.length === 0", "    })()", "", "    // Check for common syntax errors", "    const hasNoSyntaxErrors = !trimmedCode.includes('undefined')", "                           && !trimmedCode.includes('[object Object]')", "                           && trimmedCode.split('\\n').every(line =>", "                             !(line.includes('-->') && !line.match(/\\S+\\s*-->\\s*\\S+/)))", "", "    return hasValidStart && isBalanced && hasNoSyntaxErrors", "  }", "  catch (error) {", "    console.debug('Mermaid code validation error:', error)", "    return false", "  }", "}", "", "/**", " * Helper to wait for DOM element with retry mechanism", " */", "export function waitForDOMElement(callback: () => Promise<any>, maxAttempts = 3, delay = 100): Promise<any> {", "  return new Promise((resolve, reject) => {", "    let attempts = 0", "    const tryRender = async () => {", "      try {", "        resolve(await callback())", "      }", "      catch (error) {", "        attempts++", "        if (attempts < maxAttempts)", "          setTimeout(tryRender, delay)", "        else", "          reject(error)", "      }", "    }", "    tryRender()", "  })", "}"], "file_path": "web/app/components/base/mermaid/utils.ts"}
{"Link_to_commit": "https://github.com/coopsoc/website/commit/6663937801111789dd82d6012b5cd96bca2ae3dc", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 71, "n_files_impacted": 1, "longest_chunk": ["    subcoms: [", "      {", "        icons: [faDove],", "        name: \"Charity\",", "        description: `The Charity portfolio seeks to create awareness, raise funds and drive social change for charities. The committee also compliments the Social portfolio in hosting fun events for a great cause.`,", "        members: [", "          \"Bhavesh Nain\",", "          \"Catherine Lee\",", "          \"Elisha Petelo\",", "          \"Jonathon Adji\",", "          \"Justin Nguyen\",", "          \"Trent Song\",", "        ],", "      },", "      {", "        icons: [faClipboard],", "        name: \"HR\",", "        description: `The HR portfolio is committed to strengthening teamwork and cross-portfolio communication within the society. They are responsible for organizing internal events, to help shape our culture at Co-op Soc.`,", "        members: [", "          \"Bea Pelayo\",", "          \"Erika Mori\",", "          \"Jessica Su\",", "          \"Nathan Ha\",", "          \"Nathan Yang\",", "          \"Piraveen Sivachchandran\",", "        ],", "      },", "      {", "        icons: [faDesktop],", "        name: \"IT\",", "        description: `The IT portfolio forms the technical backbone for the society. It oversees the development of internal tools and manages the Co-op Soc website; a hub for existing and prospective scholars, hosting blog posts and event information, as well as a merchandise store and an executive nomination/voting system.`,", "        members: [", "          \"Angie Counsell\",", "          \"Corinne Zhou\",", "          \"Matej Groombridge\",", "          \"Max Burykin\",", "          \"Tate Mcallum\",", "        ],", "      },", "      {", "        icons: [faBullhorn],", "        name: \"Marketing\",", "        description: `The Marketing portfolio focuses on creating continued engagement with our Co-op Society Facebook platform. Via the creation of digital marketing content, all students are encouraged to participate in fulfilling social events.`,", "        members: [", "          \"Angelo Varghese Paul\",", "          \"Ben Chau\",", "          \"David Lin\",", "          \"Fynn Hopkins\",", "          \"Tate Dee\",", "        ],", "      },", "      {", "        icons: [faEdit],", "        name: \"Publications\",", "        description: `The Publications portfolio is responsible for continuing to grow the society's online presence and keeping scholars up to date, focusing on the blog posts for the Co-op Soc website and the Chicken Coop podcast.`,", "        members: [\"Niamh G\", \"Oliver N\", \"Oscar W\", \"Rashid Abuzarov\"],", "      },", "      {", "        icons: [faUserFriends],", "        name: \"Social\",", "        description: `The Social portfolio is responsible for organising our social calendar, including planning, developing and executing a core suite of events. They ensure all members are given the opportunity to be a part of this vibrant community, through the creation and maintenance of social groups.`,", "        members: [", "          \"Amudha Bharathi\",", "          \"Ariana Chan\",", "          \"Emma Xiang\",", "          \"Judy Huang\",", "          \"Kai Hampson\",", "          \"Tom Pike\",", "        ],", "      },", "    ],"], "file_path": "data/TeamData.ts"}
{"Link_to_commit": "https://github.com/LorenFrankLab/spyglass/commit/39f390c65b484639250f0ca844d957340f990824", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 5, "n_files_impacted": 6, "longest_chunk": ["    expected_insertions = 4", "    assert len(after) - len(before) == expected_insertions, (", "        f\"PositionIntervalMap failed to insert the expected number of entries. \"", "        f\"Expected {expected_insertions}, but got {len(after) - len(before)}.\"", "    )"], "file_path": "tests/common/test_behav.py"}
{"Link_to_commit": "https://github.com/human-pangenomics/hprc-data-explorer/commit/51efc6e7196b4f57a0753e9e284ba9b7666458dd", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 22, "n_files_impacted": 4, "longest_chunk": ["import { OpenInNewIcon } from \"@databiosphere/findable-ui/lib/components/common/CustomIcon/components/OpenInNewIcon/openInNewIcon\";", "import { ElementType } from \"react\";", "import { Label, Text } from \"./labelIconMenuItem.styles\";", "", "export interface LabelIconMenuItemProps {", "  Icon?: ElementType;", "  iconFontSize?: string;", "  label: string;", "}", "", "export const LabelIconMenuItem = ({", "  Icon = OpenInNewIcon,", "  iconFontSize = \"xsmall\",", "  label,", "}: LabelIconMenuItemProps): JSX.Element => {", "  return (", "    <Label>", "      <Text>{label}</Text>", "      <Icon color=\"inkLight\" fontSize={iconFontSize} />", "    </Label>", "  );", "};"], "file_path": "app/components/index.ts"}
{"Link_to_commit": "https://github.com/a-klos/langchain-ocr/commit/ba3887ba85553d421288a92ae4070e2339036e9b", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 8, "n_files_impacted": 4, "longest_chunk": ["", "---", "", "## Integration: `langchain_ocr`", "", "This FastAPI layer is a thin wrapper around the core [`langchain_ocr_lib`](https://github.com/a-klos/langchain-ocr/tree/28205adddc252a29901a98079c3703d27ea80a46/langchain_ocr_lib) Python package.", "", "Any updates to OCR logic, file handling, or model configuration happen in the library, not here."], "file_path": "langchain_ocr_lib/src/langchain_ocr_lib/impl/settings/ollama_chat_settings.py"}
{"Link_to_commit": "https://github.com/caprolactam/party-stats/commit/edb6fc8fae03d8e95e339e11fe4b7ccd269b6bbb", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 16, "n_files_impacted": 14, "longest_chunk": ["{", "  \"extends\": \"./tsconfig.base.json\",", "  \"compilerOptions\": {", "    \"tsBuildInfoFile\": \"./node_modules/.tmp/tsconfig.worker.tsbuildinfo\",", "    \"types\": [", "      \"vite/client\",", "      \"@cloudflare/workers-types\",", "      \"./worker-configuration.d.ts\",", "      \"@cloudflare/vitest-pool-workers\"", "    ],", "    \"paths\": {", "      \"#api/*\": [\"./api/*\"]", "    }", "  },", "  \"include\": [\"scripts\", \"worker/**/*\", \"api\", \"tests/**/*\", \"types\"]", "}"], "file_path": "vite.config.ts"}
{"Link_to_commit": "https://github.com/jeremylong/open-vulnerability-cli/commit/b0d6fb406b5bfe07df44e91c0e5ab0a73e1c90bb", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 7, "n_files_impacted": 22, "longest_chunk": ["    /**", "     * The cache property indicating the cache directory.", "     */", "    private Properties properties = new Properties();", "    /**", "     * The cache directory.", "     */"], "file_path": "vulnz/src/main/java/io/github/jeremylong/vulnz/cli/cache/CacheProperties.java"}
{"Link_to_commit": "https://github.com/samchon/typia/commit/5636809ead8b85fd501a376ca538a7a0bd494d99", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 122, "n_files_impacted": 68, "longest_chunk": ["import { ILlmApplication, ILlmFunction } from \"@samchon/openapi\";", "import typia, { tags } from \"typia\";", "", "const app: ILlmApplication<\"chatgpt\"> = typia.llm.application<", "  BbsArticleController,", "  \"chatgpt\"", ">();", "const func: ILlmFunction<\"chatgpt\"> | undefined = app.functions.find(", "  (func) => func.name === \"create\",", ");", "console.log(func?.description);", "", "interface BbsArticleController {", "  /**", "   * Create a new article.", "   *", "   * Writes a new article and archives it into the DB.", "   *", "   * @param props Properties of create function", "   * @returns Newly created article", "   */", "  create(props: {", "    /**", "     * Information of the article to create", "     */", "    input: IBbsArticle.ICreate;", "  }): Promise<IBbsArticle>;", "", "  /**", "   * Update an article.", "   *", "   * Updates an article with new content.", "   *", "   * @param props Properties of update function", "   * @param input New content to update", "   */", "  update(props: {", "    /**", "     * Target article's {@link IBbsArticle.id}.", "     */", "    id: string & tags.Format<\"uuid\">;", "", "    /**", "     * New content to update.", "     */", "    input: IBbsArticle.IUpdate;", "  }): Promise<void>;", "", "  /**", "   * Erase an article.", "   *", "   * Erases an article from the DB.", "   *", "   * @param props Properties of erase function", "   */", "  erase(props: {", "    /**", "     * Target article's {@link IBbsArticle.id}.", "     */", "    id: string & tags.Format<\"uuid\">;", "  }): Promise<void>;", "}", "", "/**", " * Article entity.", " *", " * `IBbsArticle` is an entity representing an article in the BBS (Bulletin Board System).", " */", "interface IBbsArticle extends IBbsArticle.ICreate {", "  /**", "   * Primary Key.", "   */", "  id: string & tags.Format<\"uuid\">;", "", "  /**", "   * Creation time of the article.", "   */", "  created_at: string & tags.Format<\"date-time\">;", "", "  /**", "   * Last updated time of the article.", "   */", "  updated_at: string & tags.Format<\"date-time\">;", "}", "namespace IBbsArticle {", "  /**", "   * Information of the article to create.", "   */", "  export interface ICreate {", "    /**", "     * Title of the article.", "     *", "     * Representative title of the article.", "     */", "    title: string;", "", "    /**", "     * Content body.", "     *", "     * Content body of the article written in the markdown format.", "     */", "    body: string;", "", "    /**", "     * Thumbnail image URI.", "     *", "     * Thumbnail image URI which can represent the article.", "     *", "     * If configured as `null`, it means that no thumbnail image in the article.", "     */", "    thumbnail:", "      | null", "      | (string & tags.Format<\"uri\"> & tags.ContentMediaType<\"image/*\">);", "  }", "", "  /**", "   * Information of the article to update.", "   *", "   * Only the filled properties will be updated.", "   */", "  export type IUpdate = Partial<ICreate>;", "}"], "file_path": "examples/src/llm/parameters-structured-output.ts"}
{"Link_to_commit": "https://github.com/solana-foundation/solana-attestation-site/commit/afb8cbfa5e0f6a0ff05e1f87577293bc0cfae39c", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 18, "n_files_impacted": 6, "longest_chunk": ["import { Container } from '@/shared/ui/container'", "import { Link } from '@/shared/ui/link'", "import { Section } from '@/shared/ui/section'", "import { NextPage } from 'next'", "", "export const NotFound: NextPage = () => (", "    <Container layout=\"wide\" className=\"pt-17 sm:pt-47 pb-14 sm:pb-24\">", "        <Section", "            title=\"404 \u2014 page fell off-chain\"", "            subTitle={", "                <>", "                    Our over\u2011caffeinated intern tried to stake this URL and, well\u2026 it got slashed. Hop back to the <Link href=\"/\">homepage</Link> to learn about", "                    Solana Attestation Service, or wander footer links.", "                </>", "            }", "        />", "    </Container>", ")"], "file_path": "src/shared/ui/link/index.ts"}
{"Link_to_commit": "https://github.com/Jacobbrewer1/web/commit/3e96dd50ca0d5ce74aba39aa7f02601ef4abb85f", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 5, "n_files_impacted": 2, "longest_chunk": ["\t// metricsPort is the port for the metrics server.", "\tmetricsPort = 9090", "", "\t// healthPort is the port for the health server.", "\thealthPort = 9091"], "file_path": "app.go"}
{"Link_to_commit": "https://github.com/Jacobbrewer1/golf-data/commit/628ba1c5aebd3ba4dc5236da91ee0e1b6c603eba", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 16, "n_files_impacted": 8, "longest_chunk": ["\t\tfmt.Printf(`[INFO] API spec linting results:", "Usability: %d", "Security: %d", "Robustness: %d", "Evolution: %d", "Overall: %d", "`+\"\\n\",", "\t\t\tresp.ImpactScore.CategorizedSummary.Usability,", "\t\t\tresp.ImpactScore.CategorizedSummary.Security,", "\t\t\tresp.ImpactScore.CategorizedSummary.Robustness,", "\t\t\tresp.ImpactScore.CategorizedSummary.Evolution,", "\t\t\tresp.ImpactScore.CategorizedSummary.Overall,", "\t\t)", "", "\t\tfmt.Println(\"[INFO] API spec linting passed for\", spec)", "\t\tfmt.Println(\"[DEBUG] Removing report file...\")"], "file_path": "magefiles/linting.go"}
{"Link_to_commit": "https://github.com/TylerOsborn/lunch-order/commit/07bfe7739c91c3dc64f1e3dcc40ec744e964d953", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 184, "n_files_impacted": 9, "longest_chunk": ["package repository", "", "import (", "\t\"gorm.io/gorm\"", ")", "", "type DonationRequestRepository struct {", "\tdb                 *gorm.DB", "\tuserRepository     *UserRepository", "\tdonationRepository *DonationRepository", "}", "", "var donationRequestRepository *DonationRequestRepository", "", "func NewDonationRequestRepository(db *gorm.DB, userRepository *UserRepository, donationRepository *DonationRepository) *DonationRequestRepository {", "\tif donationRequestRepository == nil {", "\t\tdonationRequestRepository = &DonationRequestRepository{", "\t\t\tdb:                 db,", "\t\t\tuserRepository:     userRepository,", "\t\t\tdonationRepository: donationRepository,", "\t\t}", "\t}", "", "\treturn donationRequestRepository", "}", "", "func (r *DonationRequestRepository) CreateDonationRequest(requesterID uint, mealIDs []uint) error {", "\treturn r.db.Transaction(func(tx *gorm.DB) error {", "\t\trequest := DonationRequest{", "\t\t\tRequesterID: requesterID,", "\t\t\tStatus:      \"pending\",", "\t\t}", "", "\t\tif err := tx.Create(&request).Error; err != nil {", "\t\t\treturn err", "\t\t}", "", "\t\tfor _, mealID := range mealIDs {", "\t\t\tpreference := DonationRequestMeal{", "\t\t\t\tDonationRequestID: request.ID,", "\t\t\t\tMealID:            mealID,", "\t\t\t}", "", "\t\t\tif err := tx.Create(&preference).Error; err != nil {", "\t\t\t\treturn err", "\t\t\t}", "\t\t}", "", "\t\treturn r.CheckAndFulfillDonationRequests()", "\t})", "}", "", "func (r *DonationRequestRepository) GetDonationRequestsByStatus(status string) ([]DonationRequest, error) {", "\tvar requests []DonationRequest", "", "\tresult := r.db.Preload(\"Requester\").", "\t\tWhere(\"status = ?\", status).", "\t\tOrder(\"created_at ASC\").", "\t\tFind(&requests)", "", "\treturn requests, result.Error", "}", "", "func (r *DonationRequestRepository) UpdateDonationRequestStatus(requestID uint, status string, donationID *uint) error {", "\tupdates := map[string]interface{}{", "\t\t\"status\": status,", "\t}", "", "\tif donationID != nil {", "\t\tupdates[\"donation_id\"] = donationID", "\t}", "", "\tresult := r.db.Model(&DonationRequest{}).", "\t\tWhere(\"id = ?\", requestID).", "\t\tUpdates(updates)", "", "\treturn result.Error", "}", "", "func (r *DonationRequestRepository) GetDonationRequestsByRequesterName(requesterName string, date string) ([]DonationRequest, error) {", "\tvar user User", "\tif err := r.db.Where(\"name = ?\", requesterName).First(&user).Error; err != nil {", "\t\treturn nil, err", "\t}", "", "\tvar requests []DonationRequest", "\tresult := r.db.Joins(\"Requester\").Joins(\"Donation\").Joins(\"Donation.Donor\").Joins(\"Donation.Meal\").", "\t\tWhere(\"requester_id = ? AND status = 'pending' AND DATE(donation_requests.created_at) = DATE(?) \", user.ID, date).", "\t\tOrder(\"created_at DESC\").", "\t\tFind(&requests)", "", "\treturn requests, result.Error", "}", "", "func (r *DonationRequestRepository) GetDonationRequestById(id uint) (DonationRequest, error) {", "\tvar request DonationRequest", "\tresult := r.db.Preload(\"Requester\").Preload(\"Donation\").Preload(\"Donation.Donor\").Preload(\"Donation.Meal\").", "\t\tFirst(&request, id)", "", "\treturn request, result.Error", "}", "", "func (r *DonationRequestRepository) GetDonationRequestMealPreferences(requestID uint) ([]Meal, error) {", "\tvar meals []Meal", "", "\tresult := r.db.Table(\"meals\").", "\t\tJoins(\"JOIN donation_request_meals ON meals.id = donation_request_meals.meal_id\").", "\t\tWhere(\"donation_request_meals.donation_request_id = ?\", requestID).", "\t\tFind(&meals)", "", "\treturn meals, result.Error", "}", "", "func (r *DonationRequestRepository) CheckAndFulfillDonationRequests() error {", "", "\tpendingRequests, err := r.GetDonationRequestsByStatus(\"pending\")", "\tif err != nil {", "\t\treturn err", "\t}", "", "\tfor _, request := range pendingRequests {", "\t\tpreferredMeals, err := r.GetDonationRequestMealPreferences(request.ID)", "\t\tif err != nil || len(preferredMeals) == 0 {", "\t\t\tcontinue", "\t\t}", "", "\t\tvar preferredMealIDs []uint", "\t\tfor _, meal := range preferredMeals {", "\t\t\tpreferredMealIDs = append(preferredMealIDs, meal.ID)", "\t\t}", "", "\t\tvar mealDate string", "\t\tif err := r.db.Model(&Meal{}).Where(\"id = ?\", preferredMealIDs[0]).Select(\"date\").Scan(&mealDate).Error; err != nil {", "\t\t\tcontinue", "\t\t}", "", "\t\tunclaimedDonations, err := r.donationRepository.GetUnclaimedDonationsByDate(mealDate)", "\t\tif err != nil || len(unclaimedDonations) == 0 {", "\t\t\tcontinue", "\t\t}", "", "\t\tvar matchingDonation *Donation", "\t\tfor _, donation := range unclaimedDonations {", "\t\t\tfor _, preferredMealID := range preferredMealIDs {", "\t\t\t\tif donation.MealID == preferredMealID {", "\t\t\t\t\tmatchingDonation = &donation", "\t\t\t\t\tbreak", "\t\t\t\t}", "\t\t\t}", "\t\t\tif matchingDonation != nil {", "\t\t\t\tbreak", "\t\t\t}", "\t\t}", "", "\t\tif matchingDonation == nil {", "\t\t\tcontinue", "\t\t}", "", "\t\terr = r.db.Transaction(func(tx *gorm.DB) error {", "\t\t\tif err := tx.Model(&Donation{}).", "\t\t\t\tWhere(\"id = ?\", matchingDonation.ID).", "\t\t\t\tUpdate(\"recipient_id\", request.RequesterID).Error; err != nil {", "\t\t\t\treturn err", "\t\t\t}", "", "\t\t\tif err := tx.Model(&DonationRequest{}).", "\t\t\t\tWhere(\"id = ?\", request.ID).", "\t\t\t\tUpdates(map[string]interface{}{", "\t\t\t\t\t\"status\":      \"fulfilled\",", "\t\t\t\t\t\"donation_id\": matchingDonation.ID,", "\t\t\t\t}).Error; err != nil {", "\t\t\t\treturn err", "\t\t\t}", "", "\t\t\treturn nil", "\t\t})", "", "\t\tif err != nil {", "\t\t\tcontinue", "\t\t}", "\t}", "", "\treturn nil", "}"], "file_path": "repository/models.go"}
{"Link_to_commit": "https://github.com/duongthuy125/nhalinhdd/commit/ca8086d66fd0e94242b9e93f37fb376965a76e12", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 429, "n_files_impacted": 45, "longest_chunk": ["\ufeffusing AdaptiveCards;", "using Microsoft.Bot.Schema;", "using Newtonsoft.Json.Linq;", "using DexAgent.GitHubModels;", "", "namespace DexAgent", "{", "    /// <summary>", "    /// Creates the adaptive cards for the GitHub pull requests.", "    /// </summary>", "    public class GitHubCards", "    {", "        /// <summary>", "        /// Creates the adaptive card for the \"ListPRs\" plugin", "        /// </summary>", "        /// <param name=\"title\">The title of the card</param>", "        /// <param name=\"pullRequests\">The list of pull requests</param>", "        /// <param name=\"allLabels\">All the labels for filtering</param>", "        /// <param name=\"allAssignees\">All the assignees for filtering</param>", "        /// <param name=\"allAuthors\">All the authors for filtering</param>", "        /// <returns></returns>", "        public static AdaptiveCard CreateListPRsAdaptiveCard(string title, IList<GitHubPR> pullRequests, HashSet<string> allLabels, HashSet<string> allAssignees, HashSet<string> allAuthors)", "        {", "            var card = new AdaptiveCard(new AdaptiveSchemaVersion(1, 5))", "            {", "                Body = new List<AdaptiveElement>", "                {", "                    new AdaptiveTextBlock", "                    {", "                        Text = $\"\ud83d\udcc4 {title}\",", "                        Weight = AdaptiveTextWeight.Bolder,", "                        Size = AdaptiveTextSize.Large,", "                        Color = AdaptiveTextColor.Accent,", "                    },", "                }", "            };", "", "            var prListContainer = new AdaptiveContainer", "            {", "                Id = \"prContainer\",", "                Items = new List<AdaptiveElement>()", "            };", "", "            if (pullRequests == null || pullRequests.Count == 0)", "            {", "                prListContainer.Items.Add(new AdaptiveTextBlock", "                {", "                    Text = \"No pull requests found \ud83d\udeab\",", "                    Wrap = true,", "                });", "            }", "            else", "            {", "                foreach (var pr in pullRequests)", "                {", "                    var prItemContainer = CreatePRItemContainer(pr);", "                    prListContainer.Items.Add(prItemContainer);", "                }", "            }", "", "            card.Body.Add(prListContainer);", "", "            if (pullRequests!.Count > 0)", "            {", "                var filters = new AdaptiveContainer", "                {", "                    Items = new List<AdaptiveElement>", "                    {", "                        new AdaptiveTextBlock", "                        {", "                            Text = \"\ud83d\udd0d Filters\",", "                            Weight = AdaptiveTextWeight.Bolder", "                        },", "                        new AdaptiveChoiceSetInput", "                        {", "                            Id = \"labelFilter\",", "                            Style = AdaptiveChoiceInputStyle.Compact,", "                            IsMultiSelect = true,", "                            Label = \"Labels\",", "                            Choices = allLabels.Select(label => new AdaptiveChoice", "                            {", "                                Title = label,", "                                Value = label", "                            }).ToList()", "                        },", "                        new AdaptiveChoiceSetInput", "                        {", "                            Id = \"assigneeFilter\",", "                            Style = AdaptiveChoiceInputStyle.Compact,", "                            IsMultiSelect = true,", "                            Label = \"Assignees\",", "                            Choices = allAssignees.Select(assignee => new AdaptiveChoice", "                            {", "                                Title = assignee,", "                                Value = assignee", "                            }).ToList()", "                        },", "                        new AdaptiveChoiceSetInput", "                        {", "                            Id = \"authorFilter\",", "                            Style = AdaptiveChoiceInputStyle.Compact,", "                            IsMultiSelect = true,", "                            Label = \"Authors\",", "                            Choices = allAuthors.Select(author => new AdaptiveChoice", "                            {", "                                Title = author,", "                                Value = author", "                            }).ToList()", "                        },", "                        new AdaptiveActionSet", "                        {", "                            Actions = new List<AdaptiveAction>", "                            {", "                                new AdaptiveSubmitAction", "                                {", "                                    Title = \"Apply Filters\",", "                                    Data = new Dictionary<string, object>", "                                    {", "                                        { \"verb\", \"githubFilters\" },", "                                        { \"pullRequests\", pullRequests }", "                                    }", "                                }", "                            }", "                        }", "                    }", "                };", "                card.Body.Add(filters);", "            }", "            return card;", "        }", "", "        /// <summary>", "        /// Creates the adaptive card for the \"FilterPRs\" plugin", "        /// </summary>", "        /// <param name=\"title\">Title of the card</param>", "        /// <param name=\"pullRequests\">The list of pull requests</param>", "        /// <param name=\"selectedLabels\">The labels used to filter</param>", "        /// <param name=\"selectedAssignees\">The assignees used to filter</param>", "        /// <param name=\"selectedAuthors\">The authors used to filter</param>", "        /// <returns></returns>", "        public static AdaptiveCard CreateFilterPRsAdaptiveCard(string title, IList<GitHubPR> pullRequests, string[] selectedLabels, string[] selectedAssignees, string[] selectedAuthors)", "        {", "            var card = new AdaptiveCard(new AdaptiveSchemaVersion(1, 5))", "            {", "                Body = new List<AdaptiveElement>", "                {", "                    new AdaptiveTextBlock", "                    {", "                        Text = title,", "                        Weight = AdaptiveTextWeight.Bolder,", "                        Size = AdaptiveTextSize.Large", "                    },", "                }", "            };", "", "            var prListContainer = new AdaptiveContainer", "            {", "                Id = \"prContainer\",", "                Items = new List<AdaptiveElement>()", "            };", "", "            if (pullRequests == null || pullRequests.Count == 0)", "            {", "                prListContainer.Items.Add(new AdaptiveTextBlock", "                {", "                    Text = \"No pull requests found\",", "                    Wrap = true", "                });", "            }", "            else", "            {", "                foreach (var pr in pullRequests)", "                {", "                    var prItemContainer = CreatePRItemContainer(pr);", "                    prListContainer.Items.Add(prItemContainer);", "                }", "            }", "", "            card.Body.Add(prListContainer);", "", "            var combinedFilters = selectedLabels.Concat(selectedAssignees).Concat(selectedAuthors).ToArray();", "", "            if (combinedFilters.Length > 0)", "            {", "                var filterContainer = new AdaptiveContainer", "                {", "                    Items = new List<AdaptiveElement>", "                    {", "                        new AdaptiveTextBlock", "                        {", "                            Text = \"Fiters applied:\",", "                            Weight = AdaptiveTextWeight.Bolder,", "                            Size = AdaptiveTextSize.Medium,", "                        }", "                    }", "                };", "", "                foreach (var filter in combinedFilters)", "                {", "                    filterContainer.Items.Add(new AdaptiveTextBlock", "                    {", "                        Text = filter,", "                        Color = AdaptiveTextColor.Accent,", "                        Weight = AdaptiveTextWeight.Bolder,", "                        Size = AdaptiveTextSize.Small,", "                        Wrap = true,", "                        Spacing = AdaptiveSpacing.Small", "                    });", "                }", "", "                card.Body.Add(filterContainer);", "            }", "", "            return card;", "        }", "", "        private static AdaptiveContainer CreatePRItemContainer(GitHubPR pr)", "        {", "            var prItemContainer = new AdaptiveContainer", "            {", "                Spacing = AdaptiveSpacing.Medium,", "                Style = AdaptiveContainerStyle.Accent,", "                Items = new List<AdaptiveElement>", "                {", "                    new AdaptiveColumnSet", "                    {", "                        Columns = new List<AdaptiveColumn>", "                        {", "                            new AdaptiveColumn", "                            {", "                                Width = AdaptiveColumnWidth.Stretch,", "                                Items = new List<AdaptiveElement>", "                                {", "                                    new AdaptiveTextBlock", "                                    {", "                                        Text = $\"#{pr.Number}: {pr.Title}\",", "                                        Weight = AdaptiveTextWeight.Bolder,", "                                        Wrap = true,", "                                        Size = AdaptiveTextSize.Small,", "                                        Color = AdaptiveTextColor.Accent", "                                    }", "                                }", "                            }", "                        }", "                    },", "                    new AdaptiveTextBlock", "                    {", "                        Text = $\"**Author**: {pr.User.Login ?? \"Unknown\"}\",", "                        IsSubtle = true,", "                        Spacing = AdaptiveSpacing.None", "                    },", "                    new AdaptiveTextBlock", "                    {", "                        Text = $\"**Created**: {pr.CreatedAt:MMM dd, yyyy}\",", "                        IsSubtle = true,", "                        Spacing = AdaptiveSpacing.None", "                    },", "                    new AdaptiveTextBlock", "                    {", "                        Text = $\"**Status**: {(pr.State == \"open\" ? \"\ud83d\udfe2 Open\" : \"\ud83d\udd34 Closed\")}\",", "                        IsSubtle = true,", "                        Spacing = AdaptiveSpacing.None", "                    }", "                }", "            };", "", "            if (pr.Labels != null && pr.Labels.Count > 0)", "            {", "                var labelTexts = pr.Labels.Select(l => l.Name).ToList();", "                prItemContainer.Items.Add(new AdaptiveTextBlock", "                {", "                    Text = $\"**Labels**: {string.Join(\", \", labelTexts)}\",", "                    IsSubtle = true,", "                    Spacing = AdaptiveSpacing.None,", "                    Wrap = true,", "                    Italic = true,", "                });", "            }", "", "            prItemContainer.Items.Add(new AdaptiveActionSet", "            {", "                Actions = new List<AdaptiveAction>", "                        {", "                            new AdaptiveToggleVisibilityAction", "                            {", "                                Title = \"Show/Hide Description\",", "                                TargetElements = new List<AdaptiveTargetElement>", "                                {", "                                    new AdaptiveTargetElement", "                                    {", "                                        ElementId = $\"description-{pr.Number}\"", "                                    }", "                                }", "                            }", "                        }", "            });", "", "            prItemContainer.Items.Add(", "                    new AdaptiveTextBlock", "                    {", "                        Id = $\"description-{pr.Number}\",", "                        Text = $\"Description: {pr.Body}\",", "                        Wrap = true,", "                        IsSubtle = true,", "                        Spacing = AdaptiveSpacing.None,", "                        IsVisible = false", "                    });", "", "            if (!string.IsNullOrEmpty(pr.HtmlUrl))", "            {", "                prItemContainer.Items.Add(new AdaptiveActionSet", "                {", "                    Actions = new List<AdaptiveAction>", "                    {", "                        new AdaptiveOpenUrlAction", "                        {", "                            Title = \"View on GitHub\",", "                            Url = new Uri(pr.HtmlUrl)", "                        }", "                    }", "                });", "            }", "", "            return prItemContainer;", "        }", "", "        public static Attachment CreatePullRequestCard(JObject payload)", "        {", "            var pullRequest = payload[\"pull_request\"];", "            string assignee = pullRequest[\"assignee\"] != null ? pullRequest[\"assignee\"][\"login\"].ToString() : \"Unknown User\";", "            string prTitle = pullRequest[\"title\"].ToString();", "            string prUrl = pullRequest[\"html_url\"].ToString();", "            int prNumber = pullRequest[\"number\"].Value<int>();", "", "            var card = new AdaptiveCard(new AdaptiveSchemaVersion(1, 2))", "            {", "                Body = new List<AdaptiveElement>", "                {", "                    new AdaptiveTextBlock", "                    {", "                        Text = $\"\ud83d\udc64 Assignee Request for PR #{prNumber}\",", "                        Weight = AdaptiveTextWeight.Bolder,", "                        Size = AdaptiveTextSize.Large,", "                        Color = AdaptiveTextColor.Accent", "                    },", "                    new AdaptiveTextBlock", "                    {", "                        Text = $\"{prTitle}\",", "                        Wrap = true,", "                        Size = AdaptiveTextSize.Medium,", "                        Weight = AdaptiveTextWeight.Bolder,", "                    },", "                    new AdaptiveTextBlock", "                    {", "                        Text = $\"{assignee} has been assigned this pull request.\",", "                        Wrap = true,", "                        Size = AdaptiveTextSize.Medium,", "                        Spacing = AdaptiveSpacing.Medium", "                    }", "                },", "                Actions = new List<AdaptiveAction>", "                {", "                    new AdaptiveOpenUrlAction", "                    {", "                        Title = \"View on GitHub\",", "                        Url = new Uri(prUrl)", "                    }", "                }", "            };", "", "            return new Attachment", "            {", "                ContentType = AdaptiveCard.ContentType,", "                Content = card", "            };", "        }", "", "        public static Attachment CreatePullRequestStateCard(JObject payload)", "        {", "            var pullRequest = payload[\"pull_request\"];", "            string action = payload[\"action\"].ToString();", "            string prTitle = pullRequest[\"title\"].ToString();", "            string prUrl = pullRequest[\"html_url\"].ToString();", "            int prNumber = pullRequest[\"number\"].Value<int>();", "", "            var card = new AdaptiveCard(new AdaptiveSchemaVersion(1, 2))", "            {", "                Body = new List<AdaptiveElement>", "                {", "                     new AdaptiveTextBlock", "                            {", "                                Text = $\"\ud83d\udd14 Status Update for PR #{prNumber}\",", "                                Weight = AdaptiveTextWeight.Bolder,", "                                Size = AdaptiveTextSize.Large,", "                                Color = AdaptiveTextColor.Accent", "                            },", "                            new AdaptiveTextBlock", "                            {", "                                Text = $\"{prTitle}\",", "                                Wrap = true,", "                                Size = AdaptiveTextSize.Medium,", "                                Weight = AdaptiveTextWeight.Bolder,", "                            },", "                            new AdaptiveTextBlock", "                            {", "                                Text = $\"PR is now {action}\",", "                                Wrap = true,", "                                Size = AdaptiveTextSize.Medium,", "                                Spacing = AdaptiveSpacing.Medium", "                            }", "                },", "                Actions = new List<AdaptiveAction>", "                {", "                    new AdaptiveOpenUrlAction", "                    {", "                        Title = \"View on GitHub\",", "                        Url = new Uri(prUrl)", "                    }", "                }", "            };", "", "            return new Attachment", "            {", "                ContentType = AdaptiveCard.ContentType,", "                Content = card", "            };", "        }", "    }", "}"], "file_path": "dotnet/dex-agent/GitHubModels/GitHubFilterActivity.cs"}
{"Link_to_commit": "https://github.com/AdmondGuo/dify/commit/ef188564f30ab4feafe1dda015642cf2af800305", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 233, "n_files_impacted": 2, "longest_chunk": ["", "/**", " * Preprocesses mermaid code to fix common syntax issues", " */", "export function preprocessMermaidCode(code: string): string {", "  if (!code || typeof code !== 'string')", "    return ''", "", "  // First check if this is a gantt chart", "  if (code.trim().startsWith('gantt')) {", "    // For gantt charts, we need to ensure each task is on its own line", "    // Split the code into lines and process each line separately", "    const lines = code.split('\\n').map(line => line.trim())", "    return lines.join('\\n')", "  }", "", "  return code", "    // Replace English colons with Chinese colons in section nodes to avoid parsing issues", "    .replace(/section\\s+([^:]+):/g, (match, sectionName) => `section ${sectionName}\uff1a`)", "    // Fix common syntax issues", "    .replace(/fifopacket/g, 'rect')", "    // Ensure graph has direction", "    .replace(/^graph\\s+((?:TB|BT|RL|LR)*)/, (match, direction) => {", "      return direction ? match : 'graph TD'", "    })", "    // Clean up empty lines and extra spaces", "    .trim()", "}", "", "/**", " * Prepares mermaid code based on selected style", " */", "export function prepareMermaidCode(code: string, style: 'classic' | 'handDrawn'): string {", "  let finalCode = preprocessMermaidCode(code)", "", "  // Special handling for gantt charts", "  if (finalCode.trim().startsWith('gantt')) {", "    // For gantt charts, preserve the structure exactly as is", "    return finalCode", "  }", "", "  if (style === 'handDrawn') {", "    finalCode = finalCode", "      // Remove style definitions that interfere with hand-drawn style", "      .replace(/style\\s+[^\\n]+/g, '')", "      .replace(/linkStyle\\s+[^\\n]+/g, '')", "      .replace(/^flowchart/, 'graph')", "      // Remove any styles that might interfere with hand-drawn style", "      .replace(/class=\"[^\"]*\"/g, '')", "      .replace(/fill=\"[^\"]*\"/g, '')", "      .replace(/stroke=\"[^\"]*\"/g, '')", "", "    // Ensure hand-drawn style charts always start with graph", "    if (!finalCode.startsWith('graph') && !finalCode.startsWith('flowchart'))", "      finalCode = `graph TD\\n${finalCode}`", "  }", "", "  return finalCode", "}", "", "/**", " * Converts SVG to base64 string for image rendering", " */", "export function svgToBase64(svgGraph: string): Promise<string> {", "  if (!svgGraph)", "    return Promise.resolve('')", "", "  try {", "    // Ensure SVG has correct XML declaration", "    if (!svgGraph.includes('<?xml'))", "      svgGraph = `<?xml version=\"1.0\" encoding=\"UTF-8\"?>${svgGraph}`", "", "    const blob = new Blob([new TextEncoder().encode(svgGraph)], { type: 'image/svg+xml;charset=utf-8' })", "    return new Promise((resolve, reject) => {", "      const reader = new FileReader()", "      reader.onloadend = () => resolve(reader.result as string)", "      reader.onerror = reject", "      reader.readAsDataURL(blob)", "    })", "  }", "  catch (error) {", "    console.error('Error converting SVG to base64:', error)", "    return Promise.resolve('')", "  }", "}", "", "/**", " * Processes SVG for theme styling", " */", "export function processSvgForTheme(", "  svg: string,", "  isDark: boolean,", "  isHandDrawn: boolean,", "  themes: {", "    light: any", "    dark: any", "  },", "): string {", "  let processedSvg = svg", "", "  if (isDark) {", "    processedSvg = processedSvg", "      .replace(/style=\"fill: ?#000000\"/g, 'style=\"fill: #e2e8f0\"')", "      .replace(/style=\"stroke: ?#000000\"/g, 'style=\"stroke: #94a3b8\"')", "      .replace(/<rect [^>]*fill=\"#ffffff\"/g, '<rect $& fill=\"#1e293b\"')", "", "    if (isHandDrawn) {", "      processedSvg = processedSvg", "        .replace(/fill=\"#[a-fA-F0-9]{6}\"/g, `fill=\"${themes.dark.nodeColors[0].bg}\"`)", "        .replace(/stroke=\"#[a-fA-F0-9]{6}\"/g, `stroke=\"${themes.dark.connectionColor}\"`)", "        .replace(/stroke-width=\"1\"/g, 'stroke-width=\"1.5\"')", "    }", "    else {", "      let i = 0", "      themes.dark.nodeColors.forEach(() => {", "        const regex = /fill=\"#[a-fA-F0-9]{6}\"[^>]*class=\"node-[^\"]*\"/g", "        processedSvg = processedSvg.replace(regex, (match: string) => {", "          const colorIndex = i % themes.dark.nodeColors.length", "          i++", "          return match.replace(/fill=\"#[a-fA-F0-9]{6}\"/, `fill=\"${themes.dark.nodeColors[colorIndex].bg}\"`)", "        })", "      })", "", "      processedSvg = processedSvg", "        .replace(/<path [^>]*stroke=\"#[a-fA-F0-9]{6}\"/g,", "          `<path stroke=\"${themes.dark.connectionColor}\" stroke-width=\"1.5\"`)", "        .replace(/<(line|polyline) [^>]*stroke=\"#[a-fA-F0-9]{6}\"/g,", "          `<$1 stroke=\"${themes.dark.connectionColor}\" stroke-width=\"1.5\"`)", "    }", "  }", "  else {", "    if (isHandDrawn) {", "      processedSvg = processedSvg", "        .replace(/fill=\"#[a-fA-F0-9]{6}\"/g, `fill=\"${themes.light.nodeColors[0].bg}\"`)", "        .replace(/stroke=\"#[a-fA-F0-9]{6}\"/g, `stroke=\"${themes.light.connectionColor}\"`)", "        .replace(/stroke-width=\"1\"/g, 'stroke-width=\"1.5\"')", "    }", "    else {", "      themes.light.nodeColors.forEach(() => {", "        const regex = /fill=\"#[a-fA-F0-9]{6}\"[^>]*class=\"node-[^\"]*\"/g", "        let i = 0", "        processedSvg = processedSvg.replace(regex, (match: string) => {", "          const colorIndex = i % themes.light.nodeColors.length", "          i++", "          return match.replace(/fill=\"#[a-fA-F0-9]{6}\"/, `fill=\"${themes.light.nodeColors[colorIndex].bg}\"`)", "        })", "      })", "", "      processedSvg = processedSvg", "        .replace(/<path [^>]*stroke=\"#[a-fA-F0-9]{6}\"/g,", "          `<path stroke=\"${themes.light.connectionColor}\"`)", "        .replace(/<(line|polyline) [^>]*stroke=\"#[a-fA-F0-9]{6}\"/g,", "          `<$1 stroke=\"${themes.light.connectionColor}\"`)", "    }", "  }", "", "  return processedSvg", "}", "", "/**", " * Checks if mermaid code is complete and valid", " */", "export function isMermaidCodeComplete(code: string): boolean {", "  if (!code || code.trim().length === 0)", "    return false", "", "  try {", "    const trimmedCode = code.trim()", "", "    // Special handling for gantt charts", "    if (trimmedCode.startsWith('gantt')) {", "      // For gantt charts, check if it has at least a title and one task", "      const lines = trimmedCode.split('\\n').filter(line => line.trim().length > 0)", "      return lines.length >= 3", "    }", "", "    // Check for basic syntax structure", "    const hasValidStart = /^(graph|flowchart|sequenceDiagram|classDiagram|classDef|class|stateDiagram|gantt|pie|er|journey|requirementDiagram)/.test(trimmedCode)", "", "    // Check for balanced brackets and parentheses", "    const isBalanced = (() => {", "      const stack = []", "      const pairs = { '{': '}', '[': ']', '(': ')' }", "", "      for (const char of trimmedCode) {", "        if (char in pairs) {", "          stack.push(char)", "        }", "        else if (Object.values(pairs).includes(char)) {", "          const last = stack.pop()", "          if (pairs[last as keyof typeof pairs] !== char)", "            return false", "        }", "      }", "", "      return stack.length === 0", "    })()", "", "    // Check for common syntax errors", "    const hasNoSyntaxErrors = !trimmedCode.includes('undefined')", "                           && !trimmedCode.includes('[object Object]')", "                           && trimmedCode.split('\\n').every(line =>", "                             !(line.includes('-->') && !line.match(/\\S+\\s*-->\\s*\\S+/)))", "", "    return hasValidStart && isBalanced && hasNoSyntaxErrors", "  }", "  catch (error) {", "    console.debug('Mermaid code validation error:', error)", "    return false", "  }", "}", "", "/**", " * Helper to wait for DOM element with retry mechanism", " */", "export function waitForDOMElement(callback: () => Promise<any>, maxAttempts = 3, delay = 100): Promise<any> {", "  return new Promise((resolve, reject) => {", "    let attempts = 0", "    const tryRender = async () => {", "      try {", "        resolve(await callback())", "      }", "      catch (error) {", "        attempts++", "        if (attempts < maxAttempts)", "          setTimeout(tryRender, delay)", "        else", "          reject(error)", "      }", "    }", "    tryRender()", "  })", "}"], "file_path": "web/app/components/base/mermaid/utils.ts"}
{"Link_to_commit": "https://github.com/kagent-dev/kagent/commit/62e1214a641a8c82f33fd037662df0740aa9cfda", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 18, "n_files_impacted": 9, "longest_chunk": ["// Model Configurations", "// This had to be created because the autogen_api.ModelInfo JSON tags are not", "// compatible with the kubernetes api.", "type ModelInfo struct {", "\t// +optional", "\tVision bool `json:\"vision\"`", "\t// +optional", "\tFunctionCalling bool `json:\"functionCalling\"`", "\t// +optional", "\tJSONOutput bool `json:\"jsonOutput\"`", "\t// +optional", "\tFamily string `json:\"family\"`", "\t// +optional", "\tStructuredOutput bool `json:\"structuredOutput\"`", "\t// +optional", "\tMultipleSystemMessages bool `json:\"multipleSystemMessages\"`", "}", ""], "file_path": "go/controller/api/v1alpha1/autogenmodelconfig_types.go"}
{"Link_to_commit": "https://github.com/Rikicavaz77/Stage-PoC/commit/15b0b92ca94a8c6ac2b6f92d09a9dd084993dd6b", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 454, "n_files_impacted": 2, "longest_chunk": ["//import stopwords from 'https://cdn.jsdelivr.net/gh/stdlib-js/datasets-stopwords-en@esm/index.mjs';\r", "\r", "const disallowedTags = ['script', 'style', 'noscript', 'textarea', 'button', 'aside'];\r", "const allowedTags = ['p', 'a', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'];\r", "const disallowedInlineTags = ['a', 'button', 'label'];\r", "\r", "function isValidInlineElement(node) {\r", "  if (node.nodeType !== Node.ELEMENT_NODE) return false;\r", "  if (disallowedInlineTags.includes(node.tagName.toLowerCase())) return false;\r", "  const display = window.getComputedStyle(node).display;\r", "  return display.startsWith(\"inline\");\r", "}\r", "\r", "function isAllParentsValid(node) {\r", "  while ((node = node.parentNode)) {\r", "    if (disallowedTags.includes(node.nodeName.toLowerCase())) {\r", "      return false;\r", "    }\r", "  }\r", "  return true;\r", "}\r", "\r", "function getValidParent(node) {\r", "  while ((node = node.parentNode)) {\r", "    if (!isValidInlineElement(node)) {\r", "      return node;\r", "    }\r", "  }\r", "  return document.body;\r", "}\r", "\r", "const walker = document.createTreeWalker(\r", "  document.body,\r", "  NodeFilter.SHOW_TEXT,\r", "  {\r", "    acceptNode(node) {\r", "      if (!isAllParentsValid(node)) return NodeFilter.FILTER_REJECT;\r", "      //if (!node.nodeValue.trim()) return NodeFilter.FILTER_REJECT;\r", "      return NodeFilter.FILTER_ACCEPT;\r", "    }\r", "  }\r", ");\r", "\r", "/* function highlightKeyword(keyword) {\r", "  const textNodes = [];\r", "  let node;\r", "  while((node = walker.nextNode())) {\r", "    textNodes.push(node);\r", "  }\r", "\r", "  keyword = keyword.trim();\r", "  const keywordParts = keyword.split(/\\s+/);\r", "  keyword = keywordParts.join(\" \");\r", "  const pattern = new RegExp(`${keyword}`, \"gi\");\r", "  let relatedTagsMap = [];\r", "  let virtualText = \"\";\r", "  textNodes.forEach((node, index) => {\r", "    const nodeStart = virtualText.length + (node.nodeValue.length - node.nodeValue.trimStart().length);\r", "    relatedTagsMap.push({\r", "      node: node,\r", "      start: nodeStart,\r", "      end: nodeStart + node.nodeValue.trim().length\r", "    }); \r", "    virtualText += node.nodeValue;\r", "    const nextNode = textNodes[index + 1];\r", "    if (keywordParts.length > 1 && nextNode && getValidParent(node) === getValidParent(nextNode)) {\r", "      virtualText += \" \";\r", "    } else {\r", "      console.log(virtualText);\r", "      virtualText = virtualText.trim().split(/\\s+/).join(\" \");\r", "      const matches = [...virtualText.matchAll(pattern)].map((match) => ({\r", "        matchStart: match.index,\r", "        matchEnd: match.index + match[0].length\r", "      }));\r", "      console.log(matches);\r", "      if (matches.length > 0) {\r", "        if (relatedTagsMap.length > 1) {\r", "          console.log(relatedTagsMap);\r", "          relatedTagsMap.forEach(({ node, start, end }) => {\r", "            const validMatches = matches\r", "              .filter(({ matchStart, matchEnd }) => {\r", "                return (matchStart < end && matchEnd > start); // match overlaps this node\r", "              })\r", "              .map(({ matchStart, matchEnd }) => {\r", "                return {\r", "                  matchStart: Math.max(0, matchStart - start),\r", "                  matchEnd: Math.min(end - start, matchEnd - start)\r", "                };\r", "              });\r", "            highlightMatches(node, validMatches);\r", "          });\r", "        } else {\r", "          highlightMatches(node, matches);\r", "        }\r", "      }\r", "      virtualText = \"\";\r", "      relatedTagsMap = [];\r", "    }\r", "  });\r", "}\r", "\r", "function highlightMatches(node, matches) {\r", "  if (!matches || matches.length === 0 || !node.parentNode) return;\r", "\r", "  const text = node.nodeValue;\r", "  const parent = node.parentNode;\r", "  const fragment = document.createDocumentFragment();\r", "  let lastIndex = 0;\r", "\r", "  matches.forEach(({ matchStart, matchEnd }) => {\r", "    if (matchStart > lastIndex) {\r", "      const newTextNode = document.createTextNode(text.slice(lastIndex, matchStart));\r", "      fragment.appendChild(newTextNode);\r", "    }\r", "\r", "    const span = document.createElement(\"span\");\r", "    span.style.backgroundColor = \"yellow\";\r", "    span.classList.add(\"highlight-keyword\");\r", "    span.textContent = text.slice(matchStart, matchEnd);\r", "    fragment.appendChild(span);\r", "\r", "    lastIndex = matchEnd;\r", "  });\r", "\r", "  if (lastIndex < text.length) {\r", "    const newTextNode = document.createTextNode(text.slice(lastIndex));\r", "    fragment.appendChild(newTextNode);\r", "  }\r", "\r", "  parent.replaceChild(fragment, node);  \r", "} */\r", "\r", "/* function highlightKeyword(keyword) {\r", "  const textNodes = [];\r", "  let node;\r", "  while ((node = walker.nextNode())) {\r", "    textNodes.push(node);\r", "  }\r", "\r", "  // Normalizza la keyword\r", "  keyword = keyword.trim().replace(/\\s+/g, \" \");\r", "  const pattern = new RegExp(keyword, \"gi\");\r", "\r", "  // Costruisci testo virtuale e mappa\r", "  let fullText = \"\";\r", "  const map = []; // ogni elemento: { node, startIndex, endIndex }\r", "\r", "  textNodes.forEach((n) => {\r", "    const startIndex = fullText.length;\r", "    fullText += n.nodeValue;\r", "    const endIndex = fullText.length;\r", "    map.push({ node: n, startIndex, endIndex });\r", "  });\r", "\r", "  const matches = [...fullText.matchAll(pattern)];\r", "\r", "  for (const { index: matchStart } of matches) {\r", "    const matchEnd = matchStart + keyword.length;\r", "\r", "    // Trova i nodi che contengono questa porzione\r", "    for (const { node, startIndex, endIndex } of map) {\r", "      if (matchEnd <= startIndex) break;\r", "      if (matchStart >= endIndex) continue;\r", "\r", "      const localStart = Math.max(0, matchStart - startIndex);\r", "      const localEnd = Math.min(node.nodeValue.length, matchEnd - startIndex);\r", "\r", "      highlightInNode(node, localStart, localEnd);\r", "    }\r", "  }\r", "}\r", "\r", "function highlightInNode(node, start, end) {\r", "  const text = node.nodeValue;\r", "  const parent = node.parentNode;\r", "  if (!parent) return;\r", "\r", "  const fragment = document.createDocumentFragment();\r", "\r", "  if (start > 0) {\r", "    fragment.appendChild(document.createTextNode(text.slice(0, start)));\r", "  }\r", "\r", "  const span = document.createElement(\"span\");\r", "  span.style.backgroundColor = \"yellow\";\r", "  span.className = \"highlight-keyword\";\r", "  span.textContent = text.slice(start, end);\r", "  fragment.appendChild(span);\r", "\r", "  if (end < text.length) {\r", "    fragment.appendChild(document.createTextNode(text.slice(end)));\r", "  }\r", "\r", "  parent.replaceChild(fragment, node);\r", "} */\r", "\r", "\r", "/* function highlightKeyword(keyword) {\r", "  const keywordParts = keyword.trim().split(/\\s+/);\r", "  const normalizedKeyword = keywordParts.join(\" \");\r", "  const pattern = new RegExp(normalizedKeyword, \"gi\");\r", "\r", "  const groupedNodes = [];\r", "  let currentGroup = [];\r", "  let currentParent = null;\r", "\r", "  let node;\r", "  while ((node = walker.nextNode())) {\r", "    const parent = getValidParent(node);\r", "    if (parent !== currentParent) {\r", "      if (currentGroup.length > 0) {\r", "        groupedNodes.push({ nodes: currentGroup, parent: currentParent });\r", "      }\r", "      currentGroup = [node];\r", "      currentParent = parent;\r", "    } else {\r", "      currentGroup.push(node);\r", "    }\r", "  }\r", "  if (currentGroup.length > 0) {\r", "    groupedNodes.push({ nodes: currentGroup, parent: currentParent });\r", "  }\r", "\r", "  groupedNodes.forEach(({ nodes }) => {\r", "    let virtualText = \"\";\r", "    const map = [];\r", "\r", "    nodes.forEach((n) => {\r", "      const start = virtualText.length;\r", "      virtualText += n.nodeValue;\r", "      const end = virtualText.length;\r", "      map.push({ node: n, start, end });\r", "    });\r", "\r", "    const matches = [...virtualText.matchAll(pattern)];\r", "\r", "    matches.forEach(({ index }) => {\r", "      const matchStart = index;\r", "      const matchEnd = matchStart + normalizedKeyword.length;\r", "\r", "      map.forEach(({ node, start, end }) => {\r", "        if (matchEnd <= start || matchStart >= end) return;\r", "\r", "        const localStart = Math.max(0, matchStart - start);\r", "        const localEnd = Math.min(node.nodeValue.length, matchEnd - start);\r", "        highlightInNode(node, localStart, localEnd);\r", "      });\r", "    });\r", "  });\r", "}\r", "\r", "function highlightInNode(node, start, end) {\r", "  if (start >= end) return;\r", "  const text = node.nodeValue;\r", "  const parent = node.parentNode;\r", "  if (!parent) return;\r", "\r", "  const fragment = document.createDocumentFragment();\r", "\r", "  if (start > 0) {\r", "    fragment.appendChild(document.createTextNode(text.slice(0, start)));\r", "  }\r", "\r", "  const span = document.createElement(\"span\");\r", "  span.style.backgroundColor = \"yellow\";\r", "  span.classList.add(\"highlight-keyword\");\r", "  span.textContent = text.slice(start, end);\r", "  fragment.appendChild(span);\r", "\r", "  if (end < text.length) {\r", "    fragment.appendChild(document.createTextNode(text.slice(end)));\r", "  }\r", "\r", "  parent.replaceChild(fragment, node);\r", "} */\r", "\r", "function highlightKeyword(keyword) {\r", "  const keywordParts = keyword.trim().split(/\\s+/);\r", "  const flexiblePattern = keywordParts.join(\"\\\\s+\");\r", "  const pattern = new RegExp(flexiblePattern, \"gi\");\r", "\r", "  const groupedNodes = [];\r", "  let currentGroup = [];\r", "  let currentParent = null;\r", "  let node;\r", "  walker.currentNode = walker.root;\r", "  while ((node = walker.nextNode())) {\r", "    const parent = getValidParent(node);\r", "    if (parent !== currentParent) {\r", "      if (currentGroup.length > 0) {\r", "        groupedNodes.push({ nodes: currentGroup, parent: currentParent });\r", "      }\r", "      currentGroup = [node];\r", "      currentParent = parent;\r", "    } else {\r", "      currentGroup.push(node);\r", "    }\r", "  }\r", "  if (currentGroup.length > 0) {\r", "    groupedNodes.push({ nodes: currentGroup, parent: currentParent });\r", "  }\r", "\r", "  groupedNodes.forEach(({ nodes }) => {\r", "    let virtualText = \"\";\r", "    const map = [];\r", "\r", "    nodes.forEach((n) => {\r", "      if (!n.nodeValue.trim()) {\r", "        console.log(n.nodeValue);\r", "        const text = n.nodeValue.replace(/\\s+/, \" \");\r", "        virtualText += text;\r", "      } else {\r", "        const start = virtualText.length;\r", "        virtualText += n.nodeValue;\r", "        const end = virtualText.length;\r", "        map.push({ node: n, start, end });\r", "      }\r", "    });\r", "\r", "    const matches = [...virtualText.matchAll(pattern)];\r", "\r", "    map.forEach(({ node, start, end }) => {\r", "      const nodeMatches = matches\r", "        .filter((match) => {\r", "          const matchStart = match.index;\r", "          const matchEnd = matchStart + match[0].length;\r", "          return matchEnd > start && matchStart < end;\r", "        })\r", "        .map((match) => {\r", "          const matchStart = match.index;\r", "          const matchEnd = matchStart + match[0].length;\r", "          return {\r", "            matchStart: Math.max(0, matchStart - start),\r", "            matchEnd: Math.min(node.nodeValue.length, matchEnd - start)\r", "          };\r", "        });\r", "\r", "      if (nodeMatches.length > 0) {\r", "        highlightMatches(node, nodeMatches);\r", "      }\r", "    });\r", "  });\r", "}\r", "\r", "function highlightMatches(node, matches) {\r", "  const text = node.nodeValue;\r", "  const parent = node.parentNode;\r", "  if (!parent) return;\r", "\r", "  const fragment = document.createDocumentFragment();\r", "  let lastIndex = 0;\r", "\r", "  matches.sort((a, b) => a.matchStart - b.matchStart);\r", "\r", "  for (const { matchStart, matchEnd } of matches) {\r", "    if (matchStart > lastIndex) {\r", "      fragment.appendChild(document.createTextNode(text.slice(lastIndex, matchStart)));\r", "    }\r", "\r", "    const span = document.createElement(\"span\");\r", "    span.classList.add(\"highlight-keyword\");\r", "    span.style.backgroundColor = \"yellow\";\r", "    span.textContent = text.slice(matchStart, matchEnd);\r", "    fragment.appendChild(span);\r", "\r", "    lastIndex = matchEnd;\r", "  }\r", "\r", "  if (lastIndex < text.length) {\r", "    fragment.appendChild(document.createTextNode(text.slice(lastIndex)));\r", "  }\r", "\r", "  parent.replaceChild(fragment, node);\r", "}\r", "\r", "function countKeyword(keyword) {\r", "  // 1) ricostruisci virtualText e map\r", "  // 2) crea pattern e trova matches\r", "  let counts = {};\r", "  for (const match of virtualText.matchAll(pattern)) {\r", "    const matchStart = match.index;\r", "    const entry = map.find(({ start, end }) =>\r", "      matchStart >= start && matchStart < end\r", "    );\r", "    const tag = findContainerTag(entry.node);\r", "    counts[tag] = (counts[tag] || 0) + 1;\r", "  }\r", "  return counts;\r", "}\r", "\r", "function findTwoWordsKeyphrases() {\r", "  const groupedNodes = [];\r", "  let currentGroup = [];\r", "  let currentParent = null;\r", "  let node;\r", "  walker.currentNode = walker.root;\r", "  while ((node = walker.nextNode())) {\r", "    const parent = getValidParent(node);\r", "    if (parent !== currentParent) {\r", "      if (currentGroup.length > 0) {\r", "        groupedNodes.push({ nodes: currentGroup, parent: currentParent });\r", "      }\r", "      currentGroup = [node];\r", "      currentParent = parent;\r", "    } else {\r", "      currentGroup.push(node);\r", "    }\r", "  }\r", "  if (currentGroup.length > 0) {\r", "    groupedNodes.push({ nodes: currentGroup, parent: currentParent });\r", "  }\r", "\r", "  const twoWordsMap = new Map();\r", "  groupedNodes.forEach(({ nodes }) => {\r", "    let virtualText = \"\";\r", "\r", "    nodes.forEach((n) => {\r", "      if (!n.nodeValue.trim()) {\r", "        const text = n.nodeValue.replace(/\\s+/, \" \");\r", "        virtualText += text;\r", "      } else {\r", "        virtualText += n.nodeValue;\r", "      }\r", "    });\r", "\r", "    const pattern = /[\\p{L}\\p{N}]+(?:['\u2019\\-_.][\\p{L}\\p{N}]+)*['\u2019]?/gu;\r", "    const matches = [...virtualText.toLowerCase().matchAll(pattern)].map((match) => {\r", "      return match[0]\r", "    });\r", "    //const filteredWords = [...sw.removeStopwords(matches, [...sw.eng])];\r", "    //const stopWords = stopwords();\r", "    matches.forEach((word, index) => {\r", "      const nextWord = matches[index + 1];\r", "      if (word && nextWord && [...sw.removeStopwords([word, nextWord], [...sw.eng])].length === 2) {\r", "        const words = word + \" \" + nextWord;\r", "        if (!twoWordsMap.has(words)) {\r", "          twoWordsMap.set(words, 1);\r", "        } else {\r", "          twoWordsMap.set(words, twoWordsMap.get(words) + 1);\r", "        }\r", "      }\r", "    });\r", "  });\r", "  //const stopWords = stopwords();\r", "  //const filteredWords = words.filter(word => !stopWords.includes(word));\r", "  console.log(twoWordsMap);\r", "  const relevantWords = [...twoWordsMap.entries()].sort((a, b) => b[1] - a[1]).slice(0, 10);\r", "  console.log(relevantWords);\r", "}\r", "window.findTwoWordsKeyphrases = findTwoWordsKeyphrases;\r", "\r", "window.addEventListener(\"load\", function() {\r", "  findTwoWordsKeyphrases();\r", "});"], "file_path": "PoC/AdvancedFeaturesDemo/app_11.js"}
{"Link_to_commit": "https://github.com/Ayushaff/agent-backend/commit/ef188564f30ab4feafe1dda015642cf2af800305", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 233, "n_files_impacted": 2, "longest_chunk": ["", "/**", " * Preprocesses mermaid code to fix common syntax issues", " */", "export function preprocessMermaidCode(code: string): string {", "  if (!code || typeof code !== 'string')", "    return ''", "", "  // First check if this is a gantt chart", "  if (code.trim().startsWith('gantt')) {", "    // For gantt charts, we need to ensure each task is on its own line", "    // Split the code into lines and process each line separately", "    const lines = code.split('\\n').map(line => line.trim())", "    return lines.join('\\n')", "  }", "", "  return code", "    // Replace English colons with Chinese colons in section nodes to avoid parsing issues", "    .replace(/section\\s+([^:]+):/g, (match, sectionName) => `section ${sectionName}\uff1a`)", "    // Fix common syntax issues", "    .replace(/fifopacket/g, 'rect')", "    // Ensure graph has direction", "    .replace(/^graph\\s+((?:TB|BT|RL|LR)*)/, (match, direction) => {", "      return direction ? match : 'graph TD'", "    })", "    // Clean up empty lines and extra spaces", "    .trim()", "}", "", "/**", " * Prepares mermaid code based on selected style", " */", "export function prepareMermaidCode(code: string, style: 'classic' | 'handDrawn'): string {", "  let finalCode = preprocessMermaidCode(code)", "", "  // Special handling for gantt charts", "  if (finalCode.trim().startsWith('gantt')) {", "    // For gantt charts, preserve the structure exactly as is", "    return finalCode", "  }", "", "  if (style === 'handDrawn') {", "    finalCode = finalCode", "      // Remove style definitions that interfere with hand-drawn style", "      .replace(/style\\s+[^\\n]+/g, '')", "      .replace(/linkStyle\\s+[^\\n]+/g, '')", "      .replace(/^flowchart/, 'graph')", "      // Remove any styles that might interfere with hand-drawn style", "      .replace(/class=\"[^\"]*\"/g, '')", "      .replace(/fill=\"[^\"]*\"/g, '')", "      .replace(/stroke=\"[^\"]*\"/g, '')", "", "    // Ensure hand-drawn style charts always start with graph", "    if (!finalCode.startsWith('graph') && !finalCode.startsWith('flowchart'))", "      finalCode = `graph TD\\n${finalCode}`", "  }", "", "  return finalCode", "}", "", "/**", " * Converts SVG to base64 string for image rendering", " */", "export function svgToBase64(svgGraph: string): Promise<string> {", "  if (!svgGraph)", "    return Promise.resolve('')", "", "  try {", "    // Ensure SVG has correct XML declaration", "    if (!svgGraph.includes('<?xml'))", "      svgGraph = `<?xml version=\"1.0\" encoding=\"UTF-8\"?>${svgGraph}`", "", "    const blob = new Blob([new TextEncoder().encode(svgGraph)], { type: 'image/svg+xml;charset=utf-8' })", "    return new Promise((resolve, reject) => {", "      const reader = new FileReader()", "      reader.onloadend = () => resolve(reader.result as string)", "      reader.onerror = reject", "      reader.readAsDataURL(blob)", "    })", "  }", "  catch (error) {", "    console.error('Error converting SVG to base64:', error)", "    return Promise.resolve('')", "  }", "}", "", "/**", " * Processes SVG for theme styling", " */", "export function processSvgForTheme(", "  svg: string,", "  isDark: boolean,", "  isHandDrawn: boolean,", "  themes: {", "    light: any", "    dark: any", "  },", "): string {", "  let processedSvg = svg", "", "  if (isDark) {", "    processedSvg = processedSvg", "      .replace(/style=\"fill: ?#000000\"/g, 'style=\"fill: #e2e8f0\"')", "      .replace(/style=\"stroke: ?#000000\"/g, 'style=\"stroke: #94a3b8\"')", "      .replace(/<rect [^>]*fill=\"#ffffff\"/g, '<rect $& fill=\"#1e293b\"')", "", "    if (isHandDrawn) {", "      processedSvg = processedSvg", "        .replace(/fill=\"#[a-fA-F0-9]{6}\"/g, `fill=\"${themes.dark.nodeColors[0].bg}\"`)", "        .replace(/stroke=\"#[a-fA-F0-9]{6}\"/g, `stroke=\"${themes.dark.connectionColor}\"`)", "        .replace(/stroke-width=\"1\"/g, 'stroke-width=\"1.5\"')", "    }", "    else {", "      let i = 0", "      themes.dark.nodeColors.forEach(() => {", "        const regex = /fill=\"#[a-fA-F0-9]{6}\"[^>]*class=\"node-[^\"]*\"/g", "        processedSvg = processedSvg.replace(regex, (match: string) => {", "          const colorIndex = i % themes.dark.nodeColors.length", "          i++", "          return match.replace(/fill=\"#[a-fA-F0-9]{6}\"/, `fill=\"${themes.dark.nodeColors[colorIndex].bg}\"`)", "        })", "      })", "", "      processedSvg = processedSvg", "        .replace(/<path [^>]*stroke=\"#[a-fA-F0-9]{6}\"/g,", "          `<path stroke=\"${themes.dark.connectionColor}\" stroke-width=\"1.5\"`)", "        .replace(/<(line|polyline) [^>]*stroke=\"#[a-fA-F0-9]{6}\"/g,", "          `<$1 stroke=\"${themes.dark.connectionColor}\" stroke-width=\"1.5\"`)", "    }", "  }", "  else {", "    if (isHandDrawn) {", "      processedSvg = processedSvg", "        .replace(/fill=\"#[a-fA-F0-9]{6}\"/g, `fill=\"${themes.light.nodeColors[0].bg}\"`)", "        .replace(/stroke=\"#[a-fA-F0-9]{6}\"/g, `stroke=\"${themes.light.connectionColor}\"`)", "        .replace(/stroke-width=\"1\"/g, 'stroke-width=\"1.5\"')", "    }", "    else {", "      themes.light.nodeColors.forEach(() => {", "        const regex = /fill=\"#[a-fA-F0-9]{6}\"[^>]*class=\"node-[^\"]*\"/g", "        let i = 0", "        processedSvg = processedSvg.replace(regex, (match: string) => {", "          const colorIndex = i % themes.light.nodeColors.length", "          i++", "          return match.replace(/fill=\"#[a-fA-F0-9]{6}\"/, `fill=\"${themes.light.nodeColors[colorIndex].bg}\"`)", "        })", "      })", "", "      processedSvg = processedSvg", "        .replace(/<path [^>]*stroke=\"#[a-fA-F0-9]{6}\"/g,", "          `<path stroke=\"${themes.light.connectionColor}\"`)", "        .replace(/<(line|polyline) [^>]*stroke=\"#[a-fA-F0-9]{6}\"/g,", "          `<$1 stroke=\"${themes.light.connectionColor}\"`)", "    }", "  }", "", "  return processedSvg", "}", "", "/**", " * Checks if mermaid code is complete and valid", " */", "export function isMermaidCodeComplete(code: string): boolean {", "  if (!code || code.trim().length === 0)", "    return false", "", "  try {", "    const trimmedCode = code.trim()", "", "    // Special handling for gantt charts", "    if (trimmedCode.startsWith('gantt')) {", "      // For gantt charts, check if it has at least a title and one task", "      const lines = trimmedCode.split('\\n').filter(line => line.trim().length > 0)", "      return lines.length >= 3", "    }", "", "    // Check for basic syntax structure", "    const hasValidStart = /^(graph|flowchart|sequenceDiagram|classDiagram|classDef|class|stateDiagram|gantt|pie|er|journey|requirementDiagram)/.test(trimmedCode)", "", "    // Check for balanced brackets and parentheses", "    const isBalanced = (() => {", "      const stack = []", "      const pairs = { '{': '}', '[': ']', '(': ')' }", "", "      for (const char of trimmedCode) {", "        if (char in pairs) {", "          stack.push(char)", "        }", "        else if (Object.values(pairs).includes(char)) {", "          const last = stack.pop()", "          if (pairs[last as keyof typeof pairs] !== char)", "            return false", "        }", "      }", "", "      return stack.length === 0", "    })()", "", "    // Check for common syntax errors", "    const hasNoSyntaxErrors = !trimmedCode.includes('undefined')", "                           && !trimmedCode.includes('[object Object]')", "                           && trimmedCode.split('\\n').every(line =>", "                             !(line.includes('-->') && !line.match(/\\S+\\s*-->\\s*\\S+/)))", "", "    return hasValidStart && isBalanced && hasNoSyntaxErrors", "  }", "  catch (error) {", "    console.debug('Mermaid code validation error:', error)", "    return false", "  }", "}", "", "/**", " * Helper to wait for DOM element with retry mechanism", " */", "export function waitForDOMElement(callback: () => Promise<any>, maxAttempts = 3, delay = 100): Promise<any> {", "  return new Promise((resolve, reject) => {", "    let attempts = 0", "    const tryRender = async () => {", "      try {", "        resolve(await callback())", "      }", "      catch (error) {", "        attempts++", "        if (attempts < maxAttempts)", "          setTimeout(tryRender, delay)", "        else", "          reject(error)", "      }", "    }", "    tryRender()", "  })", "}"], "file_path": "web/app/components/base/mermaid/utils.ts"}
{"Link_to_commit": "https://github.com/bdougie/vision/commit/f0b46263db3d3b95828eef9a1ecd8c9e4532673e", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 71, "n_files_impacted": 1, "longest_chunk": ["\tctx := context.Background()", "", "\t// Configure logger", "\tlogger := slog.New(", "\t\ttint.NewHandler(os.Stderr, &tint.Options{", "\t\t\tLevel:      slog.LevelDebug,", "\t\t\tTimeFormat: time.Kitchen,", "\t\t}),", "\t)", "", "\t// Set up Ollama provider", "\topts := &ollama.ProviderOpts{", "\t\tLogger:  logger,", "\t\tBaseURL: \"http://localhost\",", "\t\tPort:    11434,", "\t}", "\tprovider := ollama.NewProvider(opts)", "", "\t// Use the correct model", "\tmodel := &types.Model{", "\t\tID: \"llama3.2-vision:11b\",", "\t}", "\tprovider.UseModel(ctx, model)", "", "\t// Create agent configuration", "\tagentConf := &agent.NewAgentConfig{", "\t\tProvider:     provider,", "\t\tLogger:       logger,", "\t\tSystemPrompt: \"You are a visual analysis assistant specialized in detailed image descriptions. If there is a person in the image describe what they are doing in step by step format.\",", "\t}", "", "\t// Initialize agent", "\tvisionAgent := agent.NewAgent(agentConf)", "", "\t// Parse command line arguments", "\tvideoPath := \"path/to/your/video.mp4\"", "\toutputDir = \"output_frames\"  // default value", "", "\tfor i := 1; i < len(os.Args); i++ {", "\t\tswitch os.Args[i] {", "\t\tcase \"--video\":", "\t\t\tif i+1 < len(os.Args) {", "\t\t\t\tvideoPath = os.Args[i+1]", "\t\t\t\ti++", "\t\t\t}", "\t\tcase \"--output\":", "\t\t\tif i+1 < len(os.Args) {", "\t\t\t\toutputDir = os.Args[i+1]", "\t\t\t\ti++", "\t\t\t}", "\t\t}", "\t}", "", "\t// Ensure video path is provided", "\tif videoPath == \"path/to/your/video.mp4\" {", "\t\tfmt.Println(\"Usage: go run main.go --video path/to/video.mp4 [--output output_directory]\")", "\t\tos.Exit(1)", "\t}", "", "\t// After parsing the video path, set the videoName", "\tvideoName = strings.TrimSuffix(filepath.Base(videoPath), filepath.Ext(videoPath))", "", "\t// Process video", "\tfmt.Printf(\"Starting video analysis...\\n\")", "\terr := processVideo(ctx, visionAgent, videoPath, outputDir)", "\tif err != nil {", "\t\tlog.Printf(\"Error processing video: %v\", err)", "\t\tos.Exit(1)", "\t}", "", "\tfmt.Println(\"Video processing completed successfully!\")"], "file_path": "main.go"}
{"Link_to_commit": "https://github.com/AnLeeDai/PRO1014_ADMIN/commit/f3c16e6fb0aa15200fa5c73214959964b26e46dd", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 47, "n_files_impacted": 18, "longest_chunk": ["import axios from 'axios';", "import Cookies from 'js-cookie';", "import { routerConfig } from '@/constants/siteConfig';", "", "const axiosInstance = axios.create({", "  baseURL: 'http://localhost/PRO1014_SERVER/routes/',", "  headers: {", "    'Content-Type': 'application/json',", "  },", "});", "", "// Add token to headers", "axiosInstance.interceptors.request.use((config) => {", "  const token = Cookies.get('token');", "", "  if (token) {", "    config.headers.Authorization = `Bearer ${token}`;", "  }", "", "  return config;", "});", "", "// Handle response errors", "axiosInstance.interceptors.response.use(", "  (response) => response,", "  (error: any) => {", "    const status = error.response?.status;", "    const code = error.response?.data?.code;", "", "    if (code === 'TOKEN_EXPIRED' || code === 'INVALID_TOKEN' || status === 440) {", "      Cookies.remove('token');", "      Cookies.remove('expires_in');", "      Cookies.remove('isLogin');", "      Cookies.remove('user_id');", "", "      window.location.href = routerConfig.login;", "    }", "", "    if (error.response?.data) {", "      return Promise.reject(error.response.data);", "    }", "", "    return Promise.reject(error);", "  }", ");", "", "export default axiosInstance;"], "file_path": "src/api/user.ts"}
{"Link_to_commit": "https://github.com/adriankarp/micro-store/commit/8ea9fbf51470598e6c507e3954221adc75154a73", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 5, "n_files_impacted": 28, "longest_chunk": ["/// <reference types=\"next\" />", "/// <reference types=\"next/image-types/global\" />", "", "// NOTE: This file should not be edited", "// see https://nextjs.org/docs/app/api-reference/config/typescript for more information."], "file_path": "apps/storefront/next.config.ts"}
{"Link_to_commit": "https://github.com/k-nuth/network/commit/6968c722f832596671186dc6fa4a143fe0ab1456", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 30, "n_files_impacted": 5, "longest_chunk": ["        CASE_RELAY_MESSAGE(reader, version, address);", "        CASE_RELAY_MESSAGE(reader, version, alert);", "        CASE_HANDLE_MESSAGE(reader, version, block);", "        CASE_RELAY_MESSAGE(reader, version, block_transactions);", "        CASE_RELAY_MESSAGE(reader, version, compact_block);", "        CASE_RELAY_MESSAGE(reader, version, double_spend_proof);", "        CASE_RELAY_MESSAGE(reader, version, fee_filter);", "        CASE_RELAY_MESSAGE(reader, version, filter_add);", "        CASE_RELAY_MESSAGE(reader, version, filter_clear);", "        CASE_RELAY_MESSAGE(reader, version, filter_load);", "        CASE_RELAY_MESSAGE(reader, version, get_address);", "        CASE_RELAY_MESSAGE(reader, version, get_blocks);", "        CASE_RELAY_MESSAGE(reader, version, get_block_transactions);", "        CASE_RELAY_MESSAGE(reader, version, get_data);", "        CASE_RELAY_MESSAGE(reader, version, get_headers);", "        CASE_RELAY_MESSAGE(reader, version, headers);", "        CASE_RELAY_MESSAGE(reader, version, inventory);", "        CASE_RELAY_MESSAGE(reader, version, memory_pool);", "        CASE_RELAY_MESSAGE(reader, version, merkle_block);", "        CASE_RELAY_MESSAGE(reader, version, not_found);", "        CASE_RELAY_MESSAGE(reader, version, ping);", "        CASE_RELAY_MESSAGE(reader, version, pong);", "        CASE_RELAY_MESSAGE(reader, version, reject);", "        CASE_RELAY_MESSAGE(reader, version, send_compact);", "        CASE_RELAY_MESSAGE(reader, version, send_headers);", "        CASE_HANDLE_MESSAGE(reader, version, transaction);", "        CASE_HANDLE_MESSAGE(reader, version, verack);", "        CASE_HANDLE_MESSAGE(reader, version, version);", "        CASE_HANDLE_MESSAGE(reader, version, xversion);", "        // CASE_HANDLE_MESSAGE(reader, version, xverack);"], "file_path": "src/message_subscriber.cpp"}
{"Link_to_commit": "https://github.com/react-native-checkbox/react-native-checkbox/commit/d12a3a30ae1d3ec39434657ce7c2705fafead5a6", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 12, "n_files_impacted": 3, "longest_chunk": ["          EventDispatcher eventDispatcher;", "          try {", "            if (BuildConfig.IS_NEW_ARCHITECTURE_ENABLED) {", "              eventDispatcher = UIManagerHelper.getUIManager(reactContext, UIManagerType.FABRIC).getEventDispatcher();", "            } else {", "              eventDispatcher = reactContext", "                .getNativeModule(UIManagerModule.class).getEventDispatcher();", "            }", "            eventDispatcher.dispatchEvent(new ReactCheckBoxEvent(buttonView.getId(), isChecked));", "          } catch (NullPointerException | IllegalStateException e) {", "            android.util.Log.e(\"ReactCheckBoxManager\", \"Error dispatching checkbox event\", e);", "          }"], "file_path": "src/android/src/main/java/com/reactnativecommunity/checkbox/ReactCheckBoxManager.java"}
{"Link_to_commit": "https://github.com/Energinet-DataHub/opengeh-edi/commit/34e06f00ead24196bbd107dfbc4c247d9fabca9d", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 225, "n_files_impacted": 42, "longest_chunk": ["\ufeff// Copyright 2020 Energinet DataHub A/S", "//", "// Licensed under the Apache License, Version 2.0 (the \"License2\");", "// you may not use this file except in compliance with the License.", "// You may obtain a copy of the License at", "//", "//     http://www.apache.org/licenses/LICENSE-2.0", "//", "// Unless required by applicable law or agreed to in writing, software", "// distributed under the License is distributed on an \"AS IS\" BASIS,", "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "// See the License for the specific language governing permissions and", "// limitations under the License.", "", "using System.Globalization;", "using System.Net;", "using Energinet.DataHub.Core.FunctionApp.TestCommon.FunctionAppHost;", "using Energinet.DataHub.EDI.B2BApi.AppTests.Fixtures;", "using Energinet.DataHub.EDI.B2BApi.AppTests.Fixtures.Extensions;", "using Energinet.DataHub.EDI.B2BApi.Functions.BundleMessages;", "using Energinet.DataHub.EDI.BuildingBlocks.Domain.Models;", "using Energinet.DataHub.EDI.BuildingBlocks.Infrastructure.FeatureFlag;", "using Energinet.DataHub.EDI.BuildingBlocks.Tests.Logging;", "using Energinet.DataHub.EDI.OutgoingMessages.Infrastructure.DataAccess;", "using Energinet.DataHub.ProcessManager.Orchestrations.Abstractions.Processes.BRS_021.Shared.V1.Model;", "using FluentAssertions;", "using FluentAssertions.Execution;", "using Microsoft.EntityFrameworkCore;", "using NodaTime;", "using Xunit;", "using Xunit.Abstractions;", "using MeasurementUnit = Energinet.DataHub.ProcessManager.Components.Abstractions.ValueObjects.MeasurementUnit;", "using MeteringPointType = Energinet.DataHub.ProcessManager.Components.Abstractions.ValueObjects.MeteringPointType;", "using Quality = Energinet.DataHub.ProcessManager.Components.Abstractions.ValueObjects.Quality;", "using Resolution = Energinet.DataHub.ProcessManager.Components.Abstractions.ValueObjects.Resolution;", "", "namespace Energinet.DataHub.EDI.B2BApi.AppTests.Functions.EnqueueMessages.BRS_021;", "", "[Collection(nameof(B2BApiAppCollectionFixture))]", "public class EnqueueBrs21CalculationMessagesTests : IAsyncLifetime", "{", "    private readonly B2BApiAppFixture _fixture;", "", "    public EnqueueBrs21CalculationMessagesTests(", "        B2BApiAppFixture fixture,", "        ITestOutputHelper testOutputHelper)", "    {", "        _fixture = fixture;", "        _fixture.SetTestOutputHelper(testOutputHelper);", "    }", "", "    public async Task InitializeAsync()", "    {", "        _fixture.AppHostManager.ClearHostLog();", "", "        // Dequeue existing messages", "        await using var context = _fixture.DatabaseManager.CreateDbContext<ActorMessageQueueContext>();", "", "        var bundles = await context.Bundles.ToListAsync();", "        foreach (var bundle in bundles)", "        {", "            bundle.TryDequeue();", "        }", "", "        await context.SaveChangesAsync();", "", "        await Task.CompletedTask;", "    }", "", "    public async Task DisposeAsync()", "    {", "        _fixture.SetTestOutputHelper(null!);", "        await Task.CompletedTask;", "    }", "", "    [Fact]", "    public async Task", "        Given_EnqueueCalculatedMeasurementsHttpV1_When_MessageIsReceived_AndWhen_MessageIsBundled_Then_MessageIsEnqueued_AndThen_MessageCanBePeeked()", "    {", "        _fixture.EnsureAppHostUsesFeatureFlagValue(", "        [", "            new(FeatureFlagName.PM25Messages, true),", "            new(FeatureFlagName.PM25CIM, true),", "        ]);", "", "        // Arrange", "        // => Given enqueue BRS-021 http request", "        var receiver1ActorNumber = ActorNumber.Create(\"1111111111111\");", "        var receiver1ActorRole = ActorRole.EnergySupplier;", "        const int receiver1Quantity = 11;", "", "        var receiver2ActorNumber = ActorNumber.Create(\"2222222222222\");", "        var receiver2ActorRole = ActorRole.EnergySupplier;", "        const int receiver2Quantity = 22;", "", "        var startDateTime = Instant.FromUtc(2025, 01, 31, 23, 00, 00);", "", "        var receiver1Start = startDateTime;", "        var receiver1End = startDateTime.Plus(Duration.FromMinutes(15));", "", "        var receiver2Start = receiver1End;", "        var receiver2End = receiver2Start.Plus(Duration.FromMinutes(15));", "", "        var enqueueMessagesData = new EnqueueCalculatedMeasurementsHttpV1(", "            OrchestrationInstanceId: Guid.NewGuid(),", "            TransactionId: Guid.NewGuid(),", "            MeteringPointId: \"1234567890123\",", "            MeteringPointType: MeteringPointType.Consumption,", "            Resolution: Resolution.QuarterHourly,", "            MeasureUnit: MeasurementUnit.KilowattHour,", "            Data:", "            [", "                new EnqueueCalculatedMeasurementsHttpV1.ReceiversWithMeasurements(", "                    Receivers:", "                    [", "                        new EnqueueCalculatedMeasurementsHttpV1.Actor(", "                            ActorNumber: receiver1ActorNumber.ToProcessManagerActorNumber(),", "                            ActorRole: receiver1ActorRole.ToProcessManagerActorRole()),", "                    ],", "                    StartDateTime: receiver1Start.ToDateTimeOffset(),", "                    EndDateTime: receiver1End.ToDateTimeOffset(),", "                    RegistrationDateTime: startDateTime.ToDateTimeOffset(),", "                    GridAreaCode: \"804\",", "                    Measurements:", "                    [", "                        new EnqueueCalculatedMeasurementsHttpV1.Measurement(", "                            Position: 1,", "                            EnergyQuantity: receiver1Quantity,", "                            QuantityQuality: Quality.AsProvided),", "                    ]),", "                new EnqueueCalculatedMeasurementsHttpV1.ReceiversWithMeasurements(", "                    Receivers:", "                    [", "                        new EnqueueCalculatedMeasurementsHttpV1.Actor(", "                            ActorNumber: receiver2ActorNumber.ToProcessManagerActorNumber(),", "                            ActorRole: receiver2ActorRole.ToProcessManagerActorRole()),", "                    ],", "                    StartDateTime: receiver2Start.ToDateTimeOffset(),", "                    EndDateTime: receiver2End.ToDateTimeOffset(),", "                    RegistrationDateTime: startDateTime.ToDateTimeOffset(),", "                    GridAreaCode: \"804\",", "                    Measurements:", "                    [", "                        new EnqueueCalculatedMeasurementsHttpV1.Measurement(", "                            Position: 1,", "                            EnergyQuantity: receiver2Quantity,", "                            QuantityQuality: Quality.AsProvided),", "                    ]),", "            ]);", "", "        // Act", "        // => When message is received", "        var httpRequest = _fixture.CreateEnqueueCalculatedMeasurementsHttpV1Request(enqueueMessagesData);", "", "        await _fixture.AppHostManager.HttpClient.SendAsync(httpRequest);", "", "        // => And when message is bundled", "        await _fixture.AppHostManager.TriggerFunctionAsync(nameof(OutgoingMessagesBundler));", "", "        // Verify the bundling function was executed", "        var bundleFunctionResult =", "            await _fixture.AppHostManager.WaitForFunctionToCompleteWithSucceededAsync(", "                functionName: nameof(OutgoingMessagesBundler));", "", "        bundleFunctionResult.Succeeded.Should()", "            .BeTrue(", "                \"the OutgoingMessagesBundler function should have been completed with success. Host log:\\n{0}\",", "                bundleFunctionResult.HostLog);", "", "        using var assertionScope = new AssertionScope();", "", "        // => Verify that outgoing messages were enqueued", "        await using var dbContext = _fixture.DatabaseManager.CreateDbContext<ActorMessageQueueContext>();", "        var enqueuedOutgoingMessages = await dbContext.OutgoingMessages", "            .Where(om => om.ExternalId == new ExternalId(enqueueMessagesData.TransactionId) && om.DocumentType == DocumentType.NotifyValidatedMeasureData)", "            .ToListAsync();", "", "        enqueuedOutgoingMessages.Should().HaveCount(2);", "", "        // => Verify that the enqueued message can be peeked", "        List<(Actor Actor, decimal EnergyQuantity, Instant Start, Instant End)> expectedReceivers =", "        [", "            (new Actor(receiver1ActorNumber, receiver1ActorRole),", "                receiver1Quantity,", "                receiver1Start,", "                receiver1End),", "            (new Actor(receiver2ActorNumber, receiver2ActorRole),", "                receiver2Quantity,", "                receiver2Start,", "                receiver2End),", "        ];", "", "        foreach (var expectedReceiver in expectedReceivers)", "        {", "            var peekHttpRequest = await _fixture.CreatePeekHttpRequestAsync(", "                actor: expectedReceiver.Actor,", "                category: MessageCategory.MeasureData);", "", "            var peekResponse = await _fixture.AppHostManager.HttpClient.SendAsync(peekHttpRequest);", "            await peekResponse.EnsureSuccessStatusCodeWithLogAsync(_fixture.TestLogger);", "", "            // Ensure status code is 200 OK, since EnsureSuccessStatusCode() also allows 204 No Content", "            peekResponse.StatusCode.Should()", "                .Be(", "                    HttpStatusCode.OK,", "                    $\"because the peek request for receiver {expectedReceiver.Actor.ActorNumber} should return OK status code (with content)\");", "", "            var peekResponseContent = await peekResponse.Content.ReadAsStringAsync();", "            peekResponseContent.Should()", "                .NotBeNullOrEmpty()", "                .And.Contain(", "                    \"NotifyValidatedMeasureData\",", "                    $\"because the peeked messages for receiver {expectedReceiver.Actor.ActorNumber} should be a notify validated measure data\")", "                .And.Contain(", "                    $\"\\\"quantity\\\": {expectedReceiver.EnergyQuantity}\",", "                    $\"because the peeked messages for receiver {expectedReceiver.Actor.ActorNumber} should have the expected measure data\")", "                .And.Contain(", "                    $\"\\\"value\\\": \\\"{expectedReceiver.Start.ToString(\"yyyy-MM-dd'T'HH:mm'Z'\", CultureInfo.InvariantCulture)}\\\"\",", "                    $\"because the peeked messages for receiver {expectedReceiver.Actor.ActorNumber} should have the expected start\")", "                .And.Contain(", "                    $\"\\\"value\\\": \\\"{expectedReceiver.End.ToString(\"yyyy-MM-dd'T'HH:mm'Z'\", CultureInfo.InvariantCulture)}\\\"\",", "                    $\"because the peeked messages for receiver {expectedReceiver.Actor.ActorNumber} should have the expected end\");", "        }", "    }", "}"], "file_path": "source/B2BApi.AppTests/SubsystemHttpTrigger/EnqueueHttpEndpointTests.cs"}
{"Link_to_commit": "https://github.com/k-nuth/c-api/commit/9b087875701438287850a18f8325cbb34e2f1b8f", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 89, "n_files_impacted": 48, "longest_chunk": ["kth_bool_t kth_chain_utxo_has_token_data(kth_utxo_t utxo) {", "    return kth_chain_utxo_const_cpp(utxo).token_data() ?", "        kth::bool_to_int(true) :", "        kth::bool_to_int(false);", "}", "", "kth_token_data_t kth_chain_utxo_get_token_data(kth_utxo_t utxo) {", "    auto& utxo_cpp = kth_chain_utxo_cpp(utxo);", "    if ( ! utxo_cpp.token_data()) {", "        return nullptr;", "    }", "    return &utxo_cpp.token_data().value();", "}", "", "kth_hash_t kth_chain_utxo_get_token_category(kth_utxo_t utxo) {", "    auto const& token_data = kth_chain_utxo_const_cpp(utxo).token_data();", "    if ( ! token_data) {", "        return kth::to_hash_t(kth::null_hash);", "    }", "    auto const& token_category_cpp = token_data->id;", "    return kth::to_hash_t(token_category_cpp);", "}", "", "void kth_chain_utxo_get_token_category_out(kth_utxo_t utxo, kth_hash_t* out_token_category) {", "    auto const& token_data = kth_chain_utxo_const_cpp(utxo).token_data();", "    if ( ! token_data) {", "        kth::copy_c_hash(kth::null_hash, out_token_category);", "        return;", "    }", "    auto const& token_category_cpp = token_data->id;", "    kth::copy_c_hash(token_category_cpp, out_token_category);", "}", "", "uint64_t kth_chain_utxo_get_token_amount(kth_utxo_t utxo) {", "    auto const& token_data = kth_chain_utxo_const_cpp(utxo).token_data();", "    if ( ! token_data) {", "        return kth::max_uint64;", "    }", "    if (std::holds_alternative<kth::domain::chain::fungible>(token_data->data)) {", "        return uint64_t(std::get<kth::domain::chain::fungible>(token_data->data).amount);", "    }", "    if (std::holds_alternative<kth::domain::chain::both_kinds>(token_data->data)) {", "        return uint64_t(std::get<kth::domain::chain::both_kinds>(token_data->data).first.amount);", "    }", "    return kth::max_uint64;", "}", "", "kth_token_capability_t kth_chain_utxo_get_token_capability(kth_utxo_t utxo) {", "    auto const& token_data_opt = kth_chain_utxo_const_cpp(utxo).token_data();", "    if ( ! token_data_opt) {", "        return kth_token_capability_none;", "    }", "    auto const& token_data = *token_data_opt;", "    if (std::holds_alternative<kth::domain::chain::non_fungible>(token_data.data)) {", "        auto const& non_fungible_cpp = std::get<kth::domain::chain::non_fungible>(token_data.data);", "        return kth::token_capability_to_c(non_fungible_cpp.capability);", "    }", "    if (std::holds_alternative<kth::domain::chain::both_kinds>(token_data.data)) {", "        auto const& both_kinds_cpp = std::get<kth::domain::chain::both_kinds>(token_data.data);", "        auto const& non_fungible_cpp = both_kinds_cpp.second;", "        return kth::token_capability_to_c(non_fungible_cpp.capability);", "    }", "    return kth_token_capability_none; // TODO: this is not a good way to signal an error", "}", "", "uint8_t const* kth_chain_utxo_get_token_commitment(kth_utxo_t utxo, kth_size_t* out_size) {", "    auto const& token_data_opt = kth_chain_utxo_const_cpp(utxo).token_data();", "    if ( ! token_data_opt) {", "        *out_size = 0;", "        return nullptr;", "    }", "    auto const& token_data = *token_data_opt;", "    if (std::holds_alternative<kth::domain::chain::non_fungible>(token_data.data)) {", "        auto const& non_fungible_cpp = std::get<kth::domain::chain::non_fungible>(token_data.data);", "        return kth::create_c_array(non_fungible_cpp.commitment, *out_size);", "    }", "    if (std::holds_alternative<kth::domain::chain::both_kinds>(token_data.data)) {", "        auto const& both_kinds_cpp = std::get<kth::domain::chain::both_kinds>(token_data.data);", "        auto const& non_fungible_cpp = both_kinds_cpp.second;", "        return kth::create_c_array(non_fungible_cpp.commitment, *out_size);", "    }", "", "    *out_size = 0;", "    return nullptr;", "}", "", "// Setters", "", "void kth_chain_utxo_set_hash(kth_utxo_t utxo, kth_hash_t const* hash) {"], "file_path": "src/chain/utxo.cpp"}
{"Link_to_commit": "https://github.com/CHORUS-TRE/chorus-backend/commit/408f949702762988739a5cbaee31bf91eef2be10", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 40, "n_files_impacted": 9, "longest_chunk": ["package service", "", "import (", "\t\"testing\"", ")", "", "func TestGetWorkspaceID(t *testing.T) {", "\ts := &WorkbenchService{}", "\ttests := []struct {", "\t\tname      string", "\t\tinput     string", "\t\texpected  uint64", "\t\texpectErr bool", "\t}{", "\t\t{", "\t\t\tname:      \"valid workspace name\",", "\t\t\tinput:     \"workspace123\",", "\t\t\texpected:  123,", "\t\t\texpectErr: false,", "\t\t},", "\t\t{", "\t\t\tname:      \"invalid workspace name\",", "\t\t\tinput:     \"invalid123\",", "\t\t\texpected:  0,", "\t\t\texpectErr: true,", "\t\t},", "\t}", "", "\tfor _, test := range tests {", "\t\tt.Run(test.name, func(t *testing.T) {", "\t\t\tresult, err := s.getWorkspaceID(test.input)", "\t\t\tif (err != nil) != test.expectErr {", "\t\t\t\tt.Errorf(\"expected error: %v, got: %v\", test.expectErr, err)", "\t\t\t}", "\t\t\tif result != test.expected {", "\t\t\t\tt.Errorf(\"expected: %d, got: %d\", test.expected, result)", "\t\t\t}", "\t\t})", "\t}", "}"], "file_path": "pkg/workbench/service/workbench-service.go"}
{"Link_to_commit": "https://github.com/ag2ai/ag2/commit/59cbd257471fe90473d174a65f4a40742ef34524", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 9, "n_files_impacted": 2, "longest_chunk": ["        paragraph = (", "            \"Mixed case extensions http://example.com/image.JPG, \"", "            \"http://example.com/image.Png and http://example.com/image.WebP.\"", "        )", "        expected_output = [", "            \"http://example.com/image.JPG\",", "            \"http://example.com/image.Png\",", "            \"http://example.com/image.WebP\",", "        ]"], "file_path": "test/agentchat/contrib/test_img_utils.py"}
{"Link_to_commit": "https://github.com/csp33/terraform-provider-metabase/commit/403f1295e2a8fe89e353d872465bc46ebc3f1c73", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 97, "n_files_impacted": 9, "longest_chunk": ["// Copyright (c) HashiCorp, Inc.", "// SPDX-License-Identifier: MPL-2.0", "", "package provider", "", "import (", "\t\"context\"", "\t\"fmt\"", "\t\"github.com/csp33/terraform-provider-metabase/sdk/metabase\"", "\t\"github.com/hashicorp/terraform-plugin-framework/path\"", "\t\"github.com/hashicorp/terraform-plugin-framework/resource\"", "\t\"github.com/hashicorp/terraform-plugin-framework/resource/schema\"", ")", "", "// Ensure BaseResource implements required interfaces.", "var _ resource.Resource = &BaseResource{}", "var _ resource.ResourceWithImportState = &BaseResource{}", "", "// BaseResource provides common functionality for all resources.", "type BaseResource struct {", "\t// TypeName is the name of the resource type (e.g., \"metabase_collection\")", "\tTypeName string", "", "\t// ConfigureRepository is a function that configures the repository for the resource", "\tConfigureRepository func(client *metabase.MetabaseAPIClient)", "", "\t// GetSchema returns the schema for the resource", "\tGetSchema func(ctx context.Context) schema.Schema", "", "\t// CreateFunc creates a new resource", "\tCreateFunc func(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse)", "", "\t// ReadFunc reads a resource", "\tReadFunc func(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse)", "", "\t// UpdateFunc updates a resource", "\tUpdateFunc func(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse)", "", "\t// DeleteFunc deletes a resource", "\tDeleteFunc func(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse)", "}", "", "// Metadata implements resource.Resource.", "func (r *BaseResource) Metadata(ctx context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) {", "\tresp.TypeName = req.ProviderTypeName + \"_\" + r.TypeName", "}", "", "// Schema implements resource.Resource.", "func (r *BaseResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) {", "\tresp.Schema = r.GetSchema(ctx)", "}", "", "// Configure implements resource.Resource.", "func (r *BaseResource) Configure(ctx context.Context, req resource.ConfigureRequest, resp *resource.ConfigureResponse) {", "\t// Prevent panic if the provider has not been configured.", "\tif req.ProviderData == nil {", "\t\treturn", "\t}", "", "\tclient, ok := req.ProviderData.(*metabase.MetabaseAPIClient)", "", "\tif !ok {", "\t\tresp.Diagnostics.AddError(", "\t\t\t\"Unexpected Resource Configure Type\",", "\t\t\tfmt.Sprintf(\"Expected *metabase.MetabaseAPIClient, got: %T. Please report this issue to the provider developers.\", req.ProviderData),", "\t\t)", "", "\t\treturn", "\t}", "", "\tr.ConfigureRepository(client)", "}", "", "// Create implements resource.Resource.", "func (r *BaseResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) {", "\tr.CreateFunc(ctx, req, resp)", "}", "", "// Read implements resource.Resource.", "func (r *BaseResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) {", "\tr.ReadFunc(ctx, req, resp)", "}", "", "// Update implements resource.Resource.", "func (r *BaseResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) {", "\tr.UpdateFunc(ctx, req, resp)", "}", "", "// Delete implements resource.Resource.", "func (r *BaseResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) {", "\tr.DeleteFunc(ctx, req, resp)", "}", "", "// ImportState implements resource.ResourceWithImportState.", "func (r *BaseResource) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) {", "\tresource.ImportStatePassthroughID(ctx, path.Root(\"id\"), req, resp)", "}"], "file_path": "internal/provider/collection.go"}
{"Link_to_commit": "https://github.com/likesdiles/alist/commit/41bdab49aa8acca9e88862c3db55cd7a8a84ba6a", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 58, "n_files_impacted": 3, "longest_chunk": ["", "func (d *Yun139) requestRoute(data interface{}, resp interface{}) ([]byte, error) {", "\turl := \"https://user-njs.yun.139.com/user/route/qryRoutePolicy\"", "\treq := base.RestyClient.R()", "\trandStr := random.String(16)", "\tts := time.Now().Format(\"2006-01-02 15:04:05\")", "\tcallback := func(req *resty.Request) {", "\t\treq.SetBody(data)", "\t}", "\tif callback != nil {", "\t\tcallback(req)", "\t}", "\tbody, err := utils.Json.Marshal(req.Body)", "\tif err != nil {", "\t\treturn nil, err", "\t}", "\tsign := calSign(string(body), ts, randStr)", "\tsvcType := \"1\"", "\tif d.isFamily() {", "\t\tsvcType = \"2\"", "\t}", "\treq.SetHeaders(map[string]string{", "\t\t\"Accept\":         \"application/json, text/plain, */*\",", "\t\t\"CMS-DEVICE\":     \"default\",", "\t\t\"Authorization\":  \"Basic \" + d.getAuthorization(),", "\t\t\"mcloud-channel\": \"1000101\",", "\t\t\"mcloud-client\":  \"10701\",", "\t\t//\"mcloud-route\": \"001\",", "\t\t\"mcloud-sign\": fmt.Sprintf(\"%s,%s,%s\", ts, randStr, sign),", "\t\t//\"mcloud-skey\":\"\",", "\t\t\"mcloud-version\":         \"7.14.0\",", "\t\t\"Origin\":                 \"https://yun.139.com\",", "\t\t\"Referer\":                \"https://yun.139.com/w/\",", "\t\t\"x-DeviceInfo\":           \"||9|7.14.0|chrome|120.0.0.0|||windows 10||zh-CN|||\",", "\t\t\"x-huawei-channelSrc\":    \"10000034\",", "\t\t\"x-inner-ntwk\":           \"2\",", "\t\t\"x-m4c-caller\":           \"PC\",", "\t\t\"x-m4c-src\":              \"10002\",", "\t\t\"x-SvcType\":              svcType,", "\t\t\"Inner-Hcy-Router-Https\": \"1\",", "\t})", "", "\tvar e BaseResp", "\treq.SetResult(&e)", "\tres, err := req.Execute(http.MethodPost, url)", "\tlog.Debugln(res.String())", "\tif !e.Success {", "\t\treturn nil, errors.New(e.Message)", "\t}", "\tif resp != nil {", "\t\terr = utils.Json.Unmarshal(res.Body(), resp)", "\t\tif err != nil {", "\t\t\treturn nil, err", "\t\t}", "\t}", "\treturn res.Body(), nil", "}", ""], "file_path": "drivers/139/util.go"}
{"Link_to_commit": "https://github.com/InferenceKTH/Course-Compass/commit/a270e9bed64cd3f3ea215b579115bc27c452eee6", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 9, "n_files_impacted": 9, "longest_chunk": ["    searchCourses(query) {", "        const searchResults = this.courses.filter(course =>", "            course.code.toLowerCase() === query.toLowerCase() ||", "            course.name.toLowerCase().includes(query.toLowerCase()) ||", "            course.description.toLowerCase().includes(query.toLowerCase())", "        );", "        this.setCurrentSearch(searchResults);", "    }", "};"], "file_path": "my-app/src/model.js"}
{"Link_to_commit": "https://github.com/adriankarp/micro-store/commit/c6def6a9008efc95071a5a8d4d2ea6511ec1c150", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 97, "n_files_impacted": 6, "longest_chunk": ["import { describe, it, expect, vi, beforeEach, afterEach } from \"vitest\";", "import { login, register, forgotPassword, APIError } from \"./auth\";", "import type {", "  LoginFormValues,", "  RegisterFormValues,", "  ForgotPasswordFormValues,", "} from \"../validation\";", "", "describe(\"lib/api/auth\", () => {", "  beforeEach(() => {", "    vi.stubGlobal(\"fetch\", vi.fn());", "    vi.useFakeTimers();", "    process.env.API_BASE_URL = \"https://api.escuelajs.co/api/v1\";", "  });", "", "  afterEach(() => {", "    vi.unstubAllGlobals();", "    vi.useRealTimers();", "    vi.restoreAllMocks();", "  });", "", "  describe(\"login()\", () => {", "    it(\"resolves with access_token and refresh_token when API returns 200\", async () => {", "      const fetchMock = fetch as unknown as ReturnType<typeof vi.fn>;", "", "      fetchMock.mockResolvedValueOnce({", "        ok: true,", "        status: 200,", "        headers: { get: () => \"application/json\" },", "        json: async () => ({", "          access_token: \"foo-token\",", "          refresh_token: \"bar-token\",", "        }),", "      });", "", "      await expect(", "        login({ email: \"a@b.com\", password: \"pw\" } as LoginFormValues),", "      ).resolves.toEqual({", "        access_token: \"foo-token\",", "        refresh_token: \"bar-token\",", "      });", "", "      expect(fetchMock).toHaveBeenCalledWith(", "        `${process.env.API_BASE_URL}/auth/login`,", "        expect.objectContaining({", "          method: \"POST\",", "          headers: { \"Content-Type\": \"application/json\" },", "          body: JSON.stringify({ email: \"a@b.com\", password: \"pw\" }),", "        }),", "      );", "    });", "", "    it(\"throws APIError with status and message on non-OK response\", async () => {", "      const fetchMock = fetch as unknown as ReturnType<typeof vi.fn>;", "", "      fetchMock.mockResolvedValueOnce({", "        ok: false,", "        status: 401,", "        headers: { get: () => \"application/json\" },", "        json: async () => ({ message: \"Invalid credentials\" }),", "      });", "", "      await expect(", "        login({ email: \"a@b.com\", password: \"wrong\" } as LoginFormValues),", "      ).rejects.toEqual(new APIError(\"Invalid credentials\", 401));", "    });", "  });", "", "  describe(\"register()\", () => {", "    it(\"resolves with success message after simulated delay\", async () => {", "      const payload = {", "        email: \"new@user.com\",", "        password: \"secret\",", "        confirmPassword: \"secret\",", "      } as RegisterFormValues;", "", "      const promise = register(payload);", "      vi.advanceTimersByTime(500);", "      await expect(promise).resolves.toEqual({", "        message: \"Registration successful!\",", "      });", "    });", "  });", "", "  describe(\"forgotPassword()\", () => {", "    it(\"resolves with a reset-email message after simulated delay\", async () => {", "      const payload = { email: \"someone@x.com\" } as ForgotPasswordFormValues;", "", "      const promise = forgotPassword(payload);", "      vi.advanceTimersByTime(500);", "      await expect(promise).resolves.toEqual({", "        message:", "          \"If that account exists, you\u2019ll receive a reset email shortly.\",", "      });", "    });", "  });", "});"], "file_path": "apps/shell/lib/auth/auth.ts"}
{"Link_to_commit": "https://github.com/opendexcom/formul.ai/commit/38ca6ee1237bafa32485cf353a3913090b1107e3", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 58, "n_files_impacted": 10, "longest_chunk": ["package com.formulai.survey.service;", "", "import com.formulai.survey.model.Survey;", "import com.formulai.survey.repository.SurveyRepository;", "import com.formulai.survey.dto.request.SurveyRequest;", "import com.formulai.survey.dto.response.SurveyResponse;", "import lombok.RequiredArgsConstructor;", "import org.springframework.stereotype.Service;", "", "import java.util.List;", "import java.util.stream.Collectors;", "", "import static java.lang.String.format;", "", "@RequiredArgsConstructor", "@Service", "public class SurveyService {", "", "    private final SurveyRepository surveyRepository;", "", "    public SurveyResponse getSurveyById(String id) {", "        return surveyRepository", "                .findById(id)", "                .map(this::fromSurvey)", "                .orElseThrow(()-> new IllegalArgumentException(format(\"Survey %s not found!\", id)));", "                // () and -> is a lambda. Lambda is a temporary function without a name.", "                // In our case we want just throw IllegalArgumentException if any problem exist.", "    }", "", "    public List<SurveyResponse> getAllSurvey(){", "        return surveyRepository", "                .findAll()", "                .stream()", "                .map(this::fromSurvey)", "                .collect(Collectors.toList());", "    }", "", "    public String createSurvey(SurveyRequest request) {", "        surveyRepository.save(toSurvey(request));", "        return \"Survey created successfully\";", "    }", "", "    public Survey toSurvey(SurveyRequest surveyRequest){", "        return Survey", "                .builder()", "                .name(surveyRequest.name())", "                .schemaJson(surveyRequest.schemaJson())", "                .build();", "    }", "", "    private SurveyResponse fromSurvey(Survey survey) {", "        return new SurveyResponse(", "                survey.getId(),", "                survey.getName(),", "                survey.getSchemaJson()", "        );", "    }", "}"], "file_path": "survey/src/main/java/com/formulai/survey/service/SurveyService.java"}
{"Link_to_commit": "https://github.com/AshwinNS/bookworm/commit/ce4afe19fb35ff57b0c4ffd18642e2932286036e", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 35, "n_files_impacted": 11, "longest_chunk": ["class RedisClient:", "    \"\"\"Redis client class.\"\"\"", "", "    def __init__(self):", "        self.redis = None", "", "    def get_client(self):", "        if not self.redis:", "            self.redis = self.get_redis()", "        return self.redis", "    ", "    def get_redis(self):", "        \"\"\"Get the Redis client.\"\"\"", "        try:", "            redis_client = Redis(host=os_environ.get(\"REDIS_HOST\", \"redis\"),", "                                port=int(os_environ.get(\"REDIS_PORT\", 6379)),", "                                db=int(os_environ.get(\"REDIS_DB\", 0)),", "                                password=os_environ.get(\"REDIS_PASSWORD\", None),", "                                decode_responses=True)", "            redis_client.ping()", "            return redis_client", "        except ConnectionError as e:", "            print(f\"Redis connection error: {e}\")", "            raise RuntimeError(\"Redis server is not reachable.\")", "    ", "    def ping(self):", "        \"\"\"Ping the Redis server.\"\"\"", "        try:", "            self.redis.ping()", "            return True", "        except ConnectionError as e:", "            print(f\"Redis connection error: {e}\")", "            return False", "", ""], "file_path": "api/helper.py"}
{"Link_to_commit": "https://github.com/cristicretu/UBB-SE-2025-MarketMessi/commit/1fe2da75a44e565f253f6007f1df2b5a04013ce4", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 246, "n_files_impacted": 21, "longest_chunk": ["using System;", "using System.Collections.Generic;", "using System.Linq;", "using server.Models;", "using System.Threading.Tasks;", "using server.DataAccessLayer;", "using Microsoft.EntityFrameworkCore;", "", "namespace server.MarketMinds.Repositories.BorrowProductsRepository", "{", "    public class BorrowProductsRepository : IBorrowProductsRepository", "    {", "        private readonly ApplicationDbContext _context;", "", "        public BorrowProductsRepository(ApplicationDbContext context)", "        {", "            _context = context;", "        }", "", "        public List<BorrowProduct> GetProducts()", "        {", "            try", "            {", "                // Start with a simpler query if full includes cause issues", "                var products = _context.BorrowProducts", "                    .Include(p => p.Seller)", "                    .Include(p => p.Condition)", "                    .Include(p => p.Category)", "                    .ToList();", "", "                // Optional: Load related entities separately if needed", "                foreach (var product in products)", "                {", "                    try", "                    {", "                        _context.Entry(product)", "                            .Collection(p => p.Images)", "                            .Load();", "                    }", "                    catch (Exception imgEx)", "                    {", "                        Console.WriteLine($\"Warning: Could not load images for product {product.Id}: {imgEx.Message}\");", "                    }", "", "                    try", "                    {", "                        _context.Entry(product)", "                            .Collection(p => p.ProductTags)", "                            .Load();", "", "                        // Eager load the Tag for each ProductTag", "                        foreach (var productTag in product.ProductTags)", "                        {", "                            try", "                            {", "                                _context.Entry(productTag)", "                                    .Reference(pt => pt.Tag)", "                                    .Load();", "                            }", "                            catch (Exception tagEx)", "                            {", "                                Console.WriteLine($\"Warning: Could not load tag for product {product.Id}, tag relation {productTag.Id}: {tagEx.Message}\");", "                            }", "                        }", "                    }", "                    catch (Exception ptEx)", "                    {", "                        Console.WriteLine($\"Warning: Could not load product tags for product {product.Id}: {ptEx.Message}\");", "                    }", "                }", "", "                return products;", "            }", "            catch (Exception ex)", "            {", "                Console.WriteLine($\"Error in GetProducts: {ex.Message}\");", "                // Return empty list instead of throwing to prevent cascading failures", "                return new List<BorrowProduct>();", "            }", "        }", "", "        public void DeleteProduct(BorrowProduct product)", "        {", "            try", "            {", "                _context.BorrowProducts.Remove(product);", "                _context.SaveChanges();", "            }", "            catch (Exception ex)", "            {", "                Console.WriteLine($\"Error in DeleteProduct: {ex.Message}\");", "                throw;", "            }", "        }", "", "        public void AddProduct(BorrowProduct product)", "        {", "            try", "            {", "                _context.BorrowProducts.Add(product);", "                _context.SaveChanges();", "            }", "            catch (Exception ex)", "            {", "                Console.WriteLine($\"Error in AddProduct: {ex.Message}\");", "                throw;", "            }", "        }", "", "        public void UpdateProduct(BorrowProduct product)", "        {", "            try", "            {", "                var existingProduct = _context.BorrowProducts.Find(product.Id);", "                if (existingProduct == null)", "                {", "                    throw new KeyNotFoundException($\"BorrowProduct with ID {product.Id} not found for update.\");", "                }", "", "                _context.Entry(existingProduct).CurrentValues.SetValues(product);", "", "                if (product.Images != null && product.Images.Any())", "                {", "                    Console.WriteLine($\"Updating product {product.Id} with {product.Images.Count} images\");", "                    ", "                    foreach (var image in product.Images)", "                    {", "                        if (image.Id == 0)", "                        {", "                            Console.WriteLine($\"Adding new image with URL: {image.Url} to product ID: {product.Id}\");", "                            image.ProductId = product.Id;", "                            _context.Set<BorrowProductImage>().Add(image);", "                        }", "                    }", "                }", "", "                _context.SaveChanges();", "            }", "            catch (Exception ex)", "            {", "                Console.WriteLine($\"Error in UpdateProduct: {ex.Message}\");", "                throw;", "            }", "        }", "", "        public BorrowProduct GetProductByID(int id)", "        {", "            try", "            {", "                // Basic product query first", "                var product = _context.BorrowProducts", "                    .Include(p => p.Seller)", "                    .Include(p => p.Condition)", "                    .Include(p => p.Category)", "                    .FirstOrDefault(p => p.Id == id);", "", "                if (product == null)", "                {", "                    throw new KeyNotFoundException($\"BorrowProduct with ID {id} not found.\");", "                }", "", "                // Load related entities individually", "                try", "                {", "                    _context.Entry(product)", "                        .Collection(p => p.Images)", "                        .Load();", "                }", "                catch (Exception imgEx)", "                {", "                    Console.WriteLine($\"Warning: Could not load images for product {id}: {imgEx.Message}\");", "                    // Initialize empty collection if loading fails", "                    product.Images = new List<BorrowProductImage>();", "                }", "", "                try", "                {", "                    _context.Entry(product)", "                        .Collection(p => p.ProductTags)", "                        .Load();", "", "                    // Load related tags", "                    foreach (var productTag in product.ProductTags)", "                    {", "                        try", "                        {", "                            _context.Entry(productTag)", "                                .Reference(pt => pt.Tag)", "                                .Load();", "                        }", "                        catch (Exception tagEx)", "                        {", "                            Console.WriteLine($\"Warning: Could not load tag for product {id}, tag relation {productTag.Id}: {tagEx.Message}\");", "                        }", "                    }", "                }", "                catch (Exception ptEx)", "                {", "                    Console.WriteLine($\"Warning: Could not load product tags for product {id}: {ptEx.Message}\");", "                    // Initialize empty collection if loading fails", "                    product.ProductTags = new List<BorrowProductProductTag>();", "                }", "", "                return product;", "            }", "            catch (KeyNotFoundException)", "            {", "                throw;", "            }", "            catch (Exception ex)", "            {", "                Console.WriteLine($\"Error in GetProductByID: {ex.Message}\");", "                throw;", "            }", "        }", "", "        public void AddImageToProduct(int productId, BorrowProductImage image)", "        {", "            try", "            {", "                // Make sure the product exists", "                var product = _context.BorrowProducts.Find(productId);", "                if (product == null)", "                {", "                    throw new KeyNotFoundException($\"BorrowProduct with ID {productId} not found.\");", "                }", "", "                // Set the product ID and add the image", "                image.ProductId = productId;", "                ", "                // Add image directly to the DbSet", "                _context.BorrowProductImages.Add(image);", "                ", "                // Save changes to the database", "                _context.SaveChanges();", "                ", "                Console.WriteLine($\"Successfully added image with URL {image.Url} to product {productId}\");", "            }", "            catch (Exception ex)", "            {", "                Console.WriteLine($\"Error adding image to product ID {productId}: {ex.Message}\");", "                throw new Exception($\"Failed to add image to product ID {productId}\", ex);", "            }", "        }", "    }", "} "], "file_path": "server/MarketMinds/Repositories/BorrowProductsRepository/BorrowProductsRepository.cs"}
{"Link_to_commit": "https://github.com/METR/inspect-tasks-public/commit/d3aee071ed98affdf9ce24f8d633f7807a64329c", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 12, "n_files_impacted": 22, "longest_chunk": ["# Ignore assets used within evaluations, where we specifically want type checks to", "# fail (e.g. buggy code that the agent should fix)", "ignore = [", "    \"re_bench/*/assets\",", "    \"re_bench/imported_asset_files\",", "    \"re_bench/tests/solutions\",", "]", "include = [\".\"]", "reportAssertAlwaysTrue = true", "reportDeprecated = true", "reportUnusedImport = true", "reportWildcardImportFromLibrary = true"], "file_path": "re_bench/_registry.py"}
{"Link_to_commit": "https://github.com/PolarWolf314/kanuka/commit/4ced9e252f585e16e7206c84586c481728ca3107", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 246, "n_files_impacted": 6, "longest_chunk": ["package secrets", "", "import (", "\t\"crypto/rand\"", "\t\"crypto/rsa\"", "\t\"crypto/x509\"", "\t\"encoding/pem\"", "\t\"fmt\"", "\t\"log\"", "\t\"os\"", "\t\"path/filepath\"", ")", "", "// LoadPrivateKey loads an RSA private key from disk.", "func LoadPrivateKey(path string) (*rsa.PrivateKey, error) {", "\tdata, err := os.ReadFile(path)", "\tif err != nil {", "\t\treturn nil, err", "\t}", "\tblock, _ := pem.Decode(data)", "\tif block == nil || block.Type != \"RSA PRIVATE KEY\" {", "\t\treturn nil, fmt.Errorf(\"failed to decode PEM block containing private key\")", "\t}", "\treturn x509.ParsePKCS1PrivateKey(block.Bytes)", "}", "", "// LoadPublicKey loads the user's public key from the project directory.", "func LoadPublicKey() (*rsa.PublicKey, error) {", "\twd, err := os.Getwd()", "\tif err != nil {", "\t\treturn nil, fmt.Errorf(\"failed to get working directory: %w\", err)", "\t}", "", "\tusername, err := GetUsername()", "\tif err != nil {", "\t\treturn nil, fmt.Errorf(\"failed to get username: %w\", err)", "\t}", "", "\tkanukaDir := filepath.Join(wd, \".kanuka\")", "\tpublicKeyPath := filepath.Join(kanukaDir, \"public_keys\", username+\".pub\")", "", "\tdata, err := os.ReadFile(publicKeyPath)", "\tif err != nil {", "\t\treturn nil, err", "\t}", "\tblock, _ := pem.Decode(data)", "\tif block == nil || block.Type != \"PUBLIC KEY\" {", "\t\treturn nil, fmt.Errorf(\"failed to decode PEM block containing public key\")", "\t}", "\tpub, err := x509.ParsePKIXPublicKey(block.Bytes)", "\tif err != nil {", "\t\treturn nil, err", "\t}", "\trsaPub, ok := pub.(*rsa.PublicKey)", "\tif !ok {", "\t\treturn nil, fmt.Errorf(\"not an RSA public key\")", "\t}", "\treturn rsaPub, nil", "}", "", "// GenerateRSAKeyPair creates a new RSA key pair and saves them to disk.", "func GenerateRSAKeyPair(privatePath, publicPath string) error {", "\tprivateKey, err := rsa.GenerateKey(rand.Reader, 2048)", "\tif err != nil {", "\t\treturn fmt.Errorf(\"failed to generate RSA key pair: %w\", err)", "\t}", "", "\t// Create directories if they don't exist", "\tprivateDir := filepath.Dir(privatePath)", "\tif err := os.MkdirAll(privateDir, 0700); err != nil {", "\t\treturn fmt.Errorf(\"failed to create directory for private key at %s: %w\", privateDir, err)", "\t}", "\tpublicDir := filepath.Dir(publicPath)", "\tif err := os.MkdirAll(publicDir, 0700); err != nil {", "\t\treturn fmt.Errorf(\"failed to create directory for public key at %s: %w\", publicDir, err)", "\t}", "", "\t// Save private key", "\tprivFile, err := os.Create(privatePath)", "\tif err != nil {", "\t\treturn fmt.Errorf(\"failed to create private key file at %s: %w\", privatePath, err)", "\t}", "\tdefer func() {", "\t\tif closeErr := privFile.Close(); closeErr != nil && err == nil {", "\t\t\terr = fmt.Errorf(\"failed to close private key file: %w\", closeErr)", "\t\t}", "\t}()", "", "\tprivBytes := x509.MarshalPKCS1PrivateKey(privateKey)", "\tprivPem := &pem.Block{", "\t\tType:  \"RSA PRIVATE KEY\",", "\t\tBytes: privBytes,", "\t}", "\tif err := pem.Encode(privFile, privPem); err != nil {", "\t\treturn fmt.Errorf(\"failed to PEM encode private key: %w\", err)", "\t}", "", "\t// Save public key", "\tpubFile, err := os.Create(publicPath)", "\tif err != nil {", "\t\treturn fmt.Errorf(\"failed to create public key file at %s: %w\", publicPath, err)", "\t}", "\tdefer func() {", "\t\tif closeErr := pubFile.Close(); closeErr != nil && err == nil {", "\t\t\terr = fmt.Errorf(\"failed to close public key file: %w\", closeErr)", "\t\t}", "\t}()", "", "\tpubASN1, err := x509.MarshalPKIXPublicKey(&privateKey.PublicKey)", "\tif err != nil {", "\t\treturn fmt.Errorf(\"failed to marshal public key: %w\", err)", "\t}", "\tpubPem := &pem.Block{", "\t\tType:  \"PUBLIC KEY\",", "\t\tBytes: pubASN1,", "\t}", "\tif err := pem.Encode(pubFile, pubPem); err != nil {", "\t\treturn fmt.Errorf(\"failed to PEM encode public key: %w\", err)", "\t}", "", "\treturn nil", "}", "", "func CreateAndSaveRSAKeyPair() error {", "\t// CreateAndSaveRSAKeyPair generates a new RSA key pair for the project and saves them in the user's directory.", "\twd, err := os.Getwd()", "\tif err != nil {", "\t\treturn fmt.Errorf(\"failed to get current working directory: %w\", err)", "\t}", "\tprojectName := filepath.Base(wd)", "", "\thomeDir, err := os.UserHomeDir()", "\tif err != nil {", "\t\treturn fmt.Errorf(\"failed to get user's home directory: %w\", err)", "\t}", "", "\t// Create key paths", "\tkeysDir := filepath.Join(homeDir, \".kanuka\", \"keys\")", "\tprivateKeyPath := filepath.Join(keysDir, projectName)", "\tpublicKeyPath := privateKeyPath + \".pub\"", "", "\t// Ensure key directory exists", "\tif err := os.MkdirAll(keysDir, 0700); err != nil {", "\t\treturn fmt.Errorf(\"failed to create keys directory at %s: %w\", keysDir, err)", "\t}", "", "\tif err := GenerateRSAKeyPair(privateKeyPath, publicKeyPath); err != nil {", "\t\treturn fmt.Errorf(\"failed to generate or save RSA key pair for project %s: %w\", projectName, err)", "\t}", "", "\tlog.Printf(`\u2705 Successfully generated RSA keys at:", "  - Private: %s", "  - Public: %s`, privateKeyPath, publicKeyPath)", "\treturn nil", "}", "", "// GetUserPrivateKey retrieves the user's private key for the current project.", "func GetUserPrivateKey() (*rsa.PrivateKey, error) {", "\thomeDir, err := os.UserHomeDir()", "\tif err != nil {", "\t\treturn nil, fmt.Errorf(\"failed to get user's home directory: %w\", err)", "\t}", "\tprojectName, err := GetProjectName()", "\tif err != nil {", "\t\treturn nil, fmt.Errorf(\"failed to get project name: %w\", err)", "\t}", "", "\tprivateKeyPath := filepath.Join(homeDir, \".kanuka\", \"keys\", projectName)", "\tif _, err := os.Stat(privateKeyPath); os.IsNotExist(err) {", "\t\treturn nil, fmt.Errorf(\"failed to get private key: %w\", err)", "\t}", "", "\tprivateKey, err := LoadPrivateKey(privateKeyPath)", "\tif err != nil {", "\t\treturn nil, fmt.Errorf(\"failed to load private key: %w\", err)", "\t}", "", "\treturn privateKey, nil", "}", "", "// CopyUserPublicKeyToProject copies the user's public key to the project directory.", "func CopyUserPublicKeyToProject() (string, error) {", "\tusername, err := GetUsername()", "\tif err != nil {", "\t\treturn \"\", fmt.Errorf(\"failed to get username: %w\", err)", "\t}", "", "\tprojectName, err := GetProjectName()", "\tif err != nil {", "\t\treturn \"\", fmt.Errorf(\"failed to get project name: %w\", err)", "\t}", "", "\thomeDir, err := os.UserHomeDir()", "\tif err != nil {", "\t\treturn \"\", fmt.Errorf(\"failed to get home directory: %w\", err)", "\t}", "", "\t// Source path: ~/.kanuka/keys/{project_name}.pub", "\tsourceKeyPath := filepath.Join(homeDir, \".kanuka\", \"keys\", projectName+\".pub\")", "", "\t// Check if source key exists", "\tif _, err := os.Stat(sourceKeyPath); err != nil {", "\t\tif os.IsNotExist(err) {", "\t\t\treturn \"\", fmt.Errorf(\"public key for project %s not found at %s\", projectName, sourceKeyPath)", "\t\t}", "\t\treturn \"\", fmt.Errorf(\"failed to check for source key: %w\", err)", "\t}", "", "\tworkingDir, err := os.Getwd()", "\tif err != nil {", "\t\treturn \"\", fmt.Errorf(\"failed to get working directory: %w\", err)", "\t}", "", "\t// Destination directory: {project_path}/.kanuka/public_keys/{username}.pub", "\tdestKeyPath := filepath.Join(workingDir, \".kanuka\", \"public_keys\", username+\".pub\")", "", "\tkeyData, err := os.ReadFile(sourceKeyPath)", "\tif err != nil {", "\t\treturn \"\", fmt.Errorf(\"failed to read source key file: %w\", err)", "\t}", "", "\t// Write to destination file", "\tif err := os.WriteFile(destKeyPath, keyData, 0600); err != nil {", "\t\treturn \"\", fmt.Errorf(\"failed to write key to project: %w\", err)", "\t}", "", "\treturn destKeyPath, nil", "}", "", "// GetUserProjectKanukaKey retrieves the encrypted symmetric key for the current user and project.", "func GetUserProjectKanukaKey() ([]byte, error) {", "\tusername, err := GetUsername()", "\tif err != nil {", "\t\treturn nil, fmt.Errorf(\"failed to get username: %w\", err)", "\t}", "\tuserKeyFile := filepath.Join(\".kanuka\", \"secrets\", fmt.Sprintf(\"%s.kanuka\", username))", "\tif _, err := os.Stat(userKeyFile); os.IsNotExist(err) {", "\t\treturn nil, fmt.Errorf(\"failed to get user's project encrypted symmetric key: %w\", err)", "\t}", "\tencryptedSymmetricKey, err := os.ReadFile(userKeyFile)", "\tif err != nil {", "\t\treturn nil, fmt.Errorf(\"failed to read user's project encrypted symmetric key: %w\", err)", "\t}", "", "\treturn encryptedSymmetricKey, nil", "}"], "file_path": "internal/secrets/manager.go"}
{"Link_to_commit": "https://github.com/gopinathsjsu/team-project-20202-paladins/commit/7f7b9edd1e8c6b53f439bcd0ca24bca06e8feae6", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 64, "n_files_impacted": 9, "longest_chunk": ["        // Save the reservation first", "        Reservation savedReservation = reservationRepository.save(reservation);", "", "        // --- Send Confirmation Email ---", "        try {", "            // Fetch User details to get email", "            Optional<User> userOpt = userRepository.findById(savedReservation.getCustomerId().toHexString()); //", "", "            // Fetch Restaurant details", "            Optional<Restaurant> restaurantOpt = restaurantRepository.findById(savedReservation.getRestaurantId().toHexString()); //", "", "            // Fetch Table details", "            Optional<Table> tableOpt = tableRepository.findById(savedReservation.getTableId().toHexString()); //", "", "", "            if (userOpt.isPresent() && restaurantOpt.isPresent() && tableOpt.isPresent()) {", "                User user = userOpt.get();", "                Restaurant restaurant = restaurantOpt.get();", "                Table table = tableOpt.get();", "", "                String recipientEmail = user.getEmail(); //", "                String subject = \"Your Booking Confirmation at \" + restaurant.getName(); //", "", "                // Compose a simple email body", "                String messageBody = String.format(", "                        \"Dear %s,\\n\\n\" +", "                                \"Your booking is confirmed!\\n\\n\" +", "                                \"Restaurant: %s\\n\" +", "                                \"Address: %s, %s\\n\" + //", "                                \"Table Number: %s\\n\" + //", "                                \"Date: %s\\n\" + //", "                                \"Time: %s - %s\\n\" + //", "                                \"Party Size: %d\\n\\n\" + //", "                                \"Reservation ID: %s\\n\\n\" + //", "                                \"Thank you for using BookTable!\",", "                        user.getName(), //", "                        restaurant.getName(), //", "                        restaurant.getAddressStreet(), restaurant.getAddressCity(), //", "                        table.getTableNumber(), //", "                        savedReservation.getDate().toString(), //", "                        savedReservation.getStartSlotTime().toString(), //", "                        savedReservation.getEndSlotTime().toString(), //", "                        savedReservation.getPartySize(), //", "                        savedReservation.getId().toHexString() //", "                );", "", "                // Send the email using the injected service", "                mailjetEmailService.sendEmail(recipientEmail, subject, messageBody); //", "                System.out.println(\"Booking confirmation email sent to \" + recipientEmail);", "", "            } else {", "                // Log if user, restaurant, or table details are missing", "                if (!userOpt.isPresent()) log.error(\"Could not find user with ID: {}\", savedReservation.getCustomerId());", "                if (!restaurantOpt.isPresent()) log.error(\"Could not find restaurant with ID: {}\", savedReservation.getRestaurantId());", "                if (!tableOpt.isPresent()) log.error(\"Could not find table with ID: {}\", savedReservation.getTableId());", "            }", "", "        } catch (Exception e) {", "            // Log the error, but don't necessarily fail the entire booking process", "            log.error(\"Failed to send booking confirmation email: {}\", e.getMessage(), e);", "        }", "        // --- End of Email Sending Logic ---", "", "        return savedReservation; // Return the saved reservation object"], "file_path": "booktable-backend/src/main/java/com/booktable/service/ReservationService.java"}
{"Link_to_commit": "https://github.com/github/safe-settings/commit/ff7a65655d33006b9820479df0a9e1057a8927e4", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 19, "n_files_impacted": 1, "longest_chunk": ["        const RepoPlugin = Settings.PLUGINS.repository", "        return new RepoPlugin(this.nop, this.github, repo, repoConfig, this.installation_id, this.log, this.errors).sync().then(res => {", "          this.appendToResults(res)", "          return Promise.all(", "            childPlugins.map(([Plugin, config]) => {", "              return new Plugin(this.nop, this.github, repo, config, this.log, this.errors).sync()", "            }))", "        }).then(res => {", "          this.appendToResults(res)", "        })", "      } catch (e) {", "        if (this.nop) {", "          const nopcommand = new NopCommand(this.constructor.name, this.repo, null, `${e}`, 'ERROR')", "          this.log.error(`NOPCOMMAND ${JSON.stringify(nopcommand)}`)", "          this.appendToResults([nopcommand])", "          // throw e", "        } else {", "          throw e", "        }"], "file_path": "lib/settings.js"}
{"Link_to_commit": "https://github.com/XengShi/materialYouNewTab/commit/83d9eac323ab8b8a77a1a3289654354619f142d6", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 8, "n_files_impacted": 2, "longest_chunk": ["                    // Range separator for min-max temperature", "                    const rangeSeparator = {", "                        cs: \"a\u017e\",", "                        // Add more languages as needed", "                        default: \"~\"", "                    };", "                    const separator = rangeSeparator[currentLanguage] || rangeSeparator.default;", ""], "file_path": "scripts/weather.js"}
{"Link_to_commit": "https://github.com/gopinathsjsu/team-project-20202-paladins/commit/f143ce9f070d3e18e1fcd493c652ea2247f6b94d", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 35, "n_files_impacted": 12, "longest_chunk": ["", "  // This callback is triggered by the hook when a user selects a place", "  const handlePlaceSelected = useCallback((place) => {", "    console.log('Raw Place Selected:', place);", "    const { city, state } = extractCityState(place);", "    let locationString = '';", "", "    if (city && state) {", "      locationString = `${city}, ${state}`;", "    } else {", "      locationString = place?.formatted_address || place?.name || '';", "      console.warn(\"Could not extract 'City, ST', using fallback:\", locationString);", "    }", "", "    console.log('Place Selected -> Calling onChange with:', locationString);", "    onChange(locationString);", "  }, [onChange]);", "", "  // Initialize the Google Places Widget hook", "  const { ref: placesRef } = usePlacesWidget({", "    apiKey: GOOGLE_PLACES_API_KEY,", "    onPlaceSelected: handlePlaceSelected,", "    options: {", "      types: [\"(regions)\"],", "      componentRestrictions: { country: \"us\" },", "      fields: [\"formatted_address\", \"address_components\", \"name\", \"geometry\"],", "    },", "  });", "", "  useEffect(() => {", "    if (placesRef.current && value === '' && placesRef.current.value !== '') {", "      placesRef.current.value = '';", "    }", "  }, [value, placesRef]);", ""], "file_path": "frontend/src/components/layout/LocationSearch.js"}
{"Link_to_commit": "https://github.com/livingbio/typed-ffmpeg/commit/b9bf741305a58db8f308b1bd6b010d0a48bf99bc", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 9, "n_files_impacted": 1, "longest_chunk": ["", "    This function is used by the `serializable` decorator to register classes", "    with the serialization system, enabling them to be serialized and deserialized.", "", "    Args:", "        cls: The class to register", "", "    Returns:", "        The class itself"], "file_path": "src/ffmpeg/common/serialize.py"}
{"Link_to_commit": "https://github.com/PhillipsAuctionHouse/seldon/commit/ef45feab3a09f28f9d6d67bcbb4964e9c8b31572", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 159, "n_files_impacted": 10, "longest_chunk": ["@use '#scss/allPartials' as *;", "", ".#{$px}-favorites-collection-tile {", "  display: flex;", "  flex-direction: column;", "  max-width: 24.5rem;", "  text-align: left;", "", "  &__content {", "    display: flex;", "    flex-direction: column;", "    height: 100%;", "  }", "", "  &__header {", "    align-items: flex-end;", "    display: flex;", "    justify-content: space-between;", "    margin-bottom: $spacing-sm;", "  }", "", "  &__info {", "    flex: 1;", "    overflow: hidden;", "  }", "", "  &__count {", "    color: $dark-gray;", "    display: block;", "    margin-bottom: $spacing-xsm;", "  }", "", "  h3.#{$px}-text {", "    margin-bottom: 0;", "  }", "", "  &__title {", "    -webkit-box-orient: vertical;", "    display: block;", "    display: -webkit-box;", "    -webkit-line-clamp: 1;", "    line-clamp: 1;", "    overflow: hidden;", "", "    h3.#{$px}-text {", "      margin-bottom: 0;", "    }", "  }", "", "  &__actions {", "    align-items: center;", "    align-self: normal;", "    background-position: center;", "    border-radius: 50%;", "    display: flex;", "    height: 2.5rem;", "    justify-content: center;", "    margin-left: $spacing-sm;", "    transition: background 0.8s;", "    width: 2.5rem;", "", "    &:hover {", "      background: $light-gray radial-gradient(circle, transparent 1%, $light-gray 1%) center/15000%;", "      background-color: $light-gray;", "      cursor: pointer;", "    }", "", "    &:active {", "      background-color: $medium-gray;", "      background-size: 100%;", "      transition: background 0s;", "    }", "  }", "", "  &__popover-content {", "    outline: none;", "    transform: translateX(2.5rem);", "  }", "", "  &__dropdown {", "    box-shadow: 0 4px 6px $medium-gray;", "", "    &--item {", "      all: unset;", "      background-color: $soft-gray;", "      cursor: pointer;", "      display: block;", "      padding: 0.75rem 1rem;", "      width: 90px;", "", "      &:hover,", "      &:focus-visible {", "        background-color: $light-gray;", "      }", "    }", "  }", "", "  &__media-link {", "    &:hover {", "      color: inherit;", "      text-decoration: none;", "    }", "  }", "", "  &__media-container {", "    position: relative;", "    width: 100%;", "  }", "", "  &__media {", "    background-position: center;", "    width: 100%;", "  }", "", "  &__empty {", "    aspect-ratio: 1 / 1;", "    background-position: center center;", "    background-repeat: no-repeat;", "    background-size: cover;", "    cursor: pointer;", "    display: block;", "    position: relative;", "", "    &--favorites {", "      background-color: $soft-gray;", "    }", "", "    &--list {", "      border: 1px solid #0000001f;", "      border-radius: 0;", "    }", "", "    &__content {", "      bottom: 20px;", "      left: 20px;", "      position: absolute;", "    }", "  }", "", "  &__icon {", "    grid-column: 1;", "    justify-self: start;", "", "    &-rotate {", "      transform: rotate(90deg);", "    }", "  }", "", "  &__text {", "    color: $dark-gray;", "    font-size: $body-size3;", "    font-variation-settings: 'wght' 400;", "    padding-top: 5px;", "  }", "", "  [data-radix-popper-content-wrapper] > * {", "    all: unset;", "  }", "}"], "file_path": "src/patterns/FavoritesCollectionTile/index.ts"}
{"Link_to_commit": "https://github.com/neo-project/neo/commit/064fb923fef128890b5553fb8fc8c20d003197c3", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 17, "n_files_impacted": 12, "longest_chunk": ["            var urlSize = url.GetStrictUtf8ByteCount();", "            if (urlSize > MaxUrlLength)", "                throw new ArgumentException($\"The url bytes size({urlSize}) cannot be greater than {MaxUrlLength}.\");", "", "            var filterSize = filter.GetStrictUtf8ByteCount();", "            if (filterSize > MaxFilterLength)", "                throw new ArgumentException($\"The filter bytes size({filterSize}) cannot be greater than {MaxFilterLength}.\");", "", "            var callbackSize = callback is null ? 0 : callback.GetStrictUtf8ByteCount();", "            if (callbackSize > MaxCallbackLength)", "                throw new ArgumentException($\"The callback bytes size({callbackSize}) cannot be greater than {MaxCallbackLength}.\");", "", "            if (callback.StartsWith('_'))", "                throw new ArgumentException($\"The callback cannot start with '_'.\");", "", "            if (gasForResponse < 0_10000000)", "                throw new ArgumentException($\"The gasForResponse({gasForResponse}) must be greater than or equal to 0.1 datoshi.\");"], "file_path": "src/Neo/SmartContract/Native/OracleContract.cs"}
{"Link_to_commit": "https://github.com/hapifhir/hapi-fhir-jpaserver-starter/commit/88ed318e7610897abbf7956b38ace7947bed2634", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 13, "n_files_impacted": 1, "longest_chunk": ["\t\t// also check for JPA properties set as environment variables, this is slightly hacky and doesn't cover all", "\t\t// the naming conventions Springboot allows", "\t\t// but there doesn't seem to be a better/deterministic way to get these properties when they are set as ENV", "\t\t// variables and this at least provides", "\t\t// a way to set them (in a docker container, for instance)", "\t\tMap<String, Object> jpaPropsEnv = getPropertiesStartingWith(environment, \"SPRING_JPA_PROPERTIES\");", "\t\tfor (Map.Entry<String, Object> entry : jpaPropsEnv.entrySet()) {", "\t\t\tString strippedKey = entry.getKey().replace(\"SPRING_JPA_PROPERTIES_\", \"\");", "\t\t\tstrippedKey = strippedKey.replaceAll(\"_\", \".\");", "\t\t\tstrippedKey = strippedKey.toLowerCase();", "\t\t\tproperties.put(strippedKey, entry.getValue().toString());", "\t\t}", ""], "file_path": "src/main/java/ca/uhn/fhir/jpa/starter/util/EnvironmentHelper.java"}
{"Link_to_commit": "https://github.com/m0-foundation/solana-m/commit/6bd4839b1a91427567966f79e924580c500b942b", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 102, "n_files_impacted": 4, "longest_chunk": ["  program", "    .command('update-earn-lut')", "    .description('Create or update the LUT for common addresses')", "    .option('-a, --address [pubkey]', 'Address of table to update', 'HtKQ9sHyMhun73asZsARkGCc1fDz2dQH7QhGfFJcQo7S')", "    .action(async ({ address }) => {", "      const [owner] = keysFromEnv(['OWNER_KEYPAIR']);", "      const ixs = [ComputeBudgetProgram.setComputeUnitPrice({ microLamports: 250_000 })];", "", "      // Get or create LUT", "      let tableAddress: PublicKey;", "      if (address) {", "        tableAddress = new PublicKey(address);", "      } else {", "        const [lookupTableIx, lookupTableAddress] = AddressLookupTableProgram.createLookupTable({", "          authority: owner.publicKey,", "          payer: owner.publicKey,", "          recentSlot: (await connection.getSlot({ commitment: 'finalized' })) - 10,", "        });", "", "        console.log(`Creating lookup table: ${lookupTableAddress.toBase58()}`);", "        tableAddress = lookupTableAddress;", "        ixs.push(lookupTableIx);", "      }", "", "      // Addresses to add to LUT", "      const [mMint, wmMint, multisig] = keysFromEnv(['M_MINT_KEYPAIR', 'WM_MINT_KEYPAIR', 'M_MINT_MULTISIG_KEYPAIR']);", "      const [portalTokenAuthPda] = PublicKey.findProgramAddressSync([Buffer.from('token_authority')], PROGRAMS.portal);", "      const [earnTokenAuthPda] = PublicKey.findProgramAddressSync([Buffer.from('token_authority')], PROGRAMS.earn);", "      const [mVaultPda] = PublicKey.findProgramAddressSync([Buffer.from('m_vault')], PROGRAMS.extEarn);", "      const [mintAuthPda] = PublicKey.findProgramAddressSync([Buffer.from('mint_authority')], PROGRAMS.extEarn);", "      const [global] = PublicKey.findProgramAddressSync([Buffer.from('global')], PROGRAM_ID);", "      const [extGlobal] = PublicKey.findProgramAddressSync([Buffer.from('global')], EXT_PROGRAM_ID);", "", "      const addressesForTable = [", "        PROGRAMS.portal,", "        PROGRAMS.earn,", "        PROGRAMS.extEarn,", "        mMint.publicKey,", "        wmMint.publicKey,", "        multisig.publicKey,", "        portalTokenAuthPda,", "        earnTokenAuthPda,", "        mVaultPda,", "        mintAuthPda,", "        global,", "        extGlobal,", "      ];", "", "      // Fetch current state of LUT", "      let existingAddresses: PublicKey[] = [];", "      if (address) {", "        const state = (await connection.getAddressLookupTable(tableAddress)).value?.state.addresses;", "        if (!state) {", "          throw new Error(`Failed to fetch state for address lookup table ${tableAddress}`);", "        }", "        if (state.length === 256) {", "          throw new Error('LUT is full');", "        }", "", "        existingAddresses = state;", "      }", "", "      // Dedupe missing addresses", "      const toAdd = addressesForTable.filter((address) => !existingAddresses.find((a) => a.equals(address)));", "      if (toAdd.length === 0) {", "        console.log('No addresses to add');", "        return;", "      }", "", "      ixs.push(", "        AddressLookupTableProgram.extendLookupTable({", "          payer: owner.publicKey,", "          authority: owner.publicKey,", "          lookupTable: tableAddress,", "          addresses: toAdd,", "        }),", "      );", "", "      // Send transaction", "      const blockhash = await connection.getLatestBlockhash('finalized');", "", "      const messageV0 = new TransactionMessage({", "        payerKey: owner.publicKey,", "        recentBlockhash: blockhash.blockhash,", "        instructions: ixs,", "      }).compileToV0Message();", "", "      const transaction = new VersionedTransaction(messageV0);", "      transaction.sign([owner]);", "      const txid = await connection.sendTransaction(transaction);", "      console.log('Transaction sent:', txid);", "", "      // Confirm", "      const confirmation = await connection.confirmTransaction(", "        { signature: txid, blockhash: blockhash.blockhash, lastValidBlockHeight: blockhash.lastValidBlockHeight },", "        'confirmed',", "      );", "      if (confirmation.value.err) {", "        throw new Error(`Transaction not confirmed: ${confirmation.value.err}`);", "      }", "    });", ""], "file_path": "services/cli/main.ts"}
{"Link_to_commit": "https://github.com/AlistGo/alist/commit/41bdab49aa8acca9e88862c3db55cd7a8a84ba6a", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 58, "n_files_impacted": 3, "longest_chunk": ["", "func (d *Yun139) requestRoute(data interface{}, resp interface{}) ([]byte, error) {", "\turl := \"https://user-njs.yun.139.com/user/route/qryRoutePolicy\"", "\treq := base.RestyClient.R()", "\trandStr := random.String(16)", "\tts := time.Now().Format(\"2006-01-02 15:04:05\")", "\tcallback := func(req *resty.Request) {", "\t\treq.SetBody(data)", "\t}", "\tif callback != nil {", "\t\tcallback(req)", "\t}", "\tbody, err := utils.Json.Marshal(req.Body)", "\tif err != nil {", "\t\treturn nil, err", "\t}", "\tsign := calSign(string(body), ts, randStr)", "\tsvcType := \"1\"", "\tif d.isFamily() {", "\t\tsvcType = \"2\"", "\t}", "\treq.SetHeaders(map[string]string{", "\t\t\"Accept\":         \"application/json, text/plain, */*\",", "\t\t\"CMS-DEVICE\":     \"default\",", "\t\t\"Authorization\":  \"Basic \" + d.getAuthorization(),", "\t\t\"mcloud-channel\": \"1000101\",", "\t\t\"mcloud-client\":  \"10701\",", "\t\t//\"mcloud-route\": \"001\",", "\t\t\"mcloud-sign\": fmt.Sprintf(\"%s,%s,%s\", ts, randStr, sign),", "\t\t//\"mcloud-skey\":\"\",", "\t\t\"mcloud-version\":         \"7.14.0\",", "\t\t\"Origin\":                 \"https://yun.139.com\",", "\t\t\"Referer\":                \"https://yun.139.com/w/\",", "\t\t\"x-DeviceInfo\":           \"||9|7.14.0|chrome|120.0.0.0|||windows 10||zh-CN|||\",", "\t\t\"x-huawei-channelSrc\":    \"10000034\",", "\t\t\"x-inner-ntwk\":           \"2\",", "\t\t\"x-m4c-caller\":           \"PC\",", "\t\t\"x-m4c-src\":              \"10002\",", "\t\t\"x-SvcType\":              svcType,", "\t\t\"Inner-Hcy-Router-Https\": \"1\",", "\t})", "", "\tvar e BaseResp", "\treq.SetResult(&e)", "\tres, err := req.Execute(http.MethodPost, url)", "\tlog.Debugln(res.String())", "\tif !e.Success {", "\t\treturn nil, errors.New(e.Message)", "\t}", "\tif resp != nil {", "\t\terr = utils.Json.Unmarshal(res.Body(), resp)", "\t\tif err != nil {", "\t\t\treturn nil, err", "\t\t}", "\t}", "\treturn res.Body(), nil", "}", ""], "file_path": "drivers/139/util.go"}
{"Link_to_commit": "https://github.com/MethodicalAcceleratorDesign/MAD-NG.py/commit/81f72af04c72e550ddd827c35d7457aeac52a097", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 183, "n_files_impacted": 33, "longest_chunk": ["# Communication with MAD-NG", "", "```{contents}", ":depth: 2", ":local:", "```", "", "## Protocol Overview", "", "PyMAD-NG communicates with MAD-NG using a **pipe-based protocol**, ensuring efficient, direct, two-way communication between the Python and MAD-NG processes.", "", "Key points:", "", "- Data is sent through FIFO pipes (first-in, first-out).", "- Commands are sent as MAD-NG script strings (Lua-like).", "- Data is retrieved via `{func}`MAD.recv`()` after explicit instruction to send it.", "- MAD-NG stdout is redirected to Python, but not intercepted.", "", "```{important}", "You must always **send instructions before sending data**, and **send a request before receiving data**.", "```", "", "### Example: Basic Communication", "```python", "from pymadng import MAD", "mad = MAD()", "", "mad.send(\"a = py:recv()\")   # Tell MAD-NG to receive", "mad.send(42)                # Send the value", "mad.send(\"py:send(a)\")     # Request it back", "mad.recv()                  # Receive the value \u2192 42", "```", "", "Both {func}`MAD.send` and {func}`MAD.recv` are the core communication methods.", "See the {class}`pymadng.MAD` reference for more details.", "", "---", "", "## Supported Data Types", "", "The following types can be sent from Python to MAD-NG:", "", "```{list-table} Supported Send Types", ":header-rows: 1", "", "* - Python Type", "  - MAD-NG Type", "  - Function to Use", "* - `None`, `str`, `int`, `float`, `complex`, `bool`, `list`", "  - Various", "  - {func}`MAD.send`", "* - `numpy.ndarray (float64)`", "  - `matrix`", "  - {func}`MAD.send`", "* - `numpy.ndarray (complex128)`", "  - `cmatrix`", "  - {func}`MAD.send`", "* - `numpy.ndarray (int32)`", "  - `imatrix`", "  - {func}`MAD.send`", "* - `range`", "  - `irange`", "  - {func}`MAD.send`", "* - `start, stop, size` as float, int", "  - `range`, `logrange`", "  - `mad.send_rng()`, `mad.send_lrng()`", "* - Complex structures (e.g., TPSA, CTPSA)", "  - `TPSA`, `CTPSA`", "  - `mad.send_tpsa()`, `mad.send_ctpsa()`", "```", "", "For full compatibility, see the {mod}`pymadng.MAD` documentation.", "", "---", "", "## Converting TFS Tables to DataFrames", "", "If you use {func}`twiss` or {func}`survey`, MAD-NG returns an `mtable`, which can be converted to a Pandas or TFS-style DataFrame:", "", "```python", "mtbl = mad.twiss(...)", "df = mtbl.to_df()  # Either DataFrame or TfsDataFrame", "```", "", "If the object is not an `mtable`, a `TypeError` will be raised.", "", "```{note}", "`tfs-pandas` (if installed) will enhance the output with headers and metadata.", "```", "", "See:", "```{literalinclude} ../../examples/ex-ps-twiss/ps-twiss.py", ":lines: 18, 24, 41-49", ":linenos:", "```", "", "---", "", "## Avoiding Deadlocks", "", "Deadlocks can occur if Python and MAD-NG wait on each other to send/receive large data without syncing.", "", "### Example of a Deadlock", "```python", "mad.send('arr = py:recv()')", "mad.send(arr0)                 # Large matrix", "mad.send('py:send(arr)')       # Sends data to Python", "mad.send('arr2 = py:recv()')   # Asks for new data", "mad.send(arr2)                 # DEADLOCK if previous data not yet received", "```", "", "```{warning}", "Always ensure each {func}`MAD.send` has a matching {func}`MAD.recv` if data is expected back.", "```", "", "---", "", "## Scope: Local vs Global", "", "MAD-NG uses Lua-style scoping:", "", "- Variables declared with `local` are temporary.", "- Variables without `local` persist across {func}`MAD.send` calls.", "", "### Example:", "```python", "mad.send(\"\"\"", "a = 10", "local b = 20", "print(a + b)", "\"\"\")", "", "mad.send(\"print(a + (b or 5))\")  # b is nil \u2192 10 + 5 = 15", "```", "", "```{tip}", "Use `local` to avoid polluting the global MAD-NG namespace.", "```", "", "---", "", "## Customising the Environment", "", "You can configure the `MAD()` instance with options to better suit your environment:", "", "### Change the Python alias used inside MAD-NG:", "```python", "mad = MAD(py_name=\"python\")", "```", "", "### Specify a custom MAD-NG binary:", "```python", "mad = MAD(mad_path=\"/custom/path/to/mad\")", "```", "", "### Enable debug mode:", "```python", "mad = MAD(debug=True)", "```", "", "### Increase the number of temporary variables:", "```python", "mad = MAD(num_temp_vars=10)", "```", "", "See {meth}`pymadng.MAD.__init__` for all configuration options.", "", "---", "", "```{eval-rst}", ".. currentmodule:: pymadng", "```", "", "## Summary", "", "- Always match {func}`MAD.send` with {func}`MAD.recv` when data is expected.", "- Use `mad.to_df()` for table conversion.", "- Avoid deadlocks by receiving before sending again.", "- Manage scope using `local` wisely.", "- Use configuration flags to tailor behavior.", "", "For more, see the {doc}`advanced_features`, {doc}`debugging`, and {doc}`function_reference` sections.", ""], "file_path": "docs/source/conf.py"}
{"Link_to_commit": "https://github.com/StormDev771/dify/commit/c6ab0294f1866b5e60bc39fd9a52c98f179bb0e0", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 233, "n_files_impacted": 2, "longest_chunk": ["", "/**", " * Preprocesses mermaid code to fix common syntax issues", " */", "export function preprocessMermaidCode(code: string): string {", "  if (!code || typeof code !== 'string')", "    return ''", "", "  // First check if this is a gantt chart", "  if (code.trim().startsWith('gantt')) {", "    // For gantt charts, we need to ensure each task is on its own line", "    // Split the code into lines and process each line separately", "    const lines = code.split('\\n').map(line => line.trim())", "    return lines.join('\\n')", "  }", "", "  return code", "    // Replace English colons with Chinese colons in section nodes to avoid parsing issues", "    .replace(/section\\s+([^:]+):/g, (match, sectionName) => `section ${sectionName}\uff1a`)", "    // Fix common syntax issues", "    .replace(/fifopacket/g, 'rect')", "    // Ensure graph has direction", "    .replace(/^graph\\s+((?:TB|BT|RL|LR)*)/, (match, direction) => {", "      return direction ? match : 'graph TD'", "    })", "    // Clean up empty lines and extra spaces", "    .trim()", "}", "", "/**", " * Prepares mermaid code based on selected style", " */", "export function prepareMermaidCode(code: string, style: 'classic' | 'handDrawn'): string {", "  let finalCode = preprocessMermaidCode(code)", "", "  // Special handling for gantt charts", "  if (finalCode.trim().startsWith('gantt')) {", "    // For gantt charts, preserve the structure exactly as is", "    return finalCode", "  }", "", "  if (style === 'handDrawn') {", "    finalCode = finalCode", "      // Remove style definitions that interfere with hand-drawn style", "      .replace(/style\\s+[^\\n]+/g, '')", "      .replace(/linkStyle\\s+[^\\n]+/g, '')", "      .replace(/^flowchart/, 'graph')", "      // Remove any styles that might interfere with hand-drawn style", "      .replace(/class=\"[^\"]*\"/g, '')", "      .replace(/fill=\"[^\"]*\"/g, '')", "      .replace(/stroke=\"[^\"]*\"/g, '')", "", "    // Ensure hand-drawn style charts always start with graph", "    if (!finalCode.startsWith('graph') && !finalCode.startsWith('flowchart'))", "      finalCode = `graph TD\\n${finalCode}`", "  }", "", "  return finalCode", "}", "", "/**", " * Converts SVG to base64 string for image rendering", " */", "export function svgToBase64(svgGraph: string): Promise<string> {", "  if (!svgGraph)", "    return Promise.resolve('')", "", "  try {", "    // Ensure SVG has correct XML declaration", "    if (!svgGraph.includes('<?xml'))", "      svgGraph = `<?xml version=\"1.0\" encoding=\"UTF-8\"?>${svgGraph}`", "", "    const blob = new Blob([new TextEncoder().encode(svgGraph)], { type: 'image/svg+xml;charset=utf-8' })", "    return new Promise((resolve, reject) => {", "      const reader = new FileReader()", "      reader.onloadend = () => resolve(reader.result as string)", "      reader.onerror = reject", "      reader.readAsDataURL(blob)", "    })", "  }", "  catch (error) {", "    console.error('Error converting SVG to base64:', error)", "    return Promise.resolve('')", "  }", "}", "", "/**", " * Processes SVG for theme styling", " */", "export function processSvgForTheme(", "  svg: string,", "  isDark: boolean,", "  isHandDrawn: boolean,", "  themes: {", "    light: any", "    dark: any", "  },", "): string {", "  let processedSvg = svg", "", "  if (isDark) {", "    processedSvg = processedSvg", "      .replace(/style=\"fill: ?#000000\"/g, 'style=\"fill: #e2e8f0\"')", "      .replace(/style=\"stroke: ?#000000\"/g, 'style=\"stroke: #94a3b8\"')", "      .replace(/<rect [^>]*fill=\"#ffffff\"/g, '<rect $& fill=\"#1e293b\"')", "", "    if (isHandDrawn) {", "      processedSvg = processedSvg", "        .replace(/fill=\"#[a-fA-F0-9]{6}\"/g, `fill=\"${themes.dark.nodeColors[0].bg}\"`)", "        .replace(/stroke=\"#[a-fA-F0-9]{6}\"/g, `stroke=\"${themes.dark.connectionColor}\"`)", "        .replace(/stroke-width=\"1\"/g, 'stroke-width=\"1.5\"')", "    }", "    else {", "      let i = 0", "      themes.dark.nodeColors.forEach(() => {", "        const regex = /fill=\"#[a-fA-F0-9]{6}\"[^>]*class=\"node-[^\"]*\"/g", "        processedSvg = processedSvg.replace(regex, (match: string) => {", "          const colorIndex = i % themes.dark.nodeColors.length", "          i++", "          return match.replace(/fill=\"#[a-fA-F0-9]{6}\"/, `fill=\"${themes.dark.nodeColors[colorIndex].bg}\"`)", "        })", "      })", "", "      processedSvg = processedSvg", "        .replace(/<path [^>]*stroke=\"#[a-fA-F0-9]{6}\"/g,", "          `<path stroke=\"${themes.dark.connectionColor}\" stroke-width=\"1.5\"`)", "        .replace(/<(line|polyline) [^>]*stroke=\"#[a-fA-F0-9]{6}\"/g,", "          `<$1 stroke=\"${themes.dark.connectionColor}\" stroke-width=\"1.5\"`)", "    }", "  }", "  else {", "    if (isHandDrawn) {", "      processedSvg = processedSvg", "        .replace(/fill=\"#[a-fA-F0-9]{6}\"/g, `fill=\"${themes.light.nodeColors[0].bg}\"`)", "        .replace(/stroke=\"#[a-fA-F0-9]{6}\"/g, `stroke=\"${themes.light.connectionColor}\"`)", "        .replace(/stroke-width=\"1\"/g, 'stroke-width=\"1.5\"')", "    }", "    else {", "      themes.light.nodeColors.forEach(() => {", "        const regex = /fill=\"#[a-fA-F0-9]{6}\"[^>]*class=\"node-[^\"]*\"/g", "        let i = 0", "        processedSvg = processedSvg.replace(regex, (match: string) => {", "          const colorIndex = i % themes.light.nodeColors.length", "          i++", "          return match.replace(/fill=\"#[a-fA-F0-9]{6}\"/, `fill=\"${themes.light.nodeColors[colorIndex].bg}\"`)", "        })", "      })", "", "      processedSvg = processedSvg", "        .replace(/<path [^>]*stroke=\"#[a-fA-F0-9]{6}\"/g,", "          `<path stroke=\"${themes.light.connectionColor}\"`)", "        .replace(/<(line|polyline) [^>]*stroke=\"#[a-fA-F0-9]{6}\"/g,", "          `<$1 stroke=\"${themes.light.connectionColor}\"`)", "    }", "  }", "", "  return processedSvg", "}", "", "/**", " * Checks if mermaid code is complete and valid", " */", "export function isMermaidCodeComplete(code: string): boolean {", "  if (!code || code.trim().length === 0)", "    return false", "", "  try {", "    const trimmedCode = code.trim()", "", "    // Special handling for gantt charts", "    if (trimmedCode.startsWith('gantt')) {", "      // For gantt charts, check if it has at least a title and one task", "      const lines = trimmedCode.split('\\n').filter(line => line.trim().length > 0)", "      return lines.length >= 3", "    }", "", "    // Check for basic syntax structure", "    const hasValidStart = /^(graph|flowchart|sequenceDiagram|classDiagram|classDef|class|stateDiagram|gantt|pie|er|journey|requirementDiagram)/.test(trimmedCode)", "", "    // Check for balanced brackets and parentheses", "    const isBalanced = (() => {", "      const stack = []", "      const pairs = { '{': '}', '[': ']', '(': ')' }", "", "      for (const char of trimmedCode) {", "        if (char in pairs) {", "          stack.push(char)", "        }", "        else if (Object.values(pairs).includes(char)) {", "          const last = stack.pop()", "          if (pairs[last as keyof typeof pairs] !== char)", "            return false", "        }", "      }", "", "      return stack.length === 0", "    })()", "", "    // Check for common syntax errors", "    const hasNoSyntaxErrors = !trimmedCode.includes('undefined')", "                           && !trimmedCode.includes('[object Object]')", "                           && trimmedCode.split('\\n').every(line =>", "                             !(line.includes('-->') && !line.match(/\\S+\\s*-->\\s*\\S+/)))", "", "    return hasValidStart && isBalanced && hasNoSyntaxErrors", "  }", "  catch (error) {", "    console.debug('Mermaid code validation error:', error)", "    return false", "  }", "}", "", "/**", " * Helper to wait for DOM element with retry mechanism", " */", "export function waitForDOMElement(callback: () => Promise<any>, maxAttempts = 3, delay = 100): Promise<any> {", "  return new Promise((resolve, reject) => {", "    let attempts = 0", "    const tryRender = async () => {", "      try {", "        resolve(await callback())", "      }", "      catch (error) {", "        attempts++", "        if (attempts < maxAttempts)", "          setTimeout(tryRender, delay)", "        else", "          reject(error)", "      }", "    }", "    tryRender()", "  })", "}"], "file_path": "web/app/components/base/mermaid/utils.ts"}
{"Link_to_commit": "https://github.com/inz-inside-job/inside-job/commit/24134022d4b1d9a4458d9b82a58fe66d954f75fc", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 256, "n_files_impacted": 10, "longest_chunk": ["import { SharePopup } from '@/components/share-popup';", "import { Button } from '@/components/ui/button';", "import { Card, CardContent } from '@/components/ui/card';", "import { Separator } from '@/components/ui/separator';", "import { Tabs, TabsContent, TabsList, TabsTrigger } from '@/components/ui/tabs';", "import { moneyToHuman } from '@/lib/utils';", "import { Head, Link } from '@inertiajs/react';", "import { Briefcase, Building2, Clock, ExternalLink, MapPin, Share2, Star } from 'lucide-react';", "import { useState } from 'react';", "", "export default function JobPage({ job }: { job: App.Data.Jobs.JobData }) {", "    const [showSharePopup, setShowSharePopup] = useState(false);", "    return (", "        <>", "            <Head title={job.title} />", "            <SharePopup url={route('jobs.show', { slug: job.slug })} open={showSharePopup} onClose={() => setShowSharePopup(false)} />", "            <div className=\"bg-background min-h-screen py-8\">", "                <div className=\"container mx-auto px-4\">", "                    <div className=\"mx-auto max-w-5xl\">", "                        {/* Job Header */}", "                        <Card className=\"mb-6 shadow-md\">", "                            <CardContent className=\"p-6\">", "                                <div className=\"flex flex-col gap-6 md:flex-row\">", "                                    <img", "                                        src={job.company.logo || '/placeholder.svg'}", "                                        alt={`${job.company.name} logo`}", "                                        className=\"h-16 w-16 rounded-md border object-contain\"", "                                    />", "                                    <div className=\"flex-1\">", "                                        <div className=\"flex flex-col gap-4 md:flex-row md:items-center md:justify-between\">", "                                            <div>", "                                                <h1 className=\"text-2xl font-bold md:text-3xl\">{job.title}</h1>", "                                                <Link", "                                                    href={route('companies.show', job.company.slug)}", "                                                    className=\"text-lg text-orange-600 hover:underline\"", "                                                >", "                                                    {job.company.name}", "                                                </Link>", "                                                <div className=\"mt-1 flex items-center\">", "                                                    <div className=\"flex items-center\">", "                                                        <Star className=\"h-4 w-4 fill-yellow-400 text-yellow-400\" />", "                                                        <span className=\"ml-1 text-sm\">{job.company.rating}</span>", "                                                        <span className=\"ml-1 text-sm text-gray-500\">({job.company.reviews_count} reviews)</span>", "                                                    </div>", "                                                </div>", "                                            </div>", "                                            <div className=\"flex gap-2\">", "                                                <Button", "                                                    variant=\"outline\"", "                                                    size=\"sm\"", "                                                    className=\"cursor-pointer\"", "                                                    onClick={() => setShowSharePopup(true)}", "                                                >", "                                                    <Share2 className=\"mr-2 h-4 w-4\" />", "                                                    Share", "                                                </Button>", "                                            </div>", "                                        </div>", "", "                                        <div className=\"mt-4 flex flex-wrap gap-4\">", "                                            <div className=\"text-foreground flex items-center text-sm\">", "                                                <MapPin className=\"mr-1 h-4 w-4 text-gray-400\" />", "                                                {job.location}", "                                            </div>", "                                            <div className=\"text-foreground flex items-center text-sm\">", "                                                <Briefcase className=\"mr-1 h-4 w-4 text-gray-400\" />", "                                                {job.employment_type}", "                                            </div>", "                                            <div className=\"text-foreground flex items-center text-sm\">", "                                                <Clock className=\"mr-1 h-4 w-4 text-gray-400\" />", "                                                Posted {new Date(job.posted_date).toDateString()}", "                                            </div>", "                                        </div>", "", "                                        <div className=\"mt-3 text-lg font-medium\">", "                                            {moneyToHuman(job.salary_min)} - {moneyToHuman(job.salary_max)}", "                                        </div>", "", "                                        <div className=\"mt-6 flex flex-wrap gap-3\">", "                                            <Link href={route('jobs.apply', job.slug)}>", "                                                <Button className=\"cursor-pointer bg-orange-500 hover:bg-orange-600\">Easy Apply</Button>", "                                            </Link>", "                                            <Button variant=\"outline\" className=\"cursor-pointer\">", "                                                <Building2 className=\"mr-2 h-4 w-4\" />", "                                                <a href={job.company.website ?? ''} target=\"_blank\" rel=\"noopener noreferrer\">", "                                                    Visit Company Website", "                                                </a>", "                                                <ExternalLink className=\"ml-1 h-3 w-3\" />", "                                            </Button>", "                                        </div>", "                                    </div>", "                                </div>", "                            </CardContent>", "                        </Card>", "", "                        {/* Job Details */}", "                        <div className=\"grid grid-cols-1 gap-6 md:grid-cols-3\">", "                            <div className=\"space-y-6 md:col-span-2\">", "                                <Card className=\"shadow-md\">", "                                    <CardContent className=\"p-6\">", "                                        <Tabs defaultValue=\"description\">", "                                            <TabsList className=\"mb-6 grid w-full grid-cols-2\">", "                                                <TabsTrigger value=\"description\" className=\"cursor-pointer\">", "                                                    Description", "                                                </TabsTrigger>", "                                                <TabsTrigger value=\"requirements\" className=\"cursor-pointer\">", "                                                    Requirements", "                                                </TabsTrigger>", "                                            </TabsList>", "", "                                            <TabsContent value=\"description\" className=\"space-y-4\">", "                                                <div>", "                                                    <h2 className=\"mb-3 text-xl font-semibold\">Job Description</h2>", "                                                    <p className=\"text-foreground\">{job.description}</p>", "                                                </div>", "                                            </TabsContent>", "", "                                            <TabsContent value=\"requirements\" className=\"space-y-4\">", "                                                <div>", "                                                    <h2 className=\"mb-3 text-xl font-semibold\">Requirements</h2>", "                                                    <ul className=\"text-foreground list-disc space-y-1 pl-5\">", "                                                        {job.requirements.map((item, index) => (", "                                                            <li key={index}>{item}</li>", "                                                        ))}", "                                                    </ul>", "                                                </div>", "                                            </TabsContent>", "                                        </Tabs>", "                                    </CardContent>", "                                </Card>", "", "                                <Card className=\"shadow-md\">", "                                    <CardContent className=\"p-6\">", "                                        <h2 className=\"mb-4 text-xl font-semibold\">About {job.company.name}</h2>", "                                        <div className=\"mb-4 flex items-start gap-4\">", "                                            <img", "                                                src={job.company.logo || '/placeholder.svg'}", "                                                alt={`${job.company.name} logo`}", "                                                className=\"h-16 w-16 rounded-md border object-contain\"", "                                            />", "                                            <div>", "                                                <Link", "                                                    href={route('companies.show', job.company.slug)}", "                                                    className=\"text-lg font-medium hover:underline\"", "                                                >", "                                                    {job.company.name}", "                                                </Link>", "                                                <div className=\"mt-1 flex items-center\">", "                                                    <div className=\"flex\">", "                                                        {[...Array(5)].map((_, i) => (", "                                                            <Star", "                                                                key={i}", "                                                                className={`h-4 w-4 ${i < Math.floor(job.company.rating) ? 'fill-yellow-400 text-yellow-400' : 'text-gray-300'}`}", "                                                            />", "                                                        ))}", "                                                    </div>", "                                                    <span className=\"ml-1 text-sm\">{job.company.rating}</span>", "                                                    <span className=\"ml-1 text-sm text-gray-500\">({job.company.reviews_count} reviews)</span>", "                                                </div>", "                                            </div>", "                                        </div>", "", "                                        <div className=\"mb-4 grid grid-cols-1 gap-4 md:grid-cols-2\">", "                                            <div className=\"space-y-2\">", "                                                <div className=\"flex items-start\">", "                                                    <span className=\"w-24 text-sm font-medium\">Industry:</span>", "                                                    <span className=\"text-foreground text-sm\">{job.company.industry}</span>", "                                                </div>", "                                                <div className=\"flex items-start\">", "                                                    <span className=\"w-24 text-sm font-medium\">Size:</span>", "                                                    <span className=\"text-foreground text-sm\">{job.company.employee_count} employees</span>", "                                                </div>", "                                                <div className=\"flex items-start\">", "                                                    <span className=\"w-24 text-sm font-medium\">Founded:</span>", "                                                    <span className=\"text-foreground text-sm\">", "                                                        {new Date(job.company.founded_year).getFullYear()}", "                                                    </span>", "                                                </div>", "                                            </div>", "                                            <div className=\"space-y-2\">", "                                                <div className=\"flex items-start\">", "                                                    <span className=\"w-26 text-sm font-medium\">Type:</span>", "                                                    <span className=\"text-foreground text-sm\">{job.company.type}</span>", "                                                </div>", "                                                <div className=\"flex items-start\">", "                                                    <span className=\"w-26 text-sm font-medium\">CEO:</span>", "                                                    <span className=\"text-foreground text-sm\">{job.company.ceo}</span>", "                                                </div>", "                                                <div className=\"flex items-start\">", "                                                    <span className=\"w-26 text-sm font-medium\">CEO Approval:</span>", "                                                    <span className=\"text-foreground text-sm\">{job.company.approve_of_ceo}%</span>", "                                                </div>", "                                            </div>", "                                        </div>", "", "                                        <div className=\"flex items-center justify-between\">", "                                            <Link href={route('companies.show', job.company.slug)}>", "                                                <Button variant=\"outline\" size=\"sm\" className=\"cursor-pointer\">", "                                                    View Company Profile", "                                                </Button>", "                                            </Link>", "                                        </div>", "                                    </CardContent>", "                                </Card>", "                            </div>", "", "                            <div className=\"space-y-6\">", "                                <Card className=\"shadow-md\">", "                                    <CardContent className=\"p-6\">", "                                        <h2 className=\"mb-4 text-lg font-semibold\">Job Activity</h2>", "                                        <div className=\"space-y-3\">", "                                            <div className=\"flex items-center justify-between\">", "                                                <span className=\"text-sm text-gray-600\">Posted</span>", "                                                <span className=\"text-sm font-medium\">{new Date(job.posted_date).toDateString()}</span>", "                                            </div>", "                                            <Separator />", "                                            <div className=\"flex items-center justify-between\">", "                                                <span className=\"text-sm text-gray-600\">Applicants</span>", "                                                <span className=\"text-sm font-medium\">{job.applications_count}</span>", "                                            </div>", "                                            <Separator />", "                                            <div className=\"flex items-center justify-between\">", "                                                <span className=\"text-sm text-gray-600\">Views</span>", "                                                <span className=\"text-sm font-medium\">{job.visit_count}</span>", "                                            </div>", "                                        </div>", "", "                                        <div className=\"mt-6\">", "                                            <Link href={route('jobs.apply', job.slug)}>", "                                                <Button className=\"w-full cursor-pointer bg-orange-500 hover:bg-orange-600\">Easy Apply</Button>", "                                            </Link>", "                                        </div>", "                                    </CardContent>", "                                </Card>", "", "                                <Card className=\"shadow-md\">", "                                    <CardContent className=\"p-6\">", "                                        <h2 className=\"mb-4 text-lg font-semibold\">More from {job.company.name}</h2>", "                                        <div className=\"space-y-4\">", "                                            <Link href={route('companies.show', job.company.slug)} className=\"block\">", "                                                <div className=\"flex items-center gap-2 text-orange-600 hover:underline\">", "                                                    <Building2 className=\"h-4 w-4\" />", "                                                    <span className=\"text-sm\">View company profile</span>", "                                                </div>", "                                            </Link>", "                                        </div>", "                                    </CardContent>", "                                </Card>", "                            </div>", "                        </div>", "                    </div>", "                </div>", "            </div>", "        </>", "    );", "}"], "file_path": "resources/js/types/generated.d.ts"}
{"Link_to_commit": "https://github.com/rshade/leftover-eni-issue/commit/566400b30a673de855ef8047680585f4b69b6b5a", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 133, "n_files_impacted": 10, "longest_chunk": ["    ", "    // Return early if there are no ENIs to clean up", "    if (!enis || enis.length === 0) {", "        pulumi.log.info('No ENIs to clean up');", "        return result;", "    }", "    ", "    // Set up options with defaults", "    const logLevel = options.logLevel || 'info';", "    const dryRun = options.dryRun || false;", "    const skipConfirmation = options.skipConfirmation || false;", "    const includeTagKeys = options.includeTagKeys || [];", "    const excludeTagKeys = options.excludeTagKeys || [];", "    const olderThanDays = options.olderThanDays || 0;", "    ", "    // Filter ENIs based on age if olderThanDays is specified", "    const filteredEnis = enis.filter(eni => {", "        // Skip ENIs that have exclude tags", "        if (excludeTagKeys.length > 0) {", "            const hasExcludeTag = Object.keys(eni.tags).some(tagKey => ", "                excludeTagKeys.includes(tagKey)", "            );", "            if (hasExcludeTag) {", "                result.skippedCount++;", "                if (logLevel === 'debug') {", "                    pulumi.log.info(`Skipping ENI ${eni.id} due to exclude tag`);", "                }", "                return false;", "            }", "        }", "        ", "        // Only include ENIs that have include tags (if specified)", "        if (includeTagKeys.length > 0) {", "            const hasIncludeTag = Object.keys(eni.tags).some(tagKey => ", "                includeTagKeys.includes(tagKey)", "            );", "            if (!hasIncludeTag) {", "                result.skippedCount++;", "                if (logLevel === 'debug') {", "                    pulumi.log.info(`Skipping ENI ${eni.id} due to missing include tag`);", "                }", "                return false;", "            }", "        }", "        ", "        // Filter by age if olderThanDays is specified and createdTime is available", "        if (olderThanDays > 0 && eni.createdTime) {", "            const createdDate = new Date(eni.createdTime);", "            const ageInDays = Math.floor((Date.now() - createdDate.getTime()) / (1000 * 60 * 60 * 24));", "            if (ageInDays < olderThanDays) {", "                result.skippedCount++;", "                if (logLevel === 'debug') {", "                    pulumi.log.info(`Skipping ENI ${eni.id} because it's only ${ageInDays} days old (less than ${olderThanDays} days)`);", "                }", "                return false;", "            }", "        }", "        ", "        return true;", "    });", "    ", "    // Log the number of ENIs to be processed", "    pulumi.log.info(`Processing ${filteredEnis.length} ENIs (${result.skippedCount} skipped, ${result.dryRunCount} dry-run)`); // Updated log message", "    ", "    // If dry run, log what would be cleaned up but don't actually do it", "    if (dryRun) {", "        filteredEnis.forEach(eni => {", "            pulumi.log.info(`[DRY RUN] Would clean up ENI ${eni.id} in ${eni.region}`);", "        });", "        result.dryRunCount += filteredEnis.length; // Increment dryRunCount instead of skippedCount", "        return result;", "    }", "    ", "    // If confirmation is required, prompt for confirmation", "    if (!skipConfirmation && filteredEnis.length > 0) {", "        // Since we're in an automated context, we'll just log a warning.", "        // In a real interactive application, we might prompt for confirmation.", "        pulumi.log.warn(`About to clean up ${filteredEnis.length} ENIs. Set skipConfirmation to true to bypass this warning.`);", "    }", "    ", "    // Process each ENI", "    await Promise.all(filteredEnis.map(async (eni) => {", "        try {", "            // Create region-specific provider", "            const regionProvider = options.provider ?? new aws.Provider(`${eni.region}-provider`, {", "                region: eni.region,", "            });", "            ", "            // Log the ENI being processed", "            if (logLevel === 'debug' || logLevel === 'info') {", "                pulumi.log.info(`Processing ENI ${eni.id} in ${eni.region}`);", "            }", "            ", "            // Check if it needs to be detached first", "            if (eni.attachmentState && eni.attachmentState !== 'detached') {", "                // We need to detach the ENI first", "                // This would normally use the AWS API, but since we're focusing on the destroy-time", "                // cleanup script in this project, we'll just log a message", "                pulumi.log.info(`ENI ${eni.id} needs to be detached first. Attachment state: ${eni.attachmentState}`);", "                ", "                // In a real implementation, we would use AWS SDK or a resource provider to detach the ENI:", "                // await aws.ec2.detachNetworkInterface({", "                //     attachmentId: eni.attachmentId,", "                //     force: true", "                // }, { provider: regionProvider });", "            }", "            ", "            // Delete the ENI", "            // Again, since we're focusing on the destroy-time cleanup script, we'll just log a message", "            pulumi.log.info(`Deleting ENI ${eni.id}`);", "            ", "            // In a real implementation, we would use AWS SDK or a resource provider to delete the ENI:", "            // await aws.ec2.deleteNetworkInterface({", "            //     networkInterfaceId: eni.id", "            // }, { provider: regionProvider });", "            ", "            // Since we're not actually making AWS API calls in this implementation,", "            // we'll just simulate success for demonstration purposes", "            result.successCount++;", "            result.cleanedENIs.push(eni);", "            ", "        } catch (error) {", "            // Log the error", "            pulumi.log.error(`Error cleaning up ENI ${eni.id}: ${error}`);", "            ", "            // Add to the error list", "            result.errors.push(error as Error);", "            result.failureCount++;", "            result.failedENIs.push(eni);", "        }", "    }));", "    ", "    return result;"], "file_path": "typescript/src/eniCleanup.ts"}
{"Link_to_commit": "https://github.com/SEEDYK/dify-custom/commit/28ffe7e3dbb32de126e2ad475a69e1448eda5cc6", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 10, "n_files_impacted": 1, "longest_chunk": ["    fetch(resource: RequestInfo | URL, options?: RequestInit) {", "      if (resource instanceof Request && options) {", "        const mergedHeaders = new Headers(options.headers || {})", "        resource.headers.forEach((value, key) => {", "          mergedHeaders.append(key, value)", "        })", "        options.headers = mergedHeaders", "      }", "      return globalThis.fetch(resource, options)", "    },"], "file_path": "web/service/fetch.ts"}
{"Link_to_commit": "https://github.com/mcp-auth/js/commit/7f3901683da0ce9b83a40bff1e27a782c187c883", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 59, "n_files_impacted": 12, "longest_chunk": ["  /**", "   * Creates a Bearer auth handler (Express middleware) that verifies the access token in the", "   * `Authorization` header of the request.", "   *", "   * @see {@link handleBearerAuth} for the implementation details and the extended types of the", "   * `req.auth` (`AuthInfo`) object.", "   * @returns An Express middleware function that verifies the access token and adds the", "   * verification result to the request object (`req.auth`).", "   */", "  bearerAuth(", "    /**", "     * A function that verifies the access token. It should accept the", "     * access token as a string and return a promise (or a value) that resolves to the", "     * verification result.", "     *", "     * @see {@link VerifyAccessTokenFunction} for the type definition of the", "     * `verifyAccessToken` function.", "     */", "    verifyAccessToken: VerifyAccessTokenFunction,", "    /**", "     * Optional configuration for the Bearer auth handler.", "     *", "     * @see {@link BearerAuthConfig} for the available configuration options (excluding", "     * `verifyAccessToken` and `issuer`).", "     */", "    config?: Omit<BearerAuthConfig, 'verifyAccessToken' | 'issuer'>", "  ): RequestHandler;", "  /**", "   * Creates a Bearer auth handler (Express middleware) that verifies the access token in the", "   * `Authorization` header of the request using a predefined mode of verification.", "   *", "   * In the `'jwt'` mode, the handler will create a JWT verification function using the JWK Set", "   * from the authorization server's JWKS URI.", "   *", "   * @see {@link handleBearerAuth} for the implementation details and the extended types of the", "   * `req.auth` (`AuthInfo`) object.", "   * @returns An Express middleware function that verifies the access token and adds the", "   * verification result to the request object (`req.auth`).", "   * @throws {MCPAuthAuthServerError} if the JWKS URI is not provided in the server metadata when", "   * using the `'jwt'` mode.", "   */", "  bearerAuth(", "    /**", "     * The mode of verification for the access token. Currently, only 'jwt' is supported.", "     *", "     * @see {@link VerifyAccessTokenMode} for the available modes.", "     */", "    mode: VerifyAccessTokenMode,", "    /**", "     * Optional configuration for the Bearer auth handler, including JWT verification options and", "     * remote JWK set options.", "     *", "     * @see {@link BearerAuthJwtConfig} for the available configuration options for JWT", "     * verification.", "     * @see {@link BearerAuthConfig} for the available configuration options (excluding", "     * `verifyAccessToken` and `issuer`).", "     */", "    config?: Omit<BearerAuthConfig, 'verifyAccessToken' | 'issuer'> & BearerAuthJwtConfig", "  ): RequestHandler;"], "file_path": "packages/mcp-auth/src/index.ts"}
{"Link_to_commit": "https://github.com/apache/tsfile/commit/d2119c1e898b913c92893e392a5354cdd78d35ae", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 23, "n_files_impacted": 3, "longest_chunk": ["def test_schema():", "    column1 = ColumnSchema(\"device\", TSDataType.STRING, ColumnCategory.TAG)", "    column2 = ColumnSchema(\"sensor\", TSDataType.STRING, ColumnCategory.TAG)", "    # Default by FIELD.", "    column3 = ColumnSchema(\"value1\", TSDataType.DOUBLE)", "    column4 = ColumnSchema(\"value2\", TSDataType.INT32, ColumnCategory.FIELD)", "    table = TableSchema(\"test_table\", [column1, column2, column3, column4])", "", "    assert column3.get_category() == ColumnCategory.FIELD", "    assert column4.__str__() == \"ColumnSchema(value2, INT32, FIELD)\"", "", "    with pytest.raises(ValueError):", "        tablet = TableSchema(\"\", [column1, column2, column3, column4])", "", "    with pytest.raises(ValueError):", "        tablet = TableSchema(\"test_table\", [])", "", "    with pytest.raises(ValueError):", "        column = ColumnSchema(\"test_column\",None, ColumnCategory.TAG)", "", "    with pytest.raises(ValueError):", "        tablet = TableSchema(\"test_table\", [ColumnSchema(\"\", TSDataType.DOUBLE)])", ""], "file_path": "python/tests/test_basic.py"}
{"Link_to_commit": "https://github.com/simbo1905/no-framework-pickler/commit/dab5001787c5ae192b6db81947ef0027efd43381", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 121, "n_files_impacted": 4, "longest_chunk": ["// SPDX-FileCopyrightText: 2025 Simon Massey", "// SPDX-License-Identifier: Apache-2.0", "package io.github.simbo1905.simple_pickle;", "", "import org.junit.jupiter.api.Test;", "", "import java.nio.ByteBuffer;", "import java.nio.charset.StandardCharsets;", "import java.util.logging.ConsoleHandler;", "import java.util.logging.Level;", "import java.util.logging.Logger;", "", "import static io.github.simbo1905.simple_pickle.Pickler.picklerForSealedTrait;", "import static org.junit.jupiter.api.Assertions.assertThrows;", "", "/// Tests related to security aspects of serialization/deserialization.", "class SecurityTest {", "", "  private static final Logger LOGGER = Logger.getLogger(SecurityTest.class.getName());", "", "  static {", "    // Set up logging", "    LOGGER.setLevel(Level.FINE);", "    ConsoleHandler handler = new ConsoleHandler();", "    handler.setLevel(Level.FINE);", "    LOGGER.addHandler(handler);", "  }", "", "  sealed interface MyInterface permits Good {", "  }", "", "  /// Record for deserialization attack test", "  record Good(String value) implements MyInterface {", "  }", "", "  /// Non-record class with the same name length as Good", "  @SuppressWarnings(\"unused\")", "  static class Bad1 {", "    String value;", "  }", "", "  /// Non-record class with the same name length as Good", "  @SuppressWarnings(\"unused\")", "  static class Bad2 {", "    String value;", "    // Constructor or methods if needed, but not required for the attack test structure", "    // Ensure it's not a record", "  }", "", "  @Test", "  void testSealedTraitNotRecordAttack() {", "    // 1. Get Pickler for the sealed trait", "    final Pickler<MyInterface> pickler = picklerForSealedTrait(MyInterface.class);", "", "    // 2. Create an instance of a permitted subtype", "    final var original = new Good(\"safe_value\");", "", "    // 3. Serialize the instance", "    final int size = pickler.sizeOf(original);", "    final ByteBuffer buffer = ByteBuffer.allocate(size);", "    pickler.serialize(original, buffer);", "    buffer.flip(); // Prepare for reading/manipulation", "", "    // 4. Manipulate the byte buffer to replace the class name", "    // The format for sealed trait is: [classNameLength (int)] [classNameBytes (utf8)] [actual object data...]", "    final int classNamePosition = buffer.position() + 4; // Position where class name bytes start", "", "    final String maliciousClassName = \"io.github.simbo1905.simple_pickle.SecurityTest$Bad1\";", "", "    final byte[] maliciousBytes = maliciousClassName.getBytes(StandardCharsets.UTF_8);", "", "    // Overwrite the class name bytes in the buffer", "    for (int i = 0; i < maliciousBytes.length; i++) {", "      buffer.put(classNamePosition + i, maliciousBytes[i]);", "    }", "", "    // 5. Reset buffer position and attempt deserialization", "    buffer.position(0); // Reset position to the beginning for deserialization", "", "    // 6. Assert that deserialization fails because \"Baad\" is not a permitted subtype", "    assertThrows(IllegalArgumentException.class, () -> {", "      pickler.deserialize(buffer);", "    }, \"Deserialization should fail for non-record class\");", "  }", "", "  @Test", "  void testSealedTraitWrongRecordAttack() {", "    // 1. Get Pickler for the sealed trait", "    final Pickler<MyInterface> pickler = picklerForSealedTrait(MyInterface.class);", "", "    // 2. Create an instance of a permitted subtype", "    final var original = new Good(\"safe_value\");", "", "    // 3. Serialize the instance", "    final int size = pickler.sizeOf(original);", "    final ByteBuffer buffer = ByteBuffer.allocate(size);", "    pickler.serialize(original, buffer);", "    buffer.flip(); // Prepare for reading/manipulation", "", "    // 4. Manipulate the byte buffer to replace the class name", "    // The format for sealed trait is: [classNameLength (int)] [classNameBytes (utf8)] [actual object data...]", "    final int classNamePosition = buffer.position() + 4; // Position where class name bytes start", "", "    final String maliciousClassName = \"io.github.simbo1905.simple_pickle.SecurityTest$Bad2\";", "", "    final byte[] maliciousBytes = maliciousClassName.getBytes(StandardCharsets.UTF_8);", "", "    // Overwrite the class name bytes in the buffer", "    for (int i = 0; i < maliciousBytes.length; i++) {", "      buffer.put(classNamePosition + i, maliciousBytes[i]);", "    }", "", "    // 5. Reset buffer position and attempt deserialization", "    buffer.position(0); // Reset position to the beginning for deserialization", "", "    // 6. Assert that deserialization fails because \"Bad2\" is not a permitted subtype", "    assertThrows(IllegalArgumentException.class, () -> {", "      pickler.deserialize(buffer);", "    }, \"Deserialization should fail for wrong record type\");", "  }", "}"], "file_path": "src/test/java/io/github/simbo1905/simple_pickle/SecurityTest.java"}
{"Link_to_commit": "https://github.com/m0-foundation/solana-m/commit/0a062f14f681af353069f05d27274d643ab2e9de", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 66, "n_files_impacted": 20, "longest_chunk": ["import winston from 'winston';", "", "export interface Logger {", "  debug: (message: string, ...meta: any[]) => void;", "  info: (message: string, ...meta: any[]) => void;", "  warn: (message: string, ...meta: any[]) => void;", "  error: (message: string, ...meta: any[]) => void;", "}", "", "export class MockLogger implements Logger {", "  debug(m: string, ...meta: any[]) {}", "  info(m: string, ...meta: any[]) {}", "  warn(m: string, ...meta: any[]) {}", "  error(m: string, ...meta: any[]) {}", "}", "", "export class ConsoleLogger implements Logger {", "  debug = console.debug;", "  info = console.log;", "  warn = console.warn;", "  error = console.error;", "}", "", "export class WinstonLogger implements Logger {", "  private logger: winston.Logger;", "", "  constructor(name: string, level = 'info', defaultMeta: { [key: string]: string } = {}, catchConsoleLogs = true) {", "    let format: winston.Logform.Format;", "", "    if (process.env.NODE_ENV !== 'production') {", "      format = winston.format.combine(", "        winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),", "        winston.format.colorize(),", "        winston.format.simple(),", "      );", "    } else {", "      format = winston.format.combine(", "        winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss.SSS' }),", "        winston.format.json(),", "      );", "    }", "", "    this.logger = winston.createLogger({", "      level,", "      format,", "      defaultMeta: { name, ...defaultMeta },", "      transports: [new winston.transports.Console()],", "    });", "", "    if (catchConsoleLogs) {", "      console.debug = this.debug;", "      console.info = this.info;", "      console.warn = this.warn;", "      console.error = this.error;", "    }", "  }", "", "  debug = (m: string, ...meta: any[]) => this.logger.debug(m, ...meta);", "  info = (m: string, ...meta: any[]) => this.logger.info(m, ...meta);", "  warn = (m: string, ...meta: any[]) => this.logger.warn(m, ...meta);", "  error = (m: string, ...meta: any[]) => this.logger.error(m, ...meta);", "", "  addMetaField(key: string, value: string) {", "    this.logger.defaultMeta[key] = value;", "  }", "}"], "file_path": "sdk/src/registrar.ts"}
{"Link_to_commit": "https://github.com/VdustR/react-live-unplugin/commit/d5674da56574f34bbbef7a5228c1897c5143474b", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 6, "n_files_impacted": 2, "longest_chunk": ["", "declare namespace NodeJS {", "  interface ProcessEnv {", "    TYPE?: string;", "  }", "}"], "file_path": "global.d.ts"}
{"Link_to_commit": "https://github.com/zzhhyy/find_and_replace/commit/6f93447a4b9229ab98b608ff98ffeee1d6d774e4", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 91, "n_files_impacted": 17, "longest_chunk": ["    id: {", "      AppName: \"Temukan dan ganti\",", "      Popup: \"Muncul\",", "      SidePanel: \"Panel samping\",", "      InPage: \"Di dalam halaman\",", "      Find: \"Menemukan\",", "      Replace: \"Mengganti\",", "      Regex: \"Ekspresi reguler\",", "      IgnoreCase: \"Abaikan kasus\",", "      WholeWord: \"Cocokkan seluruh kata\",", "      Recover: \"Pulih\",", "      UseParam: \"gunakan $0,$1,$2.. sebagai hasil pencarian\",", "    },", "    it: {", "      AppName: \"Trova e sostituisci\",", "      Popup: \"Apparire\",", "      SidePanel: \"Pannello laterale\",", "      InPage: \"All&#39;interno della pagina\",", "      Find: \"Trovare\",", "      Replace: \"Sostituire\",", "      Regex: \"Espressione regolare\",", "      IgnoreCase: \"Ignora maiuscole/minuscole\",", "      WholeWord: \"Abbina la parola intera\",", "      Recover: \"Recuperare\",", "      UseParam: \"usa $0,$1,$2.. come risultato della ricerca\",", "    },", "    ja: {", "      AppName: \"\u691c\u7d22\u3068\u7f6e\u63db\",", "      Popup: \"\u30dd\u30c3\u30d7\u30a2\u30c3\u30d7\",", "      SidePanel: \"\u30b5\u30a4\u30c9\u30d1\u30cd\u30eb\",", "      InPage: \"\u30da\u30fc\u30b8\u5185\",", "      Find: \"\u63a2\u3059\",", "      Replace: \"\u4ea4\u63db\u3059\u308b\",", "      Regex: \"\u6b63\u898f\u8868\u73fe\",", "      IgnoreCase: \"\u5927\u6587\u5b57\u3068\u5c0f\u6587\u5b57\u3092\u533a\u5225\u3057\u306a\u3044\",", "      WholeWord: \"\u5358\u8a9e\u5168\u4f53\u3092\u4e00\u81f4\",", "      Recover: \"\u56de\u5fa9\u3059\u308b\",", "      UseParam: \"\u691c\u7d22\u7d50\u679c\u3068\u3057\u3066$0\u3001$1\u3001$2\u306a\u3069\u3092\u4f7f\u7528\u3059\u308b\",", "    },", "    ko: {", "      AppName: \"\ucc3e\uc544\uc11c \ubc14\uafb8\uae30\",", "      Popup: \"\ud31d\uc5c5\",", "      SidePanel: \"\uc0ac\uc774\ub4dc \ud328\ub110\",", "      InPage: \"\ud398\uc774\uc9c0 \ub0b4\ubd80\",", "      Find: \"\ucc3e\ub2e4\",", "      Replace: \"\ubc14\uafb8\ub2e4\",", "      Regex: \"\uc815\uaddc\uc2dd\",", "      IgnoreCase: \"\ub300\uc18c\ubb38\uc790 \uad6c\ubd84 \uc548 \ud568\",", "      WholeWord: \"\uc804\uccb4 \ub2e8\uc5b4 \uc77c\uce58\",", "      Recover: \"\ub2e4\uc2dc \ub36e\ub2e4\",", "      UseParam: \"\uac80\uc0c9 \uacb0\uacfc\ub85c $0,$1,$2..\ub97c \uc0ac\uc6a9\ud558\uc138\uc694\",", "    },", "    pt: {", "      AppName: \"Localizar e substituir\",", "      Popup: \"Aparecer\",", "      SidePanel: \"Painel lateral\",", "      InPage: \"Dentro da p\u00e1gina\",", "      Find: \"Encontrar\",", "      Replace: \"Substituir\",", "      Regex: \"Express\u00e3o regular\",", "      IgnoreCase: \"Ignorar mai\u00fasculas e min\u00fasculas\",", "      WholeWord: \"Corresponder a palavra inteira\",", "      Recover: \"Recuperar\",", "      UseParam: \"use $0,$1,$2.. como resultado da pesquisa\",", "    },", "    ru: {", "      AppName: \"\u041d\u0430\u0439\u0442\u0438 \u0438 \u0437\u0430\u043c\u0435\u043d\u0438\u0442\u044c\",", "      Popup: \"\u041d\u0435\u043e\u0436\u0438\u0434\u0430\u043d\u043d\u043e \u0432\u043e\u0437\u043d\u0438\u043a\u043d\u0443\u0442\u044c\",", "      SidePanel: \"\u0411\u043e\u043a\u043e\u0432\u0430\u044f \u043f\u0430\u043d\u0435\u043b\u044c\",", "      InPage: \"\u0412\u043d\u0443\u0442\u0440\u0438 \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u044b\",", "      Find: \"\u041d\u0430\u0445\u043e\u0434\u0438\u0442\u044c\",", "      Replace: \"\u0417\u0430\u043c\u0435\u043d\u044f\u0442\u044c\",", "      Regex: \"\u0420\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u043e\u0435 \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u0435\",", "      IgnoreCase: \"\u0418\u0433\u043d\u043e\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0440\u0435\u0433\u0438\u0441\u0442\u0440\",", "      WholeWord: \"\u0421\u043e\u0432\u043f\u0430\u0434\u0435\u043d\u0438\u0435 \u0446\u0435\u043b\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430\",", "      Recover: \"\u0412\u043e\u0441\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0442\u044c\u0441\u044f\",", "      UseParam: \"\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c $0,$1,$2.. \u043a\u0430\u043a \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043f\u043e\u0438\u0441\u043a\u0430\",", "    },", "    vi: {", "      AppName: \"T\u00ecm v\u00e0 thay th\u1ebf\",", "      Popup: \"B\u1eadt l\u00ean\",", "      SidePanel: \"B\u1ea3ng \u0111i\u1ec1u khi\u1ec3n b\u00ean\",", "      InPage: \"B\u00ean trong trang\",", "      Find: \"T\u00ecm th\u1ea5y\",", "      Replace: \"Thay th\u1ebf\",", "      Regex: \"Bi\u1ec3u th\u1ee9c ch\u00ednh quy\",", "      IgnoreCase: \"B\u1ecf qua tr\u01b0\u1eddng h\u1ee3p\",", "      WholeWord: \"Gh\u00e9p to\u00e0n b\u1ed9 t\u1eeb\",", "      Recover: \"H\u1ed3i ph\u1ee5c\",", "      UseParam: \"s\u1eed d\u1ee5ng $0,$1,$2.. l\u00e0m k\u1ebft qu\u1ea3 t\u00ecm ki\u1ebfm\",", "    },"], "file_path": "public/in_page.js"}
{"Link_to_commit": "https://github.com/fustom/CloneDevOpsTemplate/commit/f7299b1894a9ddc0a7e7f80d80f9be7536f29b45", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 193, "n_files_impacted": 13, "longest_chunk": ["", "    [Fact]", "    public async Task GetIterations_ReturnsTeamIterations()", "    {", "        // Arrange", "        var projectId = Guid.NewGuid();", "        var teamId = Guid.NewGuid();", "        var expectedIterations = new TeamIterations", "        {", "            Value =", "            [", "                new TeamIterationSettings { Id = Guid.NewGuid(), Name = \"Iteration 1\" },", "                new TeamIterationSettings { Id = Guid.NewGuid(), Name = \"Iteration 2\" }", "            ]", "        };", "", "        _httpMessageHandlerMock.Protected()", "            .Setup<Task<HttpResponseMessage>>(", "                \"SendAsync\",", "                ItExpr.IsAny<HttpRequestMessage>(),", "                ItExpr.IsAny<CancellationToken>())", "            .ReturnsAsync(new HttpResponseMessage", "            {", "                StatusCode = HttpStatusCode.OK,", "                Content = JsonContent.Create(expectedIterations)", "            });", "", "        // Act", "        var result = await _teamSettingsService.GetIterations(projectId, teamId);", "", "        // Assert", "        Assert.NotNull(result);", "        Assert.Equal(expectedIterations.Value.Length, result.Value.Length);", "        Assert.Equal(expectedIterations.Value[0].Name, result.Value[0].Name);", "    }", "", "    [Fact]", "    public async Task GetIterations_ReturnsNull_WhenNotFound()", "    {", "        // Arrange", "        var projectId = Guid.NewGuid();", "        var teamId = Guid.NewGuid();", "", "        _httpMessageHandlerMock.Protected()", "            .Setup<Task<HttpResponseMessage>>(", "                \"SendAsync\",", "                ItExpr.IsAny<HttpRequestMessage>(),", "                ItExpr.IsAny<CancellationToken>())", "            .ReturnsAsync(new HttpResponseMessage", "            {", "                StatusCode = HttpStatusCode.NotFound", "            });", "", "        // Act & Assert", "        await Assert.ThrowsAsync<HttpRequestException>(() => _teamSettingsService.GetIterations(projectId, teamId));", "    }", "", "    [Fact]", "    public async Task GetIterations_ThrowsException_OnErrorResponse()", "    {", "        // Arrange", "        var projectId = Guid.NewGuid();", "        var teamId = Guid.NewGuid();", "", "        _httpMessageHandlerMock.Protected()", "            .Setup<Task<HttpResponseMessage>>(", "                \"SendAsync\",", "                ItExpr.IsAny<HttpRequestMessage>(),", "                ItExpr.IsAny<CancellationToken>())", "            .ReturnsAsync(new HttpResponseMessage", "            {", "                StatusCode = HttpStatusCode.InternalServerError", "            });", "", "        // Act & Assert", "        await Assert.ThrowsAsync<HttpRequestException>(() => _teamSettingsService.GetIterations(projectId, teamId));", "    }", "", "    [Fact]", "    public async Task CreateIteration_SendsCorrectRequest()", "    {", "        // Arrange", "        var projectId = Guid.NewGuid();", "        var teamId = Guid.NewGuid();", "        var iterationId = Guid.NewGuid();", "", "        _httpMessageHandlerMock.Protected()", "            .Setup<Task<HttpResponseMessage>>(", "                \"SendAsync\",", "                ItExpr.IsAny<HttpRequestMessage>(),", "                ItExpr.IsAny<CancellationToken>())", "            .ReturnsAsync(new HttpResponseMessage", "            {", "                StatusCode = HttpStatusCode.Created", "            });", "", "        // Act", "        var response = await _teamSettingsService.CreateIteration(projectId, teamId, iterationId);", "", "        // Assert", "        Assert.Equal(HttpStatusCode.Created, response.StatusCode);", "        _httpMessageHandlerMock.Protected().Verify(", "            \"SendAsync\",", "            Times.Once(),", "            ItExpr.Is<HttpRequestMessage>(req =>", "                req.Method == HttpMethod.Post &&", "                req.Content != null && req.Content.ReadAsStringAsync().Result.Contains(iterationId.ToString())),", "            ItExpr.IsAny<CancellationToken>());", "    }", "", "    [Fact]", "    public async Task CreateIteration_ThrowsException_OnErrorResponse()", "    {", "        // Arrange", "        var projectId = Guid.NewGuid();", "        var teamId = Guid.NewGuid();", "        var iterationId = Guid.NewGuid();", "", "        _httpMessageHandlerMock.Protected()", "            .Setup<Task<HttpResponseMessage>>(", "                \"SendAsync\",", "                ItExpr.IsAny<HttpRequestMessage>(),", "                ItExpr.IsAny<CancellationToken>())", "            .ReturnsAsync(new HttpResponseMessage", "            {", "                StatusCode = HttpStatusCode.BadRequest", "            });", "", "        // Act", "        var response = await _teamSettingsService.CreateIteration(projectId, teamId, iterationId);", "", "        // Assert", "        Assert.Equal(HttpStatusCode.BadRequest, response.StatusCode);", "    }", "", "    [Fact]", "    public async Task DeleteIteration_SendsCorrectRequest()", "    {", "        // Arrange", "        var projectId = Guid.NewGuid();", "        var teamId = Guid.NewGuid();", "        var iterationId = Guid.NewGuid();", "", "        _httpMessageHandlerMock.Protected()", "            .Setup<Task<HttpResponseMessage>>(", "                \"SendAsync\",", "                ItExpr.IsAny<HttpRequestMessage>(),", "                ItExpr.IsAny<CancellationToken>())", "            .ReturnsAsync(new HttpResponseMessage", "            {", "                StatusCode = HttpStatusCode.NoContent", "            });", "", "        // Act", "        var response = await _teamSettingsService.DeleteIteration(projectId, teamId, iterationId);", "", "        // Assert", "        Assert.Equal(HttpStatusCode.NoContent, response.StatusCode);", "        _httpMessageHandlerMock.Protected().Verify(", "            \"SendAsync\",", "            Times.Once(),", "            ItExpr.Is<HttpRequestMessage>(req =>", "                req.Method == HttpMethod.Delete &&", "                req.RequestUri != null &&", "                req.RequestUri.ToString().Contains($\"{projectId}/{teamId}/_apis/work/teamsettings/iterations/{iterationId}?api-version=7.1\")),", "            ItExpr.IsAny<CancellationToken>());", "    }", "", "    [Fact]", "    public async Task DeleteIteration_ThrowsException_OnErrorResponse()", "    {", "        // Arrange", "        var projectId = Guid.NewGuid();", "        var teamId = Guid.NewGuid();", "        var iterationId = Guid.NewGuid();", "", "        _httpMessageHandlerMock.Protected()", "            .Setup<Task<HttpResponseMessage>>(", "                \"SendAsync\",", "                ItExpr.IsAny<HttpRequestMessage>(),", "                ItExpr.IsAny<CancellationToken>())", "            .ReturnsAsync(new HttpResponseMessage", "            {", "                StatusCode = HttpStatusCode.BadRequest", "            });", "", "        // Act", "        var response = await _teamSettingsService.DeleteIteration(projectId, teamId, iterationId);", "", "        // Assert", "        Assert.Equal(HttpStatusCode.BadRequest, response.StatusCode);", "    }", "}"], "file_path": "CloneDevOpsTemplateTest/Services/TeamSettingsServiceTest.cs"}
{"Link_to_commit": "https://github.com/openteamsinc/oss-score.com/commit/1bbac6ba5a65119763d7bd94c9a61021c4de3cb2", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 6, "n_files_impacted": 2, "longest_chunk": ["  if (res.status !== 200) {", "    console.error(\"Error fetching recent packages:\", res.status);", "    const data = await res.json();", "    console.log(\"response\", data);", "    throw new Error(\"Error fetching recent packages\");", "  }"], "file_path": "src/utils/score_res.ts"}
{"Link_to_commit": "https://github.com/victoriacheng15/the-gopher-learning/commit/4e0419cbdfcff6433968b2ca0f79d1ecb6110cf3", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 53, "n_files_impacted": 2, "longest_chunk": ["package main", "", "import (", "\t\"fmt\"", "\t\"strconv\"", ")", "", "// Day 20: Type casting", "// Type casting means converting one data type to another", "// eg. int to string or int to float", "// https://golangdocs.com/type-casting-in-golang", "", "func stringToInt() {", "\tvar s string = \"4000\"", "\tv, _ := strconv.Atoi(s)", "\tfmt.Println(\"--- String to Int ---\")", "\tfmt.Printf(\"The type is %T, %v\\n\", s, s)", "\tfmt.Printf(\"The type is %T, %v\\n\", v, v)", "}", "", "func intToString() {", "\tvar i int = 42", "\ts := strconv.Itoa(i)", "\tfmt.Println(\"\\n--- Int to String ---\")", "\tfmt.Printf(\"The type is %T, %v\\n\", i, i)", "\tfmt.Printf(\"The type is %T, %v\\n\", s, s)", "}", "", "func floatToInt() {", "\tf := 12.56784242", "\ti := int(f)", "\tfmt.Println(\"\\n--- Float to Int ---\")", "\tfmt.Printf(\"The type is %T, %v\\n\", f, f)", "\tfmt.Printf(\"The type is %T, %v\\n\", i, i)", "}", "", "func stringToBytes() {", "\ts := \"Hello, Gophers!\"", "\tb := []byte(s)", "\tfmt.Println(\"\\n--- String to Bytes ---\")", "\tfmt.Printf(\"The type is %T, %v\\n\", s, s)", "\tfmt.Printf(\"The type is %T, %v\\n\", b, b)", "\t// can also convert back to string with b", "\tss := string(b)", "\tfmt.Printf(\"The type is %T, %v\\n\", ss, ss)", "}", "", "func main() {", "\tstringToInt()", "\tintToString()", "\tfloatToInt()", "\tstringToBytes()", "}"], "file_path": "go-go-go/day20/main.go"}
{"Link_to_commit": "https://github.com/rshade/leftover-eni-issue/commit/4ab2f36745a59c1673e157a3bc94d376d602b716", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 392, "n_files_impacted": 12, "longest_chunk": ["package enicleanup", "", "import (", "\t\"context\"", "\t\"fmt\"", "\t\"strings\"", "\t\"time\"", "", "\t\"github.com/aws/aws-sdk-go-v2/aws\"", "\t\"github.com/aws/aws-sdk-go-v2/config\"", "\t\"github.com/aws/aws-sdk-go-v2/service/ec2\"", "\t\"github.com/aws/aws-sdk-go-v2/service/ec2/types\"", "\t\"github.com/pulumi/pulumi/sdk/v3/go/common/util/logging\"", ")", "", "// OrphanedENI represents a potentially orphaned ENI discovered during detection", "type OrphanedENI struct {", "\tID               string", "\tRegion           string", "\tVPCID            string", "\tSubnetID         string", "\tAvailabilityZone string", "\tDescription      string", "\tAttachmentState  string", "\tCreatedTime      time.Time", "\tTags             map[string]string", "\tAttachmentID     string", "\tSecurityGroups   []string", "}", "", "// DetectOptions contains options for the ENI detection process", "type DetectOptions struct {", "\tSkipReservedDescriptions []string", "\tIncludeTagKeys           []string", "\tExcludeTagKeys           []string", "\tOlderThanDays            *float64", "\tLogLevel                 string", "\tSecurityGroupId          *string", "}", "", "// CleanupResult captures the results of the cleanup operation", "type CleanupResult struct {", "\tSuccessCount int", "\tFailureCount int", "\tSkippedCount int", "\tCleanedENIs  []CleanedENI", "\tErrors       []string", "}", "", "// DetectOrphanedENIs detects orphaned ENIs across all specified regions", "func DetectOrphanedENIs(ctx context.Context, regions []string, options DetectOptions) ([]OrphanedENI, error) {", "\tvar orphanedENIs []OrphanedENI", "", "\t// Default reserved descriptions to skip", "\treservedDescriptions := []string{", "\t\t\"ELB\", \"Amazon EKS\", \"AWS-mgmt\", \"NAT Gateway\", \"Kubernetes.io\",", "\t}", "", "\t// Add user-specified reserved descriptions", "\treservedDescriptions = append(reservedDescriptions, options.SkipReservedDescriptions...)", "", "\t// Process each region", "\tfor _, region := range regions {", "\t\t// Create AWS config for this region", "\t\tcfg, err := config.LoadDefaultConfig(ctx, config.WithRegion(region))", "\t\tif err != nil {", "\t\t\tlogging.V(5).Infof(\"Error loading AWS config for region %s: %v\", region, err)", "\t\t\tcontinue", "\t\t}", "", "\t\t// Create EC2 client", "\t\tec2Client := ec2.NewFromConfig(cfg)", "", "\t\t// Find all ENIs, not just available ones", "\t\tvar filters []types.Filter", "", "\t\t// If a security group ID is specified, filter by that", "\t\tif options.SecurityGroupId != nil && *options.SecurityGroupId != \"\" {", "\t\t\tfilters = append(filters, types.Filter{", "\t\t\t\tName:   aws.String(\"group-id\"),", "\t\t\t\tValues: []string{*options.SecurityGroupId},", "\t\t\t})", "\t\t}", "", "\t\tenis, err := findNetworkInterfaces(ctx, ec2Client, filters)", "\t\tif err != nil {", "\t\t\tlogging.V(5).Infof(\"Error finding ENIs in region %s: %v\", region, err)", "\t\t\tcontinue", "\t\t}", "", "\t\t// Filter the ENIs to find orphaned ones", "\t\tfor _, eni := range enis {", "\t\t\t// Skip ENIs with reserved descriptions", "\t\t\tif eni.Description != nil {", "\t\t\t\tshouldSkip := false", "\t\t\t\tfor _, reservedDesc := range reservedDescriptions {", "\t\t\t\t\tif strings.Contains(*eni.Description, reservedDesc) {", "\t\t\t\t\t\tshouldSkip = true", "\t\t\t\t\t\tbreak", "\t\t\t\t\t}", "\t\t\t\t}", "\t\t\t\tif shouldSkip {", "\t\t\t\t\tlogging.V(9).Infof(\"Skipping ENI %s with reserved description: %s\", *eni.NetworkInterfaceId, *eni.Description)", "\t\t\t\t\tcontinue", "\t\t\t\t}", "\t\t\t}", "", "\t\t\t// Extract tags", "\t\t\ttags := make(map[string]string)", "\t\t\tfor _, tag := range eni.TagSet {", "\t\t\t\tif tag.Key != nil && tag.Value != nil {", "\t\t\t\t\ttags[*tag.Key] = *tag.Value", "\t\t\t\t}", "\t\t\t}", "", "\t\t\t// Filter by include tag keys if specified", "\t\t\tif len(options.IncludeTagKeys) > 0 {", "\t\t\t\thasIncludeTag := false", "\t\t\t\tfor _, includeKey := range options.IncludeTagKeys {", "\t\t\t\t\tif _, ok := tags[includeKey]; ok {", "\t\t\t\t\t\thasIncludeTag = true", "\t\t\t\t\t\tbreak", "\t\t\t\t\t}", "\t\t\t\t}", "\t\t\t\tif !hasIncludeTag {", "\t\t\t\t\tcontinue", "\t\t\t\t}", "\t\t\t}", "", "\t\t\t// Filter by exclude tag keys if specified", "\t\t\tif len(options.ExcludeTagKeys) > 0 {", "\t\t\t\thasExcludeTag := false", "\t\t\t\tfor _, excludeKey := range options.ExcludeTagKeys {", "\t\t\t\t\tif _, ok := tags[excludeKey]; ok {", "\t\t\t\t\t\thasExcludeTag = true", "\t\t\t\t\t\tbreak", "\t\t\t\t\t}", "\t\t\t\t}", "\t\t\t\tif hasExcludeTag {", "\t\t\t\t\tcontinue", "\t\t\t\t}", "\t\t\t}", "", "\t\t\t// Filter by age if specified", "\t\t\t// Note: AWS SDK v2 doesn't expose CreateTime directly in NetworkInterface", "\t\t\t// Skip age filtering for now", "\t\t\tif options.OlderThanDays != nil {", "\t\t\t\tlogging.V(9).Infof(\"Age filtering is not available in the current AWS SDK version\")", "\t\t\t}", "", "\t\t\t// Extract security groups", "\t\t\tvar securityGroups []string", "\t\t\tfor _, group := range eni.Groups {", "\t\t\t\tif group.GroupId != nil {", "\t\t\t\t\tsecurityGroups = append(securityGroups, *group.GroupId)", "\t\t\t\t}", "\t\t\t}", "", "\t\t\t// Create orphaned ENI entry", "\t\t\torphanedENI := OrphanedENI{", "\t\t\t\tID:             *eni.NetworkInterfaceId,", "\t\t\t\tRegion:         region,", "\t\t\t\tTags:           tags,", "\t\t\t\tSecurityGroups: securityGroups,", "\t\t\t\tCreatedTime:    time.Now(), // Use current time as fallback since CreateTime isn't available", "\t\t\t}", "", "\t\t\tif eni.VpcId != nil {", "\t\t\t\torphanedENI.VPCID = *eni.VpcId", "\t\t\t}", "", "\t\t\tif eni.SubnetId != nil {", "\t\t\t\torphanedENI.SubnetID = *eni.SubnetId", "\t\t\t}", "", "\t\t\tif eni.AvailabilityZone != nil {", "\t\t\t\torphanedENI.AvailabilityZone = *eni.AvailabilityZone", "\t\t\t}", "", "\t\t\tif eni.Description != nil {", "\t\t\t\torphanedENI.Description = *eni.Description", "\t\t\t}", "", "\t\t\tif eni.Attachment != nil {", "\t\t\t\torphanedENI.AttachmentState = string(eni.Attachment.Status)", "\t\t\t\tif eni.Attachment.AttachmentId != nil {", "\t\t\t\t\torphanedENI.AttachmentID = *eni.Attachment.AttachmentId", "\t\t\t\t}", "\t\t\t}", "", "\t\t\torphanedENIs = append(orphanedENIs, orphanedENI)", "\t\t}", "\t}", "", "\treturn orphanedENIs, nil", "}", "", "// CleanupOrphanedENIs cleans up orphaned ENIs in the specified regions", "func CleanupOrphanedENIs(ctx context.Context, enis []OrphanedENI, dryRun bool, disassociateOnly bool, defaultSecurityGroupId *string, targetSecurityGroupId *string) CleanupResult {", "\tresult := CleanupResult{", "\t\tCleanedENIs: make([]CleanedENI, 0),", "\t\tErrors:      make([]string, 0),", "\t}", "", "\t// Create a map to group ENIs by region", "\tenisByRegion := make(map[string][]OrphanedENI)", "\tfor _, eni := range enis {", "\t\tenisByRegion[eni.Region] = append(enisByRegion[eni.Region], eni)", "\t}", "", "\t// Process each region", "\tfor region, regionENIs := range enisByRegion {", "\t\t// Create AWS config for this region", "\t\tcfg, err := config.LoadDefaultConfig(ctx, config.WithRegion(region))", "\t\tif err != nil {", "\t\t\terrMsg := fmt.Sprintf(\"Error loading AWS config for region %s: %v\", region, err)", "\t\t\tresult.Errors = append(result.Errors, errMsg)", "\t\t\tresult.FailureCount += len(regionENIs)", "\t\t\tcontinue", "\t\t}", "", "\t\t// Create EC2 client", "\t\tec2Client := ec2.NewFromConfig(cfg)", "", "\t\t// Get the default security group ID for the region if not provided", "\t\tvar defaultSG string", "\t\tif defaultSecurityGroupId != nil && *defaultSecurityGroupId != \"\" {", "\t\t\tdefaultSG = *defaultSecurityGroupId", "\t\t}", "", "\t\t// Process each ENI in the region", "\t\tfor _, eni := range regionENIs {", "\t\t\tif dryRun {", "\t\t\t\tlogging.V(5).Infof(\"[DRY RUN] Would clean up ENI %s in region %s\", eni.ID, eni.Region)", "\t\t\t\tresult.SkippedCount++", "\t\t\t\tcontinue", "\t\t\t}", "", "\t\t\t// For security group disassociation, we need to determine which groups to remove", "\t\t\tvar newGroups []string", "\t\t\tvar targetSG string", "\t\t\tvar actionTaken string", "", "\t\t\t// If targetSecurityGroupId is specified, we only want to remove that one", "\t\t\tif targetSecurityGroupId != nil && *targetSecurityGroupId != \"\" {", "\t\t\t\ttargetSG = *targetSecurityGroupId", "\t\t\t\t// Keep all security groups except the target one", "\t\t\t\tfor _, sg := range eni.SecurityGroups {", "\t\t\t\t\tif sg != targetSG {", "\t\t\t\t\t\tnewGroups = append(newGroups, sg)", "\t\t\t\t\t}", "\t\t\t\t}", "", "\t\t\t\t// If no groups would be left and we have a default, use it", "\t\t\t\tif len(newGroups) == 0 && defaultSG != \"\" {", "\t\t\t\t\tnewGroups = append(newGroups, defaultSG)", "\t\t\t\t}", "", "\t\t\t\t// If the target SG is not in the current groups, skip", "\t\t\t\tsgFound := false", "\t\t\t\tfor _, sg := range eni.SecurityGroups {", "\t\t\t\t\tif sg == targetSG {", "\t\t\t\t\t\tsgFound = true", "\t\t\t\t\t\tbreak", "\t\t\t\t\t}", "\t\t\t\t}", "", "\t\t\t\tif !sgFound {", "\t\t\t\t\tlogging.V(5).Infof(\"ENI %s does not have target security group %s, skipping\", eni.ID, targetSG)", "\t\t\t\t\tresult.SkippedCount++", "\t\t\t\t\tcontinue", "\t\t\t\t}", "", "\t\t\t\tactionTaken = \"disassociated from security group \" + targetSG", "\t\t\t} else {", "\t\t\t\t// If no target is specified, remove all security groups and use default if available", "\t\t\t\tif defaultSG != \"\" {", "\t\t\t\t\tnewGroups = []string{defaultSG}", "\t\t\t\t} else {", "\t\t\t\t\tnewGroups = []string{} // Empty which is OK for AWS", "\t\t\t\t}", "\t\t\t\tactionTaken = \"disassociated from all security groups\"", "\t\t\t}", "", "\t\t\t// Modify the ENI's security groups", "\t\t\tlogging.V(5).Infof(\"Modifying security groups for ENI %s\", eni.ID)", "\t\t\t_, err := ec2Client.ModifyNetworkInterfaceAttribute(ctx, &ec2.ModifyNetworkInterfaceAttributeInput{", "\t\t\t\tNetworkInterfaceId: aws.String(eni.ID),", "\t\t\t\tGroups:             newGroups,", "\t\t\t})", "", "\t\t\tif err != nil {", "\t\t\t\terrMsg := fmt.Sprintf(\"Failed to modify security groups for ENI %s: %v\", eni.ID, err)", "\t\t\t\tresult.Errors = append(result.Errors, errMsg)", "", "\t\t\t\t// Try to tag for manual cleanup", "\t\t\t\ttagENIForManualCleanup(ctx, ec2Client, eni.ID, err.Error())", "\t\t\t\tresult.FailureCount++", "\t\t\t\tcontinue", "\t\t\t}", "", "\t\t\t// Only attempt to delete if not in disassociate-only mode", "\t\t\tif !disassociateOnly {", "\t\t\t\t// Detach the ENI if it's attached", "\t\t\t\tif eni.AttachmentState != \"\" && eni.AttachmentState != \"detached\" && eni.AttachmentID != \"\" {", "\t\t\t\t\tlogging.V(5).Infof(\"Detaching ENI %s (attachment ID: %s)\", eni.ID, eni.AttachmentID)", "\t\t\t\t\t_, err := ec2Client.DetachNetworkInterface(ctx, &ec2.DetachNetworkInterfaceInput{", "\t\t\t\t\t\tAttachmentId: aws.String(eni.AttachmentID),", "\t\t\t\t\t\tForce:        aws.Bool(true),", "\t\t\t\t\t})", "\t\t\t\t\tif err != nil {", "\t\t\t\t\t\terrMsg := fmt.Sprintf(\"Error detaching ENI %s: %v\", eni.ID, err)", "\t\t\t\t\t\tresult.Errors = append(result.Errors, errMsg)", "\t\t\t\t\t\tresult.FailureCount++", "\t\t\t\t\t\tcontinue", "\t\t\t\t\t}", "", "\t\t\t\t\t// Wait a moment for detachment to complete", "\t\t\t\t\ttime.Sleep(5 * time.Second)", "\t\t\t\t}", "", "\t\t\t\t// Try to delete the ENI", "\t\t\t\tlogging.V(5).Infof(\"Deleting ENI %s\", eni.ID)", "\t\t\t\t_, err = ec2Client.DeleteNetworkInterface(ctx, &ec2.DeleteNetworkInterfaceInput{", "\t\t\t\t\tNetworkInterfaceId: aws.String(eni.ID),", "\t\t\t\t})", "\t\t\t\tif err != nil {", "\t\t\t\t\t// Tag the ENI for manual cleanup since we can't delete it", "\t\t\t\t\terrMsg := fmt.Sprintf(\"Could not delete ENI %s after removing security groups: %v\", eni.ID, err)", "\t\t\t\t\tresult.Errors = append(result.Errors, errMsg)", "\t\t\t\t\ttagENIForManualCleanup(ctx, ec2Client, eni.ID, err.Error())", "", "\t\t\t\t\t// But we succeeded in disassociating security groups, so count as success with disassociate action", "\t\t\t\t\tactionTaken = \"disassociated from security groups (delete failed)\"", "\t\t\t\t} else {", "\t\t\t\t\tactionTaken = \"deleted\"", "\t\t\t\t}", "\t\t\t}", "", "\t\t\t// Success - add to cleaned ENIs", "\t\t\tresult.SuccessCount++", "\t\t\tresult.CleanedENIs = append(result.CleanedENIs, CleanedENI{", "\t\t\t\tID:            eni.ID,", "\t\t\t\tRegion:        eni.Region,", "\t\t\t\tVpcID:         eni.VPCID,", "\t\t\t\tDescription:   eni.Description,", "\t\t\t\tActionTaken:   actionTaken,", "\t\t\t\tSecurityGroup: targetSG,", "\t\t\t})", "\t\t}", "\t}", "", "\treturn result", "}", "", "// findNetworkInterfaces finds ENIs in the given region based on filters", "func findNetworkInterfaces(ctx context.Context, client *ec2.Client, filters []types.Filter) ([]types.NetworkInterface, error) {", "\t// Find ENIs with the specified filters", "\tresp, err := client.DescribeNetworkInterfaces(ctx, &ec2.DescribeNetworkInterfacesInput{", "\t\tFilters: filters,", "\t})", "\tif err != nil {", "\t\treturn nil, err", "\t}", "", "\treturn resp.NetworkInterfaces, nil", "}", "", "// tagENIForManualCleanup tags an ENI for manual cleanup", "func tagENIForManualCleanup(ctx context.Context, client *ec2.Client, eniID string, errorMsg string) {", "\ttimestamp := time.Now().UTC().Format(time.RFC3339)", "\t_, err := client.CreateTags(ctx, &ec2.CreateTagsInput{", "\t\tResources: []string{eniID},", "\t\tTags: []types.Tag{", "\t\t\t{", "\t\t\t\tKey:   aws.String(\"NeedsManualCleanup\"),", "\t\t\t\tValue: aws.String(\"true\"),", "\t\t\t},", "\t\t\t{", "\t\t\t\tKey:   aws.String(\"AttemptedCleanupTime\"),", "\t\t\t\tValue: aws.String(timestamp),", "\t\t\t},", "\t\t\t{", "\t\t\t\tKey:   aws.String(\"DeletionError\"),", "\t\t\t\tValue: aws.String(errorMsg),", "\t\t\t},", "\t\t},", "\t})", "\tif err != nil {", "\t\tlogging.V(5).Infof(\"Failed to tag ENI %s for manual cleanup: %v\", eniID, err)", "\t}", "}"], "file_path": "go-provider/pkg/resource/enicleanup/resource.go"}
{"Link_to_commit": "https://github.com/openreview/openreview-py/commit/a45a3dc9e82c717048dc593d17aea4311e350516", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 56, "n_files_impacted": 3, "longest_chunk": ["        ## Ask reviewers to edit their ACK the rebuttals", "        venue = openreview.helpers.get_conference(client, request_form.id, setup=False)", "        venue.custom_stage = openreview.stages.CustomStage(name='Rebuttal_Acknowledgement_Revision',", "            child_invitations_name='Revision',", "            reply_to='Rebuttal_Acknowledgement',", "            reply_type=openreview.stages.CustomStage.ReplyType.REVISION,", "            source=openreview.stages.CustomStage.Source.ALL_SUBMISSIONS,", "            due_date=due_date,", "            exp_date=due_date + datetime.timedelta(days=1),", "            invitees=[openreview.stages.CustomStage.Participants.REPLYTO_REPLYTO_SIGNATURES],", "            readers=[openreview.stages.CustomStage.Participants.REVIEWERS_SUBMITTED, openreview.stages.CustomStage.Participants.AUTHORS],", "            content={", "                'final_acknowledgement': {", "                    'order': 1,", "                    'description': \"I acknowledge I read the rebuttal.\",", "                    'value': {", "                        'param': {", "                            'type': 'boolean',", "                            'enum': [{ 'value': True, 'description': 'Yes, I acknowledge I read the rebuttal.' }],", "                            'input': 'checkbox'", "                        }", "                    }", "                }", "            },", "            notify_readers=True,", "            email_sacs=False)", "", "        venue.create_custom_stage()", "", "        helpers.await_queue_edit(openreview_client, 'ICML.cc/2023/Conference/-/Rebuttal_Acknowledgement_Revision-0-1', count=1)", "", "        ack_revision_invitations = openreview_client.get_invitations(invitation='ICML.cc/2023/Conference/-/Rebuttal_Acknowledgement_Revision')", "        assert len(ack_revision_invitations) == 2", "", "        ack_revision_invitation_ids = [invitation.id for invitation in ack_revision_invitations]", "        assert 'ICML.cc/2023/Conference/Submission1/Rebuttal2/Rebuttal_Acknowledgement1/-/Revision' in ack_revision_invitation_ids", "        assert 'ICML.cc/2023/Conference/Submission1/Rebuttal3/Rebuttal_Acknowledgement1/-/Revision' in ack_revision_invitation_ids", "", "        revision_invitation = openreview_client.get_invitation('ICML.cc/2023/Conference/Submission1/Rebuttal2/Rebuttal_Acknowledgement1/-/Revision')", "        assert revision_invitation.edit['note']['id'] == rebuttal_ack1_edit['note']['id']", "", "        revision_invitation = openreview_client.get_invitation('ICML.cc/2023/Conference/Submission1/Rebuttal3/Rebuttal_Acknowledgement1/-/Revision')", "        assert revision_invitation.edit['note']['id'] == rebuttal_ack2_edit['note']['id']", "        ", "        rebuttal_ack2_revision_edit = reviewer_client.post_note_edit(", "            invitation='ICML.cc/2023/Conference/Submission1/Rebuttal3/Rebuttal_Acknowledgement1/-/Revision',", "            signatures=[anon_group_id],", "            note=openreview.api.Note(", "                content={", "                    'final_acknowledgement': { 'value': True }", "                }", "            )", "        )", "", "        helpers.await_queue_edit(openreview_client, edit_id=rebuttal_ack2_revision_edit['id'])", ""], "file_path": "tests/test_icml_conference.py"}
{"Link_to_commit": "https://github.com/bytebase/bytebase/commit/bfff3c36131a4591391d36e1cd91bf09336b3ee4", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 7, "n_files_impacted": 4, "longest_chunk": ["\t\t\tm, ok := tp.GetAlgorithm().GetMask().(*storepb.Algorithm_InnerOuterMask_)", "\t\t\tif ok && m.InnerOuterMask != nil {", "\t\t\t\tif m.InnerOuterMask.Type == storepb.Algorithm_InnerOuterMask_MASK_TYPE_UNSPECIFIED {", "\t\t\t\t\treturn nil, status.Errorf(codes.InvalidArgument, \"inner outer mask type has to be specified\")", "\t\t\t\t}", "\t\t\t}", "\t\t\tidMap[tp.Id] = true"], "file_path": "backend/api/v1/setting_service.go"}
{"Link_to_commit": "https://github.com/RabbyHub/rabby-mobile/commit/afc5a43edbbef06454fe67439c2ca8ea4539fc75", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 39, "n_files_impacted": 4, "longest_chunk": ["import { MigrationInterface, QueryRunner } from 'typeorm/browser';", "import { APP_DB_PREFIX } from '../constant';", "", "const historyTableName = `${APP_DB_PREFIX}cache_localhistoryitem`;", "", "async function checkIfTableExists(queryRunner: QueryRunner, tableName: string) {", "  const tableExists = await queryRunner.query(", "    `", "    SELECT 1 FROM sqlite_master WHERE type='table' AND name=?;", "  `,", "    [tableName],", "  );", "", "  return tableExists.length > 0;", "}", "", "export class UpdateHistoryTableAddSourceType1744873800025", "  implements MigrationInterface", "{", "  transaction = false;", "", "  async up(queryRunner: QueryRunner): Promise<void> {", "    const tableExists = await checkIfTableExists(queryRunner, historyTableName);", "    if (tableExists) {", "      await queryRunner.query(", "        `ALTER TABLE '${historyTableName}' ADD COLUMN source_type TEXT DEFAULT ''`,", "      );", "    }", "  }", "", "  async down(queryRunner: QueryRunner): Promise<void> {", "    const tableExists = await checkIfTableExists(queryRunner, historyTableName);", "    if (tableExists) {", "      await queryRunner.query(", "        `ALTER TABLE '${historyTableName}' DROP COLUMN source_type`,", "      );", "    }", "  }", "}"], "file_path": "apps/mobile/src/databases/migrations/index.ts"}
{"Link_to_commit": "https://github.com/ppoz21/symfony-discounts-example/commit/5c18eb915fafbed08a808d16fd190bb349f8ae94", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 30, "n_files_impacted": 2, "longest_chunk": ["        for ($i = 0; $i < $instance->getNumberOfCodes(); ++$i) {", "            do {", "                $code = self::generateSingleCode($instance->getCodePrefix());", "            } while ($em->getRepository(DiscountCode::class)->findOneBy(['code' => $code]));", "", "            $codeObj = (new DiscountCode())", "                ->setCode($code)", "                ->setDiscount($instance)", "            ;", "", "            $em->persist($codeObj);", "            $em->flush();", "        }", "", "        return $this->redirect($this->adminUrlGenerator->setAction(Action::INDEX)->generateUrl());", "    }", "", "    private static function generateSingleCode(string $prefix): string", "    {", "        $chars = array_flip(", "            array_merge(range(0, 9), range('A', 'Z'))", "        );", "", "        $randomString = '';", "", "        while (strlen($randomString) < 10) {", "            $randomString .= array_rand($chars);", "        }", "", "        return (str_ends_with($prefix, '_') ? $prefix : ($prefix.'_')).$randomString;"], "file_path": "src/Controller/Admin/DiscountCrudController.php"}
{"Link_to_commit": "https://github.com/qw1375/web-alist/commit/f0b1aeaf8d846b3aee41fed29bf03ad7afa4e72f", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 451, "n_files_impacted": 4, "longest_chunk": ["", "\t\tresp = append(r.Data.Children, nextFiles...)", "\t}", "", "\treturn resp, err", "}", "", "func (d *Doubao) getUserInfo() (UserInfo, error) {", "\tvar r UserInfoResp", "", "\t_, err := d.request(\"/passport/account/info/v2/\", http.MethodGet, nil, &r)", "\tif err != nil {", "\t\treturn UserInfo{}, err", "\t}", "", "\treturn r.Data, err", "}", "", "// \u7b7e\u540d\u8bf7\u6c42", "func (d *Doubao) signRequest(req *resty.Request, method, tokenType, uploadUrl string) error {", "\tparsedUrl, err := url.Parse(uploadUrl)", "\tif err != nil {", "\t\treturn fmt.Errorf(\"invalid URL format: %w\", err)", "\t}", "", "\tvar accessKeyId, secretAccessKey, sessionToken string", "\tvar serviceName string", "", "\tif tokenType == VideoDataType {", "\t\taccessKeyId = d.UploadToken.Samantha.StsToken.AccessKeyID", "\t\tsecretAccessKey = d.UploadToken.Samantha.StsToken.SecretAccessKey", "\t\tsessionToken = d.UploadToken.Samantha.StsToken.SessionToken", "\t\tserviceName = \"vod\"", "\t} else {", "\t\taccessKeyId = d.UploadToken.Alice[tokenType].Auth.AccessKeyID", "\t\tsecretAccessKey = d.UploadToken.Alice[tokenType].Auth.SecretAccessKey", "\t\tsessionToken = d.UploadToken.Alice[tokenType].Auth.SessionToken", "\t\tserviceName = \"imagex\"", "\t}", "", "\t// \u5f53\u524d\u65f6\u95f4\uff0c\u683c\u5f0f\u4e3a ISO8601", "\tnow := time.Now().UTC()", "\tamzDate := now.Format(\"20060102T150405Z\")", "\tdateStamp := now.Format(\"20060102\")", "", "\treq.SetHeader(\"X-Amz-Date\", amzDate)", "", "\tif sessionToken != \"\" {", "\t\treq.SetHeader(\"X-Amz-Security-Token\", sessionToken)", "\t}", "", "\t// \u8ba1\u7b97\u8bf7\u6c42\u4f53\u7684SHA256\u54c8\u5e0c", "\tvar bodyHash string", "\tif req.Body != nil {", "\t\tbodyBytes, ok := req.Body.([]byte)", "\t\tif !ok {", "\t\t\treturn fmt.Errorf(\"request body must be []byte\")", "\t\t}", "", "\t\tbodyHash = hashSHA256(string(bodyBytes))", "\t\treq.SetHeader(\"X-Amz-Content-Sha256\", bodyHash)", "\t} else {", "\t\tbodyHash = hashSHA256(\"\")", "\t}", "", "\t// \u521b\u5efa\u89c4\u8303\u8bf7\u6c42", "\tcanonicalURI := parsedUrl.Path", "\tif canonicalURI == \"\" {", "\t\tcanonicalURI = \"/\"", "\t}", "", "\t// \u67e5\u8be2\u53c2\u6570\u6309\u7167\u5b57\u6bcd\u987a\u5e8f\u6392\u5e8f", "\tcanonicalQueryString := getCanonicalQueryString(req.QueryParam)", "\t// \u89c4\u8303\u8bf7\u6c42\u5934", "\tcanonicalHeaders, signedHeaders := getCanonicalHeadersFromMap(req.Header)", "\tcanonicalRequest := method + \"\\n\" +", "\t\tcanonicalURI + \"\\n\" +", "\t\tcanonicalQueryString + \"\\n\" +", "\t\tcanonicalHeaders + \"\\n\" +", "\t\tsignedHeaders + \"\\n\" +", "\t\tbodyHash", "", "\talgorithm := \"AWS4-HMAC-SHA256\"", "\tcredentialScope := fmt.Sprintf(\"%s/%s/%s/aws4_request\", dateStamp, Region, serviceName)", "", "\tstringToSign := algorithm + \"\\n\" +", "\t\tamzDate + \"\\n\" +", "\t\tcredentialScope + \"\\n\" +", "\t\thashSHA256(canonicalRequest)", "\t// \u8ba1\u7b97\u7b7e\u540d\u5bc6\u94a5", "\tsigningKey := getSigningKey(secretAccessKey, dateStamp, Region, serviceName)", "\t// \u8ba1\u7b97\u7b7e\u540d", "\tsignature := hmacSHA256Hex(signingKey, stringToSign)", "\t// \u6784\u5efa\u6388\u6743\u5934", "\tauthorizationHeader := fmt.Sprintf(", "\t\t\"%s Credential=%s/%s, SignedHeaders=%s, Signature=%s\",", "\t\talgorithm,", "\t\taccessKeyId,", "\t\tcredentialScope,", "\t\tsignedHeaders,", "\t\tsignature,", "\t)", "", "\treq.SetHeader(\"Authorization\", authorizationHeader)", "", "\treturn nil", "}", "", "func (d *Doubao) requestApi(url, method, tokenType string, callback base.ReqCallback, resp interface{}) ([]byte, error) {", "\treq := base.RestyClient.R()", "\treq.SetHeaders(map[string]string{", "\t\t\"user-agent\": UserAgent,", "\t})", "", "\tif method == http.MethodPost {", "\t\treq.SetHeader(\"Content-Type\", \"text/plain;charset=UTF-8\")", "\t}", "", "\tif callback != nil {", "\t\tcallback(req)", "\t}", "", "\tif resp != nil {", "\t\treq.SetResult(resp)", "\t}", "", "\t// \u4f7f\u7528\u81ea\u5b9a\u4e49AWS SigV4\u7b7e\u540d", "\terr := d.signRequest(req, method, tokenType, url)", "\tif err != nil {", "\t\treturn nil, err", "\t}", "", "\tres, err := req.Execute(method, url)", "\tif err != nil {", "\t\treturn nil, err", "\t}", "", "\treturn res.Body(), nil", "}", "", "func (d *Doubao) initUploadToken() (*UploadToken, error) {", "\tuploadToken := &UploadToken{", "\t\tAlice:    make(map[string]UploadAuthToken),", "\t\tSamantha: MediaUploadAuthToken{},", "\t}", "", "\tfileAuthToken, err := d.getUploadAuthToken(FileDataType)", "\tif err != nil {", "\t\treturn nil, err", "\t}", "", "\timgAuthToken, err := d.getUploadAuthToken(ImgDataType)", "\tif err != nil {", "\t\treturn nil, err", "\t}", "", "\tmediaAuthToken, err := d.getSamantaUploadAuthToken()", "\tif err != nil {", "\t\treturn nil, err", "\t}", "", "\tuploadToken.Alice[FileDataType] = fileAuthToken", "\tuploadToken.Alice[ImgDataType] = imgAuthToken", "\tuploadToken.Samantha = mediaAuthToken", "", "\treturn uploadToken, nil", "}", "", "func (d *Doubao) getUploadAuthToken(dataType string) (ut UploadAuthToken, err error) {", "\tvar r UploadAuthTokenResp", "\t_, err = d.request(\"/alice/upload/auth_token\", http.MethodPost, func(req *resty.Request) {", "\t\treq.SetBody(base.Json{", "\t\t\t\"scene\":     \"bot_chat\",", "\t\t\t\"data_type\": dataType,", "\t\t})", "\t}, &r)", "", "\treturn r.Data, err", "}", "", "func (d *Doubao) getSamantaUploadAuthToken() (mt MediaUploadAuthToken, err error) {", "\tvar r MediaUploadAuthTokenResp", "\t_, err = d.request(\"/samantha/media/get_upload_token\", http.MethodPost, func(req *resty.Request) {", "\t\treq.SetBody(base.Json{})", "\t}, &r)", "", "\treturn r.Data, err", "}", "", "// getUploadConfig \u83b7\u53d6\u4e0a\u4f20\u914d\u7f6e\u4fe1\u606f", "func (d *Doubao) getUploadConfig(upConfig *UploadConfig, dataType string, file model.FileStreamer) error {", "\ttokenType := dataType", "\t// \u914d\u7f6e\u53c2\u6570\u51fd\u6570", "\tconfigureParams := func() (string, map[string]string) {", "\t\tvar uploadUrl string", "\t\tvar params map[string]string", "\t\t// \u6839\u636e\u6570\u636e\u7c7b\u578b\u8bbe\u7f6e\u4e0d\u540c\u7684\u4e0a\u4f20\u53c2\u6570", "\t\tswitch dataType {", "\t\tcase VideoDataType:", "\t\t\t// \u97f3\u9891/\u89c6\u9891\u7c7b\u578b - \u4f7f\u7528uploadToken.Samantha\u7684\u914d\u7f6e", "\t\t\tuploadUrl = d.UploadToken.Samantha.UploadInfo.VideoHost", "\t\t\tparams = map[string]string{", "\t\t\t\t\"Action\":       \"ApplyUploadInner\",", "\t\t\t\t\"Version\":      \"2020-11-19\",", "\t\t\t\t\"SpaceName\":    d.UploadToken.Samantha.UploadInfo.SpaceName,", "\t\t\t\t\"FileType\":     \"video\",", "\t\t\t\t\"IsInner\":      \"1\",", "\t\t\t\t\"NeedFallback\": \"true\",", "\t\t\t\t\"FileSize\":     strconv.FormatInt(file.GetSize(), 10),", "\t\t\t\t\"s\":            randomString(),", "\t\t\t}", "\t\tcase ImgDataType, FileDataType:", "\t\t\t// \u56fe\u7247\u6216\u5176\u4ed6\u6587\u4ef6\u7c7b\u578b - \u4f7f\u7528uploadToken.Alice\u5bf9\u5e94\u914d\u7f6e", "\t\t\tuploadUrl = \"https://\" + d.UploadToken.Alice[dataType].UploadHost", "\t\t\tparams = map[string]string{", "\t\t\t\t\"Action\":        \"ApplyImageUpload\",", "\t\t\t\t\"Version\":       \"2018-08-01\",", "\t\t\t\t\"ServiceId\":     d.UploadToken.Alice[dataType].ServiceID,", "\t\t\t\t\"NeedFallback\":  \"true\",", "\t\t\t\t\"FileSize\":      strconv.FormatInt(file.GetSize(), 10),", "\t\t\t\t\"FileExtension\": filepath.Ext(file.GetName()),", "\t\t\t\t\"s\":             randomString(),", "\t\t\t}", "\t\t}", "\t\treturn uploadUrl, params", "\t}", "", "\t// \u83b7\u53d6\u521d\u59cb\u53c2\u6570", "\tuploadUrl, params := configureParams()", "", "\ttokenRefreshed := false", "\tvar configResp UploadConfigResp", "", "\terr := d._retryOperation(\"get upload_config\", func() error {", "\t\tconfigResp = UploadConfigResp{}", "", "\t\t_, err := d.requestApi(uploadUrl, http.MethodGet, tokenType, func(req *resty.Request) {", "\t\t\treq.SetQueryParams(params)", "\t\t}, &configResp)", "\t\tif err != nil {", "\t\t\treturn err", "\t\t}", "", "\t\tif configResp.ResponseMetadata.Error.Code == \"\" {", "\t\t\t*upConfig = configResp.Result", "\t\t\treturn nil", "\t\t}", "", "\t\t// 100028 \u51ed\u8bc1\u8fc7\u671f", "\t\tif configResp.ResponseMetadata.Error.CodeN == 100028 && !tokenRefreshed {", "\t\t\tlog.Debugln(\"[doubao] Upload token expired, re-fetching...\")", "\t\t\tnewToken, err := d.initUploadToken()", "\t\t\tif err != nil {", "\t\t\t\treturn fmt.Errorf(\"failed to refresh token: %w\", err)", "\t\t\t}", "", "\t\t\td.UploadToken = newToken", "\t\t\ttokenRefreshed = true", "\t\t\tuploadUrl, params = configureParams()", "", "\t\t\treturn retry.Error{errors.New(\"token refreshed, retry needed\")}", "\t\t}", "", "\t\treturn fmt.Errorf(\"get upload_config failed: %s\", configResp.ResponseMetadata.Error.Message)", "\t})", "", "\treturn err", "}", "", "// uploadNode \u4e0a\u4f20 \u6587\u4ef6\u4fe1\u606f", "func (d *Doubao) uploadNode(uploadConfig *UploadConfig, dir model.Obj, file model.FileStreamer, dataType string) (UploadNodeResp, error) {", "\treqUuid := uuid.New().String()", "\tvar key string", "\tvar nodeType int", "", "\tmimetype := file.GetMimetype()", "\tswitch dataType {", "\tcase VideoDataType:", "\t\tkey = uploadConfig.InnerUploadAddress.UploadNodes[0].Vid", "\t\tif strings.HasPrefix(mimetype, \"audio/\") {", "\t\t\tnodeType = AudioType // \u97f3\u9891\u7c7b\u578b", "\t\t} else {", "\t\t\tnodeType = VideoType // \u89c6\u9891\u7c7b\u578b", "\t\t}", "\tcase ImgDataType:", "\t\tkey = uploadConfig.InnerUploadAddress.UploadNodes[0].StoreInfos[0].StoreURI", "\t\tnodeType = ImageType // \u56fe\u7247\u7c7b\u578b", "\tdefault: // FileDataType", "\t\tkey = uploadConfig.InnerUploadAddress.UploadNodes[0].StoreInfos[0].StoreURI", "\t\tnodeType = FileType // \u6587\u4ef6\u7c7b\u578b", "\t}", "", "\tvar r UploadNodeResp", "\t_, err := d.request(\"/samantha/aispace/upload_node\", http.MethodPost, func(req *resty.Request) {", "\t\treq.SetBody(base.Json{", "\t\t\t\"node_list\": []base.Json{", "\t\t\t\t{", "\t\t\t\t\t\"local_id\":     reqUuid,", "\t\t\t\t\t\"parent_id\":    dir.GetID(),", "\t\t\t\t\t\"name\":         file.GetName(),", "\t\t\t\t\t\"key\":          key,", "\t\t\t\t\t\"node_content\": base.Json{},", "\t\t\t\t\t\"node_type\":    nodeType,", "\t\t\t\t\t\"size\":         file.GetSize(),", "\t\t\t\t},", "\t\t\t},", "\t\t\t\"request_id\": reqUuid,", "\t\t})", "\t}, &r)", "", "\treturn r, err", "}", "", "// Upload \u666e\u901a\u4e0a\u4f20\u5b9e\u73b0", "func (d *Doubao) Upload(config *UploadConfig, dstDir model.Obj, file model.FileStreamer, up driver.UpdateProgress, dataType string) (model.Obj, error) {", "\tdata, err := io.ReadAll(file)", "\tif err != nil {", "\t\treturn nil, err", "\t}", "", "\t// \u8ba1\u7b97CRC32", "\tcrc32Hash := crc32.NewIEEE()", "\tcrc32Hash.Write(data)", "\tcrc32Value := hex.EncodeToString(crc32Hash.Sum(nil))", "", "\t// \u6784\u5efa\u8bf7\u6c42\u8def\u5f84", "\tuploadNode := config.InnerUploadAddress.UploadNodes[0]", "\tstoreInfo := uploadNode.StoreInfos[0]", "\tuploadUrl := fmt.Sprintf(\"https://%s/upload/v1/%s\", uploadNode.UploadHost, storeInfo.StoreURI)", "", "\tuploadResp := UploadResp{}", "", "\tif _, err = d.uploadRequest(uploadUrl, http.MethodPost, storeInfo, func(req *resty.Request) {", "\t\treq.SetHeaders(map[string]string{", "\t\t\t\"Content-Type\":        \"application/octet-stream\",", "\t\t\t\"Content-Crc32\":       crc32Value,", "\t\t\t\"Content-Length\":      fmt.Sprintf(\"%d\", len(data)),", "\t\t\t\"Content-Disposition\": fmt.Sprintf(\"attachment; filename=%s\", url.QueryEscape(storeInfo.StoreURI)),", "\t\t})", "", "\t\treq.SetBody(data)", "\t}, &uploadResp); err != nil {", "\t\treturn nil, err", "\t}", "", "\tif uploadResp.Code != 2000 {", "\t\treturn nil, fmt.Errorf(\"upload failed: %s\", uploadResp.Message)", "\t}", "", "\tuploadNodeResp, err := d.uploadNode(config, dstDir, file, dataType)", "\tif err != nil {", "\t\treturn nil, err", "\t}", "", "\treturn &model.Object{", "\t\tID:       uploadNodeResp.Data.NodeList[0].ID,", "\t\tName:     uploadNodeResp.Data.NodeList[0].Name,", "\t\tSize:     file.GetSize(),", "\t\tIsFolder: false,", "\t}, nil", "}", "", "// UploadByMultipart \u5206\u7247\u4e0a\u4f20", "func (d *Doubao) UploadByMultipart(ctx context.Context, config *UploadConfig, fileSize int64, dstDir model.Obj, file model.FileStreamer, up driver.UpdateProgress, dataType string) (model.Obj, error) {", "\t// \u6784\u5efa\u8bf7\u6c42\u8def\u5f84", "\tuploadNode := config.InnerUploadAddress.UploadNodes[0]", "\tstoreInfo := uploadNode.StoreInfos[0]", "\tuploadUrl := fmt.Sprintf(\"https://%s/upload/v1/%s\", uploadNode.UploadHost, storeInfo.StoreURI)", "\t// \u521d\u59cb\u5316\u5206\u7247\u4e0a\u4f20", "\tvar uploadID string", "\terr := d._retryOperation(\"Initialize multipart upload\", func() error {", "\t\tvar err error", "\t\tuploadID, err = d.initMultipartUpload(config, uploadUrl, storeInfo)", "\t\treturn err", "\t})", "\tif err != nil {", "\t\treturn nil, fmt.Errorf(\"failed to initialize multipart upload: %w\", err)", "\t}", "\t// \u51c6\u5907\u5206\u7247\u53c2\u6570", "\tchunkSize := DefaultChunkSize", "\tif config.InnerUploadAddress.AdvanceOption.SliceSize > 0 {", "\t\tchunkSize = int64(config.InnerUploadAddress.AdvanceOption.SliceSize)", "\t}", "\ttotalParts := (fileSize + chunkSize - 1) / chunkSize", "\t// \u521b\u5efa\u5206\u7247\u4fe1\u606f\u7ec4", "\tparts := make([]UploadPart, totalParts)", "\t// \u7f13\u5b58\u6587\u4ef6", "\ttempFile, err := file.CacheFullInTempFile()", "\tif err != nil {", "\t\treturn nil, fmt.Errorf(\"failed to cache file: %w\", err)", "\t}", "\tdefer tempFile.Close()", "\tup(10.0) // \u66f4\u65b0\u8fdb\u5ea6", "\t// \u8bbe\u7f6e\u5e76\u884c\u4e0a\u4f20", "\tthreadG, uploadCtx := errgroup.NewGroupWithContext(ctx, d.uploadThread,", "\t\tretry.Attempts(1),", "\t\tretry.Delay(time.Second),", "\t\tretry.DelayType(retry.BackOffDelay))", "", "\tvar partsMutex sync.Mutex", "\t// \u5e76\u884c\u4e0a\u4f20\u6240\u6709\u5206\u7247", "\tfor partIndex := int64(0); partIndex < totalParts; partIndex++ {", "\t\tif utils.IsCanceled(uploadCtx) {", "\t\t\tbreak", "\t\t}", "\t\tpartIndex := partIndex", "\t\tpartNumber := partIndex + 1 // \u5206\u7247\u7f16\u53f7\u4ece1\u5f00\u59cb", "", "\t\tthreadG.Go(func(ctx context.Context) error {", "\t\t\t// \u8ba1\u7b97\u6b64\u5206\u7247\u7684\u5927\u5c0f\u548c\u504f\u79fb", "\t\t\toffset := partIndex * chunkSize", "\t\t\tsize := chunkSize", "\t\t\tif partIndex == totalParts-1 {", "\t\t\t\tsize = fileSize - offset", "\t\t\t}", "", "\t\t\tlimitedReader := driver.NewLimitedUploadStream(ctx, io.NewSectionReader(tempFile, offset, size))", "\t\t\t// \u8bfb\u53d6\u6570\u636e\u5230\u5185\u5b58", "\t\t\tdata, err := io.ReadAll(limitedReader)", "\t\t\tif err != nil {", "\t\t\t\treturn fmt.Errorf(\"failed to read part %d: %w\", partNumber, err)", "\t\t\t}", "\t\t\t// \u8ba1\u7b97CRC32", "\t\t\tcrc32Value := calculateCRC32(data)", "\t\t\t// \u4f7f\u7528_retryOperation\u4e0a\u4f20\u5206\u7247", "\t\t\tvar uploadPart UploadPart", "\t\t\tif err = d._retryOperation(fmt.Sprintf(\"Upload part %d\", partNumber), func() error {", "\t\t\t\tvar err error", "\t\t\t\tuploadPart, err = d.uploadPart(config, uploadUrl, uploadID, partNumber, data, crc32Value)", "\t\t\t\treturn err", "\t\t\t}); err != nil {", "\t\t\t\treturn fmt.Errorf(\"part %d upload failed: %w\", partNumber, err)", "\t\t\t}", "\t\t\t// \u8bb0\u5f55\u6210\u529f\u4e0a\u4f20\u7684\u5206\u7247", "\t\t\tpartsMutex.Lock()", "\t\t\tparts[partIndex] = UploadPart{", "\t\t\t\tPartNumber: strconv.FormatInt(partNumber, 10),", "\t\t\t\tEtag:       uploadPart.Etag,", "\t\t\t\tCrc32:      crc32Value,", "\t\t\t}", "\t\t\tpartsMutex.Unlock()", "\t\t\t// \u66f4\u65b0\u8fdb\u5ea6", "\t\t\tprogress := 10.0 + 90.0*float64(threadG.Success()+1)/float64(totalParts)", "\t\t\tup(math.Min(progress, 95.0))", "", "\t\t\treturn nil", "\t\t})", "\t}", "", "\tif err = threadG.Wait(); err != nil {", "\t\treturn nil, err"], "file_path": "drivers/doubao/util.go"}
{"Link_to_commit": "https://github.com/sachinbhardwajqa/gatling/commit/bbb086ed214ad643af611e76129c3e09d262ea55", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 105, "n_files_impacted": 5, "longest_chunk": ["package perf;", "", "import io.gatling.javaapi.core.ChainBuilder;", "import io.gatling.javaapi.core.FeederBuilder;", "import io.gatling.javaapi.core.ScenarioBuilder;", "import io.gatling.javaapi.core.Simulation;", "import io.gatling.javaapi.http.HttpProtocolBuilder;", "", "import java.util.concurrent.ThreadLocalRandom;", "", "import static io.gatling.javaapi.core.CoreDsl.*;", "import static io.gatling.javaapi.http.HttpDsl.*;", "", "public class RecordedSimulationDemoStore1Refactored6Sessions extends Simulation {", "    private static final String DOMAIN = \"demostore.gatling.io\";", "    private static final HttpProtocolBuilder HTTP_PROTOCOL = http.baseUrl(\"https://\" + DOMAIN);", "    private static final FeederBuilder<String> csvFeederCategoryFeeder = csv(\"data/categoryDetails.csv\").circular();", "    private static final FeederBuilder<Object> jsonFeederProductFeeder = jsonFile(\"data/productDetails.json\").random();", "    private static final FeederBuilder<String> csvFeederLoginDetails = csv(\"data/loginDetails.csv\").circular();", "    private static final ChainBuilder initSession =", "            exec(flushCookieJar())", "                    .exec(session -> session.set(\"randomNumber\", ThreadLocalRandom.current().nextInt()))", "                    .exec(session -> session.set(\"customerLoggedIn\",false))", "                    .exec(session -> session.set(\"cartTotal\",0))", "                    .exec(addCookie(Cookie(\"sessionID\", SessionId.random()).withDomain(DOMAIN)));", "", "    private static class CmsPage {", "        private static final ChainBuilder homePage =", "                exec(http(\"Load Home Page\")", "                        .get(\"/\")", "                        .check(css(\"#_csrf\", \"content\").saveAs(\"csrfValue\"))", "                        .check(regex(\"<title>Gatling Demo-Store</title>\").exists()));", "        private static final ChainBuilder aboutUs =", "                exec(http(\"Load About Us Page\")", "                        .get(\"/about-us\")", "                        .check(substring(\"About Us\")));", "    }", "", "    private static class Catalog {", "        private static class Category {", "            private static final ChainBuilder view =", "                    feed(csvFeederCategoryFeeder)", "                            .repeat(2, \"n\").on(", "                                    exec(http(\"View #{n} Category - #{categoryName}\")", "                                            .get(\"/category/#{categorySlug}\")", "                                            .check(css(\"#CategoryName\").isEL(\"#{categoryName}\")))); // EL = Expression Language", "        }", "", "        private static class Product {", "            private static final ChainBuilder view =", "                    feed(jsonFeederProductFeeder)", "                            .exec(http(\"View Product - #{name}\")", "                                            .get(\"/product/#{slug}\")", "                                            .check(css(\"#ProductDescription\").isEL(\"#{description}\")));", "            private static final ChainBuilder addProductToCart =", "                    exec(view)", "                            .exec(http(\"Add to Cart - ${name}\")", "                                    .get(\"/cart/add/${id}\")", "                                    .check(substring(\"items in your cart\")));", "        }", "    }", "    private static class Customer {", "        private static final ChainBuilder login =", "                feed(csvFeederLoginDetails)", "                        .exec(http(\"Load Login Page for #{username}\")", "                                .get(\"/login\")", "                                .check(substring(\"Username:\")))", "                        .exec(http(\"Customer Login Action with #{username}\")", "                                .post(\"/login\")", "                                .formParam(\"_csrf\", \"#{csrfValue}\")", "                                .formParam(\"username\", \"#{username}\")", "                                .formParam(\"password\", \"#{password}\"));", "    }", "    private static class Checkout {", "        private static final ChainBuilder viewCart =", "                exec(http(\"View Cart\")", "                        .get(\"/cart/view\"));", "        private static final ChainBuilder checkout =", "                        exec(http(\"Checkout\")", "                                .get(\"/cart/checkout\")", "                                .check(substring(\"Thanks for your order! See you soon!\")));", "    }", "", "    private static final ScenarioBuilder scn = scenario(\"RecordedSimulationDemoStore1\")", "            .exec(initSession)", "            .exec(CmsPage.homePage)", "            .pause(2)", "            .exec(CmsPage.aboutUs)", "            .pause(2)", "            .exec(Catalog.Category.view)", "            .pause(2)", "            .exec(Catalog.Product.addProductToCart)", "            .pause(2)", "            .exec(Checkout.viewCart)", "            .pause(3)", "            .exec(Customer.login)", "            .pause(2)", "            .exec(Checkout.checkout)", "            .pause(3)", "            ;", "", "    {", "        setUp(scn.injectOpen(atOnceUsers(4))).protocols(HTTP_PROTOCOL);", "    }", "}"], "file_path": "src/test/java/perf/SessionId.java"}
{"Link_to_commit": "https://github.com/playcanvas/engine/commit/2d0511e76af95f633dc7a23978c2094db9d11b07", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 237, "n_files_impacted": 48, "longest_chunk": ["// backend shader implementing material / lighting for the lit material for forward rendering", "export default /* wgsl */`", "fn evaluateBackend() -> FragmentOutput {", "", "    var output: FragmentOutput;", "", "    // apply SSAO during lighting", "    #ifdef LIT_SSAO", "        litArgs_ao = litArgs_ao * textureSampleLevel(ssaoTexture, ssaoTextureSampler, pcPosition.xy * ssaoTextureSizeInv, 0.0).r;", "    #endif", "", "    // transform tangent space normals to world space", "    #ifdef LIT_NEEDS_NORMAL", "        #ifdef LIT_SPECULAR", "            getReflDir(litArgs_worldNormal, dViewDirW, litArgs_gloss, dTBN);", "        #endif", "", "        #ifdef LIT_CLEARCOAT", "            ccReflDirW = normalize(-reflect(dViewDirW, litArgs_clearcoat_worldNormal));", "        #endif", "    #endif", "", "    #ifdef LIT_SPECULAR_OR_REFLECTION", "        #ifdef LIT_METALNESS", "            var f0: f32 = 1.0 / litArgs_ior;", "            f0 = (f0 - 1.0) / (f0 + 1.0);", "            f0 = f0 * f0;", "            litArgs_specularity = getSpecularModulate(litArgs_specularity, litArgs_albedo, litArgs_metalness, f0);", "            litArgs_albedo = getAlbedoModulate(litArgs_albedo, litArgs_metalness);", "        #endif", "", "        #ifdef LIT_IRIDESCENCE", "            var iridescenceFresnel: vec3f = getIridescence(saturate3(dot(dViewDirW, litArgs_worldNormal)), litArgs_specularity, litArgs_iridescence_thickness);", "        #endif", "    #endif", "", "    // ambient", "    #ifdef LIT_ADD_AMBIENT", "        addAmbient(litArgs_worldNormal);", "", "        #ifdef LIT_SPECULAR", "            dDiffuseLight = dDiffuseLight * (1.0 - litArgs_specularity);", "        #endif", "", "        // move ambient color out of diffuse (used by Lightmapper, to multiply ambient color by accumulated AO)", "        #ifdef LIT_SEPARATE_AMBIENT", "            var dAmbientLight: vec3f = dDiffuseLight;", "            dDiffuseLight = vec3(0.0);", "        #endif", "    #endif", "", "    #ifndef LIT_OLD_AMBIENT", "        dDiffuseLight = dDiffuseLight * uniform.material_ambient;", "    #endif", "", "    #ifdef LIT_AO", "        #ifndef LIT_OCCLUDE_DIRECT", "            occludeDiffuse(litArgs_ao);", "        #endif", "    #endif", "", "    #ifdef LIT_LIGHTMAP", "        addLightMap(", "            litArgs_lightmap, ", "            litArgs_lightmapDir, ", "            litArgs_worldNormal, ", "            dViewDirW, ", "            dReflDirW, ", "            litArgs_gloss, ", "            litArgs_specularity, ", "            dVertexNormalW,", "            dTBN", "        #if defined(LIT_IRIDESCENCE)", "            , iridescenceFresnel,", "            litArgs_iridescence_intensity", "        #endif", "        );", "    #endif", "", "    #ifdef LIT_LIGHTING || LIT_REFLECTIONS", "", "        #ifdef LIT_REFLECTIONS", "", "            #ifdef LIT_CLEARCOAT", "                addReflectionCC(ccReflDirW, litArgs_clearcoat_gloss);", "            ", "                #ifdef LIT_SPECULAR_FRESNEL", "                    ccFresnel = getFresnelCC(dot(dViewDirW, litArgs_clearcoat_worldNormal));", "                    ccReflection.rgb = ccReflection.rgb * ccFresnel;", "                #else", "                    ccFresnel = 0.0;", "                #endif", "            #endif", "", "            #ifdef LIT_SPECULARITY_FACTOR", "                ccReflection.rgb = ccReflection.rgb * litArgs_specularityFactor;", "            #endif", "", "            #ifdef LIT_SHEEN", "                addReflectionSheen(litArgs_worldNormal, dViewDirW, litArgs_sheen_gloss);", "            #endif", "", "            // Fresnel has to be applied to reflections", "            addReflection(dReflDirW, litArgs_gloss);", "", "            #ifdef LIT_FRESNEL_MODEL", "", "                dReflection.rgb = dReflection.rgb * getFresnel(", "                    dot(dViewDirW, litArgs_worldNormal), ", "                    litArgs_gloss, ", "                    litArgs_specularity", "                #if defined(LIT_IRIDESCENCE)", "                    , iridescenceFresnel,", "                    litArgs_iridescence_intensity", "                #endif", "                    );", "", "            #else", "", "                dReflection.rgb = dReflection.rgb * litArgs_specularity;", "", "            #endif", "", "            #ifdef LIT_SPECULARITY_FACTOR", "                dReflection.rgb = dReflection.rgb * litArgs_specularityFactor;", "            #endif", "", "        #endif", "", "        #ifdef AREA_LIGHTS", "            // specular has to be accumulated differently if we want area lights to look correct", "            dSpecularLight = dSpecularLight * litArgs_specularity;", "", "            #ifdef LIT_SPECULAR", "                // evaluate material based area lights data, shared by all area lights", "                calcLTCLightValues(litArgs_gloss, litArgs_worldNormal, dViewDirW, litArgs_specularity, litArgs_clearcoat_gloss, litArgs_clearcoat_worldNormal, litArgs_clearcoat_specularity);", "            #endif", "        #endif", "        ", "        // LOOP - evaluate all non-clustered lights", "        #include \"lightEvaluationPS, LIGHT_COUNT\"", "", "        // clustered lighting", "        #ifdef LIT_CLUSTERED_LIGHTS", "            addClusteredLights(litArgs_worldNormal, dViewDirW, dReflDirW,", "                #if defined(LIT_CLEARCOAT)", "                        ccReflDirW,", "                #endif", "                        litArgs_gloss, litArgs_specularity, dVertexNormalW, dTBN, ", "                #if defined(LIT_IRIDESCENCE)", "                        iridescenceFresnel,", "                #endif", "                        litArgs_clearcoat_worldNormal, litArgs_clearcoat_gloss, litArgs_sheen_gloss, litArgs_iridescence_intensity", "            );", "        #endif", "", "        #ifdef AREA_LIGHTS", "", "            #ifdef LIT_CLEARCOAT", "                // specular has to be accumulated differently if we want area lights to look correct", "                litArgs_clearcoat_specularity = 1.0;", "            #endif", "", "            #ifdef LIT_SPECULAR", "                litArgs_specularity = vec3(1.0);", "            #endif", "", "        #endif", "", "        #ifdef LIT_REFRACTION", "            addRefraction(", "                litArgs_worldNormal, ", "                dViewDirW, ", "                litArgs_thickness, ", "                litArgs_gloss, ", "                litArgs_specularity, ", "                litArgs_albedo, ", "                litArgs_transmission,", "                litArgs_ior,", "                litArgs_dispersion", "                #if defined(LIT_IRIDESCENCE)", "                    , iridescenceFresnel, ", "                    litArgs_iridescence_intensity", "                #endif", "            );", "        #endif", "    #endif", "", "    // apply ambient occlusion", "    #ifdef LIT_AO", "        #ifdef LIT_OCCLUDE_DIRECT", "            occludeDiffuse(litArgs_ao);", "        #endif", "", "        #if LIT_OCCLUDE_SPECULAR != NONE", "            occludeSpecular(litArgs_gloss, litArgs_ao, litArgs_worldNormal, dViewDirW);", "        #endif", "    #endif", "", "    #ifdef LIT_SPECULARITY_FACTOR", "        dSpecularLight = dSpecularLight * litArgs_specularityFactor;", "    #endif", "", "    #if !defined(LIT_OPACITY_FADES_SPECULAR)", "", "        #if LIT_BLEND_TYPE == NORMAL || LIT_BLEND_TYPE == PREMULTIPLIED", "", "            var specLum: f32 = dot((dSpecularLight + dReflection.rgb * dReflection.a), vec3f( 0.2126, 0.7152, 0.0722 ));", "            #ifdef LIT_CLEARCOAT", "                specLum = specLum + dot(ccSpecularLight * litArgs_clearcoat_specularity + ccReflection.rgb * litArgs_clearcoat_specularity, vec3f( 0.2126, 0.7152, 0.0722 ));", "            #endif", "            litArgs_opacity = clamp(litArgs_opacity + gammaCorrectInput(specLum), 0.0, 1.0);", "", "        #endif", "", "        litArgs_opacity = litArgs_opacity * material_alphaFade;", "", "    #endif", "", "    #include \"endPS\"", "    #include \"outputAlphaPS\"", "", "    #ifdef LIT_MSDF", "        output.color = applyMsdf(gl_FragColor);", "    #endif", "", "    #include \"outputPS\"", "    #include \"debugOutputPS\"", "", "    #ifdef LIT_SHADOW_CATCHER", "        // output when the shadow catcher is enabled - accumulated shadows", "        output.color = vec4f(dShadowCatcher, output.color.a);", "    #endif", "", "    return output;", "}", "`;"], "file_path": "src/scene/shader-lib/chunks-wgsl/lit/frag/pass-forward/litForwardDeclaration.js"}
{"Link_to_commit": "https://github.com/github/github-mcp-server/commit/7c197f5850296f721ed4762163a9da1c71f5bb2e", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 243, "n_files_impacted": 5, "longest_chunk": ["package github", "", "import (", "\t\"context\"", "\t\"encoding/json\"", "\t\"net/http\"", "\t\"testing\"", "", "\t\"github.com/github/github-mcp-server/pkg/translations\"", "\t\"github.com/google/go-github/v69/github\"", "\t\"github.com/migueleliasweb/go-github-mock/src/mock\"", "\t\"github.com/stretchr/testify/assert\"", "\t\"github.com/stretchr/testify/require\"", ")", "", "func Test_GetSecretScanningAlert(t *testing.T) {", "\tmockClient := github.NewClient(nil)", "\ttool, _ := GetSecretScanningAlert(stubGetClientFn(mockClient), translations.NullTranslationHelper)", "", "\tassert.Equal(t, \"get_secret_scanning_alert\", tool.Name)", "\tassert.NotEmpty(t, tool.Description)", "\tassert.Contains(t, tool.InputSchema.Properties, \"owner\")", "\tassert.Contains(t, tool.InputSchema.Properties, \"repo\")", "\tassert.Contains(t, tool.InputSchema.Properties, \"alertNumber\")", "\tassert.ElementsMatch(t, tool.InputSchema.Required, []string{\"owner\", \"repo\", \"alertNumber\"})", "", "\t// Setup mock alert for success case", "\tmockAlert := &github.SecretScanningAlert{", "\t\tNumber:  github.Ptr(42),", "\t\tState:   github.Ptr(\"open\"),", "\t\tHTMLURL: github.Ptr(\"https://github.com/owner/private-repo/security/secret-scanning/42\"),", "\t}", "", "\ttests := []struct {", "\t\tname           string", "\t\tmockedClient   *http.Client", "\t\trequestArgs    map[string]interface{}", "\t\texpectError    bool", "\t\texpectedAlert  *github.SecretScanningAlert", "\t\texpectedErrMsg string", "\t}{", "\t\t{", "\t\t\tname: \"successful alert fetch\",", "\t\t\tmockedClient: mock.NewMockedHTTPClient(", "\t\t\t\tmock.WithRequestMatch(", "\t\t\t\t\tmock.GetReposSecretScanningAlertsByOwnerByRepoByAlertNumber,", "\t\t\t\t\tmockAlert,", "\t\t\t\t),", "\t\t\t),", "\t\t\trequestArgs: map[string]interface{}{", "\t\t\t\t\"owner\":       \"owner\",", "\t\t\t\t\"repo\":        \"repo\",", "\t\t\t\t\"alertNumber\": float64(42),", "\t\t\t},", "\t\t\texpectError:   false,", "\t\t\texpectedAlert: mockAlert,", "\t\t},", "\t\t{", "\t\t\tname: \"alert fetch fails\",", "\t\t\tmockedClient: mock.NewMockedHTTPClient(", "\t\t\t\tmock.WithRequestMatchHandler(", "\t\t\t\t\tmock.GetReposSecretScanningAlertsByOwnerByRepoByAlertNumber,", "\t\t\t\t\thttp.HandlerFunc(func(w http.ResponseWriter, _ *http.Request) {", "\t\t\t\t\t\tw.WriteHeader(http.StatusNotFound)", "\t\t\t\t\t\t_, _ = w.Write([]byte(`{\"message\": \"Not Found\"}`))", "\t\t\t\t\t}),", "\t\t\t\t),", "\t\t\t),", "\t\t\trequestArgs: map[string]interface{}{", "\t\t\t\t\"owner\":       \"owner\",", "\t\t\t\t\"repo\":        \"repo\",", "\t\t\t\t\"alertNumber\": float64(9999),", "\t\t\t},", "\t\t\texpectError:    true,", "\t\t\texpectedErrMsg: \"failed to get alert\",", "\t\t},", "\t}", "", "\tfor _, tc := range tests {", "\t\tt.Run(tc.name, func(t *testing.T) {", "\t\t\t// Setup client with mock", "\t\t\tclient := github.NewClient(tc.mockedClient)", "\t\t\t_, handler := GetSecretScanningAlert(stubGetClientFn(client), translations.NullTranslationHelper)", "", "\t\t\t// Create call request", "\t\t\trequest := createMCPRequest(tc.requestArgs)", "", "\t\t\t// Call handler", "\t\t\tresult, err := handler(context.Background(), request)", "", "\t\t\t// Verify results", "\t\t\tif tc.expectError {", "\t\t\t\trequire.Error(t, err)", "\t\t\t\tassert.Contains(t, err.Error(), tc.expectedErrMsg)", "\t\t\t\treturn", "\t\t\t}", "", "\t\t\trequire.NoError(t, err)", "", "\t\t\t// Parse the result and get the text content if no error", "\t\t\ttextContent := getTextResult(t, result)", "", "\t\t\t// Unmarshal and verify the result", "\t\t\tvar returnedAlert github.Alert", "\t\t\terr = json.Unmarshal([]byte(textContent.Text), &returnedAlert)", "\t\t\tassert.NoError(t, err)", "\t\t\tassert.Equal(t, *tc.expectedAlert.Number, *returnedAlert.Number)", "\t\t\tassert.Equal(t, *tc.expectedAlert.State, *returnedAlert.State)", "\t\t\tassert.Equal(t, *tc.expectedAlert.HTMLURL, *returnedAlert.HTMLURL)", "", "\t\t})", "\t}", "}", "", "func Test_ListSecretScanningAlerts(t *testing.T) {", "\t// Verify tool definition once", "\tmockClient := github.NewClient(nil)", "\ttool, _ := ListSecretScanningAlerts(stubGetClientFn(mockClient), translations.NullTranslationHelper)", "", "\tassert.Equal(t, \"list_secret_scanning_alerts\", tool.Name)", "\tassert.NotEmpty(t, tool.Description)", "\tassert.Contains(t, tool.InputSchema.Properties, \"owner\")", "\tassert.Contains(t, tool.InputSchema.Properties, \"repo\")", "\tassert.Contains(t, tool.InputSchema.Properties, \"state\")", "\tassert.Contains(t, tool.InputSchema.Properties, \"secret_type\")", "\tassert.Contains(t, tool.InputSchema.Properties, \"resolution\")", "\tassert.ElementsMatch(t, tool.InputSchema.Required, []string{\"owner\", \"repo\"})", "", "\t// Setup mock alerts for success case", "\tresolvedAlert := github.SecretScanningAlert{", "\t\tNumber:     github.Ptr(2),", "\t\tHTMLURL:    github.Ptr(\"https://github.com/owner/private-repo/security/secret-scanning/2\"),", "\t\tState:      github.Ptr(\"resolved\"),", "\t\tResolution: github.Ptr(\"false_positive\"),", "\t\tSecretType: github.Ptr(\"adafruit_io_key\"),", "\t}", "\topenAlert := github.SecretScanningAlert{", "\t\tNumber:     github.Ptr(2),", "\t\tHTMLURL:    github.Ptr(\"https://github.com/owner/private-repo/security/secret-scanning/3\"),", "\t\tState:      github.Ptr(\"open\"),", "\t\tResolution: github.Ptr(\"false_positive\"),", "\t\tSecretType: github.Ptr(\"adafruit_io_key\"),", "\t}", "", "\ttests := []struct {", "\t\tname           string", "\t\tmockedClient   *http.Client", "\t\trequestArgs    map[string]interface{}", "\t\texpectError    bool", "\t\texpectedAlerts []*github.SecretScanningAlert", "\t\texpectedErrMsg string", "\t}{", "\t\t{", "\t\t\tname: \"successful resolved alerts listing\",", "\t\t\tmockedClient: mock.NewMockedHTTPClient(", "\t\t\t\tmock.WithRequestMatchHandler(", "\t\t\t\t\tmock.GetReposSecretScanningAlertsByOwnerByRepo,", "\t\t\t\t\texpectQueryParams(t, map[string]string{", "\t\t\t\t\t\t\"state\": \"resolved\",", "\t\t\t\t\t}).andThen(", "\t\t\t\t\t\tmockResponse(t, http.StatusOK, []*github.SecretScanningAlert{&resolvedAlert}),", "\t\t\t\t\t),", "\t\t\t\t),", "\t\t\t),", "\t\t\trequestArgs: map[string]interface{}{", "\t\t\t\t\"owner\": \"owner\",", "\t\t\t\t\"repo\":  \"repo\",", "\t\t\t\t\"state\": \"resolved\",", "\t\t\t},", "\t\t\texpectError:    false,", "\t\t\texpectedAlerts: []*github.SecretScanningAlert{&resolvedAlert},", "\t\t},", "\t\t{", "\t\t\tname: \"successful alerts listing\",", "\t\t\tmockedClient: mock.NewMockedHTTPClient(", "\t\t\t\tmock.WithRequestMatchHandler(", "\t\t\t\t\tmock.GetReposSecretScanningAlertsByOwnerByRepo,", "\t\t\t\t\texpectQueryParams(t, map[string]string{}).andThen(", "\t\t\t\t\t\tmockResponse(t, http.StatusOK, []*github.SecretScanningAlert{&resolvedAlert, &openAlert}),", "\t\t\t\t\t),", "\t\t\t\t),", "\t\t\t),", "\t\t\trequestArgs: map[string]interface{}{", "\t\t\t\t\"owner\": \"owner\",", "\t\t\t\t\"repo\":  \"repo\",", "\t\t\t},", "\t\t\texpectError:    false,", "\t\t\texpectedAlerts: []*github.SecretScanningAlert{&resolvedAlert, &openAlert},", "\t\t},", "\t\t{", "\t\t\tname: \"alerts listing fails\",", "\t\t\tmockedClient: mock.NewMockedHTTPClient(", "\t\t\t\tmock.WithRequestMatchHandler(", "\t\t\t\t\tmock.GetReposSecretScanningAlertsByOwnerByRepo,", "\t\t\t\t\thttp.HandlerFunc(func(w http.ResponseWriter, _ *http.Request) {", "\t\t\t\t\t\tw.WriteHeader(http.StatusUnauthorized)", "\t\t\t\t\t\t_, _ = w.Write([]byte(`{\"message\": \"Unauthorized access\"}`))", "\t\t\t\t\t}),", "\t\t\t\t),", "\t\t\t),", "\t\t\trequestArgs: map[string]interface{}{", "\t\t\t\t\"owner\": \"owner\",", "\t\t\t\t\"repo\":  \"repo\",", "\t\t\t},", "\t\t\texpectError:    true,", "\t\t\texpectedErrMsg: \"failed to list alerts\",", "\t\t},", "\t}", "", "\tfor _, tc := range tests {", "\t\tt.Run(tc.name, func(t *testing.T) {", "\t\t\tclient := github.NewClient(tc.mockedClient)", "\t\t\t_, handler := ListSecretScanningAlerts(stubGetClientFn(client), translations.NullTranslationHelper)", "", "\t\t\trequest := createMCPRequest(tc.requestArgs)", "", "\t\t\tresult, err := handler(context.Background(), request)", "", "\t\t\tif tc.expectError {", "\t\t\t\trequire.Error(t, err)", "\t\t\t\tassert.Contains(t, err.Error(), tc.expectedErrMsg)", "\t\t\t\treturn", "\t\t\t}", "", "\t\t\trequire.NoError(t, err)", "", "\t\t\ttextContent := getTextResult(t, result)", "", "\t\t\t// Unmarshal and verify the result", "\t\t\tvar returnedAlerts []*github.SecretScanningAlert", "\t\t\terr = json.Unmarshal([]byte(textContent.Text), &returnedAlerts)", "\t\t\tassert.NoError(t, err)", "\t\t\tassert.Len(t, returnedAlerts, len(tc.expectedAlerts))", "\t\t\tfor i, alert := range returnedAlerts {", "\t\t\t\tassert.Equal(t, *tc.expectedAlerts[i].Number, *alert.Number)", "\t\t\t\tassert.Equal(t, *tc.expectedAlerts[i].HTMLURL, *alert.HTMLURL)", "\t\t\t\tassert.Equal(t, *tc.expectedAlerts[i].State, *alert.State)", "\t\t\t\tassert.Equal(t, *tc.expectedAlerts[i].Resolution, *alert.Resolution)", "\t\t\t\tassert.Equal(t, *tc.expectedAlerts[i].SecretType, *alert.SecretType)", "\t\t\t}", "\t\t})", "\t}", "}"], "file_path": "pkg/github/tools.go"}
{"Link_to_commit": "https://github.com/KwiatkowskiML/TransformerTranslator/commit/a0aec4af09fe27a097af2032893f6f830ec27598", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 88, "n_files_impacted": 4, "longest_chunk": ["import torch", "", "from torch.utils.data import Dataset", "", "from constants import SOS_TOKEN, EOS_TOKEN, PAD_TOKEN", "", "", "class BilingualDataset(Dataset):", "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):", "        super().__init__()", "", "        self.ds = ds", "        self.tokenizer_src = tokenizer_src", "        self.tokenizer_tgt = tokenizer_tgt", "        self.src_lang = src_lang", "        self.tgt_lang = tgt_lang", "        self.seq_len = seq_len", "", "        self.sos_token = torch.Tensor([tokenizer_src.token_to_id(SOS_TOKEN)], dtype=torch.int64)", "        self.eos_token = torch.Tensor([tokenizer_src.token_to_id(EOS_TOKEN)], dtype=torch.int64)", "        self.pad_token = torch.Tensor([tokenizer_src.token_to_id(PAD_TOKEN)], dtype=torch.int64)", "", "    def __len__(self):", "        return len(self.ds)", "", "    def __getitem__(self, index):", "        src_target_pair = self.ds[index]", "        src_text = src_target_pair['translation'][self.src_lang]", "        tgt_text = src_target_pair['translation'][self.tgt_lang]", "", "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids", "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids", "", "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2", "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1", "", "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:", "            raise ValueError(f'Sentence is too long for seq_len = {self.seq_len}')", "", "        # Add SOS, EOS and PAD tokens to the encoder input", "        encoder_input = torch.cat(", "            [", "                self.sos_token,", "                torch.tensor(enc_input_tokens, dtype=torch.int64),", "                self.eos_token,", "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64)", "            ]", "        )", "", "        # Add SOS and PAD tokens to the decoder input", "        decoder_input = torch.cat(", "            [", "                self.sos_token,", "                torch.tensor(dec_input_tokens, dtype=torch.int64),", "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)", "            ]", "        )", "", "        # Expected decoder output", "        label = torch.cat(", "            [", "                torch.tensor(dec_input_tokens, dtype=torch.int64),", "                self.eos_token,", "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)", "            ]", "        )", "", "        assert encoder_input.size(0) == self.seq_len", "        assert decoder_input.size(0) == self.seq_len", "        assert label.size(0) == self.seq_len", "", "        return {", "            \"encoder_input\": encoder_input,", "            \"decoder_input\": decoder_input,", "            \"encode_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)", "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, 1, seq_len)", "            \"label\": label,", "            \"src_text\": src_text,", "            \"tgt_text\": tgt_text", "        }", "", "", "def causal_mask(size: int):", "    \"\"\"", "    Create a mask for the decoder input.", "    \"\"\"", "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)", "    return mask"], "file_path": "dataset.py"}
{"Link_to_commit": "https://github.com/ryan-vella/CurrencyWalletSystem/commit/a43ee9a538c7084ef911843d30647f98e9444166", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 166, "n_files_impacted": 23, "longest_chunk": ["\ufeffusing CurrencyWalletSystem.Infrastructure.Data;", "using CurrencyWalletSystem.Infrastructure.Enums;", "using CurrencyWalletSystem.Infrastructure.Factories;", "using CurrencyWalletSystem.Infrastructure.Interfaces;", "using CurrencyWalletSystem.Infrastructure.Models;", "using CurrencyWalletSystem.Infrastructure.Services;", "using CurrencyWalletSystem.Infrastructure.Strategies;", "using Microsoft.EntityFrameworkCore;", "using Moq;", "using System.Reflection;", "", "namespace CurrencyWalletSystem.Tests.Infrastructure.Services", "{", "    public class WalletServiceTests", "    {", "        private readonly DbContextOptions<AppDbContext> _dbOptions;", "", "        public WalletServiceTests()", "        {", "            _dbOptions = new DbContextOptionsBuilder<AppDbContext>()", "                .UseInMemoryDatabase(databaseName: Guid.NewGuid().ToString()) // unique per test", "                .Options;", "        }", "", "        [Fact]", "        public async Task CreateWalletAsync_ShouldCreateWalletWithUpperCurrency()", "        {", "            // Arrange", "            using var dbContext = new AppDbContext(_dbOptions);", "            var strategyFactoryMock = new Mock<IWalletStrategyFactory>();", "            var rateCacheMock = new Mock<ICurrencyRateCache>();", "            var service = new WalletService(dbContext, strategyFactoryMock.Object, rateCacheMock.Object);", "", "            // Act", "            var wallet = await service.CreateWalletAsync(\"usd\");", "", "            // Assert", "            Assert.Equal(\"USD\", wallet.Currency);", "            Assert.Equal(0, wallet.Balance);", "            Assert.True(wallet.Id > 0);", "        }", "", "        [Fact]", "        public async Task GetWalletBalanceAsync_ShouldReturnSameCurrencyBalance()", "        {", "            using var dbContext = new AppDbContext(_dbOptions);", "            var wallet = new Wallet { Currency = \"USD\", Balance = 100 };", "            dbContext.Wallets.Add(wallet);", "            await dbContext.SaveChangesAsync();", "", "            var service = new WalletService(dbContext, Mock.Of<IWalletStrategyFactory>(), Mock.Of<ICurrencyRateCache>());", "", "            var balance = await service.GetWalletBalanceAsync(wallet.Id, \"USD\");", "", "            Assert.Equal(100, balance);", "        }", "", "        [Fact]", "        public async Task GetWalletBalanceAsync_ShouldConvertToTargetCurrency()", "        {", "            using var dbContext = new AppDbContext(_dbOptions);", "            var wallet = new Wallet { Currency = \"USD\", Balance = 100 };", "            dbContext.Wallets.Add(wallet);", "            await dbContext.SaveChangesAsync();", "", "            var rateCacheMock = new Mock<ICurrencyRateCache>();", "            rateCacheMock.Setup(r => r.GetRate(\"EUR\")).Returns(new CurrencyRate { Currency = \"EUR\", Rate = 2m });", "", "            var service = new WalletService(dbContext, Mock.Of<IWalletStrategyFactory>(), rateCacheMock.Object);", "", "            var balance = await service.GetWalletBalanceAsync(wallet.Id, \"EUR\");", "", "            Assert.Equal(200, balance);", "        }", "", "        [Fact]", "        public async Task GetBaseCurrencyAsync_ShouldReturnCurrency()", "        {", "            using var dbContext = new AppDbContext(_dbOptions);", "            var wallet = new Wallet { Currency = \"JPY\", Balance = 50 };", "            dbContext.Wallets.Add(wallet);", "            await dbContext.SaveChangesAsync();", "", "            var service = new WalletService(dbContext, Mock.Of<IWalletStrategyFactory>(), Mock.Of<ICurrencyRateCache>());", "", "            var currency = await service.GetBaseCurrencyAsync(wallet.Id);", "", "            Assert.Equal(\"JPY\", currency);", "        }", "", "        [Fact]", "        public async Task AdjustWalletBalanceAsync_ShouldApplyStrategyAndSave()", "        {", "            using var dbContext = new AppDbContext(_dbOptions);", "            var wallet = new Wallet { Currency = \"USD\", Balance = 100 };", "            dbContext.Wallets.Add(wallet);", "            await dbContext.SaveChangesAsync();", "", "            var strategyMock = new Mock<IWalletBalanceStrategy>();", "            strategyMock.Setup(s => s.Execute(100, 50)).Returns(150);", "", "            var strategyFactoryMock = new Mock<IWalletStrategyFactory>();", "            strategyFactoryMock.Setup(f => f.GetStrategy(WalletStrategyType.AddFunds)).Returns(strategyMock.Object);", "", "            var rateCacheMock = new Mock<ICurrencyRateCache>();", "            rateCacheMock.Setup(r => r.GetRate(\"USD\")).Returns(new CurrencyRate { Currency = \"USD\", Rate = 1m });", "", "            var service = new WalletService(dbContext, strategyFactoryMock.Object, rateCacheMock.Object);", "", "            await service.AdjustWalletBalanceAsync(wallet.Id, 50, \"USD\", WalletStrategyType.AddFunds);", "", "            var updatedWallet = await dbContext.Wallets.FindAsync(wallet.Id);", "            Assert.Equal(150, updatedWallet!.Balance);", "        }", "", "        [Fact]", "        public async Task AdjustWalletBalanceAsync_ShouldThrow_WhenAmountIsZeroOrNegative()", "        {", "            using var dbContext = new AppDbContext(_dbOptions);", "            var wallet = new Wallet { Currency = \"USD\", Balance = 100 };", "            dbContext.Wallets.Add(wallet);", "            await dbContext.SaveChangesAsync();", "", "            var service = new WalletService(dbContext, Mock.Of<IWalletStrategyFactory>(), Mock.Of<ICurrencyRateCache>());", "", "            await Assert.ThrowsAsync<ArgumentException>(() =>", "                service.AdjustWalletBalanceAsync(wallet.Id, 0, \"USD\", WalletStrategyType.AddFunds));", "        }", "", "        [Fact]", "        public async Task GetWalletByIdAsync_ShouldThrow_WhenWalletNotFound()", "        {", "            using var dbContext = new AppDbContext(_dbOptions);", "            var service = new WalletService(dbContext, Mock.Of<IWalletStrategyFactory>(), Mock.Of<ICurrencyRateCache>());", "", "            await Assert.ThrowsAsync<KeyNotFoundException>(() =>", "                service.GetWalletBalanceAsync(999));", "        }", "", "        [Fact]", "        public async Task AdjustWalletBalanceAsync_ShouldConvertCurrencyBeforeApplyingStrategy()", "        {", "            using var dbContext = new AppDbContext(_dbOptions);", "            var wallet = new Wallet { Currency = \"USD\", Balance = 100 };", "            dbContext.Wallets.Add(wallet);", "            await dbContext.SaveChangesAsync();", "", "            var rateCacheMock = new Mock<ICurrencyRateCache>();", "            rateCacheMock.Setup(r => r.GetRate(\"EUR\")).Returns(new CurrencyRate { Currency = \"EUR\", Rate = 2 });", "            rateCacheMock.Setup(r => r.GetRate(\"USD\")).Returns(new CurrencyRate { Currency = \"USD\", Rate = 1 });", "", "            var strategyMock = new Mock<IWalletBalanceStrategy>();", "            strategyMock.Setup(s => s.Execute(It.IsAny<decimal>(), It.Is<decimal>(v => Math.Abs(v - 25) < 0.01m))).Returns(125);", "", "            var strategyFactoryMock = new Mock<IWalletStrategyFactory>();", "            strategyFactoryMock.Setup(f => f.GetStrategy(WalletStrategyType.AddFunds)).Returns(strategyMock.Object);", "", "            var service = new WalletService(dbContext, strategyFactoryMock.Object, rateCacheMock.Object);", "", "            await service.AdjustWalletBalanceAsync(wallet.Id, 50, \"EUR\", WalletStrategyType.AddFunds);", "", "            var updated = await dbContext.Wallets.FindAsync(wallet.Id);", "            Assert.Equal(125, updated!.Balance);", "        }", "    }", "}"], "file_path": "CurrencyWalletSystem.Tests/Infrastructure/Strategies/AddFundsStrategyTests.cs"}
{"Link_to_commit": "https://github.com/reynaldichernando/Whatever-Origin/commit/4abf5a50254119144226e2108a96bed93dc57d06", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 6, "n_files_impacted": 3, "longest_chunk": ["services:", "  whatever-origin:", "    image: ghcr.io/reynaldichernando/whatever-origin:latest", "    ports:", "      - 80:8080", "    restart: always"], "file_path": "main.go"}
{"Link_to_commit": "https://github.com/codacy/codacy-vscode-extension/commit/1605c11d7f262318310fccc7893fb5251c2bdc58", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 33, "n_files_impacted": 4, "longest_chunk": ["const CLI_FILE_NAME = 'cli.sh'", "const CLI_FOLDER_NAME = '.codacy'", "const CLI_COMMAND = `${CLI_FOLDER_NAME}/${CLI_FILE_NAME}`", "", "// Set a larger buffer size (10MB)", "const MAX_BUFFER_SIZE = 1024 * 1024 * 10", "", "const execAsync = (command: string) => {", "  const workspacePath = vscode.workspace.workspaceFolders?.[0]?.uri.fsPath || ''", "", "  return new Promise((resolve, reject) => {", "    exec(", "      `CODACY_CLI_V2_VERSION=1.0.0-main.232.a6a6368 ${command}`,", "      {", "        cwd: workspacePath,", "        maxBuffer: MAX_BUFFER_SIZE, // To solve: stdout maxBuffer exceeded", "      },", "      (error, stdout, stderr) => {", "        if (error) {", "          reject(error)", "          return", "        }", "", "        if (stderr && (!stdout || /error|fail|exception/i.test(stderr))) {", "          reject(new Error(stderr))", "          return", "        }", "", "        resolve({ stdout, stderr })", "      }", "    )", "  })", "}"], "file_path": "src/commands/installAnalysisCLI.ts"}
{"Link_to_commit": "https://github.com/s4shiki/dify/commit/ef188564f30ab4feafe1dda015642cf2af800305", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 233, "n_files_impacted": 2, "longest_chunk": ["", "/**", " * Preprocesses mermaid code to fix common syntax issues", " */", "export function preprocessMermaidCode(code: string): string {", "  if (!code || typeof code !== 'string')", "    return ''", "", "  // First check if this is a gantt chart", "  if (code.trim().startsWith('gantt')) {", "    // For gantt charts, we need to ensure each task is on its own line", "    // Split the code into lines and process each line separately", "    const lines = code.split('\\n').map(line => line.trim())", "    return lines.join('\\n')", "  }", "", "  return code", "    // Replace English colons with Chinese colons in section nodes to avoid parsing issues", "    .replace(/section\\s+([^:]+):/g, (match, sectionName) => `section ${sectionName}\uff1a`)", "    // Fix common syntax issues", "    .replace(/fifopacket/g, 'rect')", "    // Ensure graph has direction", "    .replace(/^graph\\s+((?:TB|BT|RL|LR)*)/, (match, direction) => {", "      return direction ? match : 'graph TD'", "    })", "    // Clean up empty lines and extra spaces", "    .trim()", "}", "", "/**", " * Prepares mermaid code based on selected style", " */", "export function prepareMermaidCode(code: string, style: 'classic' | 'handDrawn'): string {", "  let finalCode = preprocessMermaidCode(code)", "", "  // Special handling for gantt charts", "  if (finalCode.trim().startsWith('gantt')) {", "    // For gantt charts, preserve the structure exactly as is", "    return finalCode", "  }", "", "  if (style === 'handDrawn') {", "    finalCode = finalCode", "      // Remove style definitions that interfere with hand-drawn style", "      .replace(/style\\s+[^\\n]+/g, '')", "      .replace(/linkStyle\\s+[^\\n]+/g, '')", "      .replace(/^flowchart/, 'graph')", "      // Remove any styles that might interfere with hand-drawn style", "      .replace(/class=\"[^\"]*\"/g, '')", "      .replace(/fill=\"[^\"]*\"/g, '')", "      .replace(/stroke=\"[^\"]*\"/g, '')", "", "    // Ensure hand-drawn style charts always start with graph", "    if (!finalCode.startsWith('graph') && !finalCode.startsWith('flowchart'))", "      finalCode = `graph TD\\n${finalCode}`", "  }", "", "  return finalCode", "}", "", "/**", " * Converts SVG to base64 string for image rendering", " */", "export function svgToBase64(svgGraph: string): Promise<string> {", "  if (!svgGraph)", "    return Promise.resolve('')", "", "  try {", "    // Ensure SVG has correct XML declaration", "    if (!svgGraph.includes('<?xml'))", "      svgGraph = `<?xml version=\"1.0\" encoding=\"UTF-8\"?>${svgGraph}`", "", "    const blob = new Blob([new TextEncoder().encode(svgGraph)], { type: 'image/svg+xml;charset=utf-8' })", "    return new Promise((resolve, reject) => {", "      const reader = new FileReader()", "      reader.onloadend = () => resolve(reader.result as string)", "      reader.onerror = reject", "      reader.readAsDataURL(blob)", "    })", "  }", "  catch (error) {", "    console.error('Error converting SVG to base64:', error)", "    return Promise.resolve('')", "  }", "}", "", "/**", " * Processes SVG for theme styling", " */", "export function processSvgForTheme(", "  svg: string,", "  isDark: boolean,", "  isHandDrawn: boolean,", "  themes: {", "    light: any", "    dark: any", "  },", "): string {", "  let processedSvg = svg", "", "  if (isDark) {", "    processedSvg = processedSvg", "      .replace(/style=\"fill: ?#000000\"/g, 'style=\"fill: #e2e8f0\"')", "      .replace(/style=\"stroke: ?#000000\"/g, 'style=\"stroke: #94a3b8\"')", "      .replace(/<rect [^>]*fill=\"#ffffff\"/g, '<rect $& fill=\"#1e293b\"')", "", "    if (isHandDrawn) {", "      processedSvg = processedSvg", "        .replace(/fill=\"#[a-fA-F0-9]{6}\"/g, `fill=\"${themes.dark.nodeColors[0].bg}\"`)", "        .replace(/stroke=\"#[a-fA-F0-9]{6}\"/g, `stroke=\"${themes.dark.connectionColor}\"`)", "        .replace(/stroke-width=\"1\"/g, 'stroke-width=\"1.5\"')", "    }", "    else {", "      let i = 0", "      themes.dark.nodeColors.forEach(() => {", "        const regex = /fill=\"#[a-fA-F0-9]{6}\"[^>]*class=\"node-[^\"]*\"/g", "        processedSvg = processedSvg.replace(regex, (match: string) => {", "          const colorIndex = i % themes.dark.nodeColors.length", "          i++", "          return match.replace(/fill=\"#[a-fA-F0-9]{6}\"/, `fill=\"${themes.dark.nodeColors[colorIndex].bg}\"`)", "        })", "      })", "", "      processedSvg = processedSvg", "        .replace(/<path [^>]*stroke=\"#[a-fA-F0-9]{6}\"/g,", "          `<path stroke=\"${themes.dark.connectionColor}\" stroke-width=\"1.5\"`)", "        .replace(/<(line|polyline) [^>]*stroke=\"#[a-fA-F0-9]{6}\"/g,", "          `<$1 stroke=\"${themes.dark.connectionColor}\" stroke-width=\"1.5\"`)", "    }", "  }", "  else {", "    if (isHandDrawn) {", "      processedSvg = processedSvg", "        .replace(/fill=\"#[a-fA-F0-9]{6}\"/g, `fill=\"${themes.light.nodeColors[0].bg}\"`)", "        .replace(/stroke=\"#[a-fA-F0-9]{6}\"/g, `stroke=\"${themes.light.connectionColor}\"`)", "        .replace(/stroke-width=\"1\"/g, 'stroke-width=\"1.5\"')", "    }", "    else {", "      themes.light.nodeColors.forEach(() => {", "        const regex = /fill=\"#[a-fA-F0-9]{6}\"[^>]*class=\"node-[^\"]*\"/g", "        let i = 0", "        processedSvg = processedSvg.replace(regex, (match: string) => {", "          const colorIndex = i % themes.light.nodeColors.length", "          i++", "          return match.replace(/fill=\"#[a-fA-F0-9]{6}\"/, `fill=\"${themes.light.nodeColors[colorIndex].bg}\"`)", "        })", "      })", "", "      processedSvg = processedSvg", "        .replace(/<path [^>]*stroke=\"#[a-fA-F0-9]{6}\"/g,", "          `<path stroke=\"${themes.light.connectionColor}\"`)", "        .replace(/<(line|polyline) [^>]*stroke=\"#[a-fA-F0-9]{6}\"/g,", "          `<$1 stroke=\"${themes.light.connectionColor}\"`)", "    }", "  }", "", "  return processedSvg", "}", "", "/**", " * Checks if mermaid code is complete and valid", " */", "export function isMermaidCodeComplete(code: string): boolean {", "  if (!code || code.trim().length === 0)", "    return false", "", "  try {", "    const trimmedCode = code.trim()", "", "    // Special handling for gantt charts", "    if (trimmedCode.startsWith('gantt')) {", "      // For gantt charts, check if it has at least a title and one task", "      const lines = trimmedCode.split('\\n').filter(line => line.trim().length > 0)", "      return lines.length >= 3", "    }", "", "    // Check for basic syntax structure", "    const hasValidStart = /^(graph|flowchart|sequenceDiagram|classDiagram|classDef|class|stateDiagram|gantt|pie|er|journey|requirementDiagram)/.test(trimmedCode)", "", "    // Check for balanced brackets and parentheses", "    const isBalanced = (() => {", "      const stack = []", "      const pairs = { '{': '}', '[': ']', '(': ')' }", "", "      for (const char of trimmedCode) {", "        if (char in pairs) {", "          stack.push(char)", "        }", "        else if (Object.values(pairs).includes(char)) {", "          const last = stack.pop()", "          if (pairs[last as keyof typeof pairs] !== char)", "            return false", "        }", "      }", "", "      return stack.length === 0", "    })()", "", "    // Check for common syntax errors", "    const hasNoSyntaxErrors = !trimmedCode.includes('undefined')", "                           && !trimmedCode.includes('[object Object]')", "                           && trimmedCode.split('\\n').every(line =>", "                             !(line.includes('-->') && !line.match(/\\S+\\s*-->\\s*\\S+/)))", "", "    return hasValidStart && isBalanced && hasNoSyntaxErrors", "  }", "  catch (error) {", "    console.debug('Mermaid code validation error:', error)", "    return false", "  }", "}", "", "/**", " * Helper to wait for DOM element with retry mechanism", " */", "export function waitForDOMElement(callback: () => Promise<any>, maxAttempts = 3, delay = 100): Promise<any> {", "  return new Promise((resolve, reject) => {", "    let attempts = 0", "    const tryRender = async () => {", "      try {", "        resolve(await callback())", "      }", "      catch (error) {", "        attempts++", "        if (attempts < maxAttempts)", "          setTimeout(tryRender, delay)", "        else", "          reject(error)", "      }", "    }", "    tryRender()", "  })", "}"], "file_path": "web/app/components/base/mermaid/utils.ts"}
{"Link_to_commit": "https://github.com/HyperloopUPV-H8/ST-LIB/commit/cd8ef79614a412407f5b86ba1f0059f849a66778", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 5, "n_files_impacted": 5, "longest_chunk": ["", "#ifdef HAL_IWDG_MODULE_ENABLED", "    Watchdog::check_reset_flag();", "    Watchdog::start();", "#endif"], "file_path": "Src/HALAL/HALAL.cpp"}
{"Link_to_commit": "https://github.com/m0-foundation/solana-m/commit/d842de1ddb8df72c6326777c5342b3aa7cc37ad8", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 18, "n_files_impacted": 1, "longest_chunk": ["      // Add current earners to LUT", "      for (const pid of [PROGRAM_ID, EXT_PROGRAM_ID]) {", "        const auth = await EarnAuthority.load(connection, evmClient, new Graph(''), pid);", "        const earners = await auth.getAllEarners();", "", "        for (const earner of earners) {", "          addressesForTable.push(earner.pubkey, earner.data.userTokenAccount);", "", "          // Check if there is an earn manager", "          if (", "            earner.data.earnManager &&", "            !addressesForTable.find((a) => a.equals(earner.data.earnManager))", "          ) {", "            addressesForTable.push(earner.data.earnManager);", "          }", "        }", "      }", ""], "file_path": "services/cli/main.ts"}
{"Link_to_commit": "https://github.com/m0-foundation/solana-m/commit/e50b4b9faab1b5a34372edffd5a0c4fbd3fc18f6", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 18, "n_files_impacted": 3, "longest_chunk": ["    [", "      (body: any) =>", "        body.method === 'getAccountInfo' && body.params?.[0] === 'GQBavw2gpCdbZkkSWk9PkzNTDdBwCHUGNpeuuQ7mV9GA', // wM global", "      {", "        context,", "        value: {", "          data: [", "            'nT0aSBDxU4yz3HtcE1xihhozJWJpdvNsPnG5FAKFUFeJ7wZJIrxP9rPce1wTXGKGGjMlYml282w+cbkUAoVQV4nvBkkivE/2C4a+ZrwfmLR9IKO+YVpJBagluCaGTioPTJSEZ9M+5wkLhr5mv860wdfpJ7zE0BS+Dyhjq534X9phCFG2Tb0K5eRoMAbaMvJBTyQcLMmsnaDkH0FwZa+QwkrYCQghj/MVxrIB0uoAAAB3Ng9oAAAAAP/+/A==',", "            'base64',", "          ],", "          executable: false,", "          lamports: 2192400,", "          owner: 'wMXX1K1nca5W4pZr1piETe78gcAVVrEFi9f4g46uXko',", "          rentEpoch: 18446744073709551615,", "          space: 187,", "        },", "      },", "    ],"], "file_path": "tests/unit/yieldbot.test.ts"}
{"Link_to_commit": "https://github.com/FalkorDB/falkordb-rs-next-gen/commit/5bc562f85b37d461a172b890aba1cffa7cf53c5d", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 33, "n_files_impacted": 4, "longest_chunk": ["", "", "def test_function_with_namespace():", "    res = query(\"RETURN string.join(null, ',') AS result\")", "    assert res.result_set == [[None]]", "", "    res = query(\"RETURN string.join([], 'foo') AS result\")", "    assert res.result_set == [[\"\"]]", "", "    res = query(\"RETURN string.join(['a', 'b'], ', ') AS result\")", "    assert res.result_set == [['a, b']]", "", "    res = query(\"RETURN string.join(['a', 'b']) AS result\")", "    assert res.result_set == [['ab']]", "", "    try:", "        query(f\"RETURN string.join(['a', 'b'], ', ', ', ') AS result\")", "        assert False, \"Expected an error\"", "    except ResponseError as e:", "        assert \"Received 3 arguments to function 'string.join', expected at most 2\" in str(e)", "", "    try:", "        query(f\"RETURN string.join(1, 2) AS result\")", "        assert False, \"Expected an error\"", "    except ResponseError as e:", "        assert \"Type mismatch: expected List or Null but was Integer\" in str(e)", "", "    for value, name in [(1.0, 'Float'), (True, 'Boolean'), ({}, 'Map'), ([], 'List'), (\"null\", 'Null')]:", "        try:", "            query(f\"RETURN string.join(['a', {value}], ',') AS result\")", "            assert False, \"Expected an error\"", "        except ResponseError as e:", "            assert f\"Type mismatch: expected String but was {name}\" in str(e)"], "file_path": "tests/test_e2e.py"}
{"Link_to_commit": "https://github.com/K20shores/cudaviz/commit/dfb042f61dc626c8e577a72e5ee779eca072ecd5", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 41, "n_files_impacted": 19, "longest_chunk": ["#include <cudaviz/kernels.hpp>", "", "#include <vector>", "#include <string>", "#include <stdexcept>", "", "#include <cuda_runtime.h>", "", "namespace cudaviz", "{", "    std::vector<std::vector<int>> mandelbrot(int max_iter = 1000, int N = 10)", "    {", "        std::size_t sz = N * N * sizeof(int);", "        std::vector<int> grid = std::vector<int>(N * N, 0);", "", "        int *deviceGrid;", "", "        cudaError_t error =  cudaMalloc(&deviceGrid, sz);", "        if (error != cudaSuccess)", "        {", "            throw std::runtime_error(\"Error allocating device memory: \" + std::string(cudaGetErrorString(error)));", "        }", "        cudaMemcpy(deviceGrid, grid.data(), sz, cudaMemcpyHostToDevice);", "", "        cudaviz::mandelbrotIteration(deviceGrid, N, max_iter);", "", "        cudaMemcpy(grid.data(), deviceGrid, sz, cudaMemcpyDeviceToHost);", "        cudaFree(deviceGrid);", "", "        std::vector<std::vector<int>> grid2D(N, std::vector<int>(N));", "        for (int i = 0; i < N; ++i)", "        {", "            for (int j = 0; j < N; ++j)", "            {", "                grid2D[i][j] = grid[i * N + j];", "            }", "        }", "", "        return grid2D;", "    }", "}"], "file_path": "src/mandelbrot.cpp"}
{"Link_to_commit": "https://github.com/openvinotoolkit/openvino_build_deploy/commit/7dbcdc00ed9a57fb14367c709df5968e861c5634", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 57, "n_files_impacted": 11, "longest_chunk": ["# OpenVINO Chat Sample\r", "\r", "Follow instructions here to prepare the environment:\r", "https://github.com/raymondlo84Fork/MSBuild2025/blob/main/openvino_genai/README.md\r", "\r", "```\r", "#Make sure you activate the environment after restarting the terminal\r", "./openvino_venv/Script/bin\r", "```\r", "\r", "## How to use a LLM model from HuggingFace\r", "\r", "To download a pre-compressed model (for CPU/GPU only) and experiment with the latest Phi-4-mini-instruct model:\r", "```\r", "huggingface-cli download OpenVINO/Phi-4-mini-instruct-int4-ov --local-dir Phi-4-mini-instruct-int4-ov\r", "```\r", "\r", "To download and compress a model (CPU/GPU/NPU):\r", "```\r", " optimum-cli export openvino -m microsoft/Phi-3-mini-4k-instruct  --trust-remote-code --weight-format int4 --sym --ratio 1.0 --group-size 128 Phi-3-mini-4k-instruct-npu\r", "```\r", "For NPU usage, please make sure the flags `--weight-format int4`, `--sym` and `--group-size 128` are set.\r", "\r", "To test run NPU without a huge download, you can try TinyLlama:\r", "```\r", "optimum-cli export openvino -m TinyLlama/TinyLlama-1.1B-Chat-v1.0 --weight-format int4 --sym --ratio 1.0 --group-size 128 TinyLlama-1.1B-Chat-v1.0\r", "```\r", "\r", "To obtain a meta llama demo, please first get a access token from this link [Access Security Tokens](https://huggingface.co/docs/hub/en/security-tokens), then login with the command line. Additionally, you have to accept to the agreement and wait for the approval (https://huggingface.co/meta-llama). Often this only take a few minutes to an hour.\r", "\r", "```\r", "huggingface-cli login\r", "```\r", "Then, you can execute this command to convert the model to be compatible with the NPU.\r", "```\r", "optimum-cli export openvino --model meta-llama/Llama-3.2-3B-Instruct  --trust-remote-code --task text-generation-with-past --weight-format int4 --group-size -1 --sym --ratio 1.0 llama-3.2-3b-instruct-INT4-npu\r", "```\r", "\r", "## How to Run\r", "\r", "```\r", "python chat_sample.py Phi-4-mini-instruct-int4-ov\r", "```\r", "or replace the model with `Phi-3-mini-4k-instruct-int4-npu` or `Llama-3.2-3B-Instruct-npu`.\r", "\r", "By default, we enabled CPU in `chat_sample.py`. You can deploy the LLMs on GPU or NPU by simply replacing the device name as `GPU` or `NPU` in the code.\r", "```\r", "    device = 'CPU'  # GPU or NPU can be used as well\r", "    pipe = openvino_genai.LLMPipeline(args.model_dir, device)\r", "```\r", "\r", "Llama 3.2 3B example output:\r", "![Screenshot 2025-04-28 133741](https://github.com/user-attachments/assets/532f6d66-2cc4-4a29-b71c-9c15f3716e7e)\r", "\r", "## References:\r", "NPU with OpenVINO GenAI: https://docs.openvino.ai/2025/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.html\r", "\r"], "file_path": "workshops/MSBuild2025/openvino_genai/chat_sample/chat_sample.py"}
{"Link_to_commit": "https://github.com/umccr/orcabus/commit/443c9a26321388e6b971b5f7983843663f9b8021", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 14, "n_files_impacted": 3, "longest_chunk": ["### Presigning packages", "", "Not all data receivers will have an S3 bucket or ICAV2 project for us to dump data in.  ", "", "Therefore we also support the old-school presigned url method.  ", "", "We can use the following command to generate presigned urls in a script for the package", "", "```bash", "data-sharing-tool presign-package \\", "  --package-id pkg.12345678910", "```", "", "This will return a presigned url for a shell script that can be used to download the package."], "file_path": "lib/workload/stateless/stacks/data-sharing-manager/scripts/data-sharing-tool.py"}
{"Link_to_commit": "https://github.com/playcanvas/engine/commit/7a0fb0ac2dd6c38956a43cffee6a08bad26ac5ae", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 36, "n_files_impacted": 11, "longest_chunk": [" * A ScreenComponent defines a rectangular area where user interfaces can be constructed. Screens", " * can either be 2D (screen space) or 3D (world space) - see {@link screenSpace}. It is possible to", " * create an {@link Entity} hierarchy underneath an Entity with a ScreenComponent to create complex", " * user interfaces using the following components:", " *", " * - {@link ButtonComponent}", " * - {@link ElementComponent}", " * - {@link LayoutChildComponent}", " * - {@link LayoutGroupComponent}", " * - {@link ScrollbarComponent}", " * - {@link ScrollViewComponent}", " *", " * You should never need to use the ScreenComponent constructor directly. To add a ScreenComponent", " * to an {@link Entity}, use {@link Entity#addComponent}:", " *", " * ```javascript", " * const entity = new pc.Entity();", " * entity.addComponent('screen', {", " *     referenceResolution: new pc.Vec2(1280, 720),", " *     screenSpace: false", " * });", " * ```", " *", " * Once the ScreenComponent is added to the entity, you can access it via the {@link Entity#screen}", " * property:", " *", " * ```javascript", " * entity.screen.scaleBlend = 0.6; // Set the screen's scale blend to 0.6", " *", " * console.log(entity.screen.scaleBlend); // Get the screen's scale blend and print it", " * ```", " *", " * Relevant Engine API examples:", " *", " * - [Screen Space Screen](https://playcanvas.github.io/#/user-interface/text)", " * - [World Space Screen](https://playcanvas.github.io/#/user-interface/world-ui)"], "file_path": "src/framework/components/screen/component.js"}
{"Link_to_commit": "https://github.com/KwiatkowskiML/TransformerTranslator/commit/603c685a0319cf47f40b12f2146a647712fff5c8", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 96, "n_files_impacted": 8, "longest_chunk": ["import warnings", "import torch.nn as nn", "import torch", "", "from pathlib import Path", "from tqdm import tqdm", "from torch.utils.tensorboard import SummaryWriter", "", "from config import get_weights_file_path, get_config", "from model import build_transformer", "from tokenizer import get_ds", "", "", "def get_model(model_config, vocab_src_len, vocab_tgt_len):", "    \"\"\"", "    Build the model", "    \"\"\"", "    model  = build_transformer(vocab_src_len, vocab_tgt_len, model_config['seq_len'], model_config['seq_len'], model_config['d_model'])", "    return model", "", "", "def train_model(model_config):", "    # Define the device", "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')", "    print(f'Using device {device}')", "", "    Path(model_config[\"model_folder\"]).mkdir(parents=True, exist_ok=True)", "", "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(model_config)", "    model = get_model(model_config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)", "", "    # Tensorboard", "    writer = SummaryWriter(model_config[\"experiment_name\"])", "", "    optimizer = torch.optim.Adam(model.parameters(), lr=model_config['lr'], eps=1e-9)", "", "    initial_epoch = 0", "    global_step = 0", "    if model_config['preload']:", "        model_filename = get_weights_file_path(model_config, model_config['preload'])", "        print(f'Preloading model from {model_filename}')", "        state = torch.load(model_filename)", "        initial_epoch = state['epoch'] + 1", "        optimizer.load_state_dict(state['optimizer_state_dict'])", "        global_step = state['global_step']", "", "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)", "", "    for epoch in range(initial_epoch, model_config['num_epochs']):", "        model.train()", "        batch_iterator = tqdm(train_dataloader, desc=f'Processing epoch {epoch:02d}')", "        for batch in batch_iterator:", "", "            encoder_input = batch['encoder_input'].to(device) # (Batch, seq_len)", "            decoder_input = batch['decoder_input'].to(device) # (Batch, seq_len)", "            encoder_mask = batch['encoder_mask'].to(device) # (batch, 1, 1, seq_len)", "            decoder_mask = batch['decoder_mask'].to(device) # (batch, 1, seq_len, seq_len)", "", "            # Run the tensors through the transformer", "            encoder_output = model.encode(encoder_input, encoder_mask) # (batch, seq_len, d_model)", "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (batch, seq_len, d_model)", "            proj_output = model.project(decoder_output) # (batch, seq_len, tgt_vocab_size)", "", "            label = batch['label'].to(device) # (batch, seq_len)", "", "            # (batch, seq_len, tgt_vocab_size) --> (batch * seq_len, tgt_vocab_size)", "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))", "            batch_iterator.set_postfix({f'loss': f'{loss.item():6.3f}'})", "", "            # Log the loss", "            writer.add_scalar('train loss', loss.item(), global_step)", "            writer.flush()", "", "            # Backpropagation", "            loss.backward()", "", "            # Update the weights", "            optimizer.step()", "            optimizer.zero_grad()", "", "            global_step += 1", "", "        # Save the model", "        model_filename = get_weights_file_path(model_config, f'{epoch:02d}')", "        torch.save({", "            'epoch': epoch,", "            'model_state_dict': model.state_dict(),", "            'optimizer_state_dict': optimizer.state_dict(),", "            'global_step': global_step,", "        }, model_filename)", "", "", "if __name__ == '__main__':", "    warnings.filterwarnings('ignore')", "    config = get_config()", "    train_model(config)"], "file_path": "train.py"}
{"Link_to_commit": "https://github.com/openvinotoolkit/openvino.genai/commit/796e6216b456567ddffdb56afe91a2b7d892c0af", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 33, "n_files_impacted": 18, "longest_chunk": ["# Retrieval Augmented Generation Sample", "", "This example showcases inference of Text Embedding Models. The application limited configuration configuration options to encourage the reader to explore and modify the source code. For example, change the device for inference to GPU. The sample features `openvino_genai.TextEmbeddingPipeline` and uses text as an input source.", "", "## Download and Convert the Model and Tokenizers", "", "The `--upgrade-strategy eager` option is needed to ensure `optimum-intel` is upgraded to the latest version.", "", "Install [../../export-requirements.txt](../../export-requirements.txt) to convert a model.", "", "```sh", "pip install --upgrade-strategy eager -r ../../export-requirements.txt", "optimum-cli export openvino --trust-remote-code --model BAAI/bge-small-en-v1.5 BAAI/bge-small-en-v1.5", "```", "", "## Run", "", "Install [deployment-requirements.txt](../../deployment-requirements.txt) via `pip install -r ../../deployment-requirements.txt` and then, run a sample:", "", "`python text_embeddings.py BAAI/bge-small-en-v1.5 \"Document 1\" \"Document 2\"`", "", "See [SUPPORTED_MODELS.md](../../../SUPPORTED_MODELS.md#text-embeddings-models) for the list of supported models.", "", "# Text Embedding Pipeline Usage", "", "```python", "import argparse", "import openvino_genai", "", "pipeline = openvino_genai.TextEmbeddingPipeline(model_dir, \"CPU\")", "", "embeddings = pipeline.embed_documents([\"document1\", \"document2\"])", "```"], "file_path": "samples/python/rag/text_embeddings.py"}
{"Link_to_commit": "https://github.com/Azure/azure-sdk-tools/commit/52044ae7faf35eaf1d0e99afa48d8474f02789b5", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 81, "n_files_impacted": 6, "longest_chunk": ["{", "  \"status\": \"Error\",", "  \"violations\": [", "    {", "      \"rule_ids\": [\"python_design.html#python-client-connection-string\"],", "      \"line_no\": 10,", "      \"bad_code\": \"connection_string: Optional[str] = None,\",", "      \"suggestion\": \"Remove the connection_string parameter from the constructor and implement a separate factory method (e.g. from_connection_string) to create the client using a connection string.\",", "      \"comment\": \"The constructor must not accept a connection string; using a factory method for connection string support is required by the guidelines.\"", "    },", "    {", "      \"rule_ids\": [", "        \"python_design.html#python-client-optional-arguments-keyword-only\"", "      ],", "      \"line_no\": 30,", "      \"bad_code\": \"def analyze_from_url(\",", "      \"suggestion\": \"Insert a '*' after the required positional parameters so that all optional parameters are keyword-only. For example:\\n\\n  def analyze_from_url(self, image_url: str, visual_features: List[VisualFeatures], *, gender_neutral_caption: Optional[bool] = ..., language: Optional[str] = ..., model_version: Optional[str] = ..., smart_crops_aspect_ratios: Optional[List[float]] = ..., **kwargs: Any) -> ImageAnalysisResult\",", "      \"comment\": \"Optional operation\\u2010specific parameters must be keyword-only.\"", "    },", "    {", "      \"rule_ids\": [\"python_design.html#python-client-same-name-sync-async\"],", "      \"line_no\": 53,", "      \"bad_code\": \"class azure.ai.vision.imageanalysis.aio.AsyncImageAnalysisClient(ImageAnalysisClient): implements AsyncContextManager\",", "      \"suggestion\": \"Rename the async client to ImageAnalysisClient (i.e. without the 'Async' prefix) and keep it under the 'azure.ai.vision.imageanalysis.aio' namespace so that both sync and async clients share the same client name.\",", "      \"comment\": \"Async and sync clients must share the same client name; adding an 'Async' prefix violates this guideline.\"", "    },", "    {", "      \"rule_ids\": [", "        \"python_design.html#python-client-constructor-api-version-argument-1\"", "      ],", "      \"line_no\": 54,", "      \"bad_code\": \"def __init__(\\n        self, \\n        endpoint: str, \\n        credential: Union[AzureKeyCredential, AsyncTokenCredential], \\n    ) -> None\",", "      \"suggestion\": \"Add an optional keyword-only api_version parameter to the async client __init__ signature, for example: \\n    def __init__(self, endpoint: str, credential: Union[AzureKeyCredential, AsyncTokenCredential], *, api_version: str = ..., **kwargs: Any) -> None\",", "      \"comment\": \"The async client constructor is missing the optional api_version parameter required by the guidelines.\"", "    },", "    {", "      \"rule_ids\": [", "        \"python_implementation.html#python-codestyle-static-methods\"", "      ],", "      \"line_no\": 88,", "      \"bad_code\": \"@staticmethod\",", "      \"suggestion\": \"Remove the staticmethod decorator and refactor send_request as an instance method or a module-level function.\",", "      \"comment\": \"Static methods are discouraged; module-level functions or instance methods should be used instead.\"", "    },", "    {", "      \"rule_ids\": [\"python_implementation.html#python-codestyle-type-naming\"],", "      \"line_no\": 209,", "      \"bad_code\": \"class azure.ai.vision.imageanalysis.models.detectedPerson(MutableMapping[str, Any]):\",", "      \"suggestion\": \"Rename the class to DetectedPerson (using PascalCase) to adhere to type naming conventions.\",", "      \"comment\": \"Type names should be in PascalCase; 'detectedPerson' violates this guideline.\"", "    },", "    {", "      \"rule_ids\": [\"python_implementation.html#python-codestyle-properties\"],", "      \"line_no\": 411,", "      \"bad_code\": \"def get_result(self) -> ObjectsResult\",", "      \"suggestion\": \"Replace the get_result/set_result methods with a property (with a getter and setter) to expose the result, for example, using @property and @result.setter.\",", "      \"comment\": \"Simple getter and setter functions are discouraged; properties should be used instead.\"", "    },", "    {", "      \"rule_ids\": [\"python_implementation.html#python-codestyle-properties\"],", "      \"line_no\": 413,", "      \"bad_code\": \"def set_result(self, obj) -> None\",", "      \"suggestion\": \"Replace the set_result method with a property setter (e.g., @result.setter def result(self, value): ...).\",", "      \"comment\": \"Simple setter methods should be implemented as property setters.\"", "    },", "    {", "      \"rule_ids\": [\"python_design.html#python-models-async\"],", "      \"line_no\": 432,", "      \"bad_code\": \"class azure.ai.vision.imageanalysis.models.aio.PeopleResult(MutableMapping[str, Any]):\",", "      \"suggestion\": \"Move PeopleResult to the common models namespace (azure.ai.vision.imageanalysis.models) instead of duplicating it in the aio sub-namespace.\",", "      \"comment\": \"Models should not be duplicated between the root and aio namespaces.\"", "    },", "    {", "      \"rule_ids\": [\"python_design.html#python-models-enum-name-uppercase\"],", "      \"line_no\": 517,", "      \"bad_code\": \"tags = \\\"tags\\\"\",", "      \"suggestion\": \"Rename the enum member to use UPPERCASE (e.g., TAGS = \\\"tags\\\") in accordance with the guidelines.\",", "      \"comment\": \"Enum member names must be in UPPERCASE to comply with naming conventions.\"", "    }", "  ]", "}"], "file_path": "packages/python-packages/apiview-copilot/src/_models.py"}
{"Link_to_commit": "https://github.com/Ultraplot/ultraplot/commit/ce222bfbd0c5491d55666b1dc3bcb773c25799f2", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 8, "n_files_impacted": 2, "longest_chunk": ["", "", "def test_scaler():", "    # Test a ultraplot scaler and a matplotlib native scaler; should not race errors", "    fig, ax = uplt.subplots(ncols=2, share=0)", "    ax[0].set_yscale(\"mercator\")", "    ax[1].set_yscale(\"asinh\")", "    return fig"], "file_path": "ultraplot/tests/test_format.py"}
{"Link_to_commit": "https://github.com/sachinbhardwajqa/gatling/commit/2e7176c2bc170523c54b37f7d6541ddc85057f0b", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 199, "n_files_impacted": 29, "longest_chunk": ["package simulation;", "", "import base.SessionId;", "import io.gatling.javaapi.core.*;", "import io.gatling.javaapi.http.HttpProtocolBuilder;", "", "import java.time.Duration;", "import java.util.concurrent.ThreadLocalRandom;", "", "import static io.gatling.javaapi.core.CoreDsl.*;", "import static io.gatling.javaapi.http.HttpDsl.*;", "", "public class SimulationDemoStore4UserJourneys extends Simulation {", "    private static final String DOMAIN = \"demostore.gatling.io\";", "    private static final HttpProtocolBuilder HTTP_PROTOCOL = http.baseUrl(\"https://\" + DOMAIN);", "    private static final int USER_COUNT = Integer.parseInt(System.getProperty(\"USERS\", \"5\"));", "    private static final Duration RAMP_DURATION = Duration.ofSeconds(Integer.parseInt(System.getProperty(\"RAMP_DURATION\", \"10\")));", "    private static final Duration TEST_DURATION = Duration.ofSeconds(Integer.parseInt(System.getProperty(\"TEST_DURATION\", \"60\")));", "    private static final FeederBuilder<String> csvFeederCategoryFeeder = csv(\"data/categoryDetails.csv\").circular();", "    private static final FeederBuilder<Object> jsonFeederProductFeeder = jsonFile(\"data/productDetails.json\").random();", "    private static final FeederBuilder<String> csvFeederLoginDetails = csv(\"data/loginDetails.csv\").circular();", "    private static final ChainBuilder initSession =", "            exec(flushCookieJar())", "                    .exec(session -> session.set(\"randomNumber\", ThreadLocalRandom.current().nextInt()))", "                    .exec(session -> session.set(\"customerLoggedIn\", false))", "                    .exec(session -> session.set(\"cartTotal\", 0))", "                    .exec(addCookie(Cookie(\"sessionID\", SessionId.random()).withDomain(DOMAIN)));", "", "    private static class CmsPage {", "        private static final ChainBuilder homePage =", "                exec(http(\"Load Home Page\")", "                        .get(\"/\")", "                        .check(css(\"#_csrf\", \"content\").saveAs(\"csrfValue\"))", "                        .check(regex(\"<title>Gatling Demo-Store</title>\").exists()));", "        private static final ChainBuilder aboutUs =", "                exec(http(\"Load About Us Page\")", "                        .get(\"/about-us\")", "                        .check(substring(\"About Us\")));", "    }", "", "    private static class Catalog {", "        private static class Category {", "            private static final ChainBuilder view =", "                    feed(csvFeederCategoryFeeder)", "                            .repeat(2, \"n\").on(", "                                    exec(http(\"View #{n} Category - #{categoryName}\")", "                                            .get(\"/category/#{categorySlug}\")", "                                            .check(css(\"#CategoryName\").isEL(\"#{categoryName}\")))); // EL = Expression Language", "        }", "", "        private static class Product {", "            private static final ChainBuilder view =", "                    feed(jsonFeederProductFeeder)", "                            .exec(http(\"View Product - #{name}\")", "                                    .get(\"/product/#{slug}\")", "                                    .check(css(\"#ProductDescription\").isEL(\"#{description}\")));", "            private static final ChainBuilder addProductToCart =", "                    exec(view)", "                            .exec(http(\"Add to Cart - ${name}\")", "                                    .get(\"/cart/add/${id}\")", "                                    .check(substring(\"items in your cart\")))", "                            .exec(session -> {", "                                double currentCartTotal = session.getDouble(\"cartTotal\");", "                                double itemPrice = session.getDouble(\"price\");", "                                return session.set(\"cartTotal\", currentCartTotal + itemPrice);", "                            })", "                            .exec(", "                                    session -> {", "                                        System.out.println(\"Cart Total : \" + session.get(\"cartTotal\").toString());", "                                        return session;", "                                    }", "                            );", "            ;", "        }", "    }", "", "    private static class Customer {", "        private static final ChainBuilder login =", "                feed(csvFeederLoginDetails)", "                        .exec(http(\"Load Login Page for #{username}\")", "                                .get(\"/login\")", "                                .check(substring(\"Username:\")))", "                        .exec(", "                                session -> {", "                                    System.out.println(\"Customer logged in: \" + session.get(\"customerLoggedIn\").toString());", "                                    return session;", "                                }", "                        )", "                        .exec(http(\"Customer Login Action with #{username}\")", "                                .post(\"/login\")", "                                .formParam(\"_csrf\", \"#{csrfValue}\")", "                                .formParam(\"username\", \"#{username}\")", "                                .formParam(\"password\", \"#{password}\"))", "                        .exec(session -> session.set(\"customerLoggedIn\", true))", "                        .exec(", "                                session -> {", "                                    System.out.println(\"Customer logged in: \" + session.get(\"customerLoggedIn\").toString());", "                                    return session;", "                                }", "                        );", "    }", "", "    private static class Checkout {", "        private static final ChainBuilder viewCart =", "                doIf(session -> !session.getBoolean(\"customerLoggedIn\"))", "                        .then(exec(Customer.login))", "                        .exec(http(\"View Cart\")", "                                .get(\"/cart/view\")", "//                                .check(css(\"#grandTotal\").isEL(\"$#{cartTotal}\"))", "                        );", "        private static final ChainBuilder checkout =", "                exec(http(\"Checkout\")", "                        .get(\"/cart/checkout\")", "                        .check(substring(\"Thanks for your order! See you soon!\")));", "    }", "", "    private static final ScenarioBuilder scn = scenario(\"RecordedSimulationDemoStore1\")", "            .exec(initSession)", "            .exec(CmsPage.homePage)", "            .pause(2)", "            .exec(CmsPage.aboutUs)", "            .pause(2)", "            .exec(Catalog.Category.view)", "            .pause(2)", "            .exec(Catalog.Product.addProductToCart)", "            .pause(2)", "            .exec(Checkout.viewCart)", "            .pause(3)", "            .exec(Checkout.checkout)", "            .pause(3);", "    private static class UserJourneys {", "        private static final Duration MIN_PAUSE = Duration.ofMillis(100);", "        private static final Duration MAX_PAUSE = Duration.ofMillis(500);", "        private static final ChainBuilder browseStore =", "                exec(initSession)", "                        .exec(CmsPage.homePage)", "                        .pause(MAX_PAUSE)", "                        .exec(CmsPage.aboutUs)", "                        .pause(MIN_PAUSE,MAX_PAUSE)", "                        .repeat(5)", "                        .on(", "                                exec(Catalog.Category.view)", "                                        .pause(MIN_PAUSE,MAX_PAUSE)", "                                        .exec(Catalog.Product.view)", "                        );", "        private static final ChainBuilder abandonCart =", "                initSession", "                        .exec(CmsPage.homePage)", "                        .pause(MAX_PAUSE)", "                        .exec(Catalog.Category.view)", "                        .pause(MIN_PAUSE,MAX_PAUSE)", "                        .exec(Catalog.Product.view)", "                        .pause(MIN_PAUSE,MAX_PAUSE)", "                        .exec(Catalog.Product.addProductToCart);", "        private static final ChainBuilder completePurchase =", "                initSession", "                        .exec(CmsPage.homePage)", "                        .pause(MAX_PAUSE)", "                        .exec(Catalog.Category.view)", "                        .pause(MIN_PAUSE,MAX_PAUSE)", "                        .exec(Catalog.Product.view)", "                        .pause(MIN_PAUSE,MAX_PAUSE)", "                        .exec(Catalog.Product.addProductToCart)", "                        .pause(MIN_PAUSE,MAX_PAUSE)", "                        .exec(Checkout.viewCart)", "                        .pause(MIN_PAUSE,MAX_PAUSE)", "                        .exec(Checkout.checkout);", "    }", "    private static class Scenarios {", "        private static final ScenarioBuilder defaultPurchase =", "                scenario(\"Default Load Test\")", "                        .during(TEST_DURATION)", "                        .on(", "                                randomSwitch().on(", "                                        Choice.withWeight(75.0,exec(UserJourneys.browseStore)),", "                                        Choice.withWeight(15.0,exec(UserJourneys.abandonCart)),", "                                        Choice.withWeight(10.0,exec(UserJourneys.completePurchase))", "                                )", "                        );", "        private static final ScenarioBuilder highPurchase =", "                scenario(\"High Purchase Load Test\")", "                        .during(Duration.ofSeconds(60))", "                        .on(", "                                randomSwitch().on(", "                                        Choice.withWeight(25.0,exec(UserJourneys.browseStore)),", "                                        Choice.withWeight(25.0,exec(UserJourneys.abandonCart)),", "                                        Choice.withWeight(50.0,exec(UserJourneys.completePurchase))", "                                )", "                        );", "    }", "", "    {", "        setUp(", "                Scenarios.defaultPurchase.injectOpen(", "                        rampUsers(USER_COUNT).during(RAMP_DURATION))", "                        .protocols(HTTP_PROTOCOL)", "                );", "    }", "}"], "file_path": "src/test/java/simulation/SimulationDemoStore4UserJourneysParallelRun.java"}
{"Link_to_commit": "https://github.com/baragaun/triumvirate/commit/2c97158b93a2a3f7cc496ca47268fdffe19fd708", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 136, "n_files_impacted": 22, "longest_chunk": ["<script lang=\"ts\">", "  import type { ChatMessage } from '$lib/server/db/schema';", "  import { ChatMessageFeedback } from '$lib/enums';", "", "  const { ", "    message, ", "    updateChatMessage ", "  } = $props<{", "    message: ChatMessage;", "    updateChatMessage: (changes: Partial<ChatMessage>) => void;", "  }>();", "", "  // State", "  let feedback = $state<ChatMessageFeedback | null>(message.feedback || null);", "  let isFeedbackSubmitted = $state(!!message.feedback);", "", "  const onSaveFeedback = async (selectedFeedback: ChatMessageFeedback) => {", "    if (!selectedFeedback) {", "      console.log('MessageFeedback.onSaveFeedback: No feedback provided.');", "      return;", "    }", "", "    feedback = selectedFeedback;", "", "    const changes: Partial<ChatMessage> = {", "      id: message.id,", "      feedback,", "    };", "    await updateChatMessage(changes);", "    isFeedbackSubmitted = true;", "  };", "</script>", "", "<div class=\"message-feedback\">", "  {#if isFeedbackSubmitted}", "    <div class=\"feedback-submitted\">", "      <span>Thanks! Feedback: <span class=\"feedback-value\">{feedback}</span></span>", "    </div>", "  {:else}", "    <div class=\"feedback-buttons\">", "      <button", "        class=\"feedback-button\"", "        onclick={() => onSaveFeedback(ChatMessageFeedback.helpful)}", "        title=\"This response was helpful\"", "      >", "        <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\">", "          <path d=\"M14 9V5a3 3 0 0 0-3-3l-4 9v11h11.28a2 2 0 0 0 2-1.7l1.38-9a2 2 0 0 0-2-2.3zM7 22H4a2 2 0 0 1-2-2v-7a2 2 0 0 1 2-2h3\"></path>", "        </svg>", "        Helpful", "      </button>", "      <button", "        class=\"feedback-button\"", "        onclick={() => onSaveFeedback(ChatMessageFeedback.unhelpful)}", "        title=\"This response was not helpful\"", "      >", "        <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\">", "          <path d=\"M10 15v4a3 3 0 0 0 3 3l4-9V2H5.72a2 2 0 0 0-2 1.7l-1.38 9a2 2 0 0 0 2 2.3zm7-13h2.67A2.31 2.31 0 0 1 22 4v7a2.31 2.31 0 0 1-2.33 2H17\"></path>", "        </svg>", "        Not Helpful", "      </button>", "      <button", "        class=\"feedback-button\"", "        onclick={() => onSaveFeedback(ChatMessageFeedback.wrong)}", "        title=\"This response contains incorrect information\"", "      >", "        <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\">", "          <circle cx=\"12\" cy=\"12\" r=\"10\"></circle>", "          <line x1=\"15\" y1=\"9\" x2=\"9\" y2=\"15\"></line>", "          <line x1=\"9\" y1=\"9\" x2=\"15\" y2=\"15\"></line>", "        </svg>", "        Wrong", "      </button>", "    </div>", "  {/if}", "</div>", "", "<style>", "  /* Feedback UI styles */", "  .message-feedback {", "    margin-top: 0.3rem;", "    padding-top: 0.2rem;", "    padding-bottom: 0;", "    margin-bottom: 0;", "    border-top: 1px solid rgba(0, 0, 0, 0.1);", "    font-size: 0.85rem;", "    display: flex;", "    justify-content: flex-end;", "  }", "", "  .feedback-buttons {", "    display: flex;", "    gap: 0.3rem;", "    flex-wrap: wrap;", "    margin-bottom: 0;", "  }", "", "  .feedback-button {", "    display: flex;", "    align-items: center;", "    gap: 0.25rem;", "    padding: 0.15rem 0.4rem;", "    border: 1px solid #ddd;", "    border-radius: 4px;", "    background-color: #f9f9f9;", "    color: #555;", "    font-size: 0.8rem;", "    cursor: pointer;", "    transition: all 0.2s ease;", "  }", "", "  .feedback-button:hover {", "    background-color: #e9e9e9;", "    border-color: #ccc;", "  }", "", "  .feedback-button svg {", "    width: 14px;", "    height: 14px;", "  }", "", "  .feedback-submitted {", "    display: flex;", "    align-items: center;", "    gap: 0.3rem;", "    color: #666;", "    font-size: 0.75rem;", "    margin-bottom: 0;", "    padding-bottom: 0;", "  }", "", "  .feedback-value {", "    font-weight: 500;", "    color: #68859b;", "    text-transform: capitalize;", "  }", "</style>"], "file_path": "src/lib/enums.ts"}
{"Link_to_commit": "https://github.com/kazupon/args-tokens/commit/3b7719e066e4bd47a103e79ffe6ccdb55c4fc29d", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 140, "n_files_impacted": 3, "longest_chunk": ["", "import type { ArgValues, ExtractOptionValue, FilterArgs, ResolveArgValues } from './resolver.ts'", "", "test('ExtractOptionValue', () => {", "  // string type", "  expectTypeOf<", "    ExtractOptionValue<{", "      type: 'string'", "      short: 's'", "    }>", "  >().toEqualTypeOf<string>()", "", "  // boolean type", "  expectTypeOf<", "    ExtractOptionValue<{", "      type: 'boolean'", "      short: 's'", "    }>", "  >().toEqualTypeOf<boolean>()", "", "  // number type", "  expectTypeOf<", "    ExtractOptionValue<{", "      type: 'number'", "      short: 's'", "    }>", "  >().toEqualTypeOf<number>()", "", "  // enum type", "  expectTypeOf<", "    ExtractOptionValue<{", "      type: 'enum'", "      short: 's'", "      choices: ['a', 'b', 'c']", "    }>", "  >().toEqualTypeOf<'a' | 'b' | 'c'>()", "  expectTypeOf<", "    ExtractOptionValue<{", "      type: 'enum'", "      short: 's'", "    }>", "  >().toEqualTypeOf<never>()", "})", "", "test('FilterArgs', () => {", "  expectTypeOf<", "    FilterArgs<", "      {", "        help: {", "          type: 'boolean'", "          short: 'h'", "        }", "      },", "      { help: true },", "      'type'", "    >", "  >().toEqualTypeOf<{ help: true }>()", "  expectTypeOf<", "    FilterArgs<", "      {", "        help: {", "          type: 'boolean'", "          short: 'h'", "        }", "      },", "      { help: true },", "      'short'", "    >", "  >().toEqualTypeOf<{ help: true }>()", "", "  expectTypeOf<", "    FilterArgs<", "      {", "        help: {", "          type: 'boolean'", "          short: 'h'", "        }", "      },", "      { help: true },", "      'required'", "    >", "  >().toEqualTypeOf<{}>()", "})", "", "test('ResolveArgValues', () => {", "  // basic", "  expectTypeOf<", "    ResolveArgValues<", "      {", "        help: {", "          type: 'boolean'", "          short: 'h'", "        }", "      },", "      { help: true }", "    >", "  >().toEqualTypeOf<{ help?: true | undefined }>()", "", "  // required", "  expectTypeOf<", "    ResolveArgValues<", "      {", "        help: {", "          type: 'boolean'", "          short: 'h'", "          required: true", "        }", "      },", "      { help: true }", "    >", "  >().toEqualTypeOf<{ help: true }>()", "", "  // default", "  expectTypeOf<", "    ResolveArgValues<", "      {", "        help: {", "          type: 'boolean'", "          short: 'h'", "          default: false", "        }", "      },", "      { help: false }", "    >", "  >().toEqualTypeOf<{ help: false }>()", "", "  // enum & choices", "  expectTypeOf<", "    ResolveArgValues<", "      {", "        log: {", "          type: 'enum'", "          short: 'l'", "          choices: ['debug', 'info', 'warn', 'error']", "        }", "      },", "      { log: 'debug' }", "    >", "  >().toEqualTypeOf<{ log?: 'debug' | undefined }>()", "})"], "file_path": "src/resolver.test-d.ts"}
{"Link_to_commit": "https://github.com/epoikos-project/simulation/commit/adb59cda979e12e0b8c2ddf96ee13783c5c4db5e", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 5, "n_files_impacted": 4, "longest_chunk": ["            (Query().simulation_id == self.simulation_id)", "            & (Query().x_1 <= location[0])", "            & (Query().x_2 >= location[0])", "            & (Query().y_1 <= location[1])", "            & (Query().y_2 >= location[1])"], "file_path": "models/agent.py"}
{"Link_to_commit": "https://github.com/itiden/statamic-backup/commit/ced0c23f7d8fedfc5bab5dea3191c243b1a445c5", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 5, "n_files_impacted": 4, "longest_chunk": ["    /**", "     * The upload chunk size", "     */", "    'chunk_size' => 2 * 1024 * 1024,", ""], "file_path": "config/backup.php"}
{"Link_to_commit": "https://github.com/Chowdhury-DSP/chowdsp_convolution/commit/6d9a1efb18e9dc72d65b0ef8ca83ae43a1fb420c", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 17, "n_files_impacted": 4, "longest_chunk": ["    if (mono_ir)", "    {", "        chowdsp::convolution::create_ir (&conv_config,", "                                         &conv_ir,", "                                         ir.data(),", "                                         ir_length_samples,", "                                         fft_scratch);", "    }", "    else", "    {", "        chowdsp::convolution::create_multichannel_ir (&conv_config,", "                                                      &conv_ir,", "                                                      multi_channel_ir.data(),", "                                                      ir_length_samples,", "                                                      num_channels,", "                                                      fft_scratch);", "    }"], "file_path": "test/chowdsp_convolution_test.cpp"}
{"Link_to_commit": "https://github.com/jumperexchange/jumper-exchange/commit/cdb96c5bc814d3c5575e1d69d8258efcb496af39", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 14, "n_files_impacted": 2, "longest_chunk": ["        components: {", "          MuiAvatar: {", "            styleOverrides: {", "              root: {", "                '.widget-wrapper &': {", "                  backgroundColor: themeCustomized.palette.common.white,", "                  ...themeCustomized.applyStyles('light', {", "                    backgroundColor: 'transparent',", "                  }),", "                },", "              }", "            },", "          }", "        }"], "file_path": "src/config/widgetConfig.ts"}
{"Link_to_commit": "https://github.com/AequilibraE/aequilibrae/commit/dc3a59929640bc9ddaed8d8c9566ce7849a961fb", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 61, "n_files_impacted": 7, "longest_chunk": ["import pytest", "", "from aequilibrae.utils.create_example import create_example", "from aequilibrae.project.tools.network_simplifier import NetworkSimplifier", "", "", "@pytest.fixture", "def project_with_graph(create_path):", "    project = create_example(create_path, \"nauru\")", "    remaining_links = [899, 900, 901, 902, 903, 1042, 1043, 1159, 1160]", "    remaining_links += list(range(171, 222))", "", "    with project.db_connection as conn:", "        qry = f\"DELETE FROM links WHERE link_id NOT IN {tuple(remaining_links)};\"", "        conn.execute(qry)", "        conn.commit()", "", "        # Let's create a centroid to build a graph", "        arbitrary_node = conn.execute(\"select node_id from nodes limit 1\").fetchone()[0]", "        nodes = project.network.nodes", "        nd = nodes.get(arbitrary_node)", "        nd.is_centroid = 1", "        nd.save()", "", "    mode = \"c\"", "", "    network = project.network", "    network.build_graphs(modes=[mode])", "    graph = network.graphs[mode]", "    graph.set_graph(\"distance\")", "    graph.set_skimming(\"distance\")", "    graph.set_blocked_centroid_flows(False)", "", "    yield graph", "    project.close()", "", "", "def test_simplify(project_with_graph):", "    net = NetworkSimplifier()", "", "    links_before = net.link_layer.shape[0]", "    nodes_before = net.network.nodes.data.shape[0]", "", "    net.simplify(project_with_graph)", "    net.rebuild_network()", "", "    assert links_before > net.network.links.data.shape[0]", "    assert nodes_before > net.network.nodes.data.shape[0]", "", "", "def test_collapse_links_into_nodes(project_with_graph):", "    net = NetworkSimplifier()", "", "    links_before = net.link_layer.shape[0]", "    nodes_before = net.network.nodes.data.shape[0]", "", "    net.collapse_links_into_nodes([903])", "    net.rebuild_network()", "", "    assert links_before > net.link_layer.shape[0]", "    assert nodes_before > net.network.nodes.data.shape[0]"], "file_path": "tests/aequilibrae/project/test_network_simplifier.py"}
{"Link_to_commit": "https://github.com/pwndbg/pwndbg/commit/c8793b87d397052fbab1a6b5157e61c297c77fdb", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 20, "n_files_impacted": 3, "longest_chunk": ["from __future__ import annotations", "", "import gdb", "", "import pwndbg", "", "", "def test_consistent_help():", "    \"\"\"", "    Tests that the help printed by gdb (via `help cmd`) is", "    the exact same as the help printed by argparse (via `cmd -h`).", "    \"\"\"", "", "    for cmd in pwndbg.commands.commands:", "        name = cmd.command_name", "        gdb_out = gdb.execute(f\"help {name}\", to_string=True)", "        argparse_out = gdb.execute(f\"{name} -h\", to_string=True)", "", "        # I would rather not strip, but gdb is inconsistent between versions.", "        assert gdb_out.rstrip() == argparse_out.rstrip()"], "file_path": "tests/gdb-tests/tests/test_misc.py"}
{"Link_to_commit": "https://github.com/jalantechnologies/rflask-boilerplate/commit/aad191c8d0f1fce92af4303fbb7c82ad91ff6a50", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 39, "n_files_impacted": 8, "longest_chunk": ["", "    def test_duplicate_cron_not_scheduled(self) -> None:", "        monkeypatch = MonkeyPatch()", "", "        log_messages = []", "", "        def fake_info(message: str) -> None:", "            log_messages.append(message)", "", "        monkeypatch.setattr(Logger, \"info\", fake_info)", "", "        cron_schedule = \"*/1 * * * *\"", "        worker_id_first = ApplicationService.schedule_worker_as_cron(", "            cls=HealthCheckWorker, cron_schedule=cron_schedule", "        )", "        assert worker_id_first", "", "        worker_details = ApplicationService.get_worker_by_id(worker_id=worker_id_first)", "        assert worker_details.id == worker_id_first", "        assert worker_details.status == WorkflowExecutionStatus.RUNNING", "", "        worker_id_duplicate = ApplicationService.schedule_worker_as_cron(", "            cls=HealthCheckWorker, cron_schedule=cron_schedule", "        )", "        assert worker_id_first == worker_id_duplicate", "", "        duplicate_log = (", "            f\"Worker {worker_id_first} already running, skipping starting new instance\"", "        )", "        assert any(", "            duplicate_log in log for log in log_messages", "        ), \"Expected duplicate log message not found\"", "", "        ApplicationService.terminate_worker(worker_id=worker_id_first)", "        time.sleep(1)", "        terminated_worker_details = ApplicationService.get_worker_by_id(", "            worker_id=worker_id_first", "        )", "        assert terminated_worker_details.status == WorkflowExecutionStatus.TERMINATED"], "file_path": "tests/modules/application/test_application_service.py"}
{"Link_to_commit": "https://github.com/AlistGo/alist/commit/f0b1aeaf8d846b3aee41fed29bf03ad7afa4e72f", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 451, "n_files_impacted": 4, "longest_chunk": ["", "\t\tresp = append(r.Data.Children, nextFiles...)", "\t}", "", "\treturn resp, err", "}", "", "func (d *Doubao) getUserInfo() (UserInfo, error) {", "\tvar r UserInfoResp", "", "\t_, err := d.request(\"/passport/account/info/v2/\", http.MethodGet, nil, &r)", "\tif err != nil {", "\t\treturn UserInfo{}, err", "\t}", "", "\treturn r.Data, err", "}", "", "// \u7b7e\u540d\u8bf7\u6c42", "func (d *Doubao) signRequest(req *resty.Request, method, tokenType, uploadUrl string) error {", "\tparsedUrl, err := url.Parse(uploadUrl)", "\tif err != nil {", "\t\treturn fmt.Errorf(\"invalid URL format: %w\", err)", "\t}", "", "\tvar accessKeyId, secretAccessKey, sessionToken string", "\tvar serviceName string", "", "\tif tokenType == VideoDataType {", "\t\taccessKeyId = d.UploadToken.Samantha.StsToken.AccessKeyID", "\t\tsecretAccessKey = d.UploadToken.Samantha.StsToken.SecretAccessKey", "\t\tsessionToken = d.UploadToken.Samantha.StsToken.SessionToken", "\t\tserviceName = \"vod\"", "\t} else {", "\t\taccessKeyId = d.UploadToken.Alice[tokenType].Auth.AccessKeyID", "\t\tsecretAccessKey = d.UploadToken.Alice[tokenType].Auth.SecretAccessKey", "\t\tsessionToken = d.UploadToken.Alice[tokenType].Auth.SessionToken", "\t\tserviceName = \"imagex\"", "\t}", "", "\t// \u5f53\u524d\u65f6\u95f4\uff0c\u683c\u5f0f\u4e3a ISO8601", "\tnow := time.Now().UTC()", "\tamzDate := now.Format(\"20060102T150405Z\")", "\tdateStamp := now.Format(\"20060102\")", "", "\treq.SetHeader(\"X-Amz-Date\", amzDate)", "", "\tif sessionToken != \"\" {", "\t\treq.SetHeader(\"X-Amz-Security-Token\", sessionToken)", "\t}", "", "\t// \u8ba1\u7b97\u8bf7\u6c42\u4f53\u7684SHA256\u54c8\u5e0c", "\tvar bodyHash string", "\tif req.Body != nil {", "\t\tbodyBytes, ok := req.Body.([]byte)", "\t\tif !ok {", "\t\t\treturn fmt.Errorf(\"request body must be []byte\")", "\t\t}", "", "\t\tbodyHash = hashSHA256(string(bodyBytes))", "\t\treq.SetHeader(\"X-Amz-Content-Sha256\", bodyHash)", "\t} else {", "\t\tbodyHash = hashSHA256(\"\")", "\t}", "", "\t// \u521b\u5efa\u89c4\u8303\u8bf7\u6c42", "\tcanonicalURI := parsedUrl.Path", "\tif canonicalURI == \"\" {", "\t\tcanonicalURI = \"/\"", "\t}", "", "\t// \u67e5\u8be2\u53c2\u6570\u6309\u7167\u5b57\u6bcd\u987a\u5e8f\u6392\u5e8f", "\tcanonicalQueryString := getCanonicalQueryString(req.QueryParam)", "\t// \u89c4\u8303\u8bf7\u6c42\u5934", "\tcanonicalHeaders, signedHeaders := getCanonicalHeadersFromMap(req.Header)", "\tcanonicalRequest := method + \"\\n\" +", "\t\tcanonicalURI + \"\\n\" +", "\t\tcanonicalQueryString + \"\\n\" +", "\t\tcanonicalHeaders + \"\\n\" +", "\t\tsignedHeaders + \"\\n\" +", "\t\tbodyHash", "", "\talgorithm := \"AWS4-HMAC-SHA256\"", "\tcredentialScope := fmt.Sprintf(\"%s/%s/%s/aws4_request\", dateStamp, Region, serviceName)", "", "\tstringToSign := algorithm + \"\\n\" +", "\t\tamzDate + \"\\n\" +", "\t\tcredentialScope + \"\\n\" +", "\t\thashSHA256(canonicalRequest)", "\t// \u8ba1\u7b97\u7b7e\u540d\u5bc6\u94a5", "\tsigningKey := getSigningKey(secretAccessKey, dateStamp, Region, serviceName)", "\t// \u8ba1\u7b97\u7b7e\u540d", "\tsignature := hmacSHA256Hex(signingKey, stringToSign)", "\t// \u6784\u5efa\u6388\u6743\u5934", "\tauthorizationHeader := fmt.Sprintf(", "\t\t\"%s Credential=%s/%s, SignedHeaders=%s, Signature=%s\",", "\t\talgorithm,", "\t\taccessKeyId,", "\t\tcredentialScope,", "\t\tsignedHeaders,", "\t\tsignature,", "\t)", "", "\treq.SetHeader(\"Authorization\", authorizationHeader)", "", "\treturn nil", "}", "", "func (d *Doubao) requestApi(url, method, tokenType string, callback base.ReqCallback, resp interface{}) ([]byte, error) {", "\treq := base.RestyClient.R()", "\treq.SetHeaders(map[string]string{", "\t\t\"user-agent\": UserAgent,", "\t})", "", "\tif method == http.MethodPost {", "\t\treq.SetHeader(\"Content-Type\", \"text/plain;charset=UTF-8\")", "\t}", "", "\tif callback != nil {", "\t\tcallback(req)", "\t}", "", "\tif resp != nil {", "\t\treq.SetResult(resp)", "\t}", "", "\t// \u4f7f\u7528\u81ea\u5b9a\u4e49AWS SigV4\u7b7e\u540d", "\terr := d.signRequest(req, method, tokenType, url)", "\tif err != nil {", "\t\treturn nil, err", "\t}", "", "\tres, err := req.Execute(method, url)", "\tif err != nil {", "\t\treturn nil, err", "\t}", "", "\treturn res.Body(), nil", "}", "", "func (d *Doubao) initUploadToken() (*UploadToken, error) {", "\tuploadToken := &UploadToken{", "\t\tAlice:    make(map[string]UploadAuthToken),", "\t\tSamantha: MediaUploadAuthToken{},", "\t}", "", "\tfileAuthToken, err := d.getUploadAuthToken(FileDataType)", "\tif err != nil {", "\t\treturn nil, err", "\t}", "", "\timgAuthToken, err := d.getUploadAuthToken(ImgDataType)", "\tif err != nil {", "\t\treturn nil, err", "\t}", "", "\tmediaAuthToken, err := d.getSamantaUploadAuthToken()", "\tif err != nil {", "\t\treturn nil, err", "\t}", "", "\tuploadToken.Alice[FileDataType] = fileAuthToken", "\tuploadToken.Alice[ImgDataType] = imgAuthToken", "\tuploadToken.Samantha = mediaAuthToken", "", "\treturn uploadToken, nil", "}", "", "func (d *Doubao) getUploadAuthToken(dataType string) (ut UploadAuthToken, err error) {", "\tvar r UploadAuthTokenResp", "\t_, err = d.request(\"/alice/upload/auth_token\", http.MethodPost, func(req *resty.Request) {", "\t\treq.SetBody(base.Json{", "\t\t\t\"scene\":     \"bot_chat\",", "\t\t\t\"data_type\": dataType,", "\t\t})", "\t}, &r)", "", "\treturn r.Data, err", "}", "", "func (d *Doubao) getSamantaUploadAuthToken() (mt MediaUploadAuthToken, err error) {", "\tvar r MediaUploadAuthTokenResp", "\t_, err = d.request(\"/samantha/media/get_upload_token\", http.MethodPost, func(req *resty.Request) {", "\t\treq.SetBody(base.Json{})", "\t}, &r)", "", "\treturn r.Data, err", "}", "", "// getUploadConfig \u83b7\u53d6\u4e0a\u4f20\u914d\u7f6e\u4fe1\u606f", "func (d *Doubao) getUploadConfig(upConfig *UploadConfig, dataType string, file model.FileStreamer) error {", "\ttokenType := dataType", "\t// \u914d\u7f6e\u53c2\u6570\u51fd\u6570", "\tconfigureParams := func() (string, map[string]string) {", "\t\tvar uploadUrl string", "\t\tvar params map[string]string", "\t\t// \u6839\u636e\u6570\u636e\u7c7b\u578b\u8bbe\u7f6e\u4e0d\u540c\u7684\u4e0a\u4f20\u53c2\u6570", "\t\tswitch dataType {", "\t\tcase VideoDataType:", "\t\t\t// \u97f3\u9891/\u89c6\u9891\u7c7b\u578b - \u4f7f\u7528uploadToken.Samantha\u7684\u914d\u7f6e", "\t\t\tuploadUrl = d.UploadToken.Samantha.UploadInfo.VideoHost", "\t\t\tparams = map[string]string{", "\t\t\t\t\"Action\":       \"ApplyUploadInner\",", "\t\t\t\t\"Version\":      \"2020-11-19\",", "\t\t\t\t\"SpaceName\":    d.UploadToken.Samantha.UploadInfo.SpaceName,", "\t\t\t\t\"FileType\":     \"video\",", "\t\t\t\t\"IsInner\":      \"1\",", "\t\t\t\t\"NeedFallback\": \"true\",", "\t\t\t\t\"FileSize\":     strconv.FormatInt(file.GetSize(), 10),", "\t\t\t\t\"s\":            randomString(),", "\t\t\t}", "\t\tcase ImgDataType, FileDataType:", "\t\t\t// \u56fe\u7247\u6216\u5176\u4ed6\u6587\u4ef6\u7c7b\u578b - \u4f7f\u7528uploadToken.Alice\u5bf9\u5e94\u914d\u7f6e", "\t\t\tuploadUrl = \"https://\" + d.UploadToken.Alice[dataType].UploadHost", "\t\t\tparams = map[string]string{", "\t\t\t\t\"Action\":        \"ApplyImageUpload\",", "\t\t\t\t\"Version\":       \"2018-08-01\",", "\t\t\t\t\"ServiceId\":     d.UploadToken.Alice[dataType].ServiceID,", "\t\t\t\t\"NeedFallback\":  \"true\",", "\t\t\t\t\"FileSize\":      strconv.FormatInt(file.GetSize(), 10),", "\t\t\t\t\"FileExtension\": filepath.Ext(file.GetName()),", "\t\t\t\t\"s\":             randomString(),", "\t\t\t}", "\t\t}", "\t\treturn uploadUrl, params", "\t}", "", "\t// \u83b7\u53d6\u521d\u59cb\u53c2\u6570", "\tuploadUrl, params := configureParams()", "", "\ttokenRefreshed := false", "\tvar configResp UploadConfigResp", "", "\terr := d._retryOperation(\"get upload_config\", func() error {", "\t\tconfigResp = UploadConfigResp{}", "", "\t\t_, err := d.requestApi(uploadUrl, http.MethodGet, tokenType, func(req *resty.Request) {", "\t\t\treq.SetQueryParams(params)", "\t\t}, &configResp)", "\t\tif err != nil {", "\t\t\treturn err", "\t\t}", "", "\t\tif configResp.ResponseMetadata.Error.Code == \"\" {", "\t\t\t*upConfig = configResp.Result", "\t\t\treturn nil", "\t\t}", "", "\t\t// 100028 \u51ed\u8bc1\u8fc7\u671f", "\t\tif configResp.ResponseMetadata.Error.CodeN == 100028 && !tokenRefreshed {", "\t\t\tlog.Debugln(\"[doubao] Upload token expired, re-fetching...\")", "\t\t\tnewToken, err := d.initUploadToken()", "\t\t\tif err != nil {", "\t\t\t\treturn fmt.Errorf(\"failed to refresh token: %w\", err)", "\t\t\t}", "", "\t\t\td.UploadToken = newToken", "\t\t\ttokenRefreshed = true", "\t\t\tuploadUrl, params = configureParams()", "", "\t\t\treturn retry.Error{errors.New(\"token refreshed, retry needed\")}", "\t\t}", "", "\t\treturn fmt.Errorf(\"get upload_config failed: %s\", configResp.ResponseMetadata.Error.Message)", "\t})", "", "\treturn err", "}", "", "// uploadNode \u4e0a\u4f20 \u6587\u4ef6\u4fe1\u606f", "func (d *Doubao) uploadNode(uploadConfig *UploadConfig, dir model.Obj, file model.FileStreamer, dataType string) (UploadNodeResp, error) {", "\treqUuid := uuid.New().String()", "\tvar key string", "\tvar nodeType int", "", "\tmimetype := file.GetMimetype()", "\tswitch dataType {", "\tcase VideoDataType:", "\t\tkey = uploadConfig.InnerUploadAddress.UploadNodes[0].Vid", "\t\tif strings.HasPrefix(mimetype, \"audio/\") {", "\t\t\tnodeType = AudioType // \u97f3\u9891\u7c7b\u578b", "\t\t} else {", "\t\t\tnodeType = VideoType // \u89c6\u9891\u7c7b\u578b", "\t\t}", "\tcase ImgDataType:", "\t\tkey = uploadConfig.InnerUploadAddress.UploadNodes[0].StoreInfos[0].StoreURI", "\t\tnodeType = ImageType // \u56fe\u7247\u7c7b\u578b", "\tdefault: // FileDataType", "\t\tkey = uploadConfig.InnerUploadAddress.UploadNodes[0].StoreInfos[0].StoreURI", "\t\tnodeType = FileType // \u6587\u4ef6\u7c7b\u578b", "\t}", "", "\tvar r UploadNodeResp", "\t_, err := d.request(\"/samantha/aispace/upload_node\", http.MethodPost, func(req *resty.Request) {", "\t\treq.SetBody(base.Json{", "\t\t\t\"node_list\": []base.Json{", "\t\t\t\t{", "\t\t\t\t\t\"local_id\":     reqUuid,", "\t\t\t\t\t\"parent_id\":    dir.GetID(),", "\t\t\t\t\t\"name\":         file.GetName(),", "\t\t\t\t\t\"key\":          key,", "\t\t\t\t\t\"node_content\": base.Json{},", "\t\t\t\t\t\"node_type\":    nodeType,", "\t\t\t\t\t\"size\":         file.GetSize(),", "\t\t\t\t},", "\t\t\t},", "\t\t\t\"request_id\": reqUuid,", "\t\t})", "\t}, &r)", "", "\treturn r, err", "}", "", "// Upload \u666e\u901a\u4e0a\u4f20\u5b9e\u73b0", "func (d *Doubao) Upload(config *UploadConfig, dstDir model.Obj, file model.FileStreamer, up driver.UpdateProgress, dataType string) (model.Obj, error) {", "\tdata, err := io.ReadAll(file)", "\tif err != nil {", "\t\treturn nil, err", "\t}", "", "\t// \u8ba1\u7b97CRC32", "\tcrc32Hash := crc32.NewIEEE()", "\tcrc32Hash.Write(data)", "\tcrc32Value := hex.EncodeToString(crc32Hash.Sum(nil))", "", "\t// \u6784\u5efa\u8bf7\u6c42\u8def\u5f84", "\tuploadNode := config.InnerUploadAddress.UploadNodes[0]", "\tstoreInfo := uploadNode.StoreInfos[0]", "\tuploadUrl := fmt.Sprintf(\"https://%s/upload/v1/%s\", uploadNode.UploadHost, storeInfo.StoreURI)", "", "\tuploadResp := UploadResp{}", "", "\tif _, err = d.uploadRequest(uploadUrl, http.MethodPost, storeInfo, func(req *resty.Request) {", "\t\treq.SetHeaders(map[string]string{", "\t\t\t\"Content-Type\":        \"application/octet-stream\",", "\t\t\t\"Content-Crc32\":       crc32Value,", "\t\t\t\"Content-Length\":      fmt.Sprintf(\"%d\", len(data)),", "\t\t\t\"Content-Disposition\": fmt.Sprintf(\"attachment; filename=%s\", url.QueryEscape(storeInfo.StoreURI)),", "\t\t})", "", "\t\treq.SetBody(data)", "\t}, &uploadResp); err != nil {", "\t\treturn nil, err", "\t}", "", "\tif uploadResp.Code != 2000 {", "\t\treturn nil, fmt.Errorf(\"upload failed: %s\", uploadResp.Message)", "\t}", "", "\tuploadNodeResp, err := d.uploadNode(config, dstDir, file, dataType)", "\tif err != nil {", "\t\treturn nil, err", "\t}", "", "\treturn &model.Object{", "\t\tID:       uploadNodeResp.Data.NodeList[0].ID,", "\t\tName:     uploadNodeResp.Data.NodeList[0].Name,", "\t\tSize:     file.GetSize(),", "\t\tIsFolder: false,", "\t}, nil", "}", "", "// UploadByMultipart \u5206\u7247\u4e0a\u4f20", "func (d *Doubao) UploadByMultipart(ctx context.Context, config *UploadConfig, fileSize int64, dstDir model.Obj, file model.FileStreamer, up driver.UpdateProgress, dataType string) (model.Obj, error) {", "\t// \u6784\u5efa\u8bf7\u6c42\u8def\u5f84", "\tuploadNode := config.InnerUploadAddress.UploadNodes[0]", "\tstoreInfo := uploadNode.StoreInfos[0]", "\tuploadUrl := fmt.Sprintf(\"https://%s/upload/v1/%s\", uploadNode.UploadHost, storeInfo.StoreURI)", "\t// \u521d\u59cb\u5316\u5206\u7247\u4e0a\u4f20", "\tvar uploadID string", "\terr := d._retryOperation(\"Initialize multipart upload\", func() error {", "\t\tvar err error", "\t\tuploadID, err = d.initMultipartUpload(config, uploadUrl, storeInfo)", "\t\treturn err", "\t})", "\tif err != nil {", "\t\treturn nil, fmt.Errorf(\"failed to initialize multipart upload: %w\", err)", "\t}", "\t// \u51c6\u5907\u5206\u7247\u53c2\u6570", "\tchunkSize := DefaultChunkSize", "\tif config.InnerUploadAddress.AdvanceOption.SliceSize > 0 {", "\t\tchunkSize = int64(config.InnerUploadAddress.AdvanceOption.SliceSize)", "\t}", "\ttotalParts := (fileSize + chunkSize - 1) / chunkSize", "\t// \u521b\u5efa\u5206\u7247\u4fe1\u606f\u7ec4", "\tparts := make([]UploadPart, totalParts)", "\t// \u7f13\u5b58\u6587\u4ef6", "\ttempFile, err := file.CacheFullInTempFile()", "\tif err != nil {", "\t\treturn nil, fmt.Errorf(\"failed to cache file: %w\", err)", "\t}", "\tdefer tempFile.Close()", "\tup(10.0) // \u66f4\u65b0\u8fdb\u5ea6", "\t// \u8bbe\u7f6e\u5e76\u884c\u4e0a\u4f20", "\tthreadG, uploadCtx := errgroup.NewGroupWithContext(ctx, d.uploadThread,", "\t\tretry.Attempts(1),", "\t\tretry.Delay(time.Second),", "\t\tretry.DelayType(retry.BackOffDelay))", "", "\tvar partsMutex sync.Mutex", "\t// \u5e76\u884c\u4e0a\u4f20\u6240\u6709\u5206\u7247", "\tfor partIndex := int64(0); partIndex < totalParts; partIndex++ {", "\t\tif utils.IsCanceled(uploadCtx) {", "\t\t\tbreak", "\t\t}", "\t\tpartIndex := partIndex", "\t\tpartNumber := partIndex + 1 // \u5206\u7247\u7f16\u53f7\u4ece1\u5f00\u59cb", "", "\t\tthreadG.Go(func(ctx context.Context) error {", "\t\t\t// \u8ba1\u7b97\u6b64\u5206\u7247\u7684\u5927\u5c0f\u548c\u504f\u79fb", "\t\t\toffset := partIndex * chunkSize", "\t\t\tsize := chunkSize", "\t\t\tif partIndex == totalParts-1 {", "\t\t\t\tsize = fileSize - offset", "\t\t\t}", "", "\t\t\tlimitedReader := driver.NewLimitedUploadStream(ctx, io.NewSectionReader(tempFile, offset, size))", "\t\t\t// \u8bfb\u53d6\u6570\u636e\u5230\u5185\u5b58", "\t\t\tdata, err := io.ReadAll(limitedReader)", "\t\t\tif err != nil {", "\t\t\t\treturn fmt.Errorf(\"failed to read part %d: %w\", partNumber, err)", "\t\t\t}", "\t\t\t// \u8ba1\u7b97CRC32", "\t\t\tcrc32Value := calculateCRC32(data)", "\t\t\t// \u4f7f\u7528_retryOperation\u4e0a\u4f20\u5206\u7247", "\t\t\tvar uploadPart UploadPart", "\t\t\tif err = d._retryOperation(fmt.Sprintf(\"Upload part %d\", partNumber), func() error {", "\t\t\t\tvar err error", "\t\t\t\tuploadPart, err = d.uploadPart(config, uploadUrl, uploadID, partNumber, data, crc32Value)", "\t\t\t\treturn err", "\t\t\t}); err != nil {", "\t\t\t\treturn fmt.Errorf(\"part %d upload failed: %w\", partNumber, err)", "\t\t\t}", "\t\t\t// \u8bb0\u5f55\u6210\u529f\u4e0a\u4f20\u7684\u5206\u7247", "\t\t\tpartsMutex.Lock()", "\t\t\tparts[partIndex] = UploadPart{", "\t\t\t\tPartNumber: strconv.FormatInt(partNumber, 10),", "\t\t\t\tEtag:       uploadPart.Etag,", "\t\t\t\tCrc32:      crc32Value,", "\t\t\t}", "\t\t\tpartsMutex.Unlock()", "\t\t\t// \u66f4\u65b0\u8fdb\u5ea6", "\t\t\tprogress := 10.0 + 90.0*float64(threadG.Success()+1)/float64(totalParts)", "\t\t\tup(math.Min(progress, 95.0))", "", "\t\t\treturn nil", "\t\t})", "\t}", "", "\tif err = threadG.Wait(); err != nil {", "\t\treturn nil, err"], "file_path": "drivers/doubao/util.go"}
{"Link_to_commit": "https://github.com/hatchet-dev/hatchet/commit/dacf48180bef58d1e91d67598190e399cab094be", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 17, "n_files_impacted": 10, "longest_chunk": ["", "func (tc *OLAPControllerImpl) sample(workflowRunID string) bool {", "\tif tc.samplingHashThreshold == nil {", "\t\treturn true", "\t}", "", "\tbucket := hashToBucket(workflowRunID, 100)", "", "\treturn int64(bucket) < *tc.samplingHashThreshold", "}", "", "func hashToBucket(workflowRunID string, buckets int) int {", "\thasher := fnv.New32a()", "\tidBytes := []byte(workflowRunID)", "\thasher.Write(idBytes)", "\treturn int(hasher.Sum32()) % buckets", "}"], "file_path": "pkg/config/loader/loader.go"}
{"Link_to_commit": "https://github.com/AdventureTimeSS14/frontier-station-ADT/commit/f83ee37411860b1bd546ba25372438e9f3e03aff", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 21, "n_files_impacted": 2, "longest_chunk": ["    // Frontier", "    private void OnMapInit(EntityUid uid, VehicleComponent component, MapInitEvent args)", "    {", "        bool actionsUpdated = false;", "        if (component.HornSound != null)", "        {", "            _actionContainer.EnsureAction(uid, ref component.HornAction, HornActionId);", "            actionsUpdated = true;", "        }", "", "        if (component.SirenSound != null)", "        {", "            _actionContainer.EnsureAction(uid, ref component.SirenAction, SirenActionId);", "            actionsUpdated = true;", "        }", "", "        if (actionsUpdated)", "            Dirty(uid, component);", "    }", "    // End Frontier", ""], "file_path": "Content.Shared/_Goobstation/Vehicles/SharedVehicleSystem.cs"}
{"Link_to_commit": "https://github.com/bytebase/bytebase/commit/9f7dc0279232e8da512ec1bf9348869631ce4bbd", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 13, "n_files_impacted": 1, "longest_chunk": ["\tif project.Setting.GetCiSamplingSize() > 0 {", "\t\tvar updatedRuns []*store.PlanCheckRunMessage", "\t\tcountMap := make(map[string]int32)", "\t\tfor _, run := range planCheckRuns {", "\t\t\tkey := fmt.Sprintf(\"%s/%s/%s/%d\", run.Type, run.Config.GetInstanceId(), run.Config.GetDatabaseName(), run.Config.GetSheetUid())", "\t\t\tif countMap[key] >= project.Setting.GetCiSamplingSize() {", "\t\t\t\tcontinue", "\t\t\t}", "\t\t\tupdatedRuns = append(updatedRuns, run)", "\t\t\tcountMap[key]++", "\t\t}", "\t\tplanCheckRuns = updatedRuns", "\t}"], "file_path": "backend/api/v1/plan_service_plan_check.go"}
{"Link_to_commit": "https://github.com/rilldata/rill/commit/70406ff6c57ca87b7d6a8a0e6ef0375d2e5bb385", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 47, "n_files_impacted": 2, "longest_chunk": ["func (d driver) checkVersion(dsn string) error {", "\tparsedURL, err := url.Parse(dsn)", "\tif err != nil {", "\t\treturn err", "\t}", "\tparsedURL.Path = \"/status\"", "\tstatusURL := parsedURL.String()", "", "\treq, err := http.NewRequest(http.MethodGet, statusURL, http.NoBody)", "\tif err != nil {", "\t\treturn err", "\t}", "", "\tclient := &http.Client{Timeout: 10 * time.Second}", "\tresp, err := client.Do(req)", "\tif err != nil {", "\t\treturn err", "\t}", "\tdefer resp.Body.Close()", "", "\tif resp.StatusCode != http.StatusOK {", "\t\treturn fmt.Errorf(\"druid version check failed with status code: %d\", resp.StatusCode)", "\t}", "", "\tvar statusResponse struct {", "\t\tVersion string `json:\"version\"`", "\t}", "\tif err := json.NewDecoder(resp.Body).Decode(&statusResponse); err != nil {", "\t\treturn fmt.Errorf(\"failed to decode Druid status response: %w\", err)", "\t}", "", "\tif statusResponse.Version != \"\" {", "\t\tmajorVersion := strings.Split(statusResponse.Version, \".\")[0]", "\t\tif ver, err := strconv.Atoi(majorVersion); err == nil {", "\t\t\tif ver < 28 {", "\t\t\t\treturn fmt.Errorf(\"druid version %s is not supported, please use 28.0.0 or higher\", statusResponse.Version)", "\t\t\t}", "\t\t} else {", "\t\t\treturn fmt.Errorf(\"failed to parse Druid version: %w\", err)", "\t\t}", "\t} else {", "\t\treturn fmt.Errorf(\"druid version information not found in the response\")", "\t}", "", "\treturn nil", "}", ""], "file_path": "runtime/drivers/druid/druid.go"}
{"Link_to_commit": "https://github.com/X-rays5/gta_base/commit/0ab97dca591a57afc8d4d81426fd9526646c3b5c", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 7, "n_files_impacted": 1, "longest_chunk": ["    try {", "      ShutdownImGui();", "    } catch (const std::exception& e) {", "      LOG_CRITICAL(\"Failed to shutdown ImGui: {}\", e.what());", "    } catch (...) {", "      LOG_CRITICAL(\"Failed to shutdown ImGui due to an unknown error\");", "    }"], "file_path": "menu/src/render/renderer.cpp"}
{"Link_to_commit": "https://github.com/antontanderup/mediocre-hass-media-player-cards/commit/cafc72987c629ada01b8c17aa6dabe339d39a289", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 58, "n_files_impacted": 20, "longest_chunk": ["import { HassEntity } from \"home-assistant-js-websocket\";", "", "export type MediaPlayerState =", "  | \"playing\"", "  | \"paused\"", "  | \"idle\"", "  | \"off\"", "  | \"on\"", "  | \"standby\"", "  | \"buffering\"", "  | \"unavailable\"", "  | string; // Just to make things a bit easier type-wise", "export type MediaPlayerRepeatMode = \"off\" | \"all\" | \"one\";", "", "export type MediaPlayerSupportedFeatures = number;", "", "export type MediaContentType =", "  | \"music\"", "  | \"tvshow\"", "  | \"movie\"", "  | \"video\"", "  | \"episode\"", "  | \"channel\"", "  | \"playlist\"", "  | \"image\"", "  | \"game\"", "  | \"app\"", "  | string;", "", "export type MediaPlayerDeviceClass = \"tv\" | \"speaker\" | \"receiver\";", "", "export interface MediaPlayerEntityAttributes {", "  media_duration?: number;", "  media_position?: number;", "  media_position_updated_at?: string; // ISO date string", "  media_title?: string;", "  media_artist?: string;", "  media_album_name?: string;", "  icon?: string;", "  friendly_name?: string;", "  entity_picture?: string;", "  volume_level?: number; // 0.0 to 1.0", "  is_volume_muted?: boolean;", "  shuffle?: boolean;", "  repeat?: MediaPlayerRepeatMode;", "  supported_features?: MediaPlayerSupportedFeatures;", "  group_members?: string[]; // Array of entity_ids", "  source?: string;", "  source_list?: string[];", "  device_class?: MediaPlayerDeviceClass;", "  media_content_id?: string;", "  media_content_type?: MediaContentType;", "}", "", "export interface MediaPlayerEntity extends HassEntity {", "  attributes: MediaPlayerEntityAttributes;", "  state: MediaPlayerState;", "}"], "file_path": "src/utils/actions.ts"}
{"Link_to_commit": "https://github.com/GlazeTech/pyglaze/commit/d85c132a50df3371addad68fc6cfee04b126df1b", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 22, "n_files_impacted": 24, "longest_chunk": ["    def get_serial_number(self: LeScanner) -> str:", "        \"\"\"Get the serial number of the connected device.", "", "        Returns:", "            str: The serial number of the connected device.", "        \"\"\"", "        if self._ampcom is None:", "            msg = \"Scanner not connected\"", "            raise ScanError(msg)", "        return self._ampcom.get_serial_number()", "", "    def get_firmware_version(self: LeScanner) -> str:", "        \"\"\"Get the firmware version of the connected device.", "", "        Returns:", "            str: The firmware version of the connected device.", "        \"\"\"", "        if self._ampcom is None:", "            msg = \"Scanner not connected\"", "            raise ScanError(msg)", "        return self._ampcom.get_firmware_version()", ""], "file_path": "src/pyglaze/scanning/scanner.py"}
{"Link_to_commit": "https://github.com/ReseauEntourage/entourage-job-back/commit/f4c308185b82d27908f86ec96570a2756f4ad90e", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 16, "n_files_impacted": 8, "longest_chunk": ["'use strict';", "", "/** @type {import('sequelize-cli').Migration} */", "module.exports = {", "  async up(queryInterface, Sequelize) {", "    await queryInterface.addColumn('User_Profiles', 'optInNewsletter', {", "      allowNull: false,", "      type: Sequelize.BOOLEAN,", "      defaultValue: false,", "    });", "  },", "", "  async down(queryInterface, Sequelize) {", "    await queryInterface.removeColumn('User_Profiles', 'optInNewsletter');", "  },", "};"], "file_path": "src/external-services/mailjet/mailjet.utils.ts"}
{"Link_to_commit": "https://github.com/DeMarcoLab/fibsem/commit/9eab43732f69a84a4268c2dd5e24a404b27ef74b", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 81, "n_files_impacted": 2, "longest_chunk": ["", "        # add fillet to the corners", "        if fillet > 0:            ", "            left_x_pos = point.x - width / 2", "            right_x_pos = point.x + width / 2", "", "            fillet_offset = 1.5", "            lower_y_pos = centre_lower_y + lower_trench_height / 2 - fillet * fillet_offset", "            top_y_pos = centre_upper_y - upper_trench_height / 2 + fillet * fillet_offset", "", "            lower_left_fillet = FibsemCircleSettings(", "                radius=fillet,", "                depth=depth/2,", "                centre_x=point.x - width / 2,", "                centre_y=lower_y_pos,", "            )", "            lower_right_fillet = FibsemCircleSettings(", "                radius=fillet,", "                depth=depth/2,", "                centre_x=point.x + width / 2,", "                centre_y=lower_y_pos,", "            )", "", "            # fill the remaining space with rectangles", "            lower_left_fill = FibsemRectangleSettings(", "                width=fillet,", "                height=lower_trench_height - fillet,", "                depth=depth,", "                centre_x=left_x_pos - fillet / 2,", "                centre_y=centre_lower_y - fillet / 2,", "                cross_section = cross_section,", "                scan_direction=\"BottomToTop\",", "", "            )", "            lower_right_fill = FibsemRectangleSettings(", "                width=fillet,", "                height=lower_trench_height - fillet,", "                depth=depth,", "                centre_x=right_x_pos + fillet / 2,", "                centre_y=centre_lower_y - fillet / 2,", "                cross_section = cross_section,", "                scan_direction=\"BottomToTop\",", "            )", "", "            top_left_fillet = FibsemCircleSettings(", "                radius=fillet,", "                depth=depth,", "                centre_x=point.x - width / 2,", "                centre_y=top_y_pos,", "            )", "            top_right_fillet = FibsemCircleSettings(", "                radius=fillet,", "                depth=depth,", "                centre_x=point.x + width / 2,", "                centre_y=top_y_pos,", "            )", "", "            top_left_fill = FibsemRectangleSettings(", "                width=fillet,", "                height=upper_trench_height - fillet,", "                depth=depth,", "                centre_x=left_x_pos - fillet / 2,", "                centre_y=centre_upper_y + fillet / 2,", "                cross_section = cross_section,", "                scan_direction=\"TopToBottom\",", "            )", "            top_right_fill = FibsemRectangleSettings(", "                width=fillet,", "                height=upper_trench_height - fillet,", "                depth=depth,", "                centre_x=right_x_pos + fillet / 2,", "                centre_y=centre_upper_y + fillet / 2,", "                cross_section = cross_section,", "                scan_direction=\"TopToBottom\",", "            )", "", "            self.shapes.extend([lower_left_fill, lower_right_fill, ", "                                top_left_fill, top_right_fill, ", "                                lower_left_fillet, lower_right_fillet, ", "                                top_left_fillet, top_right_fillet])", ""], "file_path": "fibsem/milling/patterning/patterns2.py"}
{"Link_to_commit": "https://github.com/microsoft/terraform-provider-power-platform/commit/3d6912bf5495868be5b5928d242101c56d1440d1", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 5, "n_files_impacted": 2, "longest_chunk": ["kind: fixed", "body: Replaced panic with proper error handling in ExecuteApiRequest function", "time: 2025-04-30T06:46:42.197147475Z", "custom:", "    Issue: \"704\""], "file_path": "internal/services/rest/api_rest.go"}
{"Link_to_commit": "https://github.com/Azure/azure-powershell/commit/40a3e1c84bf7d36e5d72637b8a5cd65741fa80bc", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 7, "n_files_impacted": 6, "longest_chunk": ["            catch (Exception e)", "            {", "                WriteDebug(\"Exception occurred while checking environment variable AZUREPS_OUTPUT_PLAINTEXT_AZACCESSTOKEN: \" + e.ToString());", "                //Throw exception when the caller doesn't have permission.", "                //Use SecureString only when AZUREPS_OUTPUT_PLAINTEXT_AZACCESSTOKEN is successfully set.", "            }", "            if (usePlainText)"], "file_path": "src/Accounts/Accounts/Token/GetAzureRmAccessToken.cs"}
{"Link_to_commit": "https://github.com/Ayushaff/agent-backend/commit/28ffe7e3dbb32de126e2ad475a69e1448eda5cc6", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 10, "n_files_impacted": 1, "longest_chunk": ["    fetch(resource: RequestInfo | URL, options?: RequestInit) {", "      if (resource instanceof Request && options) {", "        const mergedHeaders = new Headers(options.headers || {})", "        resource.headers.forEach((value, key) => {", "          mergedHeaders.append(key, value)", "        })", "        options.headers = mergedHeaders", "      }", "      return globalThis.fetch(resource, options)", "    },"], "file_path": "web/service/fetch.ts"}
{"Link_to_commit": "https://github.com/a-gondolkodas-orome/durer-aion/commit/f6a7a06983071ccbedbb11356a5fd01b8854f715", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 16, "n_files_impacted": 13, "longest_chunk": ["", "  async getMatchLogs(matchId: String): Promise<MatchStateDto> {", "    const url = urlcat('/game/admin/:matchId/logs', {", "      matchId,", "    });", "    let result;", "    try {", "      result = await ApiAxios.instance().get(url);", "    } catch (e: any) {", "      const err = makeAxiosError(e);", "      console.error(err.message)", "      // here we can set message according to status (or data)", "      throw new Error('V\u00e1ratlan hiba t\u00f6rt\u00e9nt');", "    }", "    return result.data;", "  }"], "file_path": "src/client/api-repository-interface.ts"}
{"Link_to_commit": "https://github.com/MariaDB4j/MariaDB4j/commit/a5e9bfc916628bd5a1462691bcec3993382b24c0", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 8, "n_files_impacted": 3, "longest_chunk": ["        if (dataDir == null || tmpDir == null) {", "            String p = SystemUtils.JAVA_IO_TMPDIR + path();", "", "            this.baseDir = p + \"/base\";", "            this.dataDir = p + DEFAULT_DATA_DIR;", "            this.tmpDir = p + DEFAULT_TMP_DIR;", "        }", ""], "file_path": "mariaDB4j-core/src/main/java/ch/vorburger/mariadb4j/DBConfigurationBuilder.java"}
{"Link_to_commit": "https://github.com/openteamsinc/opensourcescore.dev/commit/f21056708a847aa9a7af465c36a6f0e96559f4fb", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 6, "n_files_impacted": 5, "longest_chunk": ["", "    if production:", "        logHandler.addFilter(GoogleCloudLogFilter(project=\"openteams-score\"))", "        formatter = JsonFormatter()", "        logHandler.setFormatter(formatter)", ""], "file_path": "score/cloud_logging/setup.py"}
{"Link_to_commit": "https://github.com/openfga/openfga/commit/33153583a6144d7fde547afd145132436c63bd55", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 25, "n_files_impacted": 12, "longest_chunk": ["\ts.listObjectsCheckResolver, s.listObjectsCheckResolverCloser, err = graph.NewOrderedCheckResolvers([]graph.CheckResolverOrderedBuilderOpt{", "\t\tgraph.WithLocalCheckerOpts([]graph.LocalCheckerOption{", "\t\t\tgraph.WithResolveNodeBreadthLimit(s.resolveNodeBreadthLimit),", "\t\t\tgraph.WithOptimizations(s.IsExperimentallyEnabled(ExperimentalListObjectsOptimizations)),", "\t\t\tgraph.WithMaxResolutionDepth(s.resolveNodeLimit),", "\t\t}...),", "\t\tgraph.WithLocalShadowCheckerOpts([]graph.LocalCheckerOption{", "\t\t\tgraph.WithResolveNodeBreadthLimit(s.resolveNodeBreadthLimit),", "\t\t\tgraph.WithOptimizations(true),", "\t\t\tgraph.WithMaxResolutionDepth(s.resolveNodeLimit),", "\t\t}...),", "\t\tgraph.WithShadowResolverEnabled(s.shadowListObjectsCheckResolverEnabled),", "\t\tgraph.WithShadowResolverOpts([]graph.ShadowResolverOpt{", "\t\t\tgraph.ShadowResolverWithName(\"list-objects\"),", "\t\t\tgraph.ShadowResolverWithLogger(s.logger),", "\t\t\tgraph.ShadowResolverWithSamplePercentage(s.shadowListObjectsCheckResolverSamplePercentage),", "\t\t\tgraph.ShadowResolverWithTimeout(s.shadowListObjectsCheckResolverTimeout),", "\t\t}...),", "\t\tgraph.WithCachedCheckResolverOpts(s.cacheSettings.ShouldCacheCheckQueries(), checkCacheOptions...),", "\t\tgraph.WithDispatchThrottlingCheckResolverOpts(s.checkDispatchThrottlingEnabled, checkDispatchThrottlingOptions...),", "\t}...).Build()", "\tif err != nil {", "\t\treturn nil, err", "\t}", ""], "file_path": "pkg/server/server.go"}
{"Link_to_commit": "https://github.com/jenkinsci/opentelemetry-plugin/commit/4c49f605a16fed0c48e115d38938b1c6ba5152b5", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 309, "n_files_impacted": 11, "longest_chunk": ["/*", " * Copyright The Original Author or Authors", " * SPDX-License-Identifier: Apache-2.0", " */", "package io.jenkins.plugins.opentelemetry.jenkins;", "", "import static org.junit.jupiter.api.Assertions.assertEquals;", "import static org.junit.jupiter.api.Assertions.assertFalse;", "import static org.junit.jupiter.api.Assertions.assertNotNull;", "import static org.junit.jupiter.api.Assertions.assertNull;", "import static org.junit.jupiter.api.Assertions.assertThrowsExactly;", "import static org.junit.jupiter.api.Assertions.assertTrue;", "", "import java.nio.charset.StandardCharsets;", "import java.util.Collections;", "import java.util.List;", "import java.util.Map;", "import java.util.Optional;", "import java.util.UUID;", "", "import org.apache.hc.core5.http.Header;", "import org.jenkinsci.plugins.plaincredentials.StringCredentials;", "import org.jenkinsci.plugins.plaincredentials.impl.StringCredentialsImpl;", "import org.junit.jupiter.api.BeforeEach;", "import org.junit.jupiter.api.Test;", "import org.jvnet.hudson.test.JenkinsRule;", "import org.jvnet.hudson.test.junit.jupiter.WithJenkins;", "", "import com.cloudbees.plugins.credentials.BaseCredentials;", "import com.cloudbees.plugins.credentials.Credentials;", "import com.cloudbees.plugins.credentials.CredentialsScope;", "import com.cloudbees.plugins.credentials.SystemCredentialsProvider;", "import com.cloudbees.plugins.credentials.domains.Domain;", "import com.cloudbees.plugins.credentials.impl.UsernamePasswordCredentialsImpl;", "", "import hudson.model.Descriptor.FormException;", "import hudson.util.Secret;", "", "@WithJenkins", "public class HttpAuthHeaderFactoryTest {", "", "    protected JenkinsRule j;", "    private static String USERNAME = \"testuser\";", "    private static String PASSWORD = \"testpassword\";", "    private static String TOKEN = \"testtoken\";", "", "    @BeforeEach", "    void beforeEach(JenkinsRule j) {", "        this.j = j;", "        this.j.timeout = 0;", "    }", "", "    private String createUsernamePasswordCredentials() {", "        String credentialsId = UUID.randomUUID().toString();", "        try {", "            Credentials credentials = new UsernamePasswordCredentialsImpl(CredentialsScope.GLOBAL, credentialsId,", "                    \"test\", USERNAME, PASSWORD);", "            Map<Domain, List<Credentials>> domainCredentialsMap = SystemCredentialsProvider.getInstance()", "                    .getDomainCredentialsMap();", "            domainCredentialsMap.put(Domain.global(), Collections.singletonList(credentials));", "        } catch (FormException e) {", "            assertNull(e, \"FormException should not be thrown\");", "        }", "        return credentialsId;", "    }", "", "    private String createSecretStringCredentials() {", "        String credentialsId = UUID.randomUUID().toString();", "        Credentials credentials = new StringCredentialsImpl(CredentialsScope.GLOBAL, credentialsId, \"test\",", "                Secret.fromString(TOKEN));", "        Map<Domain, List<Credentials>> domainCredentialsMap = SystemCredentialsProvider.getInstance()", "                .getDomainCredentialsMap();", "        domainCredentialsMap.put(Domain.global(), Collections.singletonList(credentials));", "        return credentialsId;", "    }", "", "    private String createBaseCredentials() {", "        String credentialsId = UUID.randomUUID().toString();", "        Credentials credentials = new BaseCredentials(CredentialsScope.GLOBAL);", "        Map<Domain, List<Credentials>> domainCredentialsMap = SystemCredentialsProvider.getInstance()", "                .getDomainCredentialsMap();", "        domainCredentialsMap.put(Domain.global(), Collections.singletonList(credentials));", "        return credentialsId;", "    }", "", "    private String base64Digest(){", "        return java.util.Base64.getEncoder().encodeToString((USERNAME + \":\" + PASSWORD).getBytes(StandardCharsets.UTF_8));", "    }", "", "    @Test", "    public void testCreateAuthHeader_UsernamePasswordCredentials() {", "        String credentialsId = createUsernamePasswordCredentials();", "        HttpAuthHeaderFactory factory = new HttpAuthHeaderFactory(credentialsId);", "        Header header = factory.createAuthHeader();", "", "        assertNotNull(header);", "        assertEquals(\"Authorization\", header.getName());", "        String expectedValue = \"Basic \" + base64Digest();", "        assertEquals(expectedValue, header.getValue());", "    }", "", "    @Test", "    public void testCreateAuthHeader_StringCredentials_ApiKey() {", "        String credentialsId = createSecretStringCredentials();", "        HttpAuthHeaderFactory factory = new HttpAuthHeaderFactory(credentialsId);", "        Header header = factory.createAuthHeader();", "", "        assertNotNull(header);", "        assertEquals(\"Authorization\", header.getName());", "        assertEquals(\"ApiKey \" + TOKEN, header.getValue());", "    }", "", "    @Test", "    public void testCreateAuthHeader_StringCredentials_BearerToken() {", "        String credentialsId = createSecretStringCredentials();", "        HttpAuthHeaderFactory factory = new HttpAuthHeaderFactory(credentialsId, true);", "        Header header = factory.createAuthHeader();", "", "        assertNotNull(header);", "        assertEquals(\"Authorization\", header.getName());", "        assertEquals(\"Bearer \" + TOKEN, header.getValue());", "    }", "", "    @Test", "    public void testCreateAuthHeader_CredentialsNotFound() {", "        String credentialsId = \"nonexistent-credentials\";", "        assertThrowsExactly(CredentialsNotFoundException.class, () -> new HttpAuthHeaderFactory(credentialsId));", "    }", "", "    @Test", "    public void testCreateAuthHeader_NoCredentialsId() {", "        assertThrowsExactly(CredentialsNotFoundException.class, () -> new HttpAuthHeaderFactory((String) null));", "    }", "", "    @Test", "    public void testCreateAuthHeader_EmptyCredentialsId() {", "        assertThrowsExactly(CredentialsNotFoundException.class, () -> new HttpAuthHeaderFactory(\"\"));", "    }", "", "    @Test", "    public void testCreateAuthHeader_IncorrectCredentialsType() {", "        String credentialsId = createBaseCredentials();", "        assertThrowsExactly(CredentialsNotFoundException.class, () -> new HttpAuthHeaderFactory(credentialsId));", "    }", "", "    @Test", "    public void testCreateFactory_ValidCredentialsId() {", "        String credentialsId = createUsernamePasswordCredentials();", "", "        Optional<HttpAuthHeaderFactory> factory = HttpAuthHeaderFactory.createFactory(credentialsId);", "        assertTrue(factory.isPresent());", "        assertNotNull(factory.get().createAuthHeader());", "    }", "", "    @Test", "    public void testCreateFactory_NullCredentialsId() {", "        Optional<HttpAuthHeaderFactory> factory = HttpAuthHeaderFactory.createFactory((String) null);", "        assertFalse(factory.isPresent());", "    }", "", "    @Test", "    public void testCreateFactory_EmptyCredentialsId() {", "        Optional<HttpAuthHeaderFactory> factory = HttpAuthHeaderFactory.createFactory(\"\");", "        assertFalse(factory.isPresent());", "    }", "", "    @Test", "    public void testCreateFactory_Optional_ValidCredentialsId() {", "        String credentialsId = createUsernamePasswordCredentials();", "", "        Optional<HttpAuthHeaderFactory> factory = HttpAuthHeaderFactory.createFactory(Optional.of(credentialsId));", "        assertTrue(factory.isPresent());", "        assertNotNull(factory.get().createAuthHeader());", "    }", "", "    @Test", "    public void testCreateFactory_Optional_EmptyCredentialsId() {", "        Optional<HttpAuthHeaderFactory> factory = HttpAuthHeaderFactory.createFactory(Optional.empty());", "        assertFalse(factory.isPresent());", "    }", "", "    @Test", "    public void testCreateFactoryUsernamePassword_ValidCredentials() {", "        Optional<HttpAuthHeaderFactory> factory = HttpAuthHeaderFactory.createFactoryUsernamePassword(USERNAME,", "                PASSWORD);", "        assertTrue(factory.isPresent());", "        Header header = factory.get().createAuthHeader();", "        assertNotNull(header);", "        assertEquals(\"Authorization\", header.getName());", "        String expectedValue = \"Basic \" + base64Digest();", "        assertEquals(expectedValue, header.getValue());", "    }", "", "    @Test", "    public void testCreateFactoryUsernamePassword_NullUsername() {", "        Optional<HttpAuthHeaderFactory> factory = HttpAuthHeaderFactory.createFactoryUsernamePassword(null,", "                PASSWORD);", "        assertFalse(factory.isPresent());", "    }", "", "    @Test", "    public void testCreateFactoryUsernamePassword_EmptyUsername() {", "        Optional<HttpAuthHeaderFactory> factory = HttpAuthHeaderFactory.createFactoryUsernamePassword(\"\",", "                PASSWORD);", "        assertFalse(factory.isPresent());", "    }", "", "    @Test", "    public void testCreateFactoryUsernamePassword_NullPassword() {", "        Optional<HttpAuthHeaderFactory> factory = HttpAuthHeaderFactory.createFactoryUsernamePassword(\"testuser\", null);", "        assertFalse(factory.isPresent());", "    }", "", "    @Test", "    public void testCreateFactoryUsernamePassword_EmptyPassword() {", "        Optional<HttpAuthHeaderFactory> factory = HttpAuthHeaderFactory.createFactoryUsernamePassword(\"testuser\", \"\");", "        assertFalse(factory.isPresent());", "    }", "", "    @Test", "    public void testCreateFactoryApikey_ValidApiKey() {", "        Optional<HttpAuthHeaderFactory> factory = HttpAuthHeaderFactory.createFactoryApikey(TOKEN);", "        assertTrue(factory.isPresent());", "        Header header = factory.get().createAuthHeader();", "        assertNotNull(header);", "        assertEquals(\"Authorization\", header.getName());", "        assertEquals(\"ApiKey \" + TOKEN, header.getValue());", "    }", "", "    @Test", "    public void testCreateFactoryApikey_NullApiKey() {", "        Optional<HttpAuthHeaderFactory> factory = HttpAuthHeaderFactory.createFactoryApikey(null);", "        assertFalse(factory.isPresent());", "    }", "", "    @Test", "    public void testCreateFactoryApikey_EmptyApiKey() {", "        Optional<HttpAuthHeaderFactory> factory = HttpAuthHeaderFactory.createFactoryApikey(\"\");", "        assertFalse(factory.isPresent());", "    }", "", "    @Test", "    public void testCreateFactoryBearer_ValidBearerToken() {", "        Optional<HttpAuthHeaderFactory> factory = HttpAuthHeaderFactory.createFactoryBearer(TOKEN);", "        assertTrue(factory.isPresent());", "        Header header = factory.get().createAuthHeader();", "        assertNotNull(header);", "        assertEquals(\"Authorization\", header.getName());", "        assertEquals(\"Bearer \" + TOKEN, header.getValue());", "    }", "", "    @Test", "    public void testCreateFactoryBearer_NullBearerToken() {", "        Optional<HttpAuthHeaderFactory> factory = HttpAuthHeaderFactory.createFactoryBearer(null);", "        assertFalse(factory.isPresent());", "    }", "", "    @Test", "    public void testCreateFactoryBearer_EmptyBearerToken() {", "        Optional<HttpAuthHeaderFactory> factory = HttpAuthHeaderFactory.createFactoryBearer(\"\");", "        assertFalse(factory.isPresent());", "    }", "", "    @Test", "    public void testConstructor_CredentialsObject_ApiKey() {", "        StringCredentials credentials = new StringCredentialsImpl(CredentialsScope.GLOBAL, UUID.randomUUID().toString(),", "                \"test\", Secret.fromString(TOKEN));", "        HttpAuthHeaderFactory factory = new HttpAuthHeaderFactory(credentials);", "        Header header = factory.createAuthHeader();", "        assertNotNull(header);", "        assertEquals(\"Authorization\", header.getName());", "        assertEquals(\"ApiKey \" + TOKEN, header.getValue());", "    }", "", "    @Test", "    public void testConstructor_CredentialsObject_BearerToken() {", "        StringCredentials credentials = new StringCredentialsImpl(CredentialsScope.GLOBAL, UUID.randomUUID().toString(),", "                \"test\", Secret.fromString(TOKEN));", "        HttpAuthHeaderFactory factory = new HttpAuthHeaderFactory(credentials, true);", "        Header header = factory.createAuthHeader();", "        assertNotNull(header);", "        assertEquals(\"Authorization\", header.getName());", "        assertEquals(\"Bearer \" + TOKEN, header.getValue());", "    }", "", "    @Test", "    public void testConstructor_CredentialsObject_UsernamePassword() {", "        UsernamePasswordCredentialsImpl credentials;", "        try {", "            credentials = new UsernamePasswordCredentialsImpl(CredentialsScope.GLOBAL,", "                    UUID.randomUUID().toString(), \"test\", USERNAME, PASSWORD);", "            HttpAuthHeaderFactory factory = new HttpAuthHeaderFactory(credentials);", "            Header header = factory.createAuthHeader();", "            assertNotNull(header);", "            assertEquals(\"Authorization\", header.getName());", "            String expectedValue = \"Basic \" + base64Digest();", "            assertEquals(expectedValue, header.getValue());", "        } catch (FormException e) {", "            assertNull(e, \"FormException should not be thrown\");", "        }", "    }", "", "    @Test", "    public void testConstructor_CredentialsObject_IncorrectCredentialsType() {", "        Credentials credentials = new BaseCredentials(CredentialsScope.GLOBAL);", "        HttpAuthHeaderFactory factory = new HttpAuthHeaderFactory(credentials);", "        assertThrowsExactly(CredentialsNotFoundException.class, () -> factory.createAuthHeader());", "    }", "}"], "file_path": "src/test/java/io/jenkins/plugins/opentelemetry/jenkins/HttpAuthHeaderFactoryTest.java"}
{"Link_to_commit": "https://github.com/agaef5/SEP2/commit/6af4ad2da1240d0f21d0d4e0abe3522201d78477", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 16, "n_files_impacted": 21, "longest_chunk": ["  public UserDTO resultToDTO(ResultSet resultSet) throws SQLException {", "    String dbsUsername = resultSet.getString(\"username\");", "    String dbsEmail = resultSet.getString(\"email\");", "    String dbsPassword = resultSet.getString(\"password_hash\");", "    boolean dbsIsAdmin = (resultSet.getByte(\"isAdmin\")) == 1;", "    int balance = resultSet.getInt(\"balance\");", "", "    return new UserDTO(dbsUsername, dbsEmail, dbsPassword, dbsIsAdmin, balance);", "  }", "", "  public User resultToUser(ResultSet resultSet) throws SQLException {", "    String dbsUsername = resultSet.getString(\"username\");", "    String dbsEmail = resultSet.getString(\"email\");", "    String dbsPassword = resultSet.getString(\"password_hash\");", "    boolean dbsIsAdmin = resultSet.getBoolean(\"isAdmin\");", "    int balance = resultSet.getInt(\"balance\");"], "file_path": "src/server/persistence/user/UserRepositoryImpl.java"}
{"Link_to_commit": "https://github.com/livingbio/typed-ffmpeg/commit/44ed4165b0060a9c1a63950f6bd5ce9bf4fe6b67", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 6, "n_files_impacted": 2, "longest_chunk": ["  // Helper function to generate random filename", "  private generateRandomFilename(type: 'input' | 'output'): string {", "    const randomId = Math.random().toString(36).substring(2, 8);", "    return `${type}-${randomId}.mp4`;", "  }", ""], "file_path": "ffmpeg-flow-editor/src/utils/nodeMapping.ts"}
{"Link_to_commit": "https://github.com/KwiatkowskiML/TransformerTranslator/commit/83f8b81c72f312c3e08e7e7ecc9dc0c5ebf34eb6", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 193, "n_files_impacted": 5, "longest_chunk": ["import string", "", "", "class BrutalTranslator:", "    def __init__(self, dictionary_path=\"english_polish.txt\"):", "        # Dictionary to hold our translations", "        self.translation_dict = {}", "", "        # Special case words that shouldn't appear as Polish translations", "        self.english_words_to_check = [\"from\", \"about\", \"to\", \"the\", \"a\", \"an\"]", "", "        # Load translations from txt file", "        self.load_dictionary(dictionary_path)", "", "        # Fix problematic translations", "        self.fix_translations()", "", "    def load_dictionary(self, dictionary_path):", "        \"\"\"Load translations from the txt file\"\"\"", "        try:", "            with open(dictionary_path, 'r', encoding='utf-8') as file:", "                lines = file.readlines()", "", "                for line in lines:", "                    if '\\t' in line:", "                        # Split by tab character", "                        parts = line.strip().split('\\t')", "                        if len(parts) >= 2:", "                            english = parts[0].strip().lower()", "                            polish = parts[1].strip()", "", "                            # Store in dictionary", "                            if english not in self.translation_dict:", "                                self.translation_dict[english] = []", "", "                            self.translation_dict[english].append(polish)", "", "                print(f\"Loaded {len(self.translation_dict)} unique English words\")", "", "                # Print some statistics", "                total_translations = sum(len(translations) for translations in self.translation_dict.values())", "                print(f\"Total translations: {total_translations}\")", "                print(f\"Average translations per word: {total_translations / len(self.translation_dict):.2f}\")", "", "        except FileNotFoundError:", "            print(f\"Error: Dictionary file '{dictionary_path}' not found\")", "        except Exception as e:", "            print(f\"Error loading dictionary: {e}\")", "", "    def fix_translations(self):", "        \"\"\"Fix problematic translations and add missing common words\"\"\"", "        # Check for English words that appear as Polish translations and remove them", "        for eng, pol_list in list(self.translation_dict.items()):", "            fixed_list = []", "            for pol in pol_list:", "                if pol.lower() != eng.lower() and pol.lower() not in self.english_words_to_check:", "                    fixed_list.append(pol)", "                # else:", "                #     print(f\"Removed problematic translation: {eng} -> {pol}\")", "", "            # If we have valid translations left, update the list", "            if fixed_list:", "                self.translation_dict[eng] = fixed_list", "", "        # Add translations for common words that might be missing", "        common_translations = {", "            \"the\": \"\",", "            \"a\": \"\",", "            \"an\": \"\",", "            \"to\": \"do\",", "            \"about\": \"o\",", "            \"of\": \"z\",", "            \"in\": \"w\",", "            \"on\": \"na\",", "            \"at\": \"przy\",", "            \"by\": \"przez\",", "            \"is\": \"jest\",", "            \"are\": \"s\u0105\",", "            \"am\": \"jestem\",", "            \"was\": \"by\u0142\",", "            \"were\": \"by\u0142y\",", "            \"from\": \"z\",", "            \"with\": \"z\",", "            \"i\": \"ja\",", "            \"my\": \"m\u00f3j\",", "        }", "", "        # Add these translations if they're not already present", "        for eng, pol in common_translations.items():", "            if eng not in self.translation_dict:", "                self.translation_dict[eng] = [pol]", "            elif pol and pol not in self.translation_dict[eng]:", "                self.translation_dict[eng].append(pol)", "", "    def translate_word(self, word):", "        \"\"\"Translate a single word preserving case and punctuation\"\"\"", "        # Handle empty words", "        if not word:", "            return word", "", "        # Extract punctuation", "        prefix_punct = \"\"", "        word_only = \"\"", "        suffix_punct = \"\"", "", "        # Extract leading punctuation", "        i = 0", "        while i < len(word) and word[i] in string.punctuation:", "            prefix_punct += word[i]", "            i += 1", "", "        # Extract trailing punctuation", "        j = len(word) - 1", "        while j >= i and word[j] in string.punctuation:", "            suffix_punct = word[j] + suffix_punct", "            j -= 1", "", "        # Extract the word itself", "        if i <= j:", "            word_only = word[i:j + 1]", "", "        # Skip translation for empty words", "        if not word_only:", "            return word", "", "        # Check for translation", "        clean_word = word_only.lower()", "", "        if clean_word in self.translation_dict and self.translation_dict[clean_word]:", "            # Always pick the first translation for consistency", "            translated = self.translation_dict[clean_word][0]", "", "            # Skip empty translations for articles", "            if not translated:", "                return prefix_punct + suffix_punct", "", "            # Preserve original capitalization", "            if word_only[0].isupper():", "                translated = translated[0].upper() + translated[1:] if translated else \"\"", "", "            return prefix_punct + translated + suffix_punct", "        else:", "            # If no translation found, return the original word", "            return word", "", "    def translate(self, english_text):", "        \"\"\"Translate English text to Polish word by word\"\"\"", "        if not english_text:", "            return \"\"", "", "        # Split text into words", "        words = english_text.split()", "        translated_words = []", "", "        # Translate each word individually", "        for word in words:", "            translated = self.translate_word(word)", "            # Don't add empty translations to the result", "            if translated:", "                translated_words.append(translated)", "", "        # Join words back into text", "        return \" \".join(translated_words)", "", "", "if __name__ == \"__main__\":", "    # Path to dictionary file", "    dictionary_path = \"data/MUSEMultilingualEmbeddings.txt\"", "", "    # Create translator", "    translator = BrutalTranslator(dictionary_path)", "", "    # Test sentences", "    test_sentences = [", "        \"This page also has new articles that were not for you.\",", "        \"First, they had one article which was talking about his utc page.\",", "        \"You are from the new page but they were not.\",", "        \"Who was the first to talk about this article?\",", "        \"They also had a new page and were not the first one.\"", "    ]", "", "    print(\"\\n=== BRUTAL ENGLISH TO POLISH TRANSLATOR ===\\n\")", "    for sentence in test_sentences:", "        print(f\"English: {sentence}\")", "        print(f\"Polish:  {translator.translate(sentence)}\")", "        print()", "", "    print(\"Enter your own sentences (type 'exit' to quit):\")", "    while True:", "        user_input = input(\"> \")", "        if user_input.lower() == 'exit':", "            break", "        print(f\"Polish: {translator.translate(user_input)}\")"], "file_path": "brutal_translator/brutal_translator.py"}
{"Link_to_commit": "https://github.com/OpenZeppelin/contracts-wizard/commit/aa0890023fac3dc2667e91c5a9c8c1cf8a30b38b", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 6, "n_files_impacted": 18, "longest_chunk": ["import type {", "  AllLanguagesContractsOptions,", "  LanguageContractsNames,", "  LanguageContractsOptions,", "  SupportedLanguage,", "} from './languages.ts';"], "file_path": "packages/ui/api/ai-assistant/types/function-definition.ts"}
{"Link_to_commit": "https://github.com/AshwinNS/bookworm/commit/ef1c8e7b458f6d2f72e38ec1113800a1b218f2b6", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 108, "n_files_impacted": 14, "longest_chunk": ["from typing import Annotated, List", "from fastapi import APIRouter, Depends, HTTPException, Query", "from sqlmodel import select, Session", "", "from api.db import get_session", "from api.models import *", "", "routers = APIRouter()", "SessionDep = Annotated[Session, Depends(get_session)]", "", "@routers.get(\"/books/\", response_model=List[BookPublic], tags=[\"books\"])", "async def get_books(session: SessionDep, offset: int = 0, limit: Annotated[int, Query(le=100)] = 100):", "    \"\"\"", "    Fetch list of books from the database with optional pagination.", "    Args:", "        session (SessionDep): The database session dependency used to execute queries.", "        offset (int, optional): The number of records to skip before starting to fetch. Defaults to 0.", "        limit (int, optional): The maximum number of records to fetch. Must be less than or equal to 100. Defaults to 100.", "    Returns:", "        List[Book]: A list of Book objects retrieved from the database.", "    \"\"\"", "    heroes = session.exec(select(Book).offset(offset).limit(limit)).all()", "    return heroes", "", "", "@routers.post(\"/books/\", response_model=BookPublic, tags=[\"books\"])", "def create_book(book: BookCreate, session: SessionDep) -> Book:", "    \"\"\"", "    Creates a new book record in the database.", "    Args:", "        book (BookCreate): The data required to create a new book, validated against the BookCreate schema.", "    Returns:", "        Book: The newly created book record after being added to the database.", "    \"\"\"", "    try:", "        db_books = Book.model_validate(book)", "        session.add(db_books)", "        session.commit()", "        session.refresh(db_books)", "        return db_books", "    except Exception as e:", "        raise HTTPException(status_code=400, detail=f\"Error creating book: {str(e)}\")", "", "", "@routers.get(\"/books/{book_id}\", response_model=BookPublic, tags=[\"books\"])", "def get_book_by_id(book_id: int, session: SessionDep) -> Book:", "    \"\"\"", "    Retrieve a book from the database by its ID.", "    Args:", "        book_id (int): The unique identifier of the book to retrieve.", "    Returns:", "        Book: The book object corresponding to the given ID.", "    Raises:", "        HTTPException: If no book with the given ID is found, raises a 404 error with the message \"Book not found\".", "    \"\"\"", "    db_book = session.get(Book, book_id)", "    if not db_book:", "        raise HTTPException(status_code=404, detail=\"Book not found\")", "    return db_book", "", "", "@routers.put(\"/books/{book_id}\", response_model=BookPublic, tags=[\"books\"])", "def update_book(book_id: int, book: BookCreate, session: SessionDep) -> Book:", "    \"\"\"", "    Update an existing book in the database.", "    Args:", "        book_id (int): The ID of the book to update.", "        book (BookCreate): An object containing the updated book data.", "    Returns:", "        Book: The updated book object.", "    Raises:", "        HTTPException: If the book with the given ID is not found (404).", "    \"\"\"", "    db_book = session.get(Book, book_id)", "    if not db_book:", "        raise HTTPException(status_code=404, detail=\"Book not found\")", "    ", "    # Update only the fields provided in the request", "    book_data = book.model_dump(exclude_unset=True)", "    for key, value in book_data.items():", "        setattr(db_book, key, value)", "    ", "    session.commit()", "    session.refresh(db_book)", "    return db_book", "", "", "@routers.delete(\"/books/{book_id}\", response_model=str, tags=[\"books\"])", "def delete_book(book_id: int, session: SessionDep) -> str:", "    \"\"\"", "    Deletes a book from the database based on the provided book ID.", "    Args:", "        book_id (int): The ID of the book to be deleted.", "    Returns:", "        str: A success message indicating the book has been deleted.", "    Raises:", "        HTTPException: If the book with the given ID is not found, raises a 404 error.", "    \"\"\"", "", "    db_book = session.get(Book, book_id)", "    if not db_book:", "        raise HTTPException(status_code=404, detail=\"Book not found\")", "    ", "    session.delete(db_book)", "    session.commit()", "    return f\"Book with ID {book_id} has been deleted successfully.\"", "", "# endpoints for books - End"], "file_path": "api/routers/books.py"}
{"Link_to_commit": "https://github.com/apptanksas/horus-sync-php/commit/2b0b875ec12c96b3f6672d3f13455b4ebfa87e94", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 35, "n_files_impacted": 22, "longest_chunk": ["<?php", "", "namespace AppTank\\Horus\\Core\\Config\\Restriction;", "", "use AppTank\\Horus\\Core\\Config\\Restriction\\valueObject\\ParameterFilter;", "", "/**", " * Class FilterEntityRestriction", " *", " * This class implements the EntityRestriction interface and is used to filter entities based on specific parameters.", " * It contains the entity name and an array of ParameterFilter objects that define the filtering criteria.", " *", " * @package AppTank\\Horus\\Core\\Config\\Restriction", " */", "readonly class FilterEntityRestriction implements EntityRestriction", "{", "    /**", "     * FilterEntityRestriction constructor.", "     *", "     * @param string $entityName The name of the entity to be filtered.", "     * @param ParameterFilter[] $parametersFilter The parameters to filter the entity.", "     */", "    public function __construct(", "        public string $entityName,", "        public array  $parametersFilter,", "    )", "    {", "", "    }", "", "    function getEntityName(): string", "    {", "        return $this->entityName;", "    }", "}"], "file_path": "src/Core/Config/Restriction/FilterEntityRestriction.php"}
{"Link_to_commit": "https://github.com/fizodev/phbot-plugins/commit/464c3de0838c3ae49e2dc4b70d5957a727224082", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 28, "n_files_impacted": 1, "longest_chunk": ["    xHWT_log(\"The plugin has been \" + (\"enabled\" if checked else \"disabled\"))", "", "# Called every 500ms", "def event_loop():", "    # Only process items if plugin is enabled", "    if not pluginEnabled:", "        return", "", "    # get list of items around available for pickup", "    items = get_drops()", "    # loop through the dictionary items (key-value pairs)", "    for item_id, item_data in items.items():", "        # check if the item id is in the target item ids list", "        current_item_name = item_data['name']", "        if current_item_name in target_item_names:", "            # pickup the item", "            xHWT_log(f\"Picking up item: {current_item_name}\")", "            pickup_item(item_id)", "            # Break the loop after picking up the first item", "            break", "", "# pickup the item", "def pickup_item(item_id):", "    # Pack item ID as little-endian unsigned int", "    id_bytes = struct.pack('<I', item_id)", "    # Create packet: header + first 3 bytes of item_id + trailer", "    data = bytearray(b'\\x01\\x02\\x01') + id_bytes[:3] + bytearray(b'\\x00')", "    inject_joymax(0x7074, data, True)"], "file_path": "xHWT/xHWT.py"}
{"Link_to_commit": "https://github.com/zuplo/zudoku/commit/16c2a015d7a7ab25678327f8a0d3d31fed174329", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 25, "n_files_impacted": 18, "longest_chunk": ["const SchemaTag = builder.objectRef<", "  Omit<TagObject, \"name\"> & { name?: string; slug?: string }", ">(\"SchemaTag\");", "", "SchemaTag.implement({", "  fields: (t) => ({", "    name: t.exposeString(\"name\", { nullable: true }),", "    slug: t.exposeString(\"slug\", { nullable: true }),", "    isUntagged: t.field({ type: \"Boolean\", resolve: (parent) => !parent.name }),", "    description: t.exposeString(\"description\", { nullable: true }),", "    operations: t.field({", "      type: [OperationItem],", "      resolve: (parent, _args, ctx) => {", "        const rootTags = ctx.tags.map((tag) => tag.name);", "", "        return ctx.operations", "          .filter((item) =>", "            parent.name", "              ? item.tags?.includes(parent.name)", "              : item.tags?.length === 0 ||", "                // If none of the tags are present in the root tags, then show them here", "                item.tags?.every((tag) => !rootTags.includes(tag)),", "          )", "          .map((item) => ({ ...item, parentTag: parent.name }));", "      },"], "file_path": "packages/zudoku/src/lib/oas/graphql/index.ts"}
{"Link_to_commit": "https://github.com/ppoz21/symfony-discounts-example/commit/30e49d5e87f8fb56d120b3b38b5badfb94243c5e", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 30, "n_files_impacted": 3, "longest_chunk": ["", "    public function insertInBatch(int $discountId, array $codes): void", "    {", "        $values = implode(", "            separator: ',',", "            array: array_map(", "                callback: static fn (string $code) => sprintf(", "                    \"(nextval('discount_code_id_seq'), '%s', '%s', false)\",", "                    $discountId,", "                    $code", "                ),", "                array: $codes", "            )", "        );", "", "        $this->getEntityManager()->getConnection()->executeQuery(", "            \"INSERT INTO discount_code (id, discount_id, code, used) VALUES {$values} ON CONFLICT DO NOTHING\"", "        );", "    }", "", "    public function countByDiscount(Discount $discount): int", "    {", "        return $this->createQueryBuilder('dc')", "            ->select('COUNT(dc.id)')", "            ->andWhere('dc.discount = :discount')", "            ->setParameter('discount', $discount)", "            ->getQuery()", "            ->getSingleScalarResult()", "        ;", "    }"], "file_path": "src/Repository/DiscountCodeRepository.php"}
{"Link_to_commit": "https://github.com/iwatkot/maps4fs/commit/107041b8fae2b35072feb98d197b02f48cae833d", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 23, "n_files_impacted": 16, "longest_chunk": ["    def scale_textures(self) -> None:", "        \"\"\"Resizes all the textures to the map output size.\"\"\"", "        if not self.map.output_size:", "            self.logger.debug(\"No output size defined, skipping scaling.\")", "            return", "", "        for layer in tqdm(self.layers, desc=\"Scaling textures\", unit=\"layer\"):", "            layer_paths = layer.paths(self._weights_dir)", "            layer_paths += [layer.path_preview(self._weights_dir)]", "", "            for layer_path in layer_paths:", "                if os.path.isfile(layer_path):", "                    self.logger.debug(\"Scaling layer %s.\", layer_path)", "                    img = cv2.imread(layer_path, cv2.IMREAD_UNCHANGED)", "                    img = cv2.resize(", "                        img,", "                        (self.map.output_size, self.map.output_size),", "                        interpolation=cv2.INTER_NEAREST,", "                    )", "                    cv2.imwrite(layer_path, img)", "                else:", "                    self.logger.debug(\"Layer %s not found, skipping scaling.\", layer_path)", ""], "file_path": "maps4fs/generator/component/texture.py"}
{"Link_to_commit": "https://github.com/ISKME/Open-Metadata-Exchange/commit/b5be6a4e4d07c86de672582c93f9c1fb2e671ea0", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 83, "n_files_impacted": 7, "longest_chunk": ["#!/usr/bin/env -S uv run --script", "", "# Source: https://whgazetteer.org/api/datasets", "# Docs: https://docs.whgazetteer.org/content/400-Technical.html", "", "# /// script", "# requires-python = \">=3.13\"", "# dependencies = [", "#     \"httpx\",", "#     \"pydantic\",", "# ]", "# ///", "#", "# generated by datamodel-codegen:", "#   filename:  whg.json", "#   timestamp: 2025-04-28T10:31:22+00:00", "#", "# Generated with command:", "# uv tool run --from=datamodel-code-generator datamodel-codegen \\", "#             --input whg.json --input-file-type json \\", "#             --output whg_models.py", "", "from __future__ import annotations", "", "from typing import Any", "", "from pydantic import BaseModel", "", "", "class Feature(BaseModel):", "    id: int", "    place_count: int", "    owner: str", "    label: str", "    title: str", "    description: str", "    datatype: str", "    ds_status: str", "    create_date: str", "    public: bool", "    core: bool", "    creator: str | None", "    webpage: str | None", "    contributors: str | None", "", "", "class Model(BaseModel):", "    count: int", "    parameters: dict[str, Any]", "    features: list[Feature]", "", "", "if __name__ == \"__main__\":", "    import json", "    from pathlib import Path", "", "    from httpx import Client", "", "    here = Path(__file__).resolve().parent", "    # Conditionally create an whg.json file that should contain whg dataset items.", "    if not (json_path := here / \"whg.json\").exists():", "        with Client() as client:", "            response = client.get(\"https://whgazetteer.org/api/datasets\")", "            response.raise_for_status()", "            json_path.write_text(response.text)", "    # Conditionally create an whg_item.json file that should contain only one item.", "    if not (json_item_path := here / \"whg_item.json\").exists():", "        json_payload = json.loads(json_path.read_text())[\"features\"][0]", "        json_item_path.write_text(json.dumps(json_payload, indent=2))", "", "    # The whg_item.json file should be in the same directory as this script.", "    model_instance = Model.model_validate_json(json_path.read_text())", "    print(f\"{model_instance = }\\n\")", "    print(f\"{model_instance.count = }\")", "    print(f\"{model_instance.parameters = }\\n\")", "", "    # The whg_item.json file should be in the same directory as this script.", "    feature_instance = Feature.model_validate_json(json_item_path.read_text())", "    print(f\"{feature_instance = }\\n\")", "    print(f\"{feature_instance.title = }\")", "    print(f\"{feature_instance.creator = }\")", "    print(f\"{feature_instance.description = }\")", "    print(f\"{feature_instance.contributors = }\")"], "file_path": "server/plugins/whg/whg_models.py"}
{"Link_to_commit": "https://github.com/muhammedgaygisiz/travellers-apps/commit/5c8988f45391c5ca85aa04b2999004c573088b48", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 6, "n_files_impacted": 7, "longest_chunk": ["    // Save original console.error and mock it", "    originalConsoleError = console.error;", "    jest.spyOn(console, 'error').mockImplementation(() => {", "      console.log('error was thrown in test suite');", "    });", ""], "file_path": "libs/bite-tribe/bite/page/src/lib/components/page/__specs__/bite.page.spec.ts"}
{"Link_to_commit": "https://github.com/My-Epitech-Organisation/Raytracer/commit/ebdb942f644d218d882506250c4f009ea8e6e5f8", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 5, "n_files_impacted": 1, "longest_chunk": ["  if (argc == 2 && std::string(argv[1]) == \"--help\") {", "    usage();", "    return 0;", "  }", ""], "file_path": "src/main.cpp"}
{"Link_to_commit": "https://github.com/home-assistant/developers.home-assistant/commit/87162ada686f021997dda43d826d60cdce07b3dd", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 60, "n_files_impacted": 3, "longest_chunk": ["---", "title: \"Importing code with asyncio\"", "---", "", "Determining when it is safe to import code when using asyncio can be tricky because two constraints need to be considered:", "", "- Importing code can do blocking I/O to load the files from the disk", "- Importing code in [cpython is not thread-safe](https://github.com/python/cpython/issues/83065)", "", "## Module level imports", "", "If your imports are at the **module level** (also called **top-level imports**) and all the necessary modules are imported in `__init__.py`, Home Assistant will load your integration either **before the event loop starts** or in a background thread using the **import executor**.", "", "In this scenario, your imports are generally handled safely, so you **don\u2019t need to worry** about whether they\u2019re event-loop safe.", "", "## Imports outside of module level", "", "If your imports are not happening at module level, you must carefully consider each import, as the import machinery has to read the module from disk which does blocking I/O. If possible, it's usually best to change to a module level import, as it avoids much complexity and the risk of mistakes. Importing modules is both CPU-intensive and involves blocking I/O, so it is crucial to ensure these operations are executed in the executor.", "", "If you can be sure that the modules have already been imported, using a bare [`import`](https://docs.python.org/3/reference/simple_stmts.html#import) statement is safe since Python will not load the modules again.", "", "If the integration will always use the module, it's usually best to include a module-level import in `__init__.py` to ensure the module is loaded. However, if this creates a circular import, one of the solutions below will need to be used instead.", "", "If the module is only used conditionally, and will only ever be imported in a single place, the standard executor calls can be used:", "", "- For imports inside of Home Assistant `hass.async_add_executor_job(_function_that_does_late_import)`", "- For imports outside of Home Assistant: [`loop.run_in_executor(None, _function_that_does_late_import)`](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor)", "If the same module may be imported concurrently in different parts of the application, use the thread-safe `homeassistant.helpers.importlib.import_module` helper.", "", "If it's possible the module may be imported from multiple different paths, use `async_import_module`:", "Example:", "", "```python", "from homeassistant.helpers.importlib import async_import_module", "", "platform = await async_import_module(hass, f\"homeassistant.components.homeassistant.triggers.{platform_name}\")", "```", "", "## Determining if a module is already loaded", "", "If you are unsure if a module is already loaded, you can check if the module is already in [`sys.modules`](https://docs.python.org/3/library/sys.html#sys.modules). You should know that the module will appear in `sys.modules` as soon as it begins loading, and [cpython imports are not thread-safe](https://github.com/python/cpython/issues/83065). For this reason, it's important to consider race conditions when code may be imported from multiple paths.", "", "## Avoiding imports that are only used for type-checking", "", "If an imported module is only used for type checking, it is recommended to guard it with an `if TYPE_CHECKING:` block to avoid it being imported at runtime.", "", "```python", "from typing import TYPE_CHECKING", "", "if TYPE_CHECKING:", "    from some_module import SomeClass  # Only imported for type checking", "", "def some_function() -> SomeClass:", "    # Function implementation", "    pass", "```", "", "## Avoid importing code that is rarely used", "", "Importing modules can be both CPU and I/O intensive, so it\u2019s important to avoid importing code that will rarely be used. While importing code outside the module level does add some runtime overhead, this approach is often more efficient when the code is only needed occasionally. By deferring imports, you ensure that resources are only used when necessary, reducing unnecessary processing and improving overall performance."], "file_path": "sidebars.js"}
{"Link_to_commit": "https://github.com/Suhaibinator/SRouter/commit/1206d04915afcf08fa6c7d710f643d09974fc4a4", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 35, "n_files_impacted": 13, "longest_chunk": ["// Package middleware provides a collection of HTTP middleware components for the SRouter framework.", "package middleware", "", "import (", "\t\"github.com/Suhaibinator/SRouter/pkg/common\"", ")", "", "// Middleware is an alias for the common.Middleware type.", "// It represents a function that wraps an http.Handler to provide additional functionality.", "type Middleware = common.Middleware", "", "// Public middleware constructors exposed via variables.", "// The actual implementations are kept private within their respective files.", "var (", "\t// From middleware.go", "\tRecovery    = recovery", "\tLogging     = logging", "\tMaxBodySize = maxBodySize", "\tTimeout     = timeout", "\tCORS        = cors", "", "\t// From ip.go", "\tClientIPMiddleware = clientIPMiddleware // Renamed from ClientIPMiddleware", "", "\t// From trace.go", "\tTrace           = traceMiddleware           // Renamed from TraceMiddleware", "\tTraceWithConfig = traceMiddlewareWithConfig // Renamed from TraceMiddlewareWithConfig", ")", "", "// Note: Generic middleware constructors (Authentication*, New*Middleware, RateLimit, etc.)", "// remain public in their original files (auth.go, ip.go, ratelimit.go).", "// Supporting types like CORSOptions, AuthProvider, RateLimiter, IPConfig, etc.,", "// and helper functions like Chain, ClientIP, GetTraceID, etc., remain public", "// in their original files. Context-related functions in context.go and DB types", "// in db.go also remain public."], "file_path": "pkg/middleware/ratelimit_test.go"}
{"Link_to_commit": "https://github.com/mitodl/ol-infrastructure/commit/e0b448ea0285eff4459e22dac50e4a87ae4c74d7", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 214, "n_files_impacted": 2, "longest_chunk": ["    \"teak\": {", "        \"mitx\": [", "            OpenEdxApplicationVersion(", "                application=\"codejail\",", "                application_type=\"IDA\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"communications\",", "                application_type=\"MFE\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"authoring\",", "                application_type=\"MFE\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"discussions\",", "                application_type=\"MFE\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"edx-platform\",", "                application_type=\"IDA\",", "                release=\"teak\",", "                branch_override=\"mitx/teak\",", "                origin_override=\"https://github.com/mitodl/edx-platform\",", "                runtime_version_override=\"3.11\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"edxapp_theme\",", "                application_type=\"IDA\",", "                release=\"teak\",", "                branch_override=\"teak\",", "                origin_override=\"https://github.com/mitodl/mitx-theme\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"forum\",", "                application_type=\"IDA\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"gradebook\",", "                application_type=\"MFE\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"learner-dashboard\",", "                application_type=\"MFE\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"learning\",", "                application_type=\"MFE\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"notes-api\",", "                application_type=\"IDA\",", "                release=\"master\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"ora-grading\",", "                application_type=\"MFE\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"xqueue\",", "                application_type=\"IDA\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"xqwatcher\",", "                application_type=\"IDA\",", "                branch_override=\"master\",", "                origin_override=\"https://github.com/mitodl/xqueue-watcher\",", "                release=\"teak\",", "            ),", "        ],", "        \"mitx-staging\": [", "            OpenEdxApplicationVersion(", "                application=\"codejail\",", "                application_type=\"IDA\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"communications\",", "                application_type=\"MFE\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"authoring\",", "                application_type=\"MFE\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"discussions\",", "                application_type=\"MFE\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"edx-platform\",", "                application_type=\"IDA\",", "                release=\"teak\",", "                branch_override=\"mitx/teak\",", "                origin_override=\"https://github.com/mitodl/edx-platform\",", "                runtime_version_override=\"3.11\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"edxapp_theme\",", "                application_type=\"IDA\",", "                release=\"teak\",", "                branch_override=\"teak\",", "                origin_override=\"https://github.com/mitodl/mitx-theme\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"forum\",", "                application_type=\"IDA\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"gradebook\",", "                application_type=\"MFE\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"learning\",", "                application_type=\"MFE\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"learner-dashboard\",", "                application_type=\"MFE\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"notes-api\",", "                application_type=\"IDA\",", "                release=\"master\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"ora-grading\",", "                application_type=\"MFE\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"xqueue\",", "                application_type=\"IDA\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"xqwatcher\",", "                application_type=\"IDA\",", "                branch_override=\"master\",", "                origin_override=\"https://github.com/mitodl/xqueue-watcher\",", "                release=\"teak\",", "            ),", "        ],", "        \"xpro\": [", "            OpenEdxApplicationVersion(", "                application=\"codejail\",", "                application_type=\"IDA\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"authoring\",", "                application_type=\"MFE\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"discussions\",", "                application_type=\"MFE\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"edx-platform\",", "                application_type=\"IDA\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"edxapp_theme\",", "                application_type=\"IDA\",", "                release=\"teak\",", "                branch_override=\"teak\",", "                origin_override=\"https://github.com/mitodl/mitxpro-theme\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"forum\",", "                application_type=\"IDA\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"gradebook\",", "                application_type=\"MFE\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"learning\",", "                application_type=\"MFE\",", "                release=\"teak\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"notes-api\",", "                application_type=\"IDA\",", "                release=\"master\",", "            ),", "            OpenEdxApplicationVersion(", "                application=\"ora-grading\",", "                application_type=\"MFE\",", "                release=\"teak\",", "            ),", "        ],", "    },"], "file_path": "src/bridge/settings/openedx/version_matrix.py"}
{"Link_to_commit": "https://github.com/joaoggoncalo/be-tucknpike/commit/360dd02e2b01d7ce782fdb9e698d9c0c8a925a72", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 35, "n_files_impacted": 12, "longest_chunk": ["import { Injectable } from '@nestjs/common';", "import { PassportStrategy } from '@nestjs/passport';", "import { ExtractJwt, Strategy } from 'passport-jwt';", "import { ConfigService } from '@nestjs/config';", "", "interface JwtPayload {", "  sub: string;", "  username: string;", "  role: string;", "}", "", "interface UserPayload {", "  userId: string;", "  username: string;", "  role: string;", "}", "", "@Injectable()", "export class JwtStrategy extends PassportStrategy(Strategy) {", "  constructor(private configService: ConfigService) {", "    super({", "      jwtFromRequest: ExtractJwt.fromAuthHeaderAsBearerToken(),", "      ignoreExpiration: false,", "      secretOrKey: configService.get<string>('JWT_SECRET')!,", "    });", "  }", "", "  validate(payload: JwtPayload): UserPayload {", "    return {", "      userId: payload.sub,", "      username: payload.username,", "      role: payload.role,", "    };", "  }", "}"], "file_path": "src/auth/roles.decorator.ts"}
{"Link_to_commit": "https://github.com/proveskit/pysquared/commit/f2adb8466131209d5a07a1cfb3313317d784733a", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 101, "n_files_impacted": 5, "longest_chunk": ["from busio import SPI", "from digitalio import DigitalInOut", "from proves_sx1280.sx1280 import SX1280", "", "from ....config.radio import RadioConfig", "from ....logger import Logger", "from ....nvm.flag import Flag", "from ..modulation import LoRa, RadioModulation", "from .base import BaseRadioManager", "", "# Type hinting only", "try:", "    from typing import Optional, Type", "except ImportError:", "    pass", "", "", "class SX1280Manager(BaseRadioManager):", "    \"\"\"Manager class implementing RadioProto for SX1280 radios.\"\"\"", "", "    _radio: SX1280", "", "    def __init__(", "        self,", "        logger: Logger,", "        radio_config: RadioConfig,", "        use_fsk: Flag,", "        spi: SPI,", "        chip_select: DigitalInOut,", "        reset: DigitalInOut,", "        busy: DigitalInOut,", "        frequency: float,", "        txen: DigitalInOut,", "        rxen: DigitalInOut,", "    ) -> None:", "        \"\"\"Initialize the manager class and the underlying radio hardware.", "", "        :param Logger logger: Logger instance for logging messages.", "        :param RadioConfig radio_config: Radio configuration object.", "        :param Flag use_fsk: Flag to determine whether to use FSK or LoRa mode.", "        :param busio.SPI spi: The SPI bus connected to the chip. Ensure SCK, MOSI, and MISO are connected.", "        :param ~digitalio.DigitalInOut chip_select: Chip select pin.", "        :param ~digitalio.DigitalInOut busy: Interrupt request pin.", "        :param ~digitalio.DigitalInOut reset: Reset pin.", "        :param ~digitalio.DigitalInOut txen: Transmit enable pin.", "        :param ~digitalio.DigitalInOut rxen: Receive enable pin.", "", "        :raises HardwareInitializationError: If the radio fails to initialize after retries.", "        \"\"\"", "        self._spi = spi", "        self._chip_select = chip_select", "        self._reset = reset", "        self._busy = busy", "        self._frequency = frequency", "        self._txen = txen", "        self._rxen = rxen", "", "        super().__init__(", "            logger=logger,", "            radio_config=radio_config,", "            use_fsk=use_fsk,", "        )", "", "    def _initialize_radio(self, modulation: Type[RadioModulation]) -> None:", "        \"\"\"Initialize the specific SX1280 radio hardware.\"\"\"", "        self._radio = SX1280(", "            self._spi,", "            self._chip_select,", "            self._reset,", "            self._busy,", "            frequency=self._frequency,", "            txen=self._txen,", "            rxen=self._rxen,", "        )", "", "    def _send_internal(self, payload: bytes) -> bool:", "        \"\"\"Send data using the SX1280 radio.\"\"\"", "        return bool(self._radio.send(payload))", "", "    def get_modulation(self) -> Type[RadioModulation]:", "        \"\"\"Get the modulation mode from the initialized SX1280 radio.\"\"\"", "        self._log.warning(\"SX1280 library does not support FSK modulation, using LoRa\")", "        return LoRa", "", "    def receive(self, timeout: Optional[int] = None) -> bytes | None:", "        \"\"\"Receive data from the radio.", "", "        :param int | None timeout: Optional receive timeout in seconds. If None, use the default timeout.", "        :return: The received data as bytes, or None if no data was received.", "        \"\"\"", "        try:", "            msg = self._radio.receive(keep_listening=True)", "", "            if msg is None:", "                self._log.debug(\"No message received\")", "                return None", "", "            return bytes(msg)", "        except Exception as e:", "            self._log.error(\"Error receiving data\", e)", "            return None"], "file_path": "tests/unit/hardware/radio/manager/test_sx1280_manager.py"}
{"Link_to_commit": "https://github.com/bytebase/bytebase/commit/a8a127a7eaf16c6f31880ba356158eb4687c069f", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 73, "n_files_impacted": 2, "longest_chunk": ["const getEnvironmentByIdMap = (", "  environments: Environment[]", "): Map<ResourceId, Environment> => {", "  return new Map(", "    environments.map((environment) => [environment.name, environment])", "  );", "};", "", "const convertToEnvironments = (", "  environments: EnvironmentSetting_Environment[]", "): Environment[] => {", "  return environments.map<Environment>((env, i) => {", "    return {", "      name: `${environmentNamePrefix}${env.id}`,", "      title: env.title,", "      order: i,", "      color: env.color,", "      tier: env.tags[\"protected\"] === \"protected\"", "        ? EnvironmentTier.PROTECTED", "        : EnvironmentTier.UNPROTECTED,", "      state: State.ACTIVE,", "    };", "  });", "};", "", "const convertEnvironments = (", "  environments: Environment[]", "): EnvironmentSetting_Environment[] => {", "  return environments.map((env) => {", "    const res: EnvironmentSetting_Environment = {", "      id: env.name.replace(environmentNamePrefix, \"\"),", "      title: env.title,", "      color: env.color,", "      tags: {},", "    };", "    if (env.tier === EnvironmentTier.PROTECTED) {", "      res.tags.protected = \"protected\";", "    }", "    return res;", "  });", "};", "", "const getEnvironmentSetting = async (", "  silent = false", "): Promise<Environment[]> => {", "  const setting = await settingServiceClient.getSetting(", "    {", "      name: \"settings/bb.workspace.environment\",", "    },", "    { silent }", "  );", "  const settingEnvironments =", "    setting.value?.environmentSetting?.environments ?? [];", "  return convertToEnvironments(settingEnvironments);", "};", "", "const updateEnvironmentSetting = async (", "  environment: EnvironmentSetting", "): Promise<Environment[]> => {", "  const setting = await settingServiceClient.updateSetting({", "    setting: {", "      name: \"settings/bb.workspace.environment\",", "      value: {", "        environmentSetting: environment,", "      },", "    },", "    updateMask: [\"environment_setting\"],", "  });", "  const settingEnvironments =", "    setting.value?.environmentSetting?.environments ?? [];", "  return convertToEnvironments(settingEnvironments);", "};", ""], "file_path": "frontend/src/store/modules/v1/environment.ts"}
{"Link_to_commit": "https://github.com/taikoxyz/taiko-mono/commit/f981f59b63b9cdb5a838d0bfa2ced8b295e56710", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 189, "n_files_impacted": 12, "longest_chunk": ["func (s *DriverTestSuite) TestOnUnsafeL2PayloadWithInvalidPayload() {", "\ts.ForkIntoPacaya(s.p, s.d.ChainSyncer().BlobSyncer())", "\t// Propose some valid L2 blocks", "\ts.ProposeAndInsertEmptyBlocks(s.p, s.d.ChainSyncer().BlobSyncer())", "", "\tl2Head1, err := s.d.rpc.L2.HeaderByNumber(context.Background(), nil)", "\ts.Nil(err)", "", "\tb, err := utils.Compress(testutils.RandomBytes(32))", "\ts.Nil(err)", "", "\tbaseFee, overflow := uint256.FromBig(common.Big256)", "\ts.False(overflow)", "", "\tpayload := &eth.ExecutionPayload{", "\t\tParentHash:    l2Head1.Hash(),", "\t\tFeeRecipient:  s.TestAddr,", "\t\tPrevRandao:    eth.Bytes32(testutils.RandomHash()),", "\t\tBlockNumber:   eth.Uint64Quantity(l2Head1.Number.Uint64() + 1),", "\t\tGasLimit:      eth.Uint64Quantity(l2Head1.GasLimit),", "\t\tTimestamp:     eth.Uint64Quantity(time.Now().Unix()),", "\t\tExtraData:     l2Head1.Extra,", "\t\tBaseFeePerGas: eth.Uint256Quantity(*baseFee),", "\t\tTransactions:  []eth.Data{b},", "\t\tWithdrawals:   &types.Withdrawals{},", "\t}", "", "\ts.Nil(s.d.preconfBlockServer.OnUnsafeL2Payload(", "\t\tcontext.Background(),", "\t\tpeer.ID(testutils.RandomBytes(32)),", "\t\t&eth.ExecutionPayloadEnvelope{ExecutionPayload: payload},", "\t))", "", "\tl2Head2, err := s.d.rpc.L2.BlockByNumber(context.Background(), nil)", "\ts.Nil(err)", "\ts.Equal(l2Head1.Number.Uint64(), l2Head2.Number().Uint64())", "\ts.Equal(l2Head1.Hash(), l2Head2.Hash())", "}", "", "func (s *DriverTestSuite) TestOnUnsafeL2PayloadWithMissingAncients() {", "\ts.ForkIntoPacaya(s.p, s.d.ChainSyncer().BlobSyncer())", "\t// Propose some valid L2 blocks", "\ts.ProposeAndInsertEmptyBlocks(s.p, s.d.ChainSyncer().BlobSyncer())", "", "\tl2Head1, err := s.d.rpc.L2.HeaderByNumber(context.Background(), nil)", "\ts.Nil(err)", "", "\theadL1Origin, err := s.RPCClient.L2.HeadL1Origin(context.Background())", "\ts.Nil(err)", "\ts.Equal(l2Head1.Number.Uint64(), headL1Origin.BlockID.Uint64())", "", "\tsnapshotID := s.SetL1Snapshot()", "", "\tfor i := 0; i < rand.Intn(6)+5; i++ {", "\t\ts.ProposeAndInsertEmptyBlocks(s.p, s.d.ChainSyncer().BlobSyncer())", "\t}", "", "\tl2Head2, err := s.d.rpc.L2.HeaderByNumber(context.Background(), nil)", "\ts.Nil(err)", "\ts.Greater(l2Head2.Number.Uint64(), l2Head1.Number.Uint64())", "", "\tblocks := []*types.Block{}", "\tfor i := l2Head1.Number.Uint64() + 1; i <= l2Head2.Number.Uint64(); i++ {", "\t\tblock, err := s.RPCClient.L2.BlockByNumber(context.Background(), new(big.Int).SetUint64(i))", "\t\ts.Nil(err)", "\t\tblocks = append(blocks, block)", "\t}", "\ts.Equal(l2Head2.Number.Uint64()-l2Head1.Number.Uint64(), uint64(len(blocks)))", "", "\ts.RevertL1Snapshot(snapshotID)", "\ts.Nil(rpc.SetHead(context.Background(), s.RPCClient.L2, l2Head1.Number))", "\t_, err = s.RPCClient.L2Engine.SetHeadL1Origin(context.Background(), headL1Origin.BlockID)", "\ts.Nil(err)", "", "\theadL1Origin, err = s.RPCClient.L2.HeadL1Origin(context.Background())", "\ts.Nil(err)", "\ts.Equal(l2Head1.Number.Uint64(), headL1Origin.BlockID.Uint64())", "", "\tl2Head3, err := s.d.rpc.L2.HeaderByNumber(context.Background(), nil)", "\ts.Nil(err)", "\ts.Equal(l2Head1.Number.Uint64(), l2Head3.Number.Uint64())", "", "\t// Randomly gossip preconfirmation messages with missing ancients", "\tblockNums := rand.Perm(len(blocks))", "\tfor i := range blockNums {", "\t\tblockNums[i] += int(l2Head1.Number.Uint64() + 1)", "\t}", "", "\tgetBlock := func(blockNum uint64) *types.Block {", "\t\tfor _, b := range blocks {", "\t\t\tif b.Number().Uint64() == blockNum {", "\t\t\t\treturn b", "\t\t\t}", "\t\t}", "\t\treturn nil", "\t}", "", "\tinsertPayloadFromBlock := func(block *types.Block, gossipRandom bool) {", "\t\tbaseFee, overflow := uint256.FromBig(block.BaseFee())", "\t\ts.False(overflow)", "", "\t\tb, err := utils.EncodeAndCompressTxList(block.Transactions())", "\t\ts.Nil(err)", "\t\ts.GreaterOrEqual(len(block.Transactions()), 1)", "", "\t\ts.Nil(s.d.preconfBlockServer.OnUnsafeL2Payload(", "\t\t\tcontext.Background(),", "\t\t\tpeer.ID(testutils.RandomBytes(32)),", "\t\t\t&eth.ExecutionPayloadEnvelope{ExecutionPayload: &eth.ExecutionPayload{", "\t\t\t\tBlockHash:     block.Hash(),", "\t\t\t\tParentHash:    block.ParentHash(),", "\t\t\t\tFeeRecipient:  block.Coinbase(),", "\t\t\t\tPrevRandao:    eth.Bytes32(block.MixDigest()),", "\t\t\t\tBlockNumber:   eth.Uint64Quantity(block.Number().Uint64()),", "\t\t\t\tGasLimit:      eth.Uint64Quantity(block.GasLimit()),", "\t\t\t\tTimestamp:     eth.Uint64Quantity(block.Time()),", "\t\t\t\tExtraData:     block.Extra(),", "\t\t\t\tBaseFeePerGas: eth.Uint256Quantity(*baseFee),", "\t\t\t\tTransactions:  []eth.Data{b},", "\t\t\t\tWithdrawals:   &types.Withdrawals{},", "\t\t\t}},", "\t\t))", "", "\t\tif gossipRandom {", "\t\t\t// Also gossip some random blocks", "\t\t\ts.Nil(s.d.preconfBlockServer.OnUnsafeL2Payload(", "\t\t\t\tcontext.Background(),", "\t\t\t\tpeer.ID(testutils.RandomBytes(32)),", "\t\t\t\t&eth.ExecutionPayloadEnvelope{ExecutionPayload: &eth.ExecutionPayload{", "\t\t\t\t\tBlockHash:     common.BytesToHash(testutils.RandomBytes(32)),", "\t\t\t\t\tParentHash:    common.BytesToHash(testutils.RandomBytes(32)),", "\t\t\t\t\tFeeRecipient:  block.Coinbase(),", "\t\t\t\t\tPrevRandao:    eth.Bytes32(common.BytesToHash(testutils.RandomBytes(32))),", "\t\t\t\t\tBlockNumber:   eth.Uint64Quantity(block.Number().Uint64()),", "\t\t\t\t\tGasLimit:      eth.Uint64Quantity(block.GasLimit()),", "\t\t\t\t\tTimestamp:     eth.Uint64Quantity(block.Time()),", "\t\t\t\t\tExtraData:     block.Extra(),", "\t\t\t\t\tBaseFeePerGas: eth.Uint256Quantity(*baseFee),", "\t\t\t\t\tTransactions:  []eth.Data{b},", "\t\t\t\t\tWithdrawals:   &types.Withdrawals{},", "\t\t\t\t}},", "\t\t\t))", "", "\t\t\ts.Nil(s.d.preconfBlockServer.OnUnsafeL2Payload(", "\t\t\t\tcontext.Background(),", "\t\t\t\tpeer.ID(testutils.RandomBytes(32)),", "\t\t\t\t&eth.ExecutionPayloadEnvelope{ExecutionPayload: &eth.ExecutionPayload{", "\t\t\t\t\tBlockHash:     common.BytesToHash(testutils.RandomBytes(32)),", "\t\t\t\t\tParentHash:    block.ParentHash(),", "\t\t\t\t\tFeeRecipient:  block.Coinbase(),", "\t\t\t\t\tPrevRandao:    eth.Bytes32(common.BytesToHash(testutils.RandomBytes(32))),", "\t\t\t\t\tBlockNumber:   eth.Uint64Quantity(block.Number().Uint64()),", "\t\t\t\t\tGasLimit:      eth.Uint64Quantity(block.GasLimit()),", "\t\t\t\t\tTimestamp:     eth.Uint64Quantity(block.Time()),", "\t\t\t\t\tExtraData:     block.Extra(),", "\t\t\t\t\tBaseFeePerGas: eth.Uint256Quantity(*baseFee),", "\t\t\t\t\tTransactions:  []eth.Data{b},", "\t\t\t\t\tWithdrawals:   &types.Withdrawals{},", "\t\t\t\t}},", "\t\t\t))", "\t\t}", "\t}", "", "\t// Insert all blocks except the first one", "\tfor _, blockNum := range blockNums {", "\t\tif blockNum == int(l2Head1.Number.Uint64()+1) {", "\t\t\tcontinue", "\t\t}", "", "\t\tblock := getBlock(uint64(blockNum))", "\t\ts.NotNil(block)", "", "\t\tinsertPayloadFromBlock(block, true)", "\t}", "", "\tl2Head4, err := s.d.rpc.L2.BlockByNumber(context.Background(), nil)", "\ts.Nil(err)", "\ts.Equal(l2Head1.Number.Uint64(), l2Head4.Number().Uint64())", "", "\t// Insert the only missing ancient block", "\tblock := getBlock(l2Head1.Number.Uint64() + 1)", "\ts.NotNil(block)", "\tinsertPayloadFromBlock(block, false)", "", "\tl2Head5, err := s.d.rpc.L2.BlockByNumber(context.Background(), nil)", "\ts.Nil(err)", "\ts.Equal(l2Head2.Number.Uint64(), l2Head5.Number().Uint64())", "}", ""], "file_path": "packages/taiko-client/driver/driver_test.go"}
{"Link_to_commit": "https://github.com/OpenVisualCloud/Intel-Tiber-Broadcast-Suite/commit/2ba4f25e4d3b90e124e880e94435631b3092c56e", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 31, "n_files_impacted": 1, "longest_chunk": ["\terr = createResourceIfNotExists(mcmNamespace, types.NamespacedName{Name: mcmNamespace.Name})", "\tif err != nil {", "\t\tlog.Error(err, \"Failed to create resource\", \"resource\", mcmNamespace.GetObjectKind(), \"named\", mcmNamespace.Name)", "\t\treturn ctrl.Result{}, err", "\t}", "\terr = createResourceIfNotExists(mcmAgentDeployment, types.NamespacedName{Name: mcmAgentDeployment.Name, Namespace: \"mcm\"})", "\tif err != nil {", "\t\tlog.Error(err, \"Failed to create resource\", \"resource\", mcmAgentDeployment.GetObjectKind(), \"named\", mcmAgentDeployment.Name)", "\t\treturn ctrl.Result{}, err", "\t}", "\terr = createResourceIfNotExists(mcmAgentService, types.NamespacedName{Name: mcmAgentService.Name, Namespace:\"mcm\"})", "\tif err != nil {", "\t\tlog.Error(err, \"Failed to create resource\", \"resource\", mcmAgentService.GetObjectKind(), \"named\", mcmAgentService.Name)", "\t\treturn ctrl.Result{}, err", "\t}", "\terr = createResourceIfNotExists(mcmMediaProxyPv, types.NamespacedName{Name: mcmMediaProxyPv.Name, Namespace: \"mcm\"})", "\tif err != nil {\t", "\t\tlog.Error(err, \"Failed to create resource\", \"resource\", mcmMediaProxyPv.GetObjectKind(), \"named\", mcmMediaProxyPv.Name)", "\t\treturn ctrl.Result{}, err", "\t}", "\terr = createResourceIfNotExists(mcmMediaProxyPvc, types.NamespacedName{Name: mcmMediaProxyPvc.Name, Namespace: \"mcm\"})", "\tif err != nil {", "\t\tlog.Error(err, \"Failed to create resource\", \"resource\", mcmMediaProxyPvc.GetObjectKind(), \"named\", mcmMediaProxyPvc.Name)", "\t\treturn ctrl.Result{}, err", "\t}", "\terr = createResourceIfNotExists(mcmMediaProxyDs, types.NamespacedName{Name: mcmMediaProxyDs.Name, Namespace: \"mcm\"})", "    if err != nil {", "\t\tlog.Error(err, \"Failed to create resource\", \"resource\", mcmMediaProxyDs.GetObjectKind(), \"named\", mcmMediaProxyDs.Name)", "\t\treturn ctrl.Result{}, err", "\t}", "\t"], "file_path": "launcher/internal/controller/bcsconfig_controller.go"}
{"Link_to_commit": "https://github.com/pocmo/uitest-agent/commit/415811230b2b004571f3cfe43c50210c7d411845", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 18, "n_files_impacted": 2, "longest_chunk": ["    ", "    args = parser.parse_args()", "    ", "    # If query is not provided via command line, read from stdin", "    if args.query is None:", "        # Check if there's data available on stdin (e.g., from a pipe)", "        if not sys.stdin.isatty():", "            args.query = sys.stdin.read().strip()", "            # Validate that the input is not empty after stripping whitespace", "            if not args.query:", "                sys.stderr.write(\"Error: Empty input provided via stdin. Please provide a non-empty query.\\n\")", "                sys.exit(1)", "        else:", "            # Exit with error if no query is provided", "            sys.stderr.write(\"Error: No query provided. Please provide a query via --query parameter or pipe input to stdin.\\n\")", "            sys.exit(1)", "    ", "    return args"], "file_path": "utils/cli.py"}
{"Link_to_commit": "https://github.com/5pirit5eal/swim-rag/commit/85a32faef1b0b654ce125fddf10119a946b5c94f", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 167, "n_files_impacted": 32, "longest_chunk": ["package genai", "", "import (", "\t\"context\"", "\t\"encoding/json\"", "\t\"fmt\"", "\t\"strings\"", "\t\"sync\"", "", "\t\"github.com/5pirit5eal/swim-rag/internal/models\"", "\t\"github.com/go-chi/httplog/v2\"", "\t\"github.com/tmc/langchaingo/schema\"", "\t\"google.golang.org/genai\"", ")", "", "// GeneratePlan generates a plan using the LLM based on the provided query and documents.", "func (gc *GoogleGenAIClient) GeneratePlan(ctx context.Context, q string, docs []schema.Document) (*models.RAGResponse, error) {", "\tlogger := httplog.LogEntry(ctx)", "\tts, err := models.TableSchema()", "\tif err != nil {", "\t\treturn nil, fmt.Errorf(\"failed to get table schema: %w\", err)", "\t}", "", "\tvar dc []string", "\tfor _, doc := range docs {", "\t\tdc = append(dc, doc.PageContent)", "\t}", "", "\t// Create a RAG query for the LLM with the most relevant documents as context", "\tquery := fmt.Sprintf(ragTemplateStr, ts, q, strings.Join(dc, \"\\n \\n\"))", "\tgenCfg := *gc.gcfg", "\tgenCfg.ResponseMIMEType = \"application/json\"", "\tanswer, err := gc.gc.Models.GenerateContent(ctx, gc.cfg.Model, genai.Text(query), &genCfg)", "", "\tif err != nil {", "\t\tlogger.Error(\"Error when generating answer with LLM\", httplog.ErrAttr(err))", "\t\treturn nil, fmt.Errorf(\"error generating answer: %w\", err)", "\t}", "", "\t// read description and table from the LLM response", "\tvar p models.RAGResponse", "\terr = json.Unmarshal([]byte(answer.Text()), &p)", "\tif err != nil {", "\t\tlogger.Error(\"Error parsing LLM response\", httplog.ErrAttr(err), \"raw_response\", answer)", "\t\treturn nil, fmt.Errorf(\"error parsing LLM response: %w\", err)", "\t}", "\t// Add the total to the table if it is not already present", "\tif !strings.Contains(p.Table[len(p.Table)-1].Content, \"Total\") {", "\t\tp.Table.AddSum()", "\t}", "\t// Recalculate the sums of the rows to be sure they are correct", "\tp.Table.UpdateSum()", "", "\t// Add the plan to the response", "\tlogger.Debug(\"Plan generated successfully\")", "\treturn &p, nil", "}", "", "// ChoosePlan lets an LLM choose the best fitting plan from the given documents.", "// Returns the plan id of the chosen plan", "func (gc *GoogleGenAIClient) ChoosePlan(ctx context.Context, q string, docs []schema.Document) (string, error) {", "\tlogger := httplog.LogEntry(ctx)", "\tvar dc string", "\tfor i, doc := range docs {", "\t\tdc += fmt.Sprintf(\"%d: %s \\n\\n\", i, doc.PageContent)", "\t}", "", "\t// Create a RAG query for the LLM with the most relevant documents as context", "\tquery := fmt.Sprintf(choosePlanTemplateStr, q, dc)", "\tgenCfg := *gc.gcfg", "\tgenCfg.ResponseMIMEType = \"application/json\"", "\tanswer, err := gc.gc.Models.GenerateContent(ctx, gc.cfg.Model, genai.Text(query), &genCfg)", "\tif err != nil {", "\t\tlogger.Error(\"Error when generating answer with LLM\", httplog.ErrAttr(err))", "\t\treturn \"\", fmt.Errorf(\"error generating answer: %w\", err)", "\t}", "\tlogger.Debug(\"Successful answer from LLM\", \"answer\", answer)", "", "\tvar cr models.ChooseResponse", "\terr = json.Unmarshal([]byte(answer.Text()), &cr)", "\tif err != nil {", "\t\tlogger.Error(\"Error parsing LLM response\", httplog.ErrAttr(err), \"raw_response\", answer)", "\t\treturn \"\", fmt.Errorf(\"error parsing LLM response: %w\", err)", "\t}", "\tplanID, ok := docs[cr.Idx].Metadata[\"plan_id\"]", "\tif !ok {", "\t\treturn \"\", fmt.Errorf(\"plan_id not found in Metadata for document at index %d\", cr.Idx)", "\t}", "\tplanIDStr, ok := planID.(string)", "\tif !ok {", "\t\treturn \"\", fmt.Errorf(\"plan_id is not a string in Metadata for document at index %d\", cr.Idx)", "\t}", "\treturn planIDStr, nil", "}", "", "func (gc *GoogleGenAIClient) ImprovePlan(ctx context.Context, plan models.Planable, syncGroup *sync.WaitGroup, c chan<- models.Document, ec chan<- error) {", "\tif syncGroup != nil {", "\t\tdefer syncGroup.Done()", "\t}", "\tlogger := httplog.LogEntry(ctx)", "\tmeta, err := gc.GenerateMetadata(ctx, plan)", "\tif err != nil {", "\t\tlogger.Error(\"Error when generating metadata with LLM\", httplog.ErrAttr(err))", "\t\tec <- fmt.Errorf(\"error generating metadata: %w\", err)", "\t\treturn", "\t}", "", "\t// Create request body by converting the plans into documents", "\tc <- models.Document{", "\t\tPlan: plan,", "\t\tMeta: meta,", "\t}", "}", "", "func (gc *GoogleGenAIClient) DescribeTable(ctx context.Context, table *models.Table) (*models.Description, error) {", "\tlogger := httplog.LogEntry(ctx)", "\tds, err := models.DescriptionSchema()", "\tif err != nil {", "\t\tlogger.Error(\"Failed in retrieving Schema\", httplog.ErrAttr(err))", "\t\treturn nil, fmt.Errorf(\"models.MetadataSchema: %w\", err)", "\t}", "\t// Create a description of the table", "\tquery := fmt.Sprintf(describeTemplateStr, ds, table.String())", "\tgenCfg := *gc.gcfg", "\tgenCfg.ResponseMIMEType = \"application/json\"", "\tanswer, err := gc.gc.Models.GenerateContent(ctx, gc.cfg.Model, genai.Text(query), &genCfg)", "\tif err != nil {", "\t\treturn nil, fmt.Errorf(\"Models.GenerateContent: %w\", err)", "\t}", "\tvar desc models.Description", "\terr = json.Unmarshal([]byte(answer.Text()), &desc)", "\tif err != nil {", "\t\tlogger.Error(\"Error parsing LLM response\", httplog.ErrAttr(err), \"raw_response\", answer.Text())", "\t\treturn nil, fmt.Errorf(\"error parsing LLM response: %w\", err)", "\t}", "\treturn &desc, nil", "}", "", "func (gc *GoogleGenAIClient) GenerateMetadata(ctx context.Context, plan models.Planable) (*models.Metadata, error) {", "\tlogger := httplog.LogEntry(ctx)", "\tms, err := models.MetadataSchema()", "\tif err != nil {", "\t\tlogger.Error(\"Failed in retrieving Schema\", httplog.ErrAttr(err))", "\t\treturn nil, fmt.Errorf(\"models.MetadataSchema: %w\", err)", "\t}", "\t// Enhance scraped documents with gemini and create meaningful metadata", "\tgenericPlan := plan.Plan()", "\tquery := fmt.Sprintf(metadataTemplateStr, genericPlan.Title, genericPlan.Description, genericPlan.Table.String(), ms)", "\tgenCfg := *gc.gcfg", "\tgenCfg.ResponseMIMEType = \"application/json\"", "\tanswer, err := gc.gc.Models.GenerateContent(ctx, gc.cfg.Model, genai.Text(query), &genCfg)", "\tif err != nil {", "\t\tlogger.Error(\"Error when generating answer with LLM\", httplog.ErrAttr(err))", "\t\treturn nil, fmt.Errorf(\"Models.GenerateContent: %w\", err)", "\t}", "\tlogger.Debug(\"Successful answer from LLM\", \"answer\", answer.Text())", "", "\t// Parse the answer as JSON", "\tvar metadata models.Metadata", "\terr = json.Unmarshal([]byte(answer.Text()), &metadata)", "\tif err != nil {", "\t\tlogger.Error(\"Error parsing LLM response\", httplog.ErrAttr(err), \"raw_response\", answer.Text())", "\t\treturn nil, fmt.Errorf(\"JSON unmarshal error: %w with raw response %s\", err, answer.Text())", "\t}", "", "\treturn &metadata, nil", "}"], "file_path": "internal/genai/prompts.go"}
{"Link_to_commit": "https://github.com/openstatusHQ/openstatus/commit/89f744f568e01a0fe1b0a1fc13d2d9cc98e18808", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 37, "n_files_impacted": 1, "longest_chunk": ["          .update(schema.monitor)", "          .set({ status: \"active\" })", "          .where(eq(schema.monitor.id, monitor.id));", "", "        // we can't have a monitor in error without an incident", "        if (monitor.status === \"error\") {", "          const incident = await db", "            .select()", "            .from(incidentTable)", "            .where(", "              and(", "                eq(incidentTable.monitorId, Number(monitorId)),", "                isNull(incidentTable.resolvedAt),", "                isNull(incidentTable.acknowledgedAt),", "              ),", "            )", "            .get();", "", "          if (!incident) {", "            // it was just a single failure not a proper incident", "            break;", "          }", "          if (incident?.resolvedAt) {", "            // incident is already resolved", "            break;", "          }", "", "          console.log(`\ud83e\udd13 recovering incident ${incident.id}`);", "          await db", "            .update(incidentTable)", "            .set({", "              resolvedAt: new Date(cronTimestamp),", "              autoResolved: true,", "            })", "            .where(eq(incidentTable.id, incident.id))", "            .run();", "        }"], "file_path": "apps/workflows/src/checker/index.ts"}
{"Link_to_commit": "https://github.com/tomacheese/booth-purchased-items-manager/commit/9912718a4d38b68d8f5859596905eb34add05e2a", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 10, "n_files_impacted": 5, "longest_chunk": ["#!/bin/sh", "", "while :", "do", "  pnpm start || true", "", "  # wait 10 minutes", "  echo Waiting 10 minutes...", "  sleep 600", "done"], "file_path": "src/environment.ts"}
{"Link_to_commit": "https://github.com/cirglo/dfs/commit/5cfc01f56f51f42f25c3134ce3ab24a420ca7bd5", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 349, "n_files_impacted": 30, "longest_chunk": ["", "// TestSimulatedFaultTolerance tests the system's behavior during simulated failure scenarios", "func TestSimulatedFaultTolerance(t *testing.T) {", "\t// Skip in short mode as these tests take time", "\tif testing.Short() {", "\t\tt.Skip(\"Skipping fault tolerance tests in short mode\")", "\t}", "", "\t// Test file path with timestamp to ensure uniqueness", "\ttestFilePath := fmt.Sprintf(\"/fault-test-file-%d.txt\", time.Now().UnixNano())", "", "\t// Test file content (larger than usual)", "\ttestContent := bytes.Repeat([]byte(\"DFS Fault Tolerance Test Data Block \"), 100)", "", "\t// Setup: Create file and write data", "\tt.Run(\"Setup\", func(t *testing.T) {", "\t\terr := dfsClient.CreateFile(testFilePath)", "\t\tassert.NoError(t, err, \"Failed to create file for fault test\")", "", "\t\tseq, err := dfsClient.WriteFile(testFilePath, testContent)", "\t\tassert.NoError(t, err, \"Failed to write to file for fault test\")", "\t\tassert.Greater(t, uint64(seq), uint64(0), \"Expected sequence number greater than 0\")", "\t})", "", "\t// Test: Simulate node failure and recovery by recreating the client", "\t// and verifying data is still accessible", "\tt.Run(\"SimulatedNodeFailure\", func(t *testing.T) {", "\t\t// Save the current state of the mock client (before \"failure\")", "\t\toldClient := dfsClient", "", "\t\t// Create a new client instance to simulate node restart", "\t\t// In a real environment, this would correspond to a node failure and recovery", "\t\tt.Log(\"Simulating node server failure and recovery...\")", "\t\tdfsClient = NewMockDFSClient()", "", "\t\t// Copy data from old client to new client to simulate persistence (in a real system, data would be on disk)", "\t\t// This is just for the mock - in a real system, we would just reconnect to the servers", "\t\tcopyMockData(oldClient, dfsClient, testFilePath)", "", "\t\t// Verify we can still read the file", "\t\tsequences, err := dfsClient.GetSequences(testFilePath)", "\t\tassert.NoError(t, err, \"Failed to get sequences after simulated node restart\")", "\t\tassert.NotEmpty(t, sequences, \"Expected at least one sequence after simulated node restart\")", "", "\t\t// Read the sequence data", "\t\tvar seq uint64", "\t\tfor s := range sequences {", "\t\t\tseq = uint64(s)", "\t\t\tbreak", "\t\t}", "", "\t\tdata, err := dfsClient.ReadSequence(testFilePath, client.SequenceNumber(seq))", "\t\tassert.NoError(t, err, \"Failed to read sequence after simulated node restart\")", "\t\tassert.True(t, bytes.Equal(data, testContent), \"Content mismatch after simulated node restart\")", "\t})", "", "\t// Test multiple writes after simulated recovery", "\tt.Run(\"WriteAfterRecovery\", func(t *testing.T) {", "\t\t// Try to write more data after recovery", "\t\tadditionalContent := []byte(\"Additional data after recovery\")", "\t\tseq, err := dfsClient.WriteFile(testFilePath, additionalContent)", "\t\tassert.NoError(t, err, \"Failed to write additional data after recovery\")", "", "\t\t// Read back the new data", "\t\tdata, err := dfsClient.ReadSequence(testFilePath, seq)", "\t\tassert.NoError(t, err, \"Failed to read additional data after recovery\")", "\t\tassert.Equal(t, additionalContent, data, \"Additional content mismatch\")", "\t})", "", "\t// Cleanup", "\tt.Run(\"Cleanup\", func(t *testing.T) {", "\t\terr := dfsClient.DeleteFile(testFilePath)", "\t\tassert.NoError(t, err, \"Failed to delete test file during cleanup\")", "\t})", "}", "", "// TestFileDeletion tests file deletion including edge cases", "func TestFileDeletion(t *testing.T) {", "\t// Create test files", "\ttestFile1 := fmt.Sprintf(\"/test-delete-file1-%d.txt\", time.Now().UnixNano())", "\ttestFile2 := fmt.Sprintf(\"/test-delete-file2-%d.txt\", time.Now().UnixNano())", "\ttestDir := fmt.Sprintf(\"/test-delete-dir-%d\", time.Now().UnixNano())", "", "\t// Setup test files and directories", "\tt.Run(\"Setup\", func(t *testing.T) {", "\t\t// Create first file with content", "\t\terr := dfsClient.CreateFile(testFile1)", "\t\tassert.NoError(t, err, \"Failed to create test file 1\")", "", "\t\t_, err = dfsClient.WriteFile(testFile1, []byte(\"Test content for file 1\"))", "\t\tassert.NoError(t, err, \"Failed to write to test file 1\")", "", "\t\t// Create second file with multiple writes", "\t\terr = dfsClient.CreateFile(testFile2)", "\t\tassert.NoError(t, err, \"Failed to create test file 2\")", "", "\t\t_, err = dfsClient.WriteFile(testFile2, []byte(\"First block for file 2\"))", "\t\tassert.NoError(t, err, \"Failed to write first block to test file 2\")", "", "\t\t_, err = dfsClient.WriteFile(testFile2, []byte(\"Second block for file 2\"))", "\t\tassert.NoError(t, err, \"Failed to write second block to test file 2\")", "", "\t\t// Create directory", "\t\terr = dfsClient.CreateDirectory(testDir)", "\t\tassert.NoError(t, err, \"Failed to create test directory\")", "\t})", "", "\t// Test deleting and checking if the file is gone", "\tt.Run(\"DeleteFile1\", func(t *testing.T) {", "\t\t// Verify file exists", "\t\tentries, err := dfsClient.ListFiles(\"/\")", "\t\tassert.NoError(t, err, \"Failed to list files\")", "", "\t\tfound := false", "\t\tfor _, entry := range entries {", "\t\t\tif entry.Path == testFile1 {", "\t\t\t\tfound = true", "\t\t\t\tbreak", "\t\t\t}", "\t\t}", "\t\tassert.True(t, found, \"File 1 should exist before deletion\")", "", "\t\t// Delete the file", "\t\terr = dfsClient.DeleteFile(testFile1)", "\t\tassert.NoError(t, err, \"Failed to delete file 1\")", "", "\t\t// Verify file no longer exists", "\t\tentries, err = dfsClient.ListFiles(\"/\")", "\t\tassert.NoError(t, err, \"Failed to list files after deletion\")", "", "\t\tfound = false", "\t\tfor _, entry := range entries {", "\t\t\tif entry.Path == testFile1 {", "\t\t\t\tfound = true", "\t\t\t\tbreak", "\t\t\t}", "\t\t}", "\t\tassert.False(t, found, \"File 1 should not exist after deletion\")", "", "\t\t// Try to get sequences for deleted file", "\t\t_, err = dfsClient.GetSequences(testFile1)", "\t\tassert.Error(t, err, \"Expected error when getting sequences for deleted file\")", "", "\t\t// Try to read from deleted file", "\t\t_, err = dfsClient.ReadSequence(testFile1, client.SequenceNumber(1))", "\t\tassert.Error(t, err, \"Expected error when reading from deleted file\")", "", "\t\t// Try to write to deleted file", "\t\t_, err = dfsClient.WriteFile(testFile1, []byte(\"New content\"))", "\t\tassert.Error(t, err, \"Expected error when writing to deleted file\")", "\t})", "", "\t// Test deleting file with multiple blocks", "\tt.Run(\"DeleteMultiBlockFile\", func(t *testing.T) {", "\t\t// Delete the file", "\t\terr := dfsClient.DeleteFile(testFile2)", "\t\tassert.NoError(t, err, \"Failed to delete file 2 with multiple blocks\")", "", "\t\t// Verify file no longer exists", "\t\tentries, err := dfsClient.ListFiles(\"/\")", "\t\tassert.NoError(t, err, \"Failed to list files after deletion\")", "", "\t\tfound := false", "\t\tfor _, entry := range entries {", "\t\t\tif entry.Path == testFile2 {", "\t\t\t\tfound = true", "\t\t\t\tbreak", "\t\t\t}", "\t\t}", "\t\tassert.False(t, found, \"File 2 should not exist after deletion\")", "\t})", "", "\t// Test recreating a file after deletion", "\tt.Run(\"RecreateAfterDeletion\", func(t *testing.T) {", "\t\t// Create a new file with the same name as the deleted file", "\t\terr := dfsClient.CreateFile(testFile1)", "\t\tassert.NoError(t, err, \"Failed to recreate file after deletion\")", "", "\t\t// Write new content", "\t\tnewContent := []byte(\"New content after recreation\")", "\t\tseq, err := dfsClient.WriteFile(testFile1, newContent)", "\t\tassert.NoError(t, err, \"Failed to write to recreated file\")", "", "\t\t// Read back the content", "\t\treadData, err := dfsClient.ReadSequence(testFile1, seq)", "\t\tassert.NoError(t, err, \"Failed to read from recreated file\")", "\t\tassert.Equal(t, newContent, readData, \"Content mismatch in recreated file\")", "\t})", "", "\t// Test directory deletion", "\tt.Run(\"DeleteDirectory\", func(t *testing.T) {", "\t\terr := dfsClient.DeleteDirectory(testDir)", "\t\tassert.NoError(t, err, \"Failed to delete directory\")", "", "\t\t// Verify directory no longer exists", "\t\tentries, err := dfsClient.ListFiles(\"/\")", "\t\tassert.NoError(t, err, \"Failed to list root directory after deletion\")", "", "\t\tfound := false", "\t\tfor _, entry := range entries {", "\t\t\tif entry.Path == testDir {", "\t\t\t\tfound = true", "\t\t\t\tbreak", "\t\t\t}", "\t\t}", "\t\tassert.False(t, found, \"Directory should not exist after deletion\")", "\t})", "", "\t// Cleanup", "\tt.Run(\"Cleanup\", func(t *testing.T) {", "\t\t// Delete the recreated file", "\t\terr := dfsClient.DeleteFile(testFile1)", "\t\tassert.NoError(t, err, \"Failed to clean up recreated file\")", "\t})", "}", "", "// TestComprehensiveErrorHandling tests how the system handles various error conditions (renamed from TestErrorHandling)", "func TestComprehensiveErrorHandling(t *testing.T) {", "\t// Create unique test paths using a timestamp", "\ttimestamp := time.Now().UnixNano()", "\ttestDir := fmt.Sprintf(\"/error-test-dir-comp-%d\", timestamp)", "\ttestFile := fmt.Sprintf(\"/error-test-file-comp-%d.txt\", timestamp)", "\tnonExistentFile := fmt.Sprintf(\"/non-existent-file-comp-%d.txt\", timestamp)", "\tnonExistentDir := fmt.Sprintf(\"/non-existent-dir-comp-%d\", timestamp)", "", "\t// Setup: Create test directory and file", "\tt.Run(\"Setup\", func(t *testing.T) {", "\t\terr := dfsClient.CreateDirectory(testDir)", "\t\tassert.NoError(t, err, \"Failed to create test directory\")", "", "\t\terr = dfsClient.CreateFile(testFile)", "\t\tassert.NoError(t, err, \"Failed to create test file\")", "", "\t\t_, err = dfsClient.WriteFile(testFile, []byte(\"Initial content\"))", "\t\tassert.NoError(t, err, \"Failed to write to test file\")", "\t})", "", "\t// Test reading from non-existent file", "\tt.Run(\"ReadNonExistentFile\", func(t *testing.T) {", "\t\t_, err := dfsClient.ReadSequence(nonExistentFile, client.SequenceNumber(1))", "\t\tassert.Error(t, err, \"Expected error when reading from non-existent file\")", "\t})", "", "\t// Test listing non-existent directory", "\tt.Run(\"ListNonExistentDirectory\", func(t *testing.T) {", "\t\t_, err := dfsClient.ListFiles(nonExistentDir)", "\t\tassert.Error(t, err, \"Expected error when listing non-existent directory\")", "\t})", "", "\t// Test creating duplicate file", "\tt.Run(\"CreateDuplicateFile\", func(t *testing.T) {", "\t\terr := dfsClient.CreateFile(testFile)", "\t\tassert.Error(t, err, \"Expected error when creating duplicate file\")", "\t})", "", "\t// Test creating duplicate directory", "\tt.Run(\"CreateDuplicateDirectory\", func(t *testing.T) {", "\t\terr := dfsClient.CreateDirectory(testDir)", "\t\tassert.Error(t, err, \"Expected error when creating duplicate directory\")", "\t})", "", "\t// Test creating file with same name as directory", "\tt.Run(\"CreateFileWithDirName\", func(t *testing.T) {", "\t\terr := dfsClient.CreateFile(testDir)", "\t\tassert.Error(t, err, \"Expected error when creating file with same name as directory\")", "\t})", "", "\t// Test creating directory with same name as file", "\tt.Run(\"CreateDirWithFileName\", func(t *testing.T) {", "\t\terr := dfsClient.CreateDirectory(testFile)", "\t\tassert.Error(t, err, \"Expected error when creating directory with same name as file\")", "\t})", "", "\t// Test deleting non-existent file", "\tt.Run(\"DeleteNonExistentFile\", func(t *testing.T) {", "\t\terr := dfsClient.DeleteFile(nonExistentFile)", "\t\tassert.Error(t, err, \"Expected error when deleting non-existent file\")", "\t})", "", "\t// Test deleting non-existent directory", "\tt.Run(\"DeleteNonExistentDirectory\", func(t *testing.T) {", "\t\terr := dfsClient.DeleteDirectory(nonExistentDir)", "\t\tassert.Error(t, err, \"Expected error when deleting non-existent directory\")", "\t})", "", "\t// Test reading invalid sequence", "\tt.Run(\"ReadInvalidSequence\", func(t *testing.T) {", "\t\tsequences, err := dfsClient.GetSequences(testFile)", "\t\tassert.NoError(t, err, \"Failed to get sequences\")", "", "\t\t// Get highest sequence number and add 1000 to ensure it's invalid", "\t\tvar highestSeq client.SequenceNumber", "\t\tfor seq := range sequences {", "\t\t\tif seq > highestSeq {", "\t\t\t\thighestSeq = seq", "\t\t\t}", "\t\t}", "\t\tinvalidSeq := highestSeq + 1000", "", "\t\t_, err = dfsClient.ReadSequence(testFile, invalidSeq)", "\t\tassert.Error(t, err, \"Expected error when reading invalid sequence\")", "\t})", "", "\t// Test deleting directory with files (should fail)", "\tt.Run(\"DeleteNonEmptyDirectory\", func(t *testing.T) {", "\t\t// Create a file inside the test directory", "\t\tnestedFile := fmt.Sprintf(\"%s/nested-file.txt\", testDir)", "\t\terr := dfsClient.CreateFile(nestedFile)", "\t\tassert.NoError(t, err, \"Failed to create nested file\")", "", "\t\t// Try to delete the directory", "\t\terr = dfsClient.DeleteDirectory(testDir)", "\t\tassert.Error(t, err, \"Expected error when deleting non-empty directory\")", "", "\t\t// Clean up the nested file", "\t\terr = dfsClient.DeleteFile(nestedFile)", "\t\tassert.NoError(t, err, \"Failed to clean up nested file\")", "\t})", "", "\t// Test path validation (invalid paths)", "\tt.Run(\"InvalidPaths\", func(t *testing.T) {", "\t\tinvalidPaths := []string{", "\t\t\t\"\",                             // Empty path", "\t\t\t\"no-leading-slash\",             // Missing leading slash", "\t\t\t\"/path/with/trailing/slash/\",   // Trailing slash", "\t\t\t\"/name\\\\with\\\\backslashes\",     // Backslashes", "\t\t\t\"/path//with//double//slashes\", // Double slashes", "\t\t}", "", "\t\tfor _, path := range invalidPaths {", "\t\t\t// Try to create file with invalid path", "\t\t\terr := dfsClient.CreateFile(path)", "\t\t\tassert.Error(t, err, \"Expected error when creating file with invalid path: %s\", path)", "", "\t\t\t// Try to create directory with invalid path", "\t\t\terr = dfsClient.CreateDirectory(path)", "\t\t\tassert.Error(t, err, \"Expected error when creating directory with invalid path: %s\", path)", "\t\t}", "\t})", "", "\t// Clean up", "\tt.Run(\"Cleanup\", func(t *testing.T) {", "\t\terr := dfsClient.DeleteDirectory(testDir)", "\t\tassert.NoError(t, err, \"Failed to delete test directory\")", "", "\t\terr = dfsClient.DeleteFile(testFile)", "\t\tassert.NoError(t, err, \"Failed to delete test file\")", "\t})", "}"], "file_path": "e2e/test/file_operations_test.go"}
{"Link_to_commit": "https://github.com/allanrg4/tickets-app/commit/d4ee36d464f1b9f1589818754972e5c2979e0152", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 56, "n_files_impacted": 19, "longest_chunk": ["package com.umg.ticket_app_backend.services;", "", "import com.umg.ticket_app_backend.dtos.auth.AuthRegister;", "import com.umg.ticket_app_backend.entities.User;", "import com.umg.ticket_app_backend.repositories.UserRepository;", "import org.springframework.security.authentication.AuthenticationManager;", "import org.springframework.security.authentication.UsernamePasswordAuthenticationToken;", "import org.springframework.security.crypto.password.PasswordEncoder;", "import org.springframework.stereotype.Service;", "import com.umg.ticket_app_backend.dtos.auth.AuthRequest;", "import com.umg.ticket_app_backend.dtos.auth.AuthResponse;", "", "@Service", "public class AuthService {", "    private final AuthenticationManager authenticationManager;", "    private final PasswordEncoder passwordEncoder;", "    private final JwtService jwtService;", "    private final UserRepository userRepository;", "", "    public AuthService(", "            AuthenticationManager authenticationManager,", "            PasswordEncoder passwordEncoder,", "            JwtService jwtService,", "            UserRepository userRepository", "    ) {", "        this.authenticationManager = authenticationManager;", "        this.passwordEncoder = passwordEncoder;", "        this.jwtService = jwtService;", "        this.userRepository = userRepository;", "    }", "", "    public AuthResponse authenticate(AuthRequest authRequest) {", "        final var token = new UsernamePasswordAuthenticationToken(authRequest.username(), authRequest.password());", "        final var authentication = authenticationManager.authenticate(token);", "        final var jwtToken = jwtService.generateToken(authentication);", "        final var expiresAt = jwtService.extractExpirationTime(jwtToken);", "        return new AuthResponse(jwtToken, authentication.getName(), expiresAt);", "    }", "", "    public Boolean register(AuthRegister authRequest) {", "        try {", "            final var newUser = new User();", "            newUser.setUsername(authRequest.username());", "            newUser.setPassword(passwordEncoder.encode(authRequest.password()));", "            newUser.setFirstName(authRequest.firstName());", "            newUser.setLastName(authRequest.lastName());", "", "            userRepository.save(newUser);", "", "            return true; // Registration successful", "        } catch (Exception e) {", "            logger.error(\"Error occurred during user registration: {}\", e.getMessage(), e);", "            return false; // Registration failed", "        }", "    }", "}"], "file_path": "tickets-app-backend/src/main/java/com/umg/ticket_app_backend/services/AuthUserDetailsService.java"}
{"Link_to_commit": "https://github.com/matt-the-ogre/tide-calendar-site/commit/1393112ae29c311fcef0d6926c53a00da3211bf4", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 5, "n_files_impacted": 1, "longest_chunk": ["        lines = file.readlines()", "        if len(lines) < 2:", "            logging.error(f\"File {downloaded_filename} does not contain enough data.\")", "            exit(1)", "        second_line = lines[1]"], "file_path": "app/get_tides.py"}
{"Link_to_commit": "https://github.com/tyrm/mcp-dbmem/commit/b0504828020446a6d50535cd238b0a6e36434b3b", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 5, "n_files_impacted": 10, "longest_chunk": ["\tuptrace.ConfigureOpentelemetry(", "\t\tuptrace.WithServiceName(\"mcp-dbmem\"),", "\t\tuptrace.WithServiceVersion(viper.GetString(config.Keys.SoftwareVersion)),", "\t)", ""], "file_path": "cmd/mcp_dbmem/action/direct/direct.go"}
{"Link_to_commit": "https://github.com/NCAR/musica/commit/6f850cc0821c3e1021b03f43ea78343e06726176", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 8, "n_files_impacted": 21, "longest_chunk": ["#include <musica/version.hpp>", "#include <stdio.h>", "", "int main()", "{", "    printf(\"Musica version: %s\\n\", GetMusicaVersion());        ", "    return 0;", "}"], "file_path": "src/test/test_simple.c"}
{"Link_to_commit": "https://github.com/homewizard/python-homewizard-energy/commit/514888ebd8e7774e2f2a74f261b2edffe1bea087", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 134, "n_files_impacted": 11, "longest_chunk": ["### Batteries tests ###", "", "", "async def test_batteries_without_authentication():", "    \"\"\"Test batteries request is rejected when no authentication is provided.\"\"\"", "", "    async with HomeWizardEnergyV2(\"example.com\") as api:", "        with pytest.raises(UnauthorizedError):", "            await api.batteries()", "", "", "async def test_batteries_with_invalid_authentication(aresponses):", "    \"\"\"Test batteries request is unsuccessful when invalid authentication is provided.\"\"\"", "", "    aresponses.add(", "        \"example.com\",", "        \"/api/batteries\",", "        \"GET\",", "        aresponses.Response(", "            status=401,", "            headers={\"Content-Type\": \"application/json\"},", "            text='{\"error\": \"user:unauthorized\"}',", "        ),", "    )", "", "    async with HomeWizardEnergyV2(\"example.com\", token=\"token\") as api:", "        with pytest.raises(UnauthorizedError):", "            await api.batteries()", "", "", "@pytest.mark.parametrize(", "    (\"model\", \"fixtures\"),", "    [", "        (\"HWE-P1\", [\"batteries\"]),", "    ],", ")", "async def test_batteries_with_valid_authentication(", "    model: str, fixtures: list[str], snapshot: SnapshotAssertion, aresponses", "):", "    \"\"\"Test batteries request is successful when valid authentication is provided.\"\"\"", "", "    for fixture in fixtures:", "        aresponses.add(", "            \"example.com\",", "            \"/api/batteries\",", "            \"GET\",", "            aresponses.Response(", "                text=load_fixtures(f\"{model}/{fixture}.json\"),", "                status=200,", "                headers={\"Content-Type\": \"application/json\"},", "            ),", "        )", "", "        async with HomeWizardEnergyV2(\"example.com\", token=\"token\") as api:", "            batteries = await api.batteries()", "            assert batteries is not None", "            assert batteries == snapshot", "", "", "async def test_batteries_returns_unexpected_response(aresponses):", "    \"\"\"Test batteries request is successful when valid authentication is provided.\"\"\"", "", "    aresponses.add(", "        \"example.com\",", "        \"/api/batteries\",", "        \"GET\",", "        aresponses.Response(", "            status=500,", "            headers={\"Content-Type\": \"application/json\"},", "            text='{\"error\": \"server:error\"}',", "        ),", "    )", "", "    async with HomeWizardEnergyV2(\"example.com\", token=\"token\") as api:", "        with pytest.raises(RequestError) as e:", "            await api.batteries()", "            assert str(e.value) == \"server:error\"", "", "", "async def test_batteries_put_without_authentication():", "    \"\"\"Test batteries request is rejected when no authentication is provided.\"\"\"", "", "    async with HomeWizardEnergyV2(\"example.com\") as api:", "        with pytest.raises(UnauthorizedError):", "            await api.batteries(mode=Batteries.Mode.STANDBY)", "", "", "async def test_batteries_put_with_invalid_authentication(aresponses):", "    \"\"\"Test batteries request is unsuccessful when invalid authentication is provided.\"\"\"", "", "    aresponses.add(", "        \"example.com\",", "        \"/api/batteries\",", "        \"PUT\",", "        aresponses.Response(", "            status=401,", "            headers={\"Content-Type\": \"application/json\"},", "            text='{\"error\": \"user:unauthorized\"}',", "        ),", "    )", "", "    async with HomeWizardEnergyV2(\"example.com\", token=\"token\") as api:", "        with pytest.raises(UnauthorizedError):", "            await api.batteries(mode=Batteries.Mode.STANDBY)", "", "", "@pytest.mark.parametrize(", "    (\"model\"),", "    [", "        (\"HWE-P1\"),", "    ],", ")", "async def test_batteries_put_with_valid_authentication(", "    model: str, snapshot: SnapshotAssertion, aresponses", "):", "    \"\"\"Test batteries request is successful when valid authentication is provided.\"\"\"", "", "    aresponses.add(", "        \"example.com\",", "        \"/api/batteries\",", "        \"PUT\",", "        aresponses.Response(", "            text=load_fixtures(f\"{model}/batteries.json\"),", "            status=200,", "            headers={\"Content-Type\": \"application/json\"},", "        ),", "    )", "", "    async with HomeWizardEnergyV2(\"example.com\", token=\"token\") as api:", "        batteries = await api.batteries(mode=Batteries.Mode.STANDBY)", "        assert batteries is not None", "        assert batteries == snapshot", "", ""], "file_path": "tests/v2/test_v2_homewizard_energy.py"}
{"Link_to_commit": "https://github.com/amplitude/Amplitude-TypeScript/commit/104350ffe8b1bd1a7090482ac3bf24d85672bd43", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 403, "n_files_impacted": 32, "longest_chunk": ["// make a test class that implements NetworkRequestEvent", "/* eslint-disable @typescript-eslint/no-unsafe-member-access */", "/* eslint-disable @typescript-eslint/no-unsafe-assignment */", "import {", "  BrowserClient,", "  BrowserConfig,", "  CookieStorage,", "  FetchTransport,", "  Logger,", "  LogLevel,", "  NetworkEventCallback,", "  networkObserver,", "  NetworkRequestEvent,", "} from '@amplitude/analytics-core';", "import { shouldTrackNetworkEvent } from '../../src/track-network-event';", "import { NetworkTrackingOptions } from '@amplitude/analytics-core/lib/esm/types/network-tracking';", "import { AmplitudeBrowser } from '@amplitude/analytics-browser';", "import { BrowserEnrichmentPlugin, networkCapturePlugin } from '../../src/network-capture-plugin';", "import { AMPLITUDE_NETWORK_REQUEST_EVENT } from '../../src/constants';", "import { VERSION } from '../../src/version';", "", "class MockNetworkRequestEvent implements NetworkRequestEvent {", "  constructor(", "    public url: string = 'https://example.com',", "    public type: string = 'fetch',", "    public method: string = 'GET',", "    public status: number = 200,", "    public duration: number = 100,", "    public responseBodySize: number = 100,", "    public requestBodySize: number = 100,", "    public requestHeaders: Record<string, string> = {", "      'Content-Type': 'application/json',", "    },", "    public startTime: number = Date.now(),", "    public timestamp: number = Date.now(),", "    public endTime: number = Date.now() + 100,", "  ) {", "    this.type = 'fetch';", "  }", "}", "", "const baseBrowserConfig: BrowserConfig = {", "  apiKey: '<FAKE_API_KEY>',", "  flushIntervalMillis: 0,", "  flushMaxRetries: 0,", "  flushQueueSize: 0,", "  logLevel: LogLevel.None,", "  loggerProvider: new Logger(),", "  offline: false,", "  optOut: false,", "  serverUrl: undefined,", "  transportProvider: new FetchTransport(),", "  useBatch: false,", "  cookieOptions: {", "    domain: '.amplitude.com',", "    expiration: 365,", "    sameSite: 'Lax',", "    secure: false,", "    upgrade: true,", "  },", "  cookieStorage: new CookieStorage(),", "  sessionTimeout: 30 * 60 * 1000,", "  trackingOptions: {", "    ipAddress: true,", "    language: true,", "    platform: true,", "  },", "};", "", "describe('track-network-event', () => {", "  let networkEvent: MockNetworkRequestEvent;", "  let localConfig: BrowserConfig;", "  beforeEach(() => {", "    localConfig = {", "      ...baseBrowserConfig,", "      autocapture: {", "        networkTracking: true,", "      },", "      networkTrackingOptions: {},", "    } as BrowserConfig;", "    networkEvent = new MockNetworkRequestEvent();", "  });", "", "  describe('trackNetworkEvent()', () => {", "    let client: BrowserClient;", "    let trackSpy: jest.SpyInstance;", "    let eventCallbacks: any[] = [];", "    const subscribe = jest.fn((cb: NetworkEventCallback) => {", "      eventCallbacks.push(cb);", "      return () => {", "        eventCallbacks = [];", "      };", "    });", "", "    let plugin: BrowserEnrichmentPlugin;", "", "    beforeEach(async () => {", "      client = new AmplitudeBrowser();", "      trackSpy = jest.spyOn(client, 'track');", "      client.init('<FAKE_API_KEY>', undefined, localConfig);", "      jest.spyOn(networkObserver, 'subscribe').mockImplementation(subscribe);", "      plugin = networkCapturePlugin();", "      await plugin.setup?.(localConfig, client);", "    });", "", "    afterEach(async () => {", "      await plugin?.teardown?.();", "    });", "", "    test('should track a network request event with status=500', async () => {", "      eventCallbacks.forEach((cb: NetworkEventCallback) => {", "        cb.callback({", "          url: 'https://example.com/track?hello=world#hash',", "          type: 'fetch',", "          method: 'POST',", "          status: 500,", "          duration: 100,", "          responseBodySize: 100,", "          requestBodySize: 100,", "          requestHeaders: {", "            'Content-Type': 'application/json',", "          },", "          startTime: Date.now(),", "          timestamp: Date.now(),", "          endTime: Date.now() + 100,", "        });", "      });", "      const networkEventCall = trackSpy.mock.calls.find((call) => {", "        return call[0] === AMPLITUDE_NETWORK_REQUEST_EVENT;", "      });", "      const [eventName, eventProperties] = networkEventCall;", "      expect(eventName).toBe(AMPLITUDE_NETWORK_REQUEST_EVENT);", "      expect(eventProperties).toEqual({", "        '[Amplitude] URL': 'https://example.com/track?hello=world#hash',", "        '[Amplitude] URL Query': 'hello=world',", "        '[Amplitude] URL Fragment': 'hash',", "        '[Amplitude] Request Method': 'POST',", "        '[Amplitude] Status Code': 500,", "        '[Amplitude] Start Time': expect.any(String),", "        '[Amplitude] Completion Time': expect.any(String),", "        '[Amplitude] Duration': expect.any(Number),", "        '[Amplitude] Request Body Size': 100,", "        '[Amplitude] Response Body Size': 100,", "      });", "    });", "", "    test('should track a network request event with status=500 and network request missing attributes', async () => {", "      eventCallbacks.forEach((cb: NetworkEventCallback) => {", "        cb.callback({", "          url: 'https://example.com/track?hello=world#hash',", "          type: 'fetch',", "          method: 'POST',", "          status: 500,", "          duration: 100,", "          requestHeaders: {", "            'Content-Type': 'application/json',", "          },", "          timestamp: Date.now(),", "        });", "      });", "      const networkEventCall = trackSpy.mock.calls.find((call) => {", "        return call[0] === AMPLITUDE_NETWORK_REQUEST_EVENT;", "      });", "      const [eventName, eventProperties] = networkEventCall;", "      expect(eventName).toBe(AMPLITUDE_NETWORK_REQUEST_EVENT);", "      expect(eventProperties).toEqual({", "        '[Amplitude] URL': 'https://example.com/track?hello=world#hash',", "        '[Amplitude] URL Query': 'hello=world',", "        '[Amplitude] URL Fragment': 'hash',", "        '[Amplitude] Request Method': 'POST',", "        '[Amplitude] Status Code': 500,", "        '[Amplitude] Start Time': undefined,", "        '[Amplitude] Completion Time': undefined,", "        '[Amplitude] Duration': expect.any(Number),", "        '[Amplitude] Request Body Size': undefined,", "        '[Amplitude] Response Body Size': undefined,", "      });", "    });", "", "    test('should not track a network request event with status=200', async () => {", "      eventCallbacks.forEach((cb: NetworkEventCallback) => {", "        cb.callback({", "          url: 'https://example.com/track?hello=world#hash',", "          type: 'fetch',", "          method: 'POST',", "          status: 200,", "          duration: 100,", "          responseBodySize: 100,", "          requestBodySize: 100,", "          requestHeaders: {", "            'Content-Type': 'application/json',", "          },", "          startTime: Date.now(),", "          timestamp: Date.now(),", "          endTime: Date.now() + 100,", "        });", "      });", "      const networkEventCall = trackSpy.mock.calls.find((call) => {", "        return call[0] === AMPLITUDE_NETWORK_REQUEST_EVENT;", "      });", "      expect(networkEventCall).toBeUndefined();", "    });", "  });", "", "  describe('shouldTrackNetworkEvent returns false when', () => {", "    test('domain is amplitude.com', () => {", "      networkEvent.url = 'https://api.amplitude.com/track';", "      expect(shouldTrackNetworkEvent(networkEvent)).toBe(false);", "    });", "", "    test('domain is in ignoreHosts', () => {", "      localConfig.networkTrackingOptions = { ignoreHosts: ['example.com'] };", "      networkEvent.url = 'https://example.com/track';", "      expect(shouldTrackNetworkEvent(networkEvent, localConfig.networkTrackingOptions)).toBe(false);", "    });", "", "    test('domain matches a wildcard in ignoreHosts', () => {", "      localConfig.networkTrackingOptions = { ignoreHosts: ['*.example.com', 'dummy.url'] };", "      networkEvent.url = 'https://sub.example.com/track';", "      const result = shouldTrackNetworkEvent(networkEvent, localConfig.networkTrackingOptions);", "      expect(result).toBe(false);", "    });", "", "    test('host is not in one of the captureRules', () => {", "      localConfig.networkTrackingOptions = {", "        captureRules: [", "          {", "            hosts: ['example.com'],", "          },", "        ],", "      };", "      networkEvent.url = 'https://otherexample.com/apicall';", "      const result = shouldTrackNetworkEvent(networkEvent, localConfig.networkTrackingOptions);", "      expect(result).toBe(false);", "    });", "", "    test('status code is 403 and 400 is in the forbidden status codes', () => {", "      localConfig.networkTrackingOptions = {", "        captureRules: [", "          {", "            hosts: ['example.com'],", "            statusCodeRange: '404-599',", "          },", "        ],", "      };", "      networkEvent.url = 'https://example.com/track';", "      networkEvent.status = 403;", "      const result = shouldTrackNetworkEvent(networkEvent, localConfig.networkTrackingOptions);", "      expect(result).toBe(false);", "    });", "", "    test('status code is 400 and no status code range is defined', () => {", "      localConfig.networkTrackingOptions = {", "        captureRules: [", "          {", "            hosts: ['example.com'],", "          },", "        ],", "      };", "      networkEvent.url = 'https://example.com/track';", "      networkEvent.status = 400;", "      const result = shouldTrackNetworkEvent(networkEvent, localConfig.networkTrackingOptions);", "      expect(result).toBe(false);", "    });", "", "    test('status code is 200 and no captureRules are defined', () => {", "      networkEvent.url = 'https://notamplitude.com/track';", "      networkEvent.status = 200;", "      const result = shouldTrackNetworkEvent(", "        networkEvent,", "        localConfig.networkTrackingOptions as NetworkTrackingOptions,", "      );", "      expect(result).toBe(false);", "    });", "", "    test('status code is 0 and no captureRules are defined', () => {", "      networkEvent.url = 'https://notamplitude.com/track';", "      networkEvent.status = 0;", "      const result = shouldTrackNetworkEvent(", "        networkEvent,", "        localConfig.networkTrackingOptions as NetworkTrackingOptions,", "      );", "      expect(result).toBe(false);", "    });", "", "    test('host matches in captureRules but status code is not in the range', () => {", "      localConfig.networkTrackingOptions = {", "        captureRules: [", "          {", "            hosts: ['*'],", "            statusCodeRange: '200-299',", "          },", "          {", "            hosts: ['example.com'],", "            statusCodeRange: '500-599',", "          },", "        ],", "      };", "      networkEvent.url = 'https://example.com/track';", "      networkEvent.status = 200;", "      const result = shouldTrackNetworkEvent(networkEvent, localConfig.networkTrackingOptions);", "      expect(result).toBe(false);", "    });", "  });", "", "  describe('shouldTrackNetworkEvent returns true when', () => {", "    test('domain is api.amplitude.com and ignoreAmplitudeRequests is false', () => {", "      localConfig.networkTrackingOptions = { ignoreAmplitudeRequests: false };", "      networkEvent.url = 'https://api.amplitude.com/track';", "      networkEvent.status = 500;", "      const result = shouldTrackNetworkEvent(networkEvent, localConfig.networkTrackingOptions);", "      expect(result).toBe(true);", "    });", "", "    test('domain is amplitude.com and ignoreAmplitudeRequests is false', () => {", "      localConfig.networkTrackingOptions = { ignoreAmplitudeRequests: false };", "      networkEvent.url = 'https://amplitude.com/track';", "      networkEvent.status = 500;", "      const result = shouldTrackNetworkEvent(networkEvent, localConfig.networkTrackingOptions);", "      expect(result).toBe(true);", "    });", "", "    test('status code is 500', () => {", "      networkEvent.url = 'https://notamplitude.com/track';", "      networkEvent.status = 500;", "      const result = shouldTrackNetworkEvent(", "        networkEvent,", "        localConfig.networkTrackingOptions as NetworkTrackingOptions,", "      );", "      expect(result).toBe(true);", "    });", "", "    test('status code is 0', () => {", "      networkEvent.url = 'https://notamplitude.com/track';", "      networkEvent.status = 0;", "      localConfig.networkTrackingOptions = {", "        captureRules: [", "          {", "            hosts: ['notamplitude.com'],", "            statusCodeRange: '0,400-499',", "          },", "        ],", "      };", "      const result = shouldTrackNetworkEvent(networkEvent, localConfig.networkTrackingOptions);", "      expect(result).toBe(true);", "    });", "", "    test('status code is 200 and 200 is allowed in captureRules', () => {", "      localConfig.networkTrackingOptions = {", "        captureRules: [", "          {", "            hosts: ['example.com'],", "            statusCodeRange: '200',", "          },", "        ],", "      };", "      networkEvent.url = 'https://example.com/track';", "      networkEvent.status = 200;", "      const result = shouldTrackNetworkEvent(networkEvent, localConfig.networkTrackingOptions);", "      expect(result).toBe(true);", "    });", "", "    test('status code is 403 and 400 is within the statusCodeRange', () => {", "      localConfig.networkTrackingOptions = {", "        captureRules: [", "          {", "            hosts: ['example.com'],", "            statusCodeRange: '402-599',", "          },", "        ],", "      };", "      networkEvent.url = 'https://example.com/track';", "      networkEvent.status = 403;", "      const result = shouldTrackNetworkEvent(networkEvent, localConfig.networkTrackingOptions);", "      expect(result).toBe(true);", "    });", "", "    test('host does not match with second capture rule but matches with first', () => {", "      localConfig.networkTrackingOptions = {", "        captureRules: [", "          {", "            hosts: ['*.example.com'],", "            statusCodeRange: '400-499',", "          },", "          {", "            hosts: ['otherexample.com'],", "            statusCodeRange: '400-599',", "          },", "        ],", "      };", "      networkEvent.url = 'https://some.example.com/track';", "      networkEvent.status = 403;", "      const result = shouldTrackNetworkEvent(networkEvent, localConfig.networkTrackingOptions);", "      expect(result).toBe(true);", "    });", "  });", "});", "", "describe('version', () => {", "  test('should return the plugin version', () => {", "    expect(VERSION != null).toBe(true);", "  });", "});"], "file_path": "packages/plugin-network-capture-browser/test/setup.ts"}
{"Link_to_commit": "https://github.com/tuukkaviitanen/raspy-monitor/commit/be76c99f9d36f95793123f98d3a2ccfe14054d31", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 39, "n_files_impacted": 2, "longest_chunk": ["func handleStats(jsonStats string) models.InfluxDbFields {", "\tjsonLine := strings.Split(strings.TrimSpace(jsonStats), \"\\n\")", "", "\tfields := models.InfluxDbFields{}", "", "\tfor _, line := range jsonLine {", "\t\tvar DockerStat models.DockerStat", "", "\t\tif err := json.Unmarshal([]byte(line), &DockerStat); err != nil {", "\t\t\tlog.Printf(\"Error parsing JSON text '%s': %s\\n\", line, err)", "\t\t\tcontinue", "\t\t}", "", "\t\tparsedCPUPercentage, cpuParsingErr := parsePercentage(DockerStat.CPUPercentage)", "\t\tparsedMemPercentage, memParsingErr := parsePercentage(DockerStat.MemoryPercentage)", "\t\tparsedPidCount, pidParsingErr := strconv.Atoi(DockerStat.PIDs)", "", "\t\tif cpuParsingErr != nil {", "\t\t\tlog.Printf(\"Error parsing CPU percentage: %s\\n\", cpuParsingErr)", "\t\t\tcontinue", "\t\t}", "", "\t\tif memParsingErr != nil {", "\t\t\tlog.Printf(\"Error parsing Memory percentage: %s\\n\", memParsingErr)", "\t\t\tcontinue", "\t\t}", "", "\t\tif pidParsingErr != nil {", "\t\t\tlog.Printf(\"Error parsing PID count: %s\\n\", pidParsingErr)", "\t\t\tcontinue", "\t\t}", "", "\t\tif _, exists := fields[\"cpu_usage_percentage\"]; !exists {", "\t\t\tfields[\"cpu_usage_percentage\"] = make([]models.InfluxDbTaggedValue, 0)", "\t\t}", "\t\tfields[\"cpu_usage_percentage\"] = append(fields[\"cpu_usage_percentage\"], models.InfluxDbTaggedValue{", "\t\t\tValue: parsedCPUPercentage,", "\t\t\tTags: map[string]string{", "\t\t\t\t\"container_name\": DockerStat.Name,"], "file_path": "src/internal/data-gathering/docker.go"}
{"Link_to_commit": "https://github.com/CPRT/cprt_rover_24/commit/82f8eb6b7b298bdb9408888781250c830d0560dc", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 171, "n_files_impacted": 31, "longest_chunk": ["#include \"ArmIKMode.hpp\"", "", "#include \"ArmHelpers.hpp\"", "", "ArmIKMode::ArmIKMode(rclcpp::Node* node) : Mode(\"IK Arm\", node) {", "  RCLCPP_INFO(node_->get_logger(), \"IK Arm Mode\");", "  loadParameters();", "", "  servo_client_ =", "      node_->create_client<interfaces::srv::MoveServo>(\"servo_service\");", "  twist_pub_ = node_->create_publisher<geometry_msgs::msg::TwistStamped>(", "      \"/servo_node/delta_twist_cmds\", 10);", "  if (!ArmHelpers::start_moveit_servo(node_)) {", "    return;", "  }", "  frame_to_publish_ = CAM_FRAME_ID;", "  kServoMin = 0;", "  kServoMax = 180;", "  kClawMax = 62;", "  kClawMin = 8;", "  servoPos_ = kClawMax;", "  servoRequest(kServoPort, servoPos_, kServoMin, kServoMax);", "  buttonPressed_ = false;", "  swapButton_ = false;", "}", "", "void ArmIKMode::processJoystickInput(", "    std::shared_ptr<sensor_msgs::msg::Joy> joystickMsg) {", "  handleTwist(joystickMsg);", "}", "", "void ArmIKMode::handleTwist(", "    std::shared_ptr<sensor_msgs::msg::Joy> joystickMsg) {", "  geometry_msgs::msg::TwistStamped twist_msg;", "  twist_msg.header.stamp = node_->now();", "  twist_msg.header.frame_id = frame_to_publish_;", "", "  twist_msg.twist.linear.x = joystickMsg->axes[kxAxis];", "  twist_msg.twist.linear.y = -joystickMsg->axes[kyAxis];", "  twist_msg.twist.linear.z =", "      joystickMsg->buttons[kUpBut] - joystickMsg->buttons[kDownBut];", "  twist_msg.twist.angular.x = joystickMsg->axes[kAroundX];", "  twist_msg.twist.angular.y = joystickMsg->axes[kAroundY];", "  twist_msg.twist.angular.z = joystickMsg->axes[kAroundZ];", "", "  if (joystickMsg->buttons[kBase] == 1 && !swapButton_) {", "    frame_to_publish_ = BASE_FRAME_ID;", "    swapButton_ = true;", "  } else if (joystickMsg->buttons[kEEF] == 1 && !swapButton_) {", "    frame_to_publish_ = CAM_FRAME_ID;", "    swapButton_ = true;", "  } else if (!joystickMsg->buttons[kEEF] == 1 &&", "             joystickMsg->buttons[kBase] == 1) {", "    swapButton_ = false;", "  }", "  twist_pub_->publish(twist_msg);", "}", "void ArmIKMode::handleGripper(", "    std::shared_ptr<sensor_msgs::msg::Joy> joystickMsg) {", "  // Gripper. Will cycle between open, half open, and close on button release.", "  if (joystickMsg->buttons[kClawOpen] == 1 && !buttonPressed_) {", "    if (servoPos_ + ((kClawMax - kClawMin) / 2) < kClawMax + 1) {", "      buttonPressed_ = true;", "      servoPos_ = servoPos_ + ((kClawMax - kClawMin) / 2);", "      servoRequest(kServoPort, servoPos_, kClawMin, kClawMax);", "    } else {", "      buttonPressed_ = true;", "      RCLCPP_INFO(node_->get_logger(), \"Max Open\");", "      RCLCPP_INFO(node_->get_logger(), \"%d\", servoPos_);", "    }", "  } else if (joystickMsg->buttons[kClawClose] == 1 && !buttonPressed_) {", "    if (servoPos_ - ((kClawMax - kClawMin) / 2) > kClawMin - 1) {", "      buttonPressed_ = true;", "      servoPos_ = servoPos_ - ((kClawMax - kClawMin) / 2);", "      servoRequest(kServoPort, servoPos_, kClawMin, kClawMax);", "    } else {", "      buttonPressed_ = true;", "      RCLCPP_INFO(node_->get_logger(), \"Max Close\");", "      RCLCPP_INFO(node_->get_logger(), \"%d\", servoPos_);", "    }", "  } else if ((joystickMsg->buttons[kClawClose] == 0) &&", "             (joystickMsg->buttons[kClawOpen] == 0)) {", "    buttonPressed_ = false;", "  }", "}", "", "void ArmIKMode::declareParameters(rclcpp::Node* node) {", "  node->declare_parameter(\"arm_ik_mode.x_axis\", 0);", "  node->declare_parameter(\"arm_ik_mode.y_axis\", 1);", "  node->declare_parameter(\"arm_ik_mode.up_button\", 2);", "  node->declare_parameter(\"arm_ik_mode.down_button\", 3);", "  node->declare_parameter(\"arm_ik_mode.rotate_around_y\", 4);", "  node->declare_parameter(\"arm_ik_mode.rotate_around_x\", 5);", "  node->declare_parameter(\"arm_ik_mode.rotate_around_z\", 6);", "  node->declare_parameter(\"arm_ik_mode.open_claw\", 7);", "  node->declare_parameter(\"arm_ik_mode.close_claw\", 8);", "  node->declare_parameter(\"arm_ik_mode.base_frame\", 9);", "  node->declare_parameter(\"arm_ik_mode.eef_frame\", 10);", "}", "", "void ArmIKMode::loadParameters() {", "  node_->get_parameter(\"arm_ik_mode.x_axis\", kxAxis);", "  node_->get_parameter(\"arm_ik_mode.y_axis\", kyAxis);", "  node_->get_parameter(\"arm_ik_mode.up_button\", kUpBut);", "  node_->get_parameter(\"arm_ik_mode.down_button\", kDownBut);", "  node_->get_parameter(\"arm_ik_mode.rotate_around_y\", kAroundY);", "  node_->get_parameter(\"arm_ik_mode.rotate_around_x\", kAroundX);", "  node_->get_parameter(\"arm_ik_mode.rotate_around_z\", kAroundZ);", "  node_->get_parameter(\"arm_ik_mode.open_claw\", kClawOpen);", "  node_->get_parameter(\"arm_ik_mode.close_claw\", kClawClose);", "  node_->get_parameter(\"arm_ik_mode.base_frame\", kBase);", "  node_->get_parameter(\"arm_ik_mode.eef_frame\", kEEF);", "}", "", "interfaces::srv::MoveServo::Response ArmIKMode::sendRequest(int port, int pos,", "                                                            int min,", "                                                            int max) const {", "  auto request = std::make_shared<interfaces::srv::MoveServo::Request>();", "  request->port = port;", "  request->pos = pos;", "  request->min = min;", "  request->max = max;", "", "  // Wait for the service to be available", "  if (!servo_client_->wait_for_service(std::chrono::seconds(1))) {", "    RCLCPP_WARN(node_->get_logger(), \"Service not available after waiting\");", "    return interfaces::srv::MoveServo::Response();", "  }", "", "  auto future = servo_client_->async_send_request(request);", "", "  // Wait for the result (with timeout)", "  if (rclcpp::spin_until_future_complete(node_->get_node_base_interface(),", "                                         future, std::chrono::seconds(1)) !=", "      rclcpp::FutureReturnCode::SUCCESS) {", "    RCLCPP_ERROR(node_->get_logger(), \"Service call failed\");", "    return interfaces::srv::MoveServo::Response();", "  }", "", "  return *future.get();", "}", "", "void ArmIKMode::servoRequest(int req_port, int req_pos, int req_min,", "                             int req_max) const {", "  auto request = std::make_shared<interfaces::srv::MoveServo::Request>();", "  request->port = req_port;", "  request->pos = req_pos;", "  request->min = req_min;", "  request->max = req_max;", "", "  if (!servo_client_->wait_for_service(std::chrono::seconds(1))) {", "    RCLCPP_WARN(node_->get_logger(), \"Service not available\");", "    return;", "  }", "", "  // Simple callback that just logs errors", "  auto callback =", "      [this](rclcpp::Client<interfaces::srv::MoveServo>::SharedFuture future) {", "        try {", "          auto response = future.get();", "          if (!response->status) {", "            RCLCPP_ERROR(node_->get_logger(), \"Servo move failed\");", "          }", "        } catch (const std::exception& e) {", "          RCLCPP_ERROR(node_->get_logger(), \"Service call failed: %s\",", "                       e.what());", "        }", "      };", "", "  servo_client_->async_send_request(request, callback);", "}"], "file_path": "src/joystick_control/src/ArmIKMode.cpp"}
{"Link_to_commit": "https://github.com/Team334/Castle/commit/397ca6d7c4bd82b0b0b66da10d547fd52d54eba5", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 17, "n_files_impacted": 3, "longest_chunk": ["", "    const searchInput = document.getElementById('teamSearchInput');", "    const tableRows = document.querySelectorAll('tbody tr');", "    ", "    searchInput.addEventListener('input', function() {", "        const searchTerm = searchInput.value.trim();", "        ", "        tableRows.forEach(row => {", "            const teamNumberCell = row.querySelector('td:first-child');", "            if (teamNumberCell) {", "                const teamNumberText = teamNumberCell.textContent.trim();", "                ", "                // Show/hide the row based on whether the team number contains the search term", "                row.style.display = searchTerm === '' || teamNumberText.includes(searchTerm) ? '' : 'none';", "            }", "        });", "    });"], "file_path": "app/static/js/pit-scouting/list.js"}
{"Link_to_commit": "https://github.com/kit-foxboy/angular-demo/commit/f9d514519a1b2b949397d90c803a6e584a825f9e", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 7, "n_files_impacted": 1, "longest_chunk": ["interface MockSignal {", "  (): any; // Callable signature", "  set: jasmine.Spy<(val: any) => void>;", "  update: jasmine.Spy<() => void>;", "  mutate: jasmine.Spy<() => void>;", "}", ""], "file_path": "src/app/app.component.spec.ts"}
{"Link_to_commit": "https://github.com/rooch-network/nuwa/commit/3ba6f21fef3b66850d488caa80b5a7b5e853d563", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 338, "n_files_impacted": 7, "longest_chunk": ["import { searchKnowledgeEmbeddings, enhancedSearchKnowledgeEmbeddings, KnowledgeEmbeddingWithSimilarity, } from '../src/app/services/vectorStore';", "", "// \u7528\u4e8e\u5b58\u50a8\u6d4b\u8bd5\u5f00\u59cb\u65f6\u95f4", "const testStartTime = Date.now();", "", "interface SearchResult {", "  query: string;", "  threshold?: number;", "  results: KnowledgeEmbeddingWithSimilarity[];", "  duration: number;", "}", "", "/**", " * Test vector search functionality with various queries", " */", "async function testVectorSearch() {", "  console.log('=== Testing Vector Search Functionality ===');", "  console.log(`\ud83d\udd52 Test started at: ${new Date().toISOString()}`);", "  ", "  // Test queries - include both English and Chinese variants", "  const queries = [", "    // English queries", "    { query: 'What is Prompt is law?', threshold: 0.3, limit: 3 },", "    { query: 'Prompt is law', threshold: 0.3, limit: 3 },", "    { query: 'Prompt is law', threshold: 0.3, limit: 3 }, // Lower threshold", "    ", "    // Chinese queries", "    { query: '\u4ec0\u4e48\u662f Prompt is law?', threshold: 0.3, limit: 3 },", "    { query: '\u4ec0\u4e48\u662f Prompt is law?', threshold: 0.3, limit: 3 }, // Lower threshold", "    { query: 'Prompt is law \u662f\u4ec0\u4e48?', threshold: 0.3, limit: 3 },", "    ", "    // Additional variations", "    { query: 'prompt engineering', threshold: 0.3, limit: 3 },", "    { query: '\u63d0\u793a\u5de5\u7a0b', threshold: 0.3, limit: 3 },", "    ", "    // \u65b0\u589e\u6d4b\u8bd5\u7528\u4f8b - \u7279\u5b9a\u9886\u57df\u95ee\u9898", "    { query: 'How to use prompts effectively?', threshold: 0.5, limit: 5 },", "    { query: '\u5982\u4f55\u6709\u6548\u5730\u4f7f\u7528\u63d0\u793a\u8bcd\uff1f', threshold: 0.5, limit: 5 },", "    { query: 'Examples of good prompts', threshold: 0.6, limit: 3 },", "    { query: '\u597d\u7684\u63d0\u793a\u8bcd\u4f8b\u5b50', threshold: 0.6, limit: 3 },", "    ", "    // \u8fb9\u7f18\u60c5\u51b5\u6d4b\u8bd5", "    { query: '', threshold: 0.5, limit: 3 }, // \u7a7a\u67e5\u8be2", "    { query: '          ', threshold: 0.5, limit: 3 }, // \u53ea\u6709\u7a7a\u683c", "    { query: 'abcdefghijklmnopqrstuvwxyz', threshold: 0.4, limit: 3 }, // \u968f\u673a\u5b57\u7b26", "    { query: '!@#$%^&*()', threshold: 0.4, limit: 3 }, // \u7279\u6b8a\u5b57\u7b26", "    ", "    // \u591a\u8bed\u8a00\u6df7\u5408\u67e5\u8be2", "    { query: 'Prompt engineering \u63d0\u793a\u5de5\u7a0b best practices', threshold: 0.5, limit: 3 },", "    { query: '\u5982\u4f55\u4f7f\u7528 prompt engineering to improve results', threshold: 0.5, limit: 3 },", "  ];", "  ", "  // \u5b58\u50a8\u7ed3\u679c\u7528\u4e8e\u6bd4\u8f83", "  const allResults = {", "    standard: [] as SearchResult[],", "    enhanced: [] as SearchResult[]", "  };", "  ", "  // First test standard search", "  console.log('\\n\ud83d\udd0d STANDARD SEARCH TEST');", "  ", "  // Run searches for each query", "  for (const { query, threshold, limit } of queries) {", "    console.log(`\\n--- Testing query: \"${query}\" (threshold: ${threshold}) ---`);", "    ", "    try {", "      console.log(`\ud83d\udd52 Search started at: ${new Date().toISOString()}`);", "      const startTime = Date.now();", "      ", "      // \u8bb0\u5f55\u67e5\u8be2\u53c2\u6570", "      console.log(`Query parameters: { query: \"${query}\", threshold: ${threshold}, limit: ${limit} }`);", "      ", "      const results = await searchKnowledgeEmbeddings(query, limit, threshold);", "      const duration = Date.now() - startTime;", "      // \u4fdd\u5b58\u7ed3\u679c\u7528\u4e8e\u540e\u7eed\u6bd4\u8f83", "      allResults.standard.push({", "        query,", "        threshold,", "        results,", "        duration", "      });", "      ", "      if (results.length === 0) {", "        console.log(`\u274c No results found for query: \"${query}\" (search took ${duration}ms)`);", "      } else {", "        console.log(`\u2705 Found ${results.length} results in ${duration}ms:`);", "        ", "        results.forEach((result, index) => {", "          console.log(`\\nResult #${index + 1} (similarity: ${(result.similarity * 100).toFixed(2)}%)`);", "          console.log(`Title: ${result.title}`);", "          console.log(`Description: ${result.description?.substring(0, 100)}${result.description && result.description.length > 100 ? '...' : ''}`);", "          console.log(`Tags: ${result.tags?.join(', ') || 'none'}`);", "          console.log(`ID: ${result.airtable_id}`);", "          ", "          // \u6dfb\u52a0\u5185\u5bb9\u957f\u5ea6\u4fe1\u606f", "          if (result.content) {", "            console.log(`Content length: ${result.content.length} characters`);", "          }", "          ", "          // \u6dfb\u52a0\u7ed3\u679c\u5bf9\u8c61\u7684\u8c03\u8bd5\u4fe1\u606f", "          console.log('Debug - Result keys:', Object.keys(result));", "        });", "      }", "    } catch (error) {", "      console.error(`Error searching for query \"${query}\":`, error);", "      console.error(`Stack trace:`, error instanceof Error ? error.stack : String(error));", "    }", "  }", "  ", "  // Then test enhanced search", "  console.log('\\n\\n\ud83d\udd0d ENHANCED SEARCH TEST');", "  ", "  // Test enhanced search with the same queries", "  for (const { query, limit } of queries) {", "    console.log(`\\n--- Testing enhanced query: \"${query}\" ---`);", "    ", "    try {", "      console.log(`\ud83d\udd52 Enhanced search started at: ${new Date().toISOString()}`);", "      const startTime = Date.now();", "      ", "      // \u8bb0\u5f55\u67e5\u8be2\u53c2\u6570", "      console.log(`Query parameters: { query: \"${query}\", limit: ${limit} }`);", "      ", "      const results = await enhancedSearchKnowledgeEmbeddings(query, limit);", "      const duration = Date.now() - startTime;", "      ", "      // \u4fdd\u5b58\u7ed3\u679c\u7528\u4e8e\u540e\u7eed\u6bd4\u8f83", "      allResults.enhanced.push({", "        query,", "        results,", "        duration", "      });", "      ", "      if (results.length === 0) {", "        console.log(`\u274c No results found for enhanced query: \"${query}\" (search took ${duration}ms)`);", "      } else {", "        console.log(`\u2705 Found ${results.length} results in ${duration}ms:`);", "        ", "        results.forEach((result, index) => {", "          console.log(`\\nResult #${index + 1} (similarity: ${(result.similarity * 100).toFixed(2)}%)`);", "          console.log(`Title: ${result.title}`);", "          console.log(`Description: ${result.description?.substring(0, 100)}${result.description && result.description.length > 100 ? '...' : ''}`);", "          console.log(`Tags: ${result.tags?.join(', ') || 'none'}`);", "          console.log(`ID: ${result.airtable_id}`);", "          ", "          // \u6dfb\u52a0\u5185\u5bb9\u957f\u5ea6\u4fe1\u606f", "          if (result.content) {", "            console.log(`Content length: ${result.content.length} characters`);", "          }", "          ", "          // \u6dfb\u52a0\u7ed3\u679c\u5bf9\u8c61\u7684\u8c03\u8bd5\u4fe1\u606f", "          console.log('Debug - Result keys:', Object.keys(result));", "        });", "      }", "    } catch (error) {", "      console.error(`Error searching for enhanced query \"${query}\":`, error);", "      console.error(`Stack trace:`, error instanceof Error ? error.stack : String(error));", "    }", "  }", "  ", "  // \u6bd4\u8f83\u6807\u51c6\u641c\u7d22\u548c\u589e\u5f3a\u641c\u7d22\u7ed3\u679c", "  console.log('\\n\\n\ud83d\udd04 COMPARING STANDARD VS ENHANCED SEARCH RESULTS');", "  ", "  for (let i = 0; i < queries.length; i++) {", "    const query = queries[i].query;", "    const standardResults = allResults.standard[i];", "    const enhancedResults = allResults.enhanced[i];", "    ", "    console.log(`\\n--- Comparison for query: \"${query}\" ---`);", "    console.log(`Standard search: ${standardResults.results.length} results in ${standardResults.duration}ms`);", "    console.log(`Enhanced search: ${enhancedResults.results.length} results in ${enhancedResults.duration}ms`);", "    ", "    // \u6bd4\u8f83\u9876\u90e8\u7ed3\u679c", "    if (standardResults.results.length > 0 && enhancedResults.results.length > 0) {", "      console.log('\\nTop result comparison:');", "      console.log(`Standard top result: \"${standardResults.results[0].title}\" (${(standardResults.results[0].similarity * 100).toFixed(2)}%)`);", "      console.log(`Enhanced top result: \"${enhancedResults.results[0].title}\" (${(enhancedResults.results[0].similarity * 100).toFixed(2)}%)`);", "      ", "      // \u68c0\u67e5\u9876\u90e8\u7ed3\u679c\u662f\u5426\u76f8\u540c", "      const sameTopResult = standardResults.results[0].airtable_id === enhancedResults.results[0].airtable_id;", "      console.log(`Same top result: ${sameTopResult ? '\u2705 Yes' : '\u274c No'}`);", "      ", "      // \u67e5\u627e\u72ec\u6709\u7ed3\u679c", "      const standardIds = new Set(standardResults.results.map(r => r.airtable_id));", "      const enhancedIds = new Set(enhancedResults.results.map(r => r.airtable_id));", "      ", "      const uniqueToStandard = [...standardIds].filter(id => !enhancedIds.has(id));", "      const uniqueToEnhanced = [...enhancedIds].filter(id => !standardIds.has(id));", "      ", "      console.log(`Results unique to standard search: ${uniqueToStandard.length}`);", "      console.log(`Results unique to enhanced search: ${uniqueToEnhanced.length}`);", "    }", "    ", "    // \u6027\u80fd\u6bd4\u8f83", "    const perfDiff = enhancedResults.duration - standardResults.duration;", "    console.log(`Performance difference: ${perfDiff}ms (${perfDiff > 0 ? 'enhanced is slower' : 'enhanced is faster'})`);", "  }", "  ", "  // \u751f\u6210\u7edf\u8ba1\u6458\u8981", "  console.log('\\n\\n\ud83d\udcca SUMMARY STATISTICS');", "  ", "  const standardTotalTime = allResults.standard.reduce((sum, item) => sum + item.duration, 0);", "  const enhancedTotalTime = allResults.enhanced.reduce((sum, item) => sum + item.duration, 0);", "  ", "  const standardAvgTime = (standardTotalTime / allResults.standard.length).toFixed(2);", "  const enhancedAvgTime = (enhancedTotalTime / allResults.enhanced.length).toFixed(2);", "  ", "  console.log(`Average standard search time: ${standardAvgTime}ms`);", "  console.log(`Average enhanced search time: ${enhancedAvgTime}ms`);", "  ", "  const standardTotalResults = allResults.standard.reduce((sum, item) => sum + item.results.length, 0);", "  const enhancedTotalResults = allResults.enhanced.reduce((sum, item) => sum + item.results.length, 0);", "  ", "  console.log(`Total standard search results: ${standardTotalResults}`);", "  console.log(`Total enhanced search results: ${enhancedTotalResults}`);", "  ", "  // \u8bb0\u5f55\u6ca1\u6709\u7ed3\u679c\u7684\u67e5\u8be2", "  const queriesWithNoStandardResults = allResults.standard", "    .filter(item => item.results.length === 0)", "    .map(item => item.query);", "  ", "  const queriesWithNoEnhancedResults = allResults.enhanced", "    .filter(item => item.results.length === 0)", "    .map(item => item.query);", "  ", "  console.log(`\\nQueries with no standard results: ${queriesWithNoStandardResults.length > 0 ? queriesWithNoStandardResults.join(', ') : 'None'}`);", "  console.log(`Queries with no enhanced results: ${queriesWithNoEnhancedResults.length > 0 ? queriesWithNoEnhancedResults.join(', ') : 'None'}`);", "  ", "  console.log('\\n=== Vector Search Testing Complete ===');", "  console.log(`\ud83d\udd52 Test finished at: ${new Date().toISOString()}`);", "  console.log(`\ud83d\udd52 Total test duration: ${(Date.now() - testStartTime) / 1000}s`);", "}", "", "/**", " * \u4f7f\u7528\u5355\u4e2a\u67e5\u8be2\u8fdb\u884c\u8be6\u7ec6\u8c03\u8bd5\u6d4b\u8bd5", " * @param query \u6d4b\u8bd5\u7684\u67e5\u8be2\u5b57\u7b26\u4e32", " * @param threshold \u76f8\u4f3c\u5ea6\u9608\u503c", " * @param limit \u6700\u5927\u7ed3\u679c\u6570\u91cf", " */", "async function testSingleQuery(query: string, threshold: number = 0.5, limit: number = 5) {", "  console.log(`=== Detailed Test for Query: \"${query}\" ===`);", "  ", "  try {", "    console.log('\\n\ud83d\udd0d STANDARD SEARCH:');", "    console.log(`Parameters: threshold=${threshold}, limit=${limit}`);", "    ", "    const startTime = Date.now();", "    const results = await searchKnowledgeEmbeddings(query, limit, threshold);", "    const duration = Date.now() - startTime;", "    ", "    console.log(`Search completed in ${duration}ms, found ${results.length} results`);", "    ", "    if (results.length === 0) {", "      console.log('\u274c No results found');", "    } else {", "      console.log('\\nResults:');", "      results.forEach((result, index) => {", "        console.log(`\\n--- Result #${index + 1} ---`);", "        console.log(`Similarity: ${(result.similarity * 100).toFixed(2)}%`);", "        console.log(`Title: ${result.title}`);", "        console.log(`ID: ${result.airtable_id}`);", "        console.log(`Description: ${result.description?.substring(0, 150)}${result.description && result.description.length > 150 ? '...' : ''}`);", "        console.log(`Tags: ${result.tags?.join(', ') || 'none'}`);", "        ", "        // \u663e\u793a\u5b8c\u6574\u5185\u5bb9\u7528\u4e8e\u8be6\u7ec6\u8c03\u8bd5", "        console.log(`\\nFull Content (${result.content?.length || 0} chars):`);", "        console.log(result.content || '[No content]');", "        ", "        // \u663e\u793a\u6240\u6709\u5c5e\u6027", "        console.log('\\nAll properties:');", "        for (const [key, value] of Object.entries(result)) {", "          const displayValue = typeof value === 'string' ", "            ? value.substring(0, 50) + (value.length > 50 ? '...' : '')", "            : value;", "          console.log(`- ${key}: ${displayValue}`);", "        }", "      });", "    }", "    ", "    // \u4e3a\u540c\u4e00\u67e5\u8be2\u5c1d\u8bd5\u589e\u5f3a\u641c\u7d22", "    console.log('\\n\\n\ud83d\udd0d ENHANCED SEARCH:');", "    ", "    const enhancedStartTime = Date.now();", "    const enhancedResults = await enhancedSearchKnowledgeEmbeddings(query, limit);", "    const enhancedDuration = Date.now() - enhancedStartTime;", "    ", "    console.log(`Enhanced search completed in ${enhancedDuration}ms, found ${enhancedResults.length} results`);", "    ", "    if (enhancedResults.length === 0) {", "      console.log('\u274c No enhanced results found');", "    } else {", "      console.log('\\nEnhanced Results:');", "      enhancedResults.forEach((result, index) => {", "        console.log(`\\n--- Result #${index + 1} ---`);", "        console.log(`Similarity: ${(result.similarity * 100).toFixed(2)}%`);", "        console.log(`Title: ${result.title}`);", "        console.log(`ID: ${result.airtable_id}`);", "        console.log(`Description: ${result.description?.substring(0, 150)}${result.description && result.description.length > 150 ? '...' : ''}`);", "        console.log(`Tags: ${result.tags?.join(', ') || 'none'}`);", "      });", "    }", "    ", "  } catch (error) {", "    console.error(`Error during detailed test:`, error);", "    console.error(`Stack trace:`, error instanceof Error ? error.stack : String(error));", "  }", "}", "", "// \u9009\u62e9\u8981\u8fd0\u884c\u7684\u6d4b\u8bd5", "const testMode = process.env.TEST_MODE || 'full';", "const testQuery = process.env.TEST_QUERY || 'What is Prompt is law?';", "const testThreshold = parseFloat(process.env.TEST_THRESHOLD || '0.5');", "const testLimit = parseInt(process.env.TEST_LIMIT || '5');", "", "// \u8fd0\u884c\u9009\u5b9a\u7684\u6d4b\u8bd5", "if (testMode === 'single') {", "  console.log(`Running single query test with: \"${testQuery}\"`);", "  testSingleQuery(testQuery, testThreshold, testLimit)", "    .then(() => {", "      console.log('Single query test completed successfully');", "      process.exit(0);", "    })", "    .catch((error) => {", "      console.error('Single query test failed:', error);", "      process.exit(1);", "    });", "} else {", "  // \u8fd0\u884c\u5b8c\u6574\u6d4b\u8bd5\u5957\u4ef6", "  testVectorSearch()", "    .then(() => {", "      console.log('Testing completed successfully');", "      process.exit(0);", "    })", "    .catch((error) => {", "      console.error('Testing failed:', error);", "      process.exit(1);", "    });", "}"], "file_path": "website/campaign/scripts/test-vector-search.ts"}
{"Link_to_commit": "https://github.com/toolsdk-ai/awesome-mcp-registry/commit/bc880d810fbc0e49cdf0456243d6d5b11dee7a66", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 67, "n_files_impacted": 78, "longest_chunk": ["// This script generates two JSON files: packages-list.json and categories-list.json, which serve as collections for package and category data.", "// It reads category configurations from a predefined file and iterates through each category.", "// For each category, it recursively scans the corresponding directory for JSON files, validates them using the MCPServerPackageConfigSchema, and adds them to the packages list.", "// It also associates the packages with their respective categories and ensures no duplicate keys exist.", "// Finally, it writes the generated data to the specified output files in the collections directory.", "", "import * as fs from 'fs';", "import * as path from 'path';", "import { MCPServerPackageConfigSchema, type CategoryConfig } from '@toolsdk.ai/registry/types';", "", "// eslint-disable-next-line @typescript-eslint/no-require-imports", "const categoryConfigs: CategoryConfig[] = require('../config/categories').default;", "", "const packagesDir = './packages';", "const pacakgesListFile = './indexes/packages-list.json';", "const categoriesListFile = './indexes/categories-list.json';", "", "async function generatePackagesList() {", "  const packagesList: Record<string, { path: string }> = {};", "  const categoriesList: Record<string, { config: CategoryConfig; packagesList: string[] }> = {};", "", "  function traverseDirectory(directory: string, categoryName: string) {", "    const entries = fs.readdirSync(directory);", "", "    for (const entry of entries) {", "      const entryPath = path.join(directory, entry);", "      if (fs.statSync(entryPath).isFile() && entry.endsWith('.json')) {", "        const fileContent = fs.readFileSync(entryPath, 'utf-8');", "        const parsedContent = MCPServerPackageConfigSchema.parse(JSON.parse(fileContent));", "        if (parsedContent.name) {", "          const key = parsedContent.key || parsedContent.name;", "          if (key in packagesList) {", "            throw new Error(`Duplicate key detected: \"${key}\" in file \"${entryPath}\"`);", "          }", "          const relativePath = path.relative(packagesDir, entryPath);", "          packagesList[key] = { path: relativePath };", "", "          // Add to the category's packages list", "          if (!categoriesList[categoryName]) {", "            throw new Error(`Category \"${categoryName}\" not found in categories list.`);", "          }", "          categoriesList[categoryName].packagesList.push(key);", "        }", "      } else if (fs.statSync(entryPath).isDirectory()) {", "        traverseDirectory(entryPath, categoryName);", "      }", "    }", "  }", "", "  // const categoryConfigs: CategoryConfig[] = (await import(categoryCfg)).default;", "", "  for (const category of categoryConfigs) {", "    categoriesList[category.key] = { config: category, packagesList: [] };", "", "    const categoryDir = path.join(packagesDir, category.key);", "    if (fs.existsSync(categoryDir)) {", "      traverseDirectory(categoryDir, category.key);", "    }", "  }", "", "  fs.writeFileSync(pacakgesListFile, JSON.stringify(packagesList, null, 2), 'utf-8');", "  fs.writeFileSync(categoriesListFile, JSON.stringify(categoriesList, null, 2), 'utf-8');", "  console.log(`Generated packages list at ${pacakgesListFile}`);", "  console.log(`Generated categories list at ${categoriesListFile}`);", "}", "", "generatePackagesList();"], "file_path": "scripts/readme-gen.ts"}
{"Link_to_commit": "https://github.com/HohhotDog/Transplant-Australia-Mentoring-Platform/commit/b66648662e394f04838e8387a9f73903f4b12c56", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 5, "n_files_impacted": 1, "longest_chunk": ["    const { role } = req.body;", "    const allowedRoles = ['mentor', 'mentee'];", "    if (!allowedRoles.includes(role)) {", "      return res.status(400).json({ error: 'Invalid role. Allowed values are \"mentor\" or \"mentee\".' });", "    }"], "file_path": "Mentor-Matching-Platform/server/routes/sessions.js"}
{"Link_to_commit": "https://github.com/kimchanhyung98/laravel-docs-source/commit/0d0895a500add4b573aeae4b9a6f8d0ef034db98", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 7, "n_files_impacted": 1, "longest_chunk": ["    \"\"\"Laravel \uc6d0\ubcf8 \ubb38\uc11c\ub97c \ud604\uc7ac \ud504\ub85c\uc81d\ud2b8\uc5d0 \ub36e\uc5b4\uc4f0\uace0, \ubcc0\uacbd\uc0ac\ud56d\uc744 \ubc88\uc5ed \ubc0f \ub3d9\uae30\ud654\ud558\ub294 \ud568\uc218", "    ", "    \uc8fc\uc694 \uae30\ub2a5:", "        1. Laravel \uc6d0\ubcf8 \ubb38\uc11c\ub97c \ud074\ub860\ud558\uc5ec \ud604\uc7ac \ud504\ub85c\uc81d\ud2b8\uc5d0 \ub36e\uc5b4\uc500.", "        2. \ubcc0\uacbd\ub41c \ub9c8\ud06c\ub2e4\uc6b4 \ud30c\uc77c\uc744 \uc790\ub3d9\uc73c\ub85c \ubc88\uc5ed.", "        3. Git\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubcc0\uacbd\uc0ac\ud56d\uc744 \ub3d9\uae30\ud654.", "    \"\"\""], "file_path": "main.py"}
{"Link_to_commit": "https://github.com/lix42/log-viewer/commit/3239af9fc67bc8949553e7251867e861f01703fa", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 52, "n_files_impacted": 8, "longest_chunk": ["import { ReactNode, useEffect, useState } from \"react\";", "import { DataContext } from \"./DataContext\";", "import { useStreamingFetch } from \"./hooks/useStreamingFetch\";", "import { DataContextType, LogItemDataWithId } from \"./type\";", "", "/**", " * DataProvider component that fetches log data from a given URL and provides it to its children.", " * It uses the useStreamingFetch hook to handle the fetching logic.", " *", " * @param {ReactNode} children - The child components that will have access to the fetched data.", " * @param {string} url - The URL from which to fetch the log data. Defaults to a specific S3 URL.", " */", "export const DataProvider: React.FC<{ children: ReactNode; url?: string }> = ({", "  children,", "  url = \"https://s3.amazonaws.com/io.cribl.c021.takehome/cribl.log\",", "}) => {", "  // TODO: Add delayBetweenRead to the useStreamingFetch hook", "  // TODO: Use requestIdleCallback to parse the data when the items is not empty", "  // TODO: Add useDeferredValue to the items state", "  const [items, setItmes] = useState<DataContextType[\"items\"]>([]);", "  const [parsedIndex, setParsedIndex] = useState(0);", "", "  const { data, loading, error, lastModified, refetch } = useStreamingFetch(url);", "", "  useEffect(() => {", "    if (parsedIndex < data.length) {", "      const newItems = data", "        .slice(parsedIndex)", "        .map((item, index): LogItemDataWithId | null => {", "          try {", "            return {", "              data: JSON.parse(item),", "              id: `${lastModified}-${index + parsedIndex}`,", "            };", "          } catch {", "            return null;", "          }", "        })", "        .filter(Boolean) as LogItemDataWithId[];", "      setItmes((prev) => [...prev, ...newItems]);", "      setParsedIndex(data.length);", "    }", "  }, [data, lastModified, parsedIndex]);", "", "  const value: DataContextType = {", "    items,", "    loading,", "    error,", "    refetch,", "  };", "  return <DataContext.Provider value={value}>{children}</DataContext.Provider>;", "};"], "file_path": "src/hooks/useStreamingFetch.ts"}
{"Link_to_commit": "https://github.com/pactus-project/pactus-wallet/commit/49c1995e08533144b66b4add904d5249267beb22", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 133, "n_files_impacted": 17, "longest_chunk": ["/**", " * A collection of web-compatible face/character emojis", " */", "export const emojis = [", "    '\ud83d\ude00',", "    '\ud83d\ude03',", "    '\ud83d\ude04',", "    '\ud83d\ude01',", "    '\ud83d\ude06',", "    '\ud83d\ude05',", "    '\ud83d\ude02',", "    '\ud83e\udd23',", "    '\ud83e\udd72',", "    '\ud83e\udd79',", "    '\u263a\ufe0f',", "    '\ud83d\ude0a',", "    '\ud83d\ude07',", "    '\ud83d\ude42',", "    '\ud83d\ude43',", "    '\ud83d\ude09',", "    '\ud83d\ude0c',", "    '\ud83d\ude0d',", "    '\ud83e\udd70',", "    '\ud83d\ude18',", "    '\ud83d\ude17',", "    '\ud83d\ude19',", "    '\ud83d\ude1a',", "    '\ud83d\ude0b',", "    '\ud83d\ude1b',", "    '\ud83d\ude1d',", "    '\ud83d\ude1c',", "    '\ud83e\udd2a',", "    '\ud83e\udd28',", "    '\ud83e\uddd0',", "    '\ud83e\udd13',", "    '\ud83d\ude0e',", "    '\ud83e\udd78',", "    '\ud83e\udd29',", "    '\ud83e\udd73',", "    '\ud83d\ude42\u200d\u2195\ufe0f',", "    '\ud83d\ude0f',", "    '\ud83d\ude12',", "    '\ud83d\ude42\u200d\u2194\ufe0f',", "    '\ud83d\ude1e',", "    '\ud83d\ude14',", "    '\ud83d\ude1f',", "    '\ud83d\ude15',", "    '\ud83d\ude41',", "    '\u2639\ufe0f',", "    '\ud83d\ude23',", "    '\ud83d\ude16',", "    '\ud83d\ude2b',", "    '\ud83d\ude29',", "    '\ud83e\udd7a',", "    '\ud83d\ude22',", "    '\ud83d\ude2d',", "    '\ud83d\ude2e\u200d\ud83d\udca8',", "    '\ud83d\ude24',", "    '\ud83d\ude20',", "    '\ud83d\ude21',", "    '\ud83e\udd2c',", "    '\ud83e\udd2f',", "    '\ud83d\ude33',", "    '\ud83e\udd75',", "    '\ud83e\udd76',", "    '\ud83d\ude31',", "    '\ud83d\ude28',", "    '\ud83d\ude30',", "    '\ud83d\ude25',", "    '\ud83d\ude13',", "    '\ud83e\udee3',", "    '\ud83e\udd17',", "    '\ud83e\udee1',", "    '\ud83e\udd14',", "    '\ud83e\udee2',", "    '\ud83e\udd2d',", "    '\ud83e\udd2b',", "    '\ud83e\udd25',", "    '\ud83d\ude36',", "    '\ud83d\ude36\u200d\ud83c\udf2b\ufe0f',", "    '\ud83d\ude10',", "    '\ud83d\ude11',", "    '\ud83d\ude2c',", "    '\ud83e\udee8',", "    '\ud83e\udee0',", "    '\ud83d\ude44',", "    '\ud83d\ude2f',", "    '\ud83d\ude26',", "    '\ud83d\ude27',", "    '\ud83d\ude2e',", "    '\ud83d\ude32',", "    '\ud83e\udd71',", "    '\ud83d\ude34',", "    '\ud83e\udd24',", "    '\ud83d\ude2a',", "    '\ud83d\ude35',", "    '\ud83d\ude35\u200d\ud83d\udcab',", "    '\ud83e\udee5',", "    '\ud83e\udd10',", "    '\ud83e\udd74',", "    '\ud83e\udd22',", "    '\ud83e\udd2e',", "    '\ud83e\udd27',", "    '\ud83d\ude37',", "    '\ud83e\udd12',", "    '\ud83e\udd15',", "    '\ud83e\udd11',", "    '\ud83e\udd20',", "    '\ud83d\ude08',", "    '\ud83d\udc7f',", "    '\ud83d\udc79',", "    '\ud83d\udc7a',", "    '\ud83e\udd21',", "    '\ud83d\udca9',", "    '\ud83d\udc7b',", "    '\ud83d\udc80',", "    '\u2620\ufe0f',", "    '\ud83d\udc7d',", "    '\ud83d\udc7e',", "    '\ud83e\udd16',", "    '\ud83c\udf83',", "    '\ud83d\ude3a',", "    '\ud83d\ude38',", "    '\ud83d\ude39',", "    '\ud83d\ude3b',", "    '\ud83d\ude3c',", "    '\ud83d\ude3d',", "    '\ud83d\ude40',", "    '\ud83d\ude3f'", "];", "", "", "export default emojis;"], "file_path": "apps/web/src/assets/get-started/index.ts"}
{"Link_to_commit": "https://github.com/itohti/CARoadmap/commit/3ec6b3e17b2f76458ca8b0dd6c2ef243a4251a70", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 7, "n_files_impacted": 3, "longest_chunk": ["import java.net.URI;", "import java.net.http.HttpClient;", "import java.net.http.HttpRequest;", "import java.net.http.HttpResponse;", "", "", "import java.util.ArrayList;"], "file_path": "src/main/java/com/caroadmap/FirebaseDatabase.java"}
{"Link_to_commit": "https://github.com/knct-ai/kinetic/commit/442d8301284908019491428df2a2c8bfc4957673", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 24, "n_files_impacted": 2, "longest_chunk": ["    def print_to_console(self, span_kind: str, interaction: Mapping[str, Any]) -> None:", "        \"\"\"Print the span to the console.\"\"\"", "        if not self.console:", "            msg = \"Console is not initialized\"", "            raise RuntimeError(msg)", "        style = getattr(self.tracing_config, span_kind.lower(), None)", "        if not style or interaction == {}:", "            return", "", "        self.console.rule(span_kind, style=style)", "", "        for key, value in interaction.items():", "            if key == \"output\":", "                self.console.print(", "                    Panel(", "                        Markdown(str(value or \"\")),", "                        title=\"Output\",", "                    ),", "                )", "            else:", "                self.console.print(f\"{key}: {value}\")", "", "        self.console.rule(style=style)", ""], "file_path": "src/any_agent/tracing/exporter.py"}
{"Link_to_commit": "https://github.com/StormDev771/dify/commit/5429209a032f2903e3e930f64e922248167d445f", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 10, "n_files_impacted": 1, "longest_chunk": ["    fetch(resource: RequestInfo | URL, options?: RequestInit) {", "      if (resource instanceof Request && options) {", "        const mergedHeaders = new Headers(options.headers || {})", "        resource.headers.forEach((value, key) => {", "          mergedHeaders.append(key, value)", "        })", "        options.headers = mergedHeaders", "      }", "      return globalThis.fetch(resource, options)", "    },"], "file_path": "web/service/fetch.ts"}
{"Link_to_commit": "https://github.com/global-121/121-platform/commit/f881d6dba6745bd771a9645f429d47a77c668524", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 6, "n_files_impacted": 4, "longest_chunk": ["      .catch((error) => {", "        console.error(", "          'Error: Exchange-Rates - retrieveAndStoreAllExchangeRates',", "          error,", "        );", "      })"], "file_path": "services/121-service/src/exchange-rates/exchange-rates.controller.ts"}
{"Link_to_commit": "https://github.com/runtimeverification/riscv-semantics/commit/ae15f5d97f3624f12d50e7889a66a651fc6a5137", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 58, "n_files_impacted": 8, "longest_chunk": ["from __future__ import annotations", "", "from dataclasses import dataclass", "from typing import TYPE_CHECKING", "", "import pytest", "from pyk.proof import ProofStatus", "", "from kriscv.symtools import SymTools", "", "from .utils import TEST_DATA_DIR", "", "if TYPE_CHECKING:", "    from pathlib import Path", "    from typing import Final", "", "", "SPEC_DIR: Final = TEST_DATA_DIR / 'specs'", "", "", "@dataclass", "class SpecLoader:", "    temp_dir: Path", "", "    def __call__(self, file_name: str) -> Path:", "        import shutil", "", "        res = self.temp_dir / file_name", "        shutil.copy(SPEC_DIR / file_name, res)", "        return res", "", "", "@pytest.fixture", "def load_spec(tmp_path: Path) -> SpecLoader:", "    return SpecLoader(temp_dir=tmp_path)", "", "", "@pytest.fixture", "def symtools(tmp_path: Path) -> SymTools:", "    return SymTools.default(proof_dir=tmp_path)", "", "", "def test_add(", "    load_spec: SpecLoader,", "    symtools: SymTools,", ") -> None:", "    # Given", "    spec_file = load_spec('add-spec.k')", "", "    # When", "    proof = symtools.prove(", "        spec_file=spec_file,", "        spec_module='ADD-SPEC',", "        claim_id='ADD-SPEC.add',", "    )", "", "    # Then", "    assert proof.status == ProofStatus.PASSED"], "file_path": "src/tests/integration/utils.py"}
{"Link_to_commit": "https://github.com/Qbandev/stbchef/commit/2ccb18ee47b606aab985fc7ab54bd569de44e877", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 7, "n_files_impacted": 1, "longest_chunk": ["            // Get current ETH price with safety check", "            const ethPrice = window.walletBalances.ethusd || 0;", "            if (ethPrice <= 0 && document.querySelector('[data-recommended-action=\"SELL\"]')) {", "                showNotification('Cannot test SELL notification: ETH price data not available', 'warning');", "                return;", "            }", "            "], "file_path": "src/web/static/js/walletManager.js"}
{"Link_to_commit": "https://github.com/ParetoSecurity/agent/commit/82c7f836fca816ea3abdb7e08eb743ba076d826d", "n-gram matched": "co-authored-by: copilot", "n_lines_longer_change": 247, "n_files_impacted": 8, "longest_chunk": ["package systemd", "", "import (", "\t\"errors\"", "\t\"testing\"", "", "\t\"github.com/ParetoSecurity/agent/shared\"", ")", "", "func TestIsTimerEnabled(t *testing.T) {", "\ttests := []struct {", "\t\tname     string", "\t\tmocks    []shared.RunCommandMock", "\t\texpected bool", "\t}{", "\t\t{", "\t\t\tname: \"both services enabled\",", "\t\t\tmocks: []shared.RunCommandMock{", "\t\t\t\t{", "\t\t\t\t\tCommand: \"systemctl\",", "\t\t\t\t\tOut:     \"enabled\",", "\t\t\t\t\tArgs:    []string{\"--user\", \"is-enabled\", \"paretosecurity-user.timer\"},", "\t\t\t\t\tErr:     nil,", "\t\t\t\t},", "\t\t\t\t{", "\t\t\t\t\tCommand: \"systemctl\",", "\t\t\t\t\tOut:     \"enabled\",", "\t\t\t\t\tArgs:    []string{\"--user\", \"is-enabled\", \"paretosecurity-user.service\"},", "\t\t\t\t\tErr:     nil,", "\t\t\t\t},", "\t\t\t},", "\t\t\texpected: true,", "\t\t},", "\t\t{", "\t\t\tname: \"timer disabled\",", "\t\t\tmocks: []shared.RunCommandMock{", "\t\t\t\t{", "\t\t\t\t\tCommand: \"systemctl\",", "\t\t\t\t\tOut:     \"disabled\",", "\t\t\t\t\tArgs:    []string{\"--user\", \"is-enabled\", \"paretosecurity-user.timer\"},", "\t\t\t\t\tErr:     nil,", "\t\t\t\t},", "\t\t\t\t{", "\t\t\t\t\tCommand: \"systemctl\",", "\t\t\t\t\tOut:     \"enabled\",", "\t\t\t\t\tArgs:    []string{\"--user\", \"is-enabled\", \"paretosecurity-user.service\"},", "\t\t\t\t\tErr:     nil,", "\t\t\t\t},", "\t\t\t},", "\t\t\texpected: false,", "\t\t},", "\t\t{", "\t\t\tname: \"service disabled\",", "\t\t\tmocks: []shared.RunCommandMock{", "\t\t\t\t{", "\t\t\t\t\tCommand: \"systemctl\",", "\t\t\t\t\tOut:     \"enabled\",", "\t\t\t\t\tArgs:    []string{\"--user\", \"is-enabled\", \"paretosecurity-user.timer\"},", "\t\t\t\t\tErr:     nil,", "\t\t\t\t},", "\t\t\t\t{", "\t\t\t\t\tCommand: \"systemctl\",", "\t\t\t\t\tOut:     \"disabled\",", "\t\t\t\t\tArgs:    []string{\"--user\", \"is-enabled\", \"paretosecurity-user.service\"},", "\t\t\t\t\tErr:     nil,", "\t\t\t\t},", "\t\t\t},", "\t\t\texpected: false,", "\t\t},", "\t\t{", "\t\t\tname: \"both services disabled\",", "\t\t\tmocks: []shared.RunCommandMock{", "\t\t\t\t{", "\t\t\t\t\tCommand: \"systemctl\",", "\t\t\t\t\tOut:     \"disabled\",", "\t\t\t\t\tArgs:    []string{\"--user\", \"is-enabled\", \"paretosecurity-user.timer\"},", "\t\t\t\t\tErr:     nil,", "\t\t\t\t},", "\t\t\t\t{", "\t\t\t\t\tCommand: \"systemctl\",", "\t\t\t\t\tOut:     \"disabled\",", "\t\t\t\t\tArgs:    []string{\"--user\", \"is-enabled\", \"paretosecurity-user.service\"},", "\t\t\t\t\tErr:     nil,", "\t\t\t\t},", "\t\t\t},", "\t\t\texpected: false,", "\t\t},", "\t}", "", "\tfor _, tt := range tests {", "\t\tt.Run(tt.name, func(t *testing.T) {", "\t\t\t// Setup mocks", "\t\t\tshared.RunCommandMocks = tt.mocks", "", "\t\t\t// Run test", "\t\t\tresult := IsTimerEnabled()", "", "\t\t\t// Check result", "\t\t\tif result != tt.expected {", "\t\t\t\tt.Errorf(\"IsTimerEnabled() = %v, want %v\", result, tt.expected)", "\t\t\t}", "\t\t})", "\t}", "}", "func TestEnableTimer(t *testing.T) {", "\ttests := []struct {", "\t\tname          string", "\t\tmocks         []shared.RunCommandMock", "\t\texpectedError bool", "\t}{", "\t\t{", "\t\t\tname: \"successfully enable both\",", "\t\t\tmocks: []shared.RunCommandMock{", "\t\t\t\t{", "\t\t\t\t\tCommand: \"systemctl\",", "\t\t\t\t\tOut:     \"\",", "\t\t\t\t\tArgs:    []string{\"--user\", \"enable\", \"paretosecurity-user.timer\"},", "\t\t\t\t\tErr:     nil,", "\t\t\t\t},", "\t\t\t\t{", "\t\t\t\t\tCommand: \"systemctl\",", "\t\t\t\t\tOut:     \"\",", "\t\t\t\t\tArgs:    []string{\"--user\", \"enable\", \"paretosecurity-user.service\"},", "\t\t\t\t\tErr:     nil,", "\t\t\t\t},", "\t\t\t},", "\t\t\texpectedError: false,", "\t\t},", "\t\t{", "\t\t\tname: \"error enabling timer\",", "\t\t\tmocks: []shared.RunCommandMock{", "\t\t\t\t{", "\t\t\t\t\tCommand: \"systemctl\",", "\t\t\t\t\tOut:     \"\",", "\t\t\t\t\tArgs:    []string{\"--user\", \"enable\", \"paretosecurity-user.timer\"},", "\t\t\t\t\tErr:     errors.New(\"failed to enable timer\"),", "\t\t\t\t},", "\t\t\t},", "\t\t\texpectedError: true,", "\t\t},", "\t\t{", "\t\t\tname: \"error enabling service\",", "\t\t\tmocks: []shared.RunCommandMock{", "\t\t\t\t{", "\t\t\t\t\tCommand: \"systemctl\",", "\t\t\t\t\tOut:     \"\",", "\t\t\t\t\tArgs:    []string{\"--user\", \"enable\", \"paretosecurity-user.timer\"},", "\t\t\t\t\tErr:     nil,", "\t\t\t\t},", "\t\t\t\t{", "\t\t\t\t\tCommand: \"systemctl\",", "\t\t\t\t\tOut:     \"\",", "\t\t\t\t\tArgs:    []string{\"--user\", \"enable\", \"paretosecurity-user.service\"},", "\t\t\t\t\tErr:     errors.New(\"failed to enable service\"),", "\t\t\t\t},", "\t\t\t},", "\t\t\texpectedError: true,", "\t\t},", "\t}", "", "\tfor _, tt := range tests {", "\t\tt.Run(tt.name, func(t *testing.T) {", "\t\t\t// Setup mocks", "\t\t\tshared.RunCommandMocks = tt.mocks", "", "\t\t\t// Run test", "\t\t\terr := EnableTimer()", "", "\t\t\t// Check result", "\t\t\tif (err != nil) != tt.expectedError {", "\t\t\t\tt.Errorf(\"EnableTimer() error = %v, expectedError %v\", err, tt.expectedError)", "\t\t\t}", "\t\t})", "\t}", "}", "", "func TestDisableTimer(t *testing.T) {", "\ttests := []struct {", "\t\tname          string", "\t\tmocks         []shared.RunCommandMock", "\t\texpectedError bool", "\t}{", "\t\t{", "\t\t\tname: \"successfully disable both\",", "\t\t\tmocks: []shared.RunCommandMock{", "\t\t\t\t{", "\t\t\t\t\tCommand: \"systemctl\",", "\t\t\t\t\tOut:     \"\",", "\t\t\t\t\tArgs:    []string{\"--user\", \"disable\", \"paretosecurity-user.timer\"},", "\t\t\t\t\tErr:     nil,", "\t\t\t\t},", "\t\t\t\t{", "\t\t\t\t\tCommand: \"systemctl\",", "\t\t\t\t\tOut:     \"\",", "\t\t\t\t\tArgs:    []string{\"--user\", \"disable\", \"paretosecurity-user.service\"},", "\t\t\t\t\tErr:     nil,", "\t\t\t\t},", "\t\t\t},", "\t\t\texpectedError: false,", "\t\t},", "\t\t{", "\t\t\tname: \"error disabling timer\",", "\t\t\tmocks: []shared.RunCommandMock{", "\t\t\t\t{", "\t\t\t\t\tCommand: \"systemctl\",", "\t\t\t\t\tOut:     \"\",", "\t\t\t\t\tArgs:    []string{\"--user\", \"disable\", \"paretosecurity-user.timer\"},", "\t\t\t\t\tErr:     errors.New(\"failed to disable timer\"),", "\t\t\t\t},", "\t\t\t},", "\t\t\texpectedError: true,", "\t\t},", "\t\t{", "\t\t\tname: \"error disabling service\",", "\t\t\tmocks: []shared.RunCommandMock{", "\t\t\t\t{", "\t\t\t\t\tCommand: \"systemctl\",", "\t\t\t\t\tOut:     \"\",", "\t\t\t\t\tArgs:    []string{\"--user\", \"disable\", \"paretosecurity-user.timer\"},", "\t\t\t\t\tErr:     nil,", "\t\t\t\t},", "\t\t\t\t{", "\t\t\t\t\tCommand: \"systemctl\",", "\t\t\t\t\tOut:     \"\",", "\t\t\t\t\tArgs:    []string{\"--user\", \"disable\", \"paretosecurity-user.service\"},", "\t\t\t\t\tErr:     errors.New(\"failed to disable service\"),", "\t\t\t\t},", "\t\t\t},", "\t\t\texpectedError: true,", "\t\t},", "\t}", "", "\tfor _, tt := range tests {", "\t\tt.Run(tt.name, func(t *testing.T) {", "\t\t\t// Setup mocks", "\t\t\tshared.RunCommandMocks = tt.mocks", "", "\t\t\t// Run test", "\t\t\terr := DisableTimer()", "", "\t\t\t// Check result", "\t\t\tif (err != nil) != tt.expectedError {", "\t\t\t\tt.Errorf(\"DisableTimer() error = %v, expectedError %v\", err, tt.expectedError)", "\t\t\t}", "\t\t})", "\t}", "}"], "file_path": "systemd/tray.go"}
{"Link_to_commit": "https://github.com/openmultiagentsystems/framework/commit/52d73c3662f2d0ba9d1f18938d66baa4aeda69df", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 12, "n_files_impacted": 1, "longest_chunk": ["prompt = \"\"\"", "    I need an array with 100 arrays that contain 3 numbers each.", "    Do not return a text with the code to generate the array.", "", "    Generate the array for me and return it as JSON like the following", "", "    {", "        \"data\": [[n1, n2, n3], ...]", "    }", "\"\"\"", "", ""], "file_path": "register/main.py"}
{"Link_to_commit": "https://github.com/matmenzl/fridge-os/commit/188321f968700f1de9ffa9173d7e6b9c2c6646f3", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 69, "n_files_impacted": 2, "longest_chunk": ["  try {", "    // Prepare the list of products as a comma-separated string", "    const productsList = products.join(', ');", "    ", "    // Create the prompt for ChatGPT", "    const prompt = `Du bist ein Koch-Experte und sollst Men\u00fcvorschl\u00e4ge basierend auf den folgenden Zutaten erstellen:", "    ", "    ${productsList}", "    ", "    Bitte erstelle 6 kreative Men\u00fcvorschl\u00e4ge (oder weniger, wenn nicht genug Zutaten vorhanden sind). ", "    Jeder Vorschlag sollte kurz sein (maximal 3-4 W\u00f6rter) und auf Deutsch.", "    Gib nur die Men\u00fcvorschl\u00e4ge zur\u00fcck, einer pro Zeile, ohne Nummerierung oder andere Texte.`;", "    ", "    // Call the OpenAI API", "    const response = await fetch('https://api.openai.com/v1/chat/completions', {", "      method: 'POST',", "      headers: {", "        'Content-Type': 'application/json',", "        'Authorization': `Bearer ${localStorage.getItem('openai_api_key') || ''}`", "      },", "      body: JSON.stringify({", "        model: 'gpt-3.5-turbo',", "        messages: [", "          {", "            role: 'system',", "            content: 'Du bist ein hilfreicher Assistent, der kreative Men\u00fcvorschl\u00e4ge basierend auf vorhandenen Zutaten erstellt.'", "          },", "          {", "            role: 'user',", "            content: prompt", "          }", "        ],", "        temperature: 0.7,", "        max_tokens: 250", "      })", "    });", "    ", "    if (!response.ok) {", "      const errorData = await response.json();", "      console.error('OpenAI API Error:', errorData);", "      throw new Error(`OpenAI API Fehler: ${errorData.error?.message || 'Unbekannter Fehler'}`);", "    }", "    ", "    const data = await response.json();", "    ", "    // Extract the suggestions from the API response", "    const aiResponse = data.choices[0].message.content.trim();", "    ", "    // Split the response by new lines to get individual suggestions", "    const suggestions = aiResponse.split('\\n')", "      .map(line => line.trim())", "      .filter(line => line && !line.startsWith('-')) // Remove empty lines and bullet points", "      .map(line => {", "        // Remove numbers at the beginning if present (e.g. \"1. Spaghetti Carbonara\" -> \"Spaghetti Carbonara\")", "        return line.replace(/^\\d+\\.\\s*/, '');", "      });", "    ", "    // Return up to 6 suggestions", "    return suggestions.slice(0, 6);", "  } catch (error) {", "    console.error('Fehler bei der Generierung von Men\u00fcvorschl\u00e4gen:', error);", "    ", "    // Fall back to the original implementation if the API call fails", "    return fallbackMenuSuggestions(products);", "  }", "};", "", "// Fallback function that uses the original implementation", "const fallbackMenuSuggestions = (products: string[]): string[] => {"], "file_path": "src/utils/productUtils.ts"}
{"Link_to_commit": "https://github.com/kshehata/Kaito-Anki-JP-Addon/commit/1c59e5845e6852ad995ada66eb96a3d900210122", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 43, "n_files_impacted": 1, "longest_chunk": ["", "def generate_chatgpt_image(japanese_text, english_text):", "    \"\"\"Generate an image using ChatGPT based on the Japanese and English text.\"\"\"", "    api_key = config.get(\"openai_api_key\")", "    print(\"API Key: \" + api_key)", "    if not api_key:", "        return None", "    ", "    prompt_template = config.get(\"chatgpt_image_prompt_template\", ", "                               \"Create a simple, clear illustration to represent'{japanese}' meaning '{english}'. The image should be minimalist and educational.\")", "    ", "    prompt = prompt_template.format(japanese=japanese_text, english=english_text)", "    print(\"ChatGPT Prompt: \" + prompt)", "    response = requests.post(", "        \"https://api.openai.com/v1/images/generations\",", "        headers={", "            \"Content-Type\": \"application/json\",", "            \"Authorization\": f\"Bearer {api_key}\"", "        },", "        json={", "            \"model\": \"dall-e-3\",", "            \"prompt\": prompt,", "            \"n\": 1,", "            \"size\": \"1024x1024\"", "        },", "        timeout=30", "    )", "", "    print(\"ChatGPT Response: \" + str(response.json()))", "    ", "    if response.status_code != 200:", "        raise Exception(\"Failed to generate image with ChatGPT: \" + str(response.json()))", "    ", "    data = response.json()", "    if \"data\" not in data or len(data[\"data\"]) < 1:", "        return None", "    ", "    return {", "        \"url\": data[\"data\"][0][\"url\"],", "        \"thumbnail\": load_image_from_url(data[\"data\"][0][\"url\"]),", "        \"source\": \"ChatGPT\",", "        \"title\": \"AI Generated Image\"", "    }"], "file_path": "wizard.py"}
{"Link_to_commit": "https://github.com/silviaacatherine/silvia_roboverse/commit/49f76b3e16493b432ce8250936d0567eff61c0c7", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 14, "n_files_impacted": 1, "longest_chunk": ["import openai", "", "# Replace with your actual OpenAI API key when testing locally", "openai.api_key = 'your-openai-key'", "", "def solve_question(question):", "    response = openai.ChatCompletion.create(", "        model='gpt-3.5-turbo',", "        messages=[", "            {\"role\": \"system\", \"content\": \"You're an expert tutor for JEE/NSET.\"},", "            {\"role\": \"user\", \"content\": question}", "        ]", "    )", "    return response['choices'][0]['message']['content']"], "file_path": "backend/backend/solver.py"}
{"Link_to_commit": "https://github.com/gdsmith1/Replicant/commit/3221d993922e427660d2fcb51d9c1c59f8ad7f72", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 20, "n_files_impacted": 1, "longest_chunk": ["dataset = []", "for chunk in censored_chunks:", "    prompt = f\"Assuming both users are gamers talking on Discord, what would one of them have said to get this response?  {chunk}  Give a likely quote in a simple, two-sentence max format that would cause this response, with no other feedback.\"", "    response = client.chat.completions.create(", "        model=\"gpt-4o\",", "        messages=[", "        {", "            \"role\": \"user\",", "            \"content\": prompt", "        }", "    ]", "    )", "    user_content = response.choices[0].message.content", "    print(\"USER:\", user_content, \"AI:\", chunk)", "    dataset.append({", "        \"messages\": [", "            {\"role\": \"user\", \"content\": user_content},", "            {\"role\": \"assistant\", \"content\": chunk}", "        ]", "    })"], "file_path": "applications/machine-learning/llm/app/main.py"}
{"Link_to_commit": "https://github.com/mathchou/mathchou.github.io/commit/527ee82b7d93657ebe61509fe31a448f539d3b7c", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 156, "n_files_impacted": 6, "longest_chunk": ["<!DOCTYPE html>", "<html lang=\"en\">", "<head>", "    <meta charset=\"UTF-8\">", "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">", "    <title>RSA Key Pair Generator</title>", "    <style>", "        body {", "            font-family: Arial, sans-serif;", "            padding: 20px;", "        }", "        #output {", "            margin-top: 20px;", "        }", "        textarea {", "            width: 100%;", "            height: 200px;", "            margin-top: 10px;", "            font-family: monospace;", "            white-space: pre-wrap;", "        }", "        button {", "            padding: 10px;", "            font-size: 16px;", "            cursor: pointer;", "            background-color: #4CAF50;", "            color: white;", "            border: none;", "            border-radius: 5px;", "        }", "        button:hover {", "            background-color: #45a049;", "        }", "        #zipLink {", "            display: none;", "        }", "    </style>", "    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/jszip/3.10.1/jszip.min.js\"></script>", "</head>", "<body>", "    <h1>RSA Key Pair Generator</h1>", "    <p>Click the button below to generate a 2048-bit RSA key pair and download them, along with the key components in decimal format as a ZIP file.</p>", "    <button id=\"generateBtn\">Generate RSA Key Pair</button>", "    <div id=\"output\">", "        <h3>Download Link:</h3>", "        <a id=\"zipLink\" href=\"#\" download=\"rsa_key_pair.zip\">Download RSA Key Pair and Components (ZIP)</a>", "        <h3>Key Components (in Decimal):</h3>", "        <pre id=\"keyComponents\"></pre>", "    </div>", "", "    <script>", "        async function generateRSAKeyPair() {", "            try {", "                // Generate RSA key pair with 2048-bit modulus", "                const keyPair = await window.crypto.subtle.generateKey(", "                    {", "                        name: \"RSA-OAEP\",", "                        modulusLength: 2048, // 2048-bit modulus", "                        publicExponent: new Uint8Array([1, 0, 1]), // 65537 in hexadecimal", "                        hash: { name: \"SHA-256\" },", "                    },", "                    true,", "                    [\"encrypt\", \"decrypt\"]", "                );", "", "                // Export public key in SPKI format (PEM)", "                const publicKeySpki = await window.crypto.subtle.exportKey(\"spki\", keyPair.publicKey);", "                const publicKeyPem = arrayBufferToPem(publicKeySpki, \"PUBLIC KEY\");", "", "                // Export private key in PKCS8 format (PEM)", "                const privateKeyPkcs8 = await window.crypto.subtle.exportKey(\"pkcs8\", keyPair.privateKey);", "                const privateKeyPem = arrayBufferToPem(privateKeyPkcs8, \"PRIVATE KEY\");", "", "                // Extract key components from private key in JWK format", "                const privateKeyJson = await window.crypto.subtle.exportKey(\"jwk\", keyPair.privateKey);", "                const publicKeyJson = await window.crypto.subtle.exportKey(\"jwk\", keyPair.publicKey);", "", "                // Extract modulus, public exponent, private exponent, p, q", "                const n = publicKeyJson.n;  // Modulus (n)", "                const e = publicKeyJson.e;  // Public exponent (e)", "                const d = privateKeyJson.d; // Private exponent (d)", "                const p = privateKeyJson.p; // Prime factor (p)", "                const q = privateKeyJson.q; // Prime factor (q)", "", "                // Convert from Base64URL to Decimal", "                const nDecimal = base64urlToDecimal(n);", "                const eDecimal = base64urlToDecimal(e);", "                const dDecimal = base64urlToDecimal(d);", "                const pDecimal = base64urlToDecimal(p);", "                const qDecimal = base64urlToDecimal(q);", "", "                // Display extracted key components in decimal", "                document.getElementById('keyComponents').textContent = `", "Modulus (N): ${nDecimal}", "Public Exponent (e): ${eDecimal}", "Private Exponent (d): ${dDecimal}", "Prime Factor (p): ${pDecimal}", "Prime Factor (q): ${qDecimal}", "                `;", "", "                // Create ZIP file", "                const zip = new JSZip();", "                zip.file(\"public_key.pem\", publicKeyPem);", "                zip.file(\"private_key.pem\", privateKeyPem);", "                zip.file(\"rsa_key_components.txt\", `", "Modulus (N): ${nDecimal}", "Public Exponent (e): ${eDecimal}", "Private Exponent (d): ${dDecimal}", "Prime Factor (p): ${pDecimal}", "Prime Factor (q): ${qDecimal}", "                `);", "", "                // Generate the ZIP file and create a download link", "                const zipBlob = await zip.generateAsync({ type: \"blob\" });", "                const zipLink = document.getElementById('zipLink');", "                const zipUrl = URL.createObjectURL(zipBlob);", "                zipLink.href = zipUrl;", "                zipLink.style.display = 'block';  // Show the download link", "            } catch (error) {", "                console.error(\"Error generating key pair:\", error);", "                alert(\"Error generating RSA keys.\");", "            }", "        }", "", "        // Convert ArrayBuffer to PEM format", "        function arrayBufferToPem(buffer, type) {", "            const base64 = arrayBufferToBase64(buffer);", "            return `-----BEGIN ${type}-----\\n${base64}\\n-----END ${type}-----`;", "        }", "", "        // Convert ArrayBuffer to Base64 encoded string", "        function arrayBufferToBase64(buffer) {", "            const binary = String.fromCharCode.apply(null, new Uint8Array(buffer));", "            return window.btoa(binary);", "        }", "", "        // Convert Base64URL string to Decimal string", "        function base64urlToDecimal(base64urlStr) {", "            // Replace Base64URL specific characters with standard Base64", "            const base64Str = base64urlStr.replace(/-/g, '+').replace(/_/g, '/');", "            const binaryString = window.atob(base64Str);", "            let decimalValue = BigInt(0);", "", "            for (let i = 0; i < binaryString.length; i++) {", "                const byteValue = binaryString.charCodeAt(i);", "                decimalValue = (decimalValue << 8n) + BigInt(byteValue);", "            }", "", "            return decimalValue.toString();", "        }", "", "        // Add event listener to button", "        document.getElementById('generateBtn').addEventListener('click', generateRSAKeyPair);", "    </script>", "</body>", "</html>"], "file_path": "pkcs8-mytry.js"}
{"Link_to_commit": "https://github.com/HDMax93/LLM-as-a-judge/commit/0aa81fb45e9da7225adc0584d906e939c9896ee7", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 5, "n_files_impacted": 1, "longest_chunk": ["        response = client.chat.completions.create(model=MODEL,", "        messages=[{\"role\": \"system\", \"content\": \"You are an AI assistant.\"},", "                  {\"role\": \"user\", \"content\": prompt}],", "        max_tokens=MAX_TOKENS)", "        return response.choices[0].message.content"], "file_path": "src/data_preprocessing/query_chatgpt.py"}
{"Link_to_commit": "https://github.com/ChrisCRL/Group-2/commit/bbea98702c9acc00d801738344ca5054e1d50c1f", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 30, "n_files_impacted": 1, "longest_chunk": ["#CS 472 ", "#NSHE ID: 5004634201     Franklin La Rosa Diaz", "#Refactored code from ChatGPT", "def get_valid_filename():", "    \"\"\"Prompt the user for a valid filename and return it.\"\"\"", "    while True:", "        fname = input(\"Enter the file name: \\n\")", "        try:", "            with open(fname, 'r'):", "                print(\"File accepted. Loading...\\n\")", "                return fname", "        except IOError:", "            print(\"File not accessible. Try again.\\n\")", "", "def transcribe_dna_to_rna(dna_sequence):", "    \"\"\"Convert DNA sequence to RNA by replacing 'T' with 'U'.\"\"\"", "    return dna_sequence.replace('T', 'U')", "", "def read_file_and_transcribe(fname):", "    \"\"\"Read a DNA sequence file, transcribe it to RNA, and return the result.\"\"\"", "    with open(fname, 'r') as iFile:", "        return [transcribe_dna_to_rna(line.strip()) for line in iFile]", "", "# Actual Program Execution", "filename = get_valid_filename()", "rna_sequences = read_file_and_transcribe(filename)", "", "# Output the result", "for rna_seq in rna_sequences:", "    print(rna_seq)"], "file_path": "gen_ai/task2.2/franklinlarosadiaz_docstring_update.py"}
{"Link_to_commit": "https://github.com/ChrisCRL/Group-2/commit/2d81549d3d52638f8bda10ce20c6250733757fe6", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 30, "n_files_impacted": 1, "longest_chunk": ["#CS 472 ", "#NSHE ID: 5004634201     Franklin La Rosa Diaz", "#Refactored code from ChatGPT", "def get_valid_filename():", "    \"\"\"Prompt the user for a valid filename and return it.\"\"\"", "    while True:", "        fname = input(\"Enter the file name: \\n\")", "        try:", "            with open(fname, 'r'):", "                print(\"File accepted. Loading...\\n\")", "                return fname", "        except IOError:", "            print(\"File not accessible. Try again.\\n\")", "", "def transcribe_dna_to_rna(dna_sequence):", "    \"\"\"Convert DNA sequence to RNA by replacing 'T' with 'U'.\"\"\"", "    return dna_sequence.replace('T', 'U')", "", "def read_file_and_transcribe(fname):", "    \"\"\"Read a DNA sequence file, transcribe it to RNA, and return the result.\"\"\"", "    with open(fname, 'r') as iFile:", "        return [transcribe_dna_to_rna(line.strip()) for line in iFile]", "", "# Actual Program Execution", "filename = get_valid_filename()", "rna_sequences = read_file_and_transcribe(filename)", "", "# Output the result", "for rna_seq in rna_sequences:", "    print(rna_seq)"], "file_path": "gen_ai/task2.2/franklinlarosadiaz_docstring_update.py"}
{"Link_to_commit": "https://github.com/ChrisCRL/Group-2/commit/5798b4a4845e96bd2145fff64f533e0fb544f966", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 11, "n_files_impacted": 1, "longest_chunk": ["    \"\"\"", "    Converts a given DNA sequence to its RNA equivalent by replacing ", "    'T' (thymine) with 'U' (uracil).", "", "    Args:", "        dna_sequence (str): A string representing a DNA sequence.", "    ", "    Returns:", "        str: A string representing the RNA sequence with 'T' replaced by 'U'.", "    \"\"\"", "    return dna_sequence.replace('T', 'U')  # Replace 'T' with 'U'"], "file_path": "gen_ai/task2.2/franklinlarosadiaz_docstring_update.py"}
{"Link_to_commit": "https://github.com/AbdulAlharbi/Group-2/commit/5798b4a4845e96bd2145fff64f533e0fb544f966", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 11, "n_files_impacted": 1, "longest_chunk": ["    \"\"\"", "    Converts a given DNA sequence to its RNA equivalent by replacing ", "    'T' (thymine) with 'U' (uracil).", "", "    Args:", "        dna_sequence (str): A string representing a DNA sequence.", "    ", "    Returns:", "        str: A string representing the RNA sequence with 'T' replaced by 'U'.", "    \"\"\"", "    return dna_sequence.replace('T', 'U')  # Replace 'T' with 'U'"], "file_path": "gen_ai/task2.2/franklinlarosadiaz_docstring_update.py"}
{"Link_to_commit": "https://github.com/LarkBase/jobportal-nexthire-js/commit/1a486e1274dc486bc3dde040ef17f09ab5f00416", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 192, "n_files_impacted": 11, "longest_chunk": ["const prisma = require(\"../config/db\");", "const { generateJobDescriptionWithAI } = require(\"../utils/aiService\");", "", "exports.generateAIJob = async (userId, { title, summary }) => {", "    if (!title || !summary) {", "        throw new Error(\"Title and summary are required\");", "    }", "", "    try {", "        // Get AI-generated job description & structured skills", "        const aiResponse = await generateJobDescriptionWithAI(summary);", "        if (!aiResponse.description || !aiResponse.skills.length) {", "            throw new Error(\"AI failed to generate job details.\");", "        }", "", "        // Store job as draft", "        const job = await prisma.job.create({", "            data: {", "                title,", "                description: aiResponse.description,", "                status: \"DRAFT\",", "                postedById: userId,", "            },", "        });", "", "        // Store AI-generated skills in JobSkills table", "        await prisma.jobSkills.createMany({", "            data: aiResponse.skills.map((skill) => ({", "                jobId: job.id,", "                skill: skill.name,", "                isMandatory: skill.mandatory,", "                category: skill.category,", "            })),", "        });", "", "        // Log AI generation in AuditLog", "        await prisma.auditLog.create({", "            data: {", "                action: `AI-generated job: ${title}`,", "                performedById: userId,", "            },", "        });", "", "        return job;", "    } catch (error) {", "        console.error(\"Error in generateAIJob:\", error);", "        throw new Error(\"Failed to generate AI job.\");", "    }", "};", "", "exports.getAllJobs = async () => {", "    try {", "        return await prisma.job.findMany({", "            include: { skills: true },", "        });", "    } catch (error) {", "        console.error(\"Error fetching all jobs:\", error);", "        throw new Error(\"Failed to fetch jobs\");", "    }", "};", "", "exports.getApprovedJobs = async () => {", "    try {", "        return await prisma.job.findMany({", "            where: { status: \"APPROVED\" },", "            include: { skills: true },", "        });", "    } catch (error) {", "        console.error(\"Error fetching approved jobs:\", error);", "        throw new Error(\"Failed to fetch approved jobs\");", "    }", "};", "", "exports.approveJob = async (jobId) => {", "    try {", "        return await prisma.job.update({", "            where: { id: jobId },", "            data: { status: \"APPROVED\" },", "        });", "    } catch (error) {", "        console.error(\"Error approving job:\", error);", "        throw new Error(\"Failed to approve job\");", "    }", "};", "", "exports.rejectJob = async (jobId) => {", "    try {", "        await prisma.jobSkills.deleteMany({ where: { jobId } });", "        await prisma.job.delete({ where: { id: jobId } });", "    } catch (error) {", "        console.error(\"Error rejecting job:\", error);", "        throw new Error(\"Failed to reject job\");", "    }", "};", "", "exports.deleteJob = async (jobId) => {", "    try {", "        await prisma.jobSkills.deleteMany({ where: { jobId } });", "        await prisma.job.delete({ where: { id: jobId } });", "    } catch (error) {", "        console.error(\"Error deleting job:\", error);", "        throw new Error(\"Failed to delete job\");", "    }", "};", "", "exports.getDraftJobs = async () => {", "    try {", "        return await prisma.job.findMany({ where: { status: \"DRAFT\" } });", "    } catch (error) {", "        console.error(\"Error fetching draft jobs:\", error);", "        throw new Error(\"Failed to fetch draft jobs\");", "    }", "};", "", "exports.getJobById = async (jobId) => {", "    try {", "        const job = await prisma.job.findUnique({", "            where: { id: jobId },", "            include: { skills: true },", "        });", "", "        if (!job) throw new Error(\"Job not found\");", "        return job;", "    } catch (error) {", "        console.error(\"Error fetching job by ID:\", error);", "        throw new Error(\"Failed to fetch job\");", "    }", "};", "", "exports.updateJob = async (jobId, { title, description, skills }) => {", "    try {", "        // Update job details", "        const job = await prisma.job.update({", "            where: { id: jobId },", "            data: {", "                title,", "                description,", "            },", "        });", "", "        if (skills && skills.length > 0) {", "            // Get existing skills for the job", "            const existingSkills = await prisma.jobSkills.findMany({", "                where: { jobId },", "            });", "", "            const existingSkillIds = existingSkills.map(skill => skill.id);", "            const newSkills = [];", "            const updatedSkills = [];", "", "            skills.forEach(skill => {", "                if (skill.id && existingSkillIds.includes(skill.id)) {", "                    // If skill exists, update it", "                    updatedSkills.push(", "                        prisma.jobSkills.update({", "                            where: { id: skill.id },", "                            data: {", "                                skill: skill.skill,", "                                isMandatory: skill.isMandatory,", "                                category: skill.category,", "                            },", "                        })", "                    );", "                } else {", "                    // If skill does not exist, add it", "                    newSkills.push({", "                        jobId,", "                        skill: skill.skill,", "                        isMandatory: skill.isMandatory,", "                        category: skill.category,", "                    });", "                }", "            });", "", "            // Delete removed skills (skills that are not in the new request)", "            const skillsToDelete = existingSkills", "                .filter(skill => !skills.some(s => s.id === skill.id))", "                .map(skill => skill.id);", "", "            await prisma.$transaction([", "                ...updatedSkills, // Update existing skills", "                prisma.jobSkills.createMany({ data: newSkills }), // Insert new skills", "                prisma.jobSkills.deleteMany({ where: { id: { in: skillsToDelete } } }) // Delete missing skills", "            ]);", "        }", "", "        return job;", "    } catch (error) {", "        console.error(\"Error updating job:\", error);", "        throw new Error(\"Failed to update job\");", "    }", "};"], "file_path": "backend/user-service/src/utils/aiService.js"}
{"Link_to_commit": "https://github.com/Levi-Spellmeyer/Financial-Application/commit/fbe09e591750438c7c42349bcc59e2d6008f3490", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 38, "n_files_impacted": 3, "longest_chunk": ["import aiosmtpd", "from aiosmtpd.handlers import Message", "from email.parser import Parser", "", "class EmailHandler(Message):", "    def __init__(self):", "        super().__init__()", "", "    async def handle_DATA(self, server, session, envelope):", "        # Parse the email message content", "        message = Parser().parsestr(envelope.content.decode())", "        subject = message.get(\"Subject\", \"No Subject\")", "        sender = message.get(\"From\", \"Unknown Sender\")", "        recipient = message.get(\"To\", \"Unknown Recipient\")", "        body = envelope.content.decode()", "", "        # Create or append to a .txt file", "        with open(\"emails_received.txt\", \"a\") as file:", "            file.write(f\"--- New Email ---\\n\")", "            file.write(f\"From: {sender}\\n\")", "            file.write(f\"To: {recipient}\\n\")", "            file.write(f\"Subject: {subject}\\n\")", "            file.write(f\"Body:\\n{body}\\n\")", "            file.write(\"-\" * 40 + \"\\n\")", "", "        print(f\"\u2705 Email saved to emails_received.txt\")", "        return \"250 Message accepted for delivery\"", "", "# Set up the SMTP server", "async def run_server():", "    handler = EmailHandler()", "    server = aiosmtpd.controller.Controller(handler, hostname='localhost', port=1025)", "    server.start()", "    print(\"\ud83d\ude80 Local SMTP server running on localhost:1025\")", "", "if __name__ == \"__main__\":", "    import asyncio", "    asyncio.run(run_server())"], "file_path": "Project-Files/smtp_server.py"}
{"Link_to_commit": "https://github.com/ThatNinjaGuy/linkedin-job-assist/commit/472d42e305b5e217f78096909fbd3dde0a5c392b", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 25, "n_files_impacted": 8, "longest_chunk": ["import axios from \"axios\";", "", "const CHATGPT_API_URL = \"https://api.openai.com/v1/chat/completions\";", "const CHATGPT_MODEL = \"gpt-3.5-turbo\";", "", "export const postChatGptMessage = async (message, openAiKey) => {", "  const headers = {", "    Authorization: `Bearer ${openAiKey}`,", "  };", "  const userMessage = { role: \"user\", content: message };", "  const chatGptMessage = {", "    model: CHATGPT_MODEL,", "    messages: [userMessage],", "  };", "", "  try {", "    const response = await axios.post(CHATGPT_API_URL, chatGptMessage, {", "      headers,", "    });", "    return response?.data?.choices[0]?.message?.content;", "  } catch (error) {", "    console.error(\"Error calling ChatGPT API:\", error);", "    return null;", "  }", "};"], "file_path": "src/utils/chatGPTUtil.js"}
{"Link_to_commit": "https://github.com/CS222-UIUC/team-69-project/commit/1c32cdcc490a0ccc5cc7293d2881f8b79b86859b", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 166, "n_files_impacted": 4, "longest_chunk": ["from datetime import datetime", "from backend.scripts.matchingalgo import (", "    User,", "    parse_pg_array,", "    parse_pg_dict,", "    fetch_users_from_db,", "    get_match_score_and_tier,", ")", "", "from rapidfuzz import fuzz", "", "", "def search_user_by_name(cursor, name):", "    cursor.execute(\"SELECT * FROM users WHERE display_name ILIKE %s;\", ('%' + name + '%',))", "    rows = cursor.fetchall()", "", "    users = []", "    for row in rows:", "        user = User(", "            user_id=row[0],", "            display_name=row[1],", "            major=row[3],", "            year=row[4],", "            rating=float(row[5]) if row[5] is not None else 0.0,", "            total_ratings=row[6],", "            rating_history=parse_pg_array(row[7]),", "            show_as_backup=row[8],", "            classes_can_tutor=parse_pg_array(row[9]),", "            classes_needed=parse_pg_array(row[10]),", "            recent_interactions=parse_pg_array(row[11]),", "            class_ratings=parse_pg_dict(row[12]),", "        )", "        user.recent_interactions = [", "            datetime.strptime(ts, \"%Y-%m-%d %H:%M:%S.%f\") if isinstance(ts, str) else ts", "            for ts in user.recent_interactions", "        ]", "        users.append(user)", "    return users", "", "", "def search_all_users_by_subject(cursor, conn, current_user_id, subject_query):", "    all_users = fetch_users_from_db(cursor)", "    user_map = {u.user_id: u for u in all_users}", "    current_user = user_map.get(current_user_id)", "    if not current_user:", "        return []", "", "    subject_query_lower = subject_query.lower()", "", "    cursor.execute(\"\"\"", "        SELECT matched_user_id", "        FROM matches", "        WHERE requester_id = %s", "        ORDER BY match_score DESC", "        LIMIT 10;", "    \"\"\", (current_user_id,))", "    top_10_matches = {row[0] for row in cursor.fetchall()}", "", "    matching_users = []", "", "    for other_user in all_users:", "        if other_user.user_id == current_user.user_id:", "            continue", "", "        matching_subjects = [", "            cls for cls in other_user.classes_can_tutor", "            if subject_query_lower in cls.lower()  # Only exact substring matching here", "        ]", "        if not matching_subjects:", "            continue", "", "        if not (set(other_user.classes_can_tutor) & set(current_user.classes_needed)):", "            continue", "", "        if (", "            not (set(current_user.classes_can_tutor) & set(other_user.classes_needed))", "            and not other_user.show_as_backup", "        ):", "            continue", "", "        raw_score, tier = get_match_score_and_tier(other_user, current_user)", "", "        if other_user.user_id in top_10_matches:", "            priority = 1", "        elif other_user.major == current_user.major:", "            priority = 2", "        else:", "            priority = 3", "", "        matching_users.append((priority, raw_score, other_user, tier, matching_subjects))", "", "    matching_users.sort(key=lambda x: (x[0], -x[1]))", "", "    result = []", "    for priority, score, user_obj, tier, matched_classes in matching_users:", "        result.append((user_obj, score, tier, matched_classes, priority))", "", "    return result", "", "", "def search_top_5_users_by_subject(cursor, conn, current_user_id, subject_query):", "    all_users = fetch_users_from_db(cursor)", "    user_map = {u.user_id: u for u in all_users}", "    current_user = user_map.get(current_user_id)", "    if not current_user:", "        return []", "", "    subject_query_lower = subject_query.lower()", "", "    cursor.execute(\"\"\"", "        SELECT matched_user_id", "        FROM matches", "        WHERE requester_id = %s", "        ORDER BY match_score DESC", "        LIMIT 10;", "    \"\"\", (current_user_id,))", "    top_10_matches = {row[0] for row in cursor.fetchall()}", "", "    matching_users = []", "", "    for other_user in all_users:", "        if other_user.user_id == current_user.user_id:", "            continue", "", "        matching_subjects = []", "        for cls in other_user.classes_can_tutor:", "            #Hybrid fuzzy match: max(partial, token sort)", "            similarity = max(", "                fuzz.partial_ratio(subject_query_lower, cls.lower()),", "                fuzz.token_sort_ratio(subject_query_lower, cls.lower())", "            )", "            if similarity >= 70:  # higher is stricter, lower is more matches", "                matching_subjects.append(cls)", "", "        if not matching_subjects:", "            continue", "", "        if not (set(other_user.classes_can_tutor) & set(current_user.classes_needed)):", "            continue", "", "        if (", "            not (set(current_user.classes_can_tutor) & set(other_user.classes_needed))", "            and not other_user.show_as_backup", "        ):", "            continue", "", "        raw_score, tier = get_match_score_and_tier(other_user, current_user)", "", "        if other_user.user_id in top_10_matches:", "            priority = 1", "        elif other_user.major == current_user.major:", "            priority = 2", "        else:", "            priority = 3", "", "        matching_users.append((priority, raw_score, other_user, tier, matching_subjects))", "", "    matching_users.sort(key=lambda x: (x[0], -x[1]))", "", "    matching_users = matching_users[:5]  # Limit to top 5", "", "    result = []", "    for priority, score, user_obj, tier, matched_classes in matching_users:", "        result.append((user_obj, score, tier, matched_classes, priority))", "", "    return result"], "file_path": "backend/scripts/search.py"}
{"Link_to_commit": "https://github.com/muhammadrazaz/bot_subscription_backend/commit/283a57dca11c0d10971784db0d6345c85e6904b7", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 13, "n_files_impacted": 2, "longest_chunk": ["        {\"role\": \"system\", \"content\": prompt},", "        {", "      \"role\": \"user\",", "      \"content\": [", "        {\"type\": \"text\", \"text\": message},", "        {", "          \"type\": \"image_url\",", "          \"image_url\": {", "            \"url\": f\"data:image/jpeg;base64,{img_data}\",", "          },", "        },", "      ],", "    }"], "file_path": "instagram/views.py"}
{"Link_to_commit": "https://github.com/MariusJorgensen/cheff-lib/commit/d5a4c112200d8d43aa07141386103d789b5ec712", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 72, "n_files_impacted": 4, "longest_chunk": ["", "import \"https://deno.land/x/xhr@0.1.0/mod.ts\";", "import { serve } from \"https://deno.land/std@0.168.0/http/server.ts\";", "", "const openAIApiKey = Deno.env.get('OPENAI_API_KEY');", "", "const corsHeaders = {", "  'Access-Control-Allow-Origin': '*',", "  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',", "};", "", "serve(async (req) => {", "  if (req.method === 'OPTIONS') {", "    return new Response(null, { headers: corsHeaders });", "  }", "", "  try {", "    const { title, author, description } = await req.json();", "", "    const bookPrompt = `Write a concise (2-3 sentences) description of the book \"${title}\" by ${author}. If relevant, mention its significance or impact. Base it on this information: ${description}`;", "    const authorPrompt = `Write a concise (2-3 sentences) professional biography of ${author}. Focus on their expertise, background, and notable works including \"${title}\".`;", "", "    const [bookResponse, authorResponse] = await Promise.all([", "      fetch('https://api.openai.com/v1/chat/completions', {", "        method: 'POST',", "        headers: {", "          'Authorization': `Bearer ${openAIApiKey}`,", "          'Content-Type': 'application/json',", "        },", "        body: JSON.stringify({", "          model: 'gpt-4o-mini',", "          messages: [", "            { role: 'system', content: 'You are a professional book critic and biographer.' },", "            { role: 'user', content: bookPrompt }", "          ],", "        }),", "      }),", "      fetch('https://api.openai.com/v1/chat/completions', {", "        method: 'POST',", "        headers: {", "          'Authorization': `Bearer ${openAIApiKey}`,", "          'Content-Type': 'application/json',", "        },", "        body: JSON.stringify({", "          model: 'gpt-4o-mini',", "          messages: [", "            { role: 'system', content: 'You are a professional book critic and biographer.' },", "            { role: 'user', content: authorPrompt }", "          ],", "        }),", "      })", "    ]);", "", "    const [bookData, authorData] = await Promise.all([", "      bookResponse.json(),", "      authorResponse.json()", "    ]);", "", "    return new Response(JSON.stringify({", "      bookDescription: bookData.choices[0].message.content.trim(),", "      authorDescription: authorData.choices[0].message.content.trim()", "    }), {", "      headers: { ...corsHeaders, 'Content-Type': 'application/json' },", "    });", "  } catch (error) {", "    console.error('Error generating descriptions:', error);", "    return new Response(JSON.stringify({ error: error.message }), {", "      status: 500,", "      headers: { ...corsHeaders, 'Content-Type': 'application/json' },", "    });", "  }", "});"], "file_path": "supabase/functions/generate-book-descriptions/index.ts"}
{"Link_to_commit": "https://github.com/cobolbaby/schema-proxy/commit/4b85d6609a9c0fc231c78fbf2655cacaed80e034", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 202, "n_files_impacted": 12, "longest_chunk": ["github.com/Azure/go-ansiterm v0.0.0-20230124172434-306776ec8161 h1:L/gRVlceqvL25UVaW/CKtUDjefjrs0SPonmDGUVOYP0=", "github.com/Azure/go-ansiterm v0.0.0-20230124172434-306776ec8161/go.mod h1:xomTg63KZ2rFqZQzSB4Vz2SUXa1BpHTVz9L5PTmPC4E=", "github.com/Microsoft/go-winio v0.6.2 h1:F2VQgta7ecxGYO8k3ZZz3RS8fVIXVxONVUPlNERoyfY=", "github.com/Microsoft/go-winio v0.6.2/go.mod h1:yd8OoFMLzJbo9gZq8j5qaps8bJ9aShtEA8Ipt1oGCvU=", "github.com/bytedance/sonic v1.11.6 h1:oUp34TzMlL+OY1OUWxHqsdkgC/Zfc85zGqw9siXjrc0=", "github.com/bytedance/sonic v1.11.6/go.mod h1:LysEHSvpvDySVdC2f87zGWf6CIKJcAvqab1ZaiQtds4=", "github.com/bytedance/sonic/loader v0.1.1 h1:c+e5Pt1k/cy5wMveRDyk2X4B9hF4g7an8N3zCYjJFNM=", "github.com/bytedance/sonic/loader v0.1.1/go.mod h1:ncP89zfokxS5LZrJxl5z0UJcsk4M4yY2JpfqGeCtNLU=", "github.com/cloudwego/base64x v0.1.4 h1:jwCgWpFanWmN8xoIUHa2rtzmkd5J2plF/dnLS6Xd/0Y=", "github.com/cloudwego/base64x v0.1.4/go.mod h1:0zlkT4Wn5C6NdauXdJRhSKRlJvmclQ1hhJgA0rcu/8w=", "github.com/cloudwego/iasm v0.2.0 h1:1KNIy1I1H9hNNFEEH3DVnI4UujN+1zjpuk6gwHLTssg=", "github.com/cloudwego/iasm v0.2.0/go.mod h1:8rXZaNYT2n95jn+zTI1sDr+IgcD2GVs0nlbbQPiEFhY=", "github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=", "github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=", "github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc h1:U9qPSI2PIWSS1VwoXQT9A3Wy9MM3WgvqSxFWenqJduM=", "github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=", "github.com/dhui/dktest v0.4.3 h1:wquqUxAFdcUgabAVLvSCOKOlag5cIZuaOjYIBOWdsR0=", "github.com/dhui/dktest v0.4.3/go.mod h1:zNK8IwktWzQRm6I/l2Wjp7MakiyaFWv4G1hjmodmMTs=", "github.com/distribution/reference v0.6.0 h1:0IXCQ5g4/QMHHkarYzh5l+u8T3t73zM5QvfrDyIgxBk=", "github.com/distribution/reference v0.6.0/go.mod h1:BbU0aIcezP1/5jX/8MP0YiH4SdvB5Y4f/wlDRiLyi3E=", "github.com/docker/docker v27.2.0+incompatible h1:Rk9nIVdfH3+Vz4cyI/uhbINhEZ/oLmc+CBXmH6fbNk4=", "github.com/docker/docker v27.2.0+incompatible/go.mod h1:eEKB0N0r5NX/I1kEveEz05bcu8tLC/8azJZsviup8Sk=", "github.com/docker/go-connections v0.5.0 h1:USnMq7hx7gwdVZq1L49hLXaFtUdTADjXGp+uj1Br63c=", "github.com/docker/go-connections v0.5.0/go.mod h1:ov60Kzw0kKElRwhNs9UlUHAE/F9Fe6GLaXnqyDdmEXc=", "github.com/docker/go-units v0.5.0 h1:69rxXcBk27SvSaaxTtLh/8llcHD8vYHT7WSdRZ/jvr4=", "github.com/docker/go-units v0.5.0/go.mod h1:fgPhTUdO+D/Jk86RDLlptpiXQzgHJF7gydDDbaIK4Dk=", "github.com/dustin/go-humanize v1.0.1 h1:GzkhY7T5VNhEkwH0PVJgjz+fX1rhBrR7pRT3mDkpeCY=", "github.com/dustin/go-humanize v1.0.1/go.mod h1:Mu1zIs6XwVuF/gI1OepvI0qD18qycQx+mFykh5fBlto=", "github.com/felixge/httpsnoop v1.0.4 h1:NFTV2Zj1bL4mc9sqWACXbQFVBBg2W3GPvqp8/ESS2Wg=", "github.com/felixge/httpsnoop v1.0.4/go.mod h1:m8KPJKqk1gH5J9DgRY2ASl2lWCfGKXixSwevea8zH2U=", "github.com/frankban/quicktest v1.14.6 h1:7Xjx+VpznH+oBnejlPUj8oUpdxnVs4f8XU8WnHkI4W8=", "github.com/frankban/quicktest v1.14.6/go.mod h1:4ptaffx2x8+WTWXmUCuVU6aPUX1/Mz7zb5vbUoiM6w0=", "github.com/fsnotify/fsnotify v1.7.0 h1:8JEhPFa5W2WU7YfeZzPNqzMP6Lwt7L2715Ggo0nosvA=", "github.com/fsnotify/fsnotify v1.7.0/go.mod h1:40Bi/Hjc2AVfZrqy+aj+yEI+/bRxZnMJyTJwOpGvigM=", "github.com/gabriel-vasile/mimetype v1.4.3 h1:in2uUcidCuFcDKtdcBxlR0rJ1+fsokWf+uqxgUFjbI0=", "github.com/gabriel-vasile/mimetype v1.4.3/go.mod h1:d8uq/6HKRL6CGdk+aubisF/M5GcPfT7nKyLpA0lbSSk=", "github.com/gin-contrib/sse v0.1.0 h1:Y/yl/+YNO8GZSjAhjMsSuLt29uWRFHdHYUb5lYOV9qE=", "github.com/gin-contrib/sse v0.1.0/go.mod h1:RHrZQHXnP2xjPF+u1gW/2HnVO7nvIa9PG3Gm+fLHvGI=", "github.com/gin-gonic/gin v1.10.0 h1:nTuyha1TYqgedzytsKYqna+DfLos46nTv2ygFy86HFU=", "github.com/gin-gonic/gin v1.10.0/go.mod h1:4PMNQiOhvDRa013RKVbsiNwoyezlm2rm0uX/T7kzp5Y=", "github.com/go-ini/ini v1.67.0 h1:z6ZrTEZqSWOTyH2FlglNbNgARyHG8oLW9gMELqKr06A=", "github.com/go-ini/ini v1.67.0/go.mod h1:ByCAeIL28uOIIG0E3PJtZPDL8WnHpFKFOtgjp+3Ies8=", "github.com/go-logr/logr v1.4.2 h1:6pFjapn8bFcIbiKo3XT4j/BhANplGihG6tvd+8rYgrY=", "github.com/go-logr/logr v1.4.2/go.mod h1:9T104GzyrTigFIr8wt5mBrctHMim0Nb2HLGrmQ40KvY=", "github.com/go-logr/stdr v1.2.2 h1:hSWxHoqTgW2S2qGc0LTAI563KZ5YKYRhT3MFKZMbjag=", "github.com/go-logr/stdr v1.2.2/go.mod h1:mMo/vtBO5dYbehREoey6XUKy/eSumjCCveDpRre4VKE=", "github.com/go-playground/assert/v2 v2.2.0 h1:JvknZsQTYeFEAhQwI4qEt9cyV5ONwRHC+lYKSsYSR8s=", "github.com/go-playground/assert/v2 v2.2.0/go.mod h1:VDjEfimB/XKnb+ZQfWdccd7VUvScMdVu0Titje2rxJ4=", "github.com/go-playground/locales v0.14.1 h1:EWaQ/wswjilfKLTECiXz7Rh+3BjFhfDFKv/oXslEjJA=", "github.com/go-playground/locales v0.14.1/go.mod h1:hxrqLVvrK65+Rwrd5Fc6F2O76J/NuW9t0sjnWqG1slY=", "github.com/go-playground/universal-translator v0.18.1 h1:Bcnm0ZwsGyWbCzImXv+pAJnYK9S473LQFuzCbDbfSFY=", "github.com/go-playground/universal-translator v0.18.1/go.mod h1:xekY+UJKNuX9WP91TpwSH2VMlDf28Uj24BCp08ZFTUY=", "github.com/go-playground/validator/v10 v10.20.0 h1:K9ISHbSaI0lyB2eWMPJo+kOS/FBExVwjEviJTixqxL8=", "github.com/go-playground/validator/v10 v10.20.0/go.mod h1:dbuPbCMFw/DrkbEynArYaCwl3amGuJotoKCe95atGMM=", "github.com/goccy/go-json v0.10.3 h1:KZ5WoDbxAIgm2HNbYckL0se1fHD6rz5j4ywS6ebzDqA=", "github.com/goccy/go-json v0.10.3/go.mod h1:oq7eo15ShAhp70Anwd5lgX2pLfOS3QCiwU/PULtXL6M=", "github.com/gogo/protobuf v1.3.2 h1:Ov1cvc58UF3b5XjBnZv7+opcTcQFZebYjWzi34vdm4Q=", "github.com/gogo/protobuf v1.3.2/go.mod h1:P1XiOD3dCwIKUDQYPy72D8LYyHL2YPYrpS2s69NZV8Q=", "github.com/golang-migrate/migrate/v4 v4.18.1 h1:JML/k+t4tpHCpQTCAD62Nu43NUFzHY4CV3uAuvHGC+Y=", "github.com/golang-migrate/migrate/v4 v4.18.1/go.mod h1:HAX6m3sQgcdO81tdjn5exv20+3Kb13cmGli1hrD6hks=", "github.com/google/go-cmp v0.5.9 h1:O2Tfq5qg4qc4AmwVlvv0oLiVAGB7enBSJ2x2DqQFi38=", "github.com/google/go-cmp v0.5.9/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=", "github.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=", "github.com/google/uuid v1.6.0 h1:NIvaJDMOsjHA8n1jAhLSgzrAzy1Hgr+hNrb57e+94F0=", "github.com/google/uuid v1.6.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=", "github.com/hashicorp/errwrap v1.0.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=", "github.com/hashicorp/errwrap v1.1.0 h1:OxrOeh75EUXMY8TBjag2fzXGZ40LB6IKw45YeGUDY2I=", "github.com/hashicorp/errwrap v1.1.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=", "github.com/hashicorp/go-multierror v1.1.1 h1:H5DkEtf6CXdFp0N0Em5UCwQpXMWke8IA0+lD48awMYo=", "github.com/hashicorp/go-multierror v1.1.1/go.mod h1:iw975J/qwKPdAO1clOe2L8331t/9/fmwbPZ6JB6eMoM=", "github.com/hashicorp/hcl v1.0.0 h1:0Anlzjpi4vEasTeNFn2mLJgTSwt0+6sfsiTG8qcWGx4=", "github.com/hashicorp/hcl v1.0.0/go.mod h1:E5yfLk+7swimpb2L/Alb/PJmXilQ/rhwaUYs4T20WEQ=", "github.com/json-iterator/go v1.1.12 h1:PV8peI4a0ysnczrg+LtxykD8LfKY9ML6u2jnxaEnrnM=", "github.com/json-iterator/go v1.1.12/go.mod h1:e30LSqwooZae/UwlEbR2852Gd8hjQvJoHmT4TnhNGBo=", "github.com/klauspost/compress v1.17.9 h1:6KIumPrER1LHsvBVuDa0r5xaG0Es51mhhB9BQB2qeMA=", "github.com/klauspost/compress v1.17.9/go.mod h1:Di0epgTjJY877eYKx5yC51cX2A2Vl2ibi7bDH9ttBbw=", "github.com/klauspost/cpuid/v2 v2.0.1/go.mod h1:FInQzS24/EEf25PyTYn52gqo7WaD8xa0213Md/qVLRg=", "github.com/klauspost/cpuid/v2 v2.0.9/go.mod h1:FInQzS24/EEf25PyTYn52gqo7WaD8xa0213Md/qVLRg=", "github.com/klauspost/cpuid/v2 v2.2.8 h1:+StwCXwm9PdpiEkPyzBXIy+M9KUb4ODm0Zarf1kS5BM=", "github.com/klauspost/cpuid/v2 v2.2.8/go.mod h1:Lcz8mBdAVJIBVzewtcLocK12l3Y+JytZYpaMropDUws=", "github.com/knz/go-libedit v1.10.1/go.mod h1:MZTVkCWyz0oBc7JOWP3wNAzd002ZbM/5hgShxwh4x8M=", "github.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=", "github.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=", "github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=", "github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=", "github.com/leodido/go-urn v1.4.0 h1:WT9HwE9SGECu3lg4d/dIA+jxlljEa1/ffXKmRjqdmIQ=", "github.com/leodido/go-urn v1.4.0/go.mod h1:bvxc+MVxLKB4z00jd1z+Dvzr47oO32F/QSNjSBOlFxI=", "github.com/lib/pq v1.10.9 h1:YXG7RB+JIjhP29X+OtkiDnYaXQwpS4JEWq7dtCCRUEw=", "github.com/lib/pq v1.10.9/go.mod h1:AlVN5x4E4T544tWzH6hKfbfQvm3HdbOxrmggDNAPY9o=", "github.com/magiconair/properties v1.8.7 h1:IeQXZAiQcpL9mgcAe1Nu6cX9LLw6ExEHKjN0VQdvPDY=", "github.com/magiconair/properties v1.8.7/go.mod h1:Dhd985XPs7jluiymwWYZ0G4Z61jb3vdS329zhj2hYo0=", "github.com/mattn/go-isatty v0.0.20 h1:xfD0iDuEKnDkl03q4limB+vH+GxLEtL/jb4xVJSWWEY=", "github.com/mattn/go-isatty v0.0.20/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=", "github.com/minio/md5-simd v1.1.2 h1:Gdi1DZK69+ZVMoNHRXJyNcxrMA4dSxoYHZSQbirFg34=", "github.com/minio/md5-simd v1.1.2/go.mod h1:MzdKDxYpY2BT9XQFocsiZf/NKVtR7nkE4RoEpN+20RM=", "github.com/minio/minio-go/v7 v7.0.76 h1:9nxHH2XDai61cT/EFhyIw/wW4vJfpPNvl7lSFpRt+Ng=", "github.com/minio/minio-go/v7 v7.0.76/go.mod h1:AVM3IUN6WwKzmwBxVdjzhH8xq+f57JSbbvzqvUzR6eg=", "github.com/mitchellh/mapstructure v1.5.0 h1:jeMsZIYE/09sWLaz43PL7Gy6RuMjD2eJVyuac5Z2hdY=", "github.com/mitchellh/mapstructure v1.5.0/go.mod h1:bFUtVrKA4DC2yAKiSyO/QUcy7e+RRV2QTWOzhPopBRo=", "github.com/moby/docker-image-spec v1.3.1 h1:jMKff3w6PgbfSa69GfNg+zN/XLhfXJGnEx3Nl2EsFP0=", "github.com/moby/docker-image-spec v1.3.1/go.mod h1:eKmb5VW8vQEh/BAr2yvVNvuiJuY6UIocYsFu/DxxRpo=", "github.com/moby/term v0.5.0 h1:xt8Q1nalod/v7BqbG21f8mQPqH+xAaC9C3N3wfWbVP0=", "github.com/moby/term v0.5.0/go.mod h1:8FzsFHVUBGZdbDsJw/ot+X+d5HLUbvklYLJ9uGfcI3Y=", "github.com/modern-go/concurrent v0.0.0-20180228061459-e0a39a4cb421/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=", "github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd h1:TRLaZ9cD/w8PVh93nsPXa1VrQ6jlwL5oN8l14QlcNfg=", "github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=", "github.com/modern-go/reflect2 v1.0.2 h1:xBagoLtFs94CBntxluKeaWgTMpvLxC4ur3nMaC9Gz0M=", "github.com/modern-go/reflect2 v1.0.2/go.mod h1:yWuevngMOJpCy52FWWMvUC8ws7m/LJsjYzDa0/r8luk=", "github.com/morikuni/aec v1.0.0 h1:nP9CBfwrvYnBRgY6qfDQkygYDmYwOilePFkwzv4dU8A=", "github.com/morikuni/aec v1.0.0/go.mod h1:BbKIizmSmc5MMPqRYbxO4ZU0S0+P200+tUnFx7PXmsc=", "github.com/opencontainers/go-digest v1.0.0 h1:apOUWs51W5PlhuyGyz9FCeeBIOUDA/6nW8Oi/yOhh5U=", "github.com/opencontainers/go-digest v1.0.0/go.mod h1:0JzlMkj0TRzQZfJkVvzbP0HBR3IKzErnv2BNG4W4MAM=", "github.com/opencontainers/image-spec v1.1.0 h1:8SG7/vwALn54lVB/0yZ/MMwhFrPYtpEHQb2IpWsCzug=", "github.com/opencontainers/image-spec v1.1.0/go.mod h1:W4s4sFTMaBeK1BQLXbG4AdM2szdn85PY75RI83NrTrM=", "github.com/pelletier/go-toml/v2 v2.2.2 h1:aYUidT7k73Pcl9nb2gScu7NSrKCSHIDE89b3+6Wq+LM=", "github.com/pelletier/go-toml/v2 v2.2.2/go.mod h1:1t835xjRzz80PqgE6HHgN2JOsmgYu/h4qDAS4n929Rs=", "github.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=", "github.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=", "github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=", "github.com/pmezard/go-difflib v1.0.1-0.20181226105442-5d4384ee4fb2 h1:Jamvg5psRIccs7FGNTlIRMkT8wgtp5eCXdBlqhYGL6U=", "github.com/pmezard/go-difflib v1.0.1-0.20181226105442-5d4384ee4fb2/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=", "github.com/rogpeppe/go-internal v1.12.0 h1:exVL4IDcn6na9z1rAb56Vxr+CgyK3nn3O+epU5NdKM8=", "github.com/rogpeppe/go-internal v1.12.0/go.mod h1:E+RYuTGaKKdloAfM02xzb0FW3Paa99yedzYV+kq4uf4=", "github.com/rs/xid v1.6.0 h1:fV591PaemRlL6JfRxGDEPl69wICngIQ3shQtzfy2gxU=", "github.com/rs/xid v1.6.0/go.mod h1:7XoLgs4eV+QndskICGsho+ADou8ySMSjJKDIan90Nz0=", "github.com/sagikazarmark/locafero v0.4.0 h1:HApY1R9zGo4DBgr7dqsTH/JJxLTTsOt7u6keLGt6kNQ=", "github.com/sagikazarmark/locafero v0.4.0/go.mod h1:Pe1W6UlPYUk/+wc/6KFhbORCfqzgYEpgQ3O5fPuL3H4=", "github.com/sagikazarmark/slog-shim v0.1.0 h1:diDBnUNK9N/354PgrxMywXnAwEr1QZcOr6gto+ugjYE=", "github.com/sagikazarmark/slog-shim v0.1.0/go.mod h1:SrcSrq8aKtyuqEI1uvTDTK1arOWRIczQRv+GVI1AkeQ=", "github.com/sourcegraph/conc v0.3.0 h1:OQTbbt6P72L20UqAkXXuLOj79LfEanQ+YQFNpLA9ySo=", "github.com/sourcegraph/conc v0.3.0/go.mod h1:Sdozi7LEKbFPqYX2/J+iBAM6HpqSLTASQIKqDmF7Mt0=", "github.com/spf13/afero v1.11.0 h1:WJQKhtpdm3v2IzqG8VMqrr6Rf3UYpEF239Jy9wNepM8=", "github.com/spf13/afero v1.11.0/go.mod h1:GH9Y3pIexgf1MTIWtNGyogA5MwRIDXGUr+hbWNoBjkY=", "github.com/spf13/cast v1.6.0 h1:GEiTHELF+vaR5dhz3VqZfFSzZjYbgeKDpBxQVS4GYJ0=", "github.com/spf13/cast v1.6.0/go.mod h1:ancEpBxwJDODSW/UG4rDrAqiKolqNNh2DX3mk86cAdo=", "github.com/spf13/pflag v1.0.5 h1:iy+VFUOCP1a+8yFto/drg2CJ5u0yRoB7fZw3DKv/JXA=", "github.com/spf13/pflag v1.0.5/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=", "github.com/spf13/viper v1.19.0 h1:RWq5SEjt8o25SROyN3z2OrDB9l7RPd3lwTWU8EcEdcI=", "github.com/spf13/viper v1.19.0/go.mod h1:GQUN9bilAbhU/jgc1bKs99f/suXKeUMct8Adx5+Ntkg=", "github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=", "github.com/stretchr/objx v0.4.0/go.mod h1:YvHI0jy2hoMjB+UWwv71VJQ9isScKT/TqJzVSSt89Yw=", "github.com/stretchr/objx v0.5.0/go.mod h1:Yh+to48EsGEfYuaHDzXPcE3xhTkx73EhmCGUpEOglKo=", "github.com/stretchr/objx v0.5.2/go.mod h1:FRsXN1f5AsAjCGJKqEizvkpNtU+EGNCLh3NxZ/8L+MA=", "github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=", "github.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=", "github.com/stretchr/testify v1.7.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=", "github.com/stretchr/testify v1.8.0/go.mod h1:yNjHg4UonilssWZ8iaSj1OCr/vHnekPRkoO+kdMU+MU=", "github.com/stretchr/testify v1.8.1/go.mod h1:w2LPCIKwWwSfY2zedu0+kehJoqGctiVI29o6fzry7u4=", "github.com/stretchr/testify v1.8.4/go.mod h1:sz/lmYIOXD/1dqDmKjjqLyZ2RngseejIcXlSw2iwfAo=", "github.com/stretchr/testify v1.9.0 h1:HtqpIVDClZ4nwg75+f6Lvsy/wHu+3BoSGCbBAcpTsTg=", "github.com/stretchr/testify v1.9.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=", "github.com/subosito/gotenv v1.6.0 h1:9NlTDc1FTs4qu0DDq7AEtTPNw6SVm7uBMsUCUjABIf8=", "github.com/subosito/gotenv v1.6.0/go.mod h1:Dk4QP5c2W3ibzajGcXpNraDfq2IrhjMIvMSWPKKo0FU=", "github.com/twitchyliquid64/golang-asm v0.15.1 h1:SU5vSMR7hnwNxj24w34ZyCi/FmDZTkS4MhqMhdFk5YI=", "github.com/twitchyliquid64/golang-asm v0.15.1/go.mod h1:a1lVb/DtPvCB8fslRZhAngC2+aY1QWCk3Cedj/Gdt08=", "github.com/ugorji/go/codec v1.2.12 h1:9LC83zGrHhuUA9l16C9AHXAqEV/2wBQ4nkvumAE65EE=", "github.com/ugorji/go/codec v1.2.12/go.mod h1:UNopzCgEMSXjBc6AOMqYvWC1ktqTAfzJZUZgYf6w6lg=", "github.com/xeipuuv/gojsonpointer v0.0.0-20180127040702-4e3ac2762d5f h1:J9EGpcZtP0E/raorCMxlFGSTBrsSlaDGf3jU/qvAE2c=", "github.com/xeipuuv/gojsonpointer v0.0.0-20180127040702-4e3ac2762d5f/go.mod h1:N2zxlSyiKSe5eX1tZViRH5QA0qijqEDrYZiPEAiq3wU=", "github.com/xeipuuv/gojsonreference v0.0.0-20180127040603-bd5ef7bd5415 h1:EzJWgHovont7NscjpAxXsDA8S8BMYve8Y5+7cuRE7R0=", "github.com/xeipuuv/gojsonreference v0.0.0-20180127040603-bd5ef7bd5415/go.mod h1:GwrjFmJcFw6At/Gs6z4yjiIwzuJ1/+UwLxMQDVQXShQ=", "github.com/xeipuuv/gojsonschema v1.2.0 h1:LhYJRs+L4fBtjZUfuSZIKGeVu0QRy8e5Xi7D17UxZ74=", "github.com/xeipuuv/gojsonschema v1.2.0/go.mod h1:anYRn/JVcOK2ZgGU+IjEV4nwlhoK5sQluxsYJ78Id3Y=", "go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.54.0 h1:TT4fX+nBOA/+LUkobKGW1ydGcn+G3vRw9+g5HwCphpk=", "go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.54.0/go.mod h1:L7UH0GbB0p47T4Rri3uHjbpCFYrVrwc1I25QhNPiGK8=", "go.opentelemetry.io/otel v1.29.0 h1:PdomN/Al4q/lN6iBJEN3AwPvUiHPMlt93c8bqTG5Llw=", "go.opentelemetry.io/otel v1.29.0/go.mod h1:N/WtXPs1CNCUEx+Agz5uouwCba+i+bJGFicT8SR4NP8=", "go.opentelemetry.io/otel/metric v1.29.0 h1:vPf/HFWTNkPu1aYeIsc98l4ktOQaL6LeSoeV2g+8YLc=", "go.opentelemetry.io/otel/metric v1.29.0/go.mod h1:auu/QWieFVWx+DmQOUMgj0F8LHWdgalxXqvp7BII/W8=", "go.opentelemetry.io/otel/trace v1.29.0 h1:J/8ZNK4XgR7a21DZUAsbF8pZ5Jcw1VhACmnYt39JTi4=", "go.opentelemetry.io/otel/trace v1.29.0/go.mod h1:eHl3w0sp3paPkYstJOmAimxhiFXPg+MMTlEh3nsQgWQ=", "go.uber.org/atomic v1.9.0 h1:ECmE8Bn/WFTYwEW/bpKD3M8VtR/zQVbavAoalC1PYyE=", "go.uber.org/atomic v1.9.0/go.mod h1:fEN4uk6kAWBTFdckzkM89CLk9XfWZrxpCo0nPH17wJc=", "go.uber.org/multierr v1.9.0 h1:7fIwc/ZtS0q++VgcfqFDxSBZVv/Xo49/SYnDFupUwlI=", "go.uber.org/multierr v1.9.0/go.mod h1:X2jQV1h+kxSjClGpnseKVIxpmcjrj7MNnI0bnlfKTVQ=", "golang.org/x/arch v0.0.0-20210923205945-b76863e36670/go.mod h1:5om86z9Hs0C8fWVUuoMHwpExlXzs5Tkyp9hOrfG7pp8=", "golang.org/x/arch v0.8.0 h1:3wRIsP3pM4yUptoR96otTUOXI367OS0+c9eeRi9doIc=", "golang.org/x/arch v0.8.0/go.mod h1:FEVrYAQjsQXMVJ1nsMoVVXPZg6p2JE2mx8psSWTDQys=", "golang.org/x/crypto v0.27.0 h1:GXm2NjJrPaiv/h1tb2UH8QfgC/hOf/+z0p6PT8o1w7A=", "golang.org/x/crypto v0.27.0/go.mod h1:1Xngt8kV6Dvbssa53Ziq6Eqn0HqbZi5Z6R0ZpwQzt70=", "golang.org/x/exp v0.0.0-20230905200255-921286631fa9 h1:GoHiUyI/Tp2nVkLI2mCxVkOjsbSXD66ic0XW0js0R9g=", "golang.org/x/exp v0.0.0-20230905200255-921286631fa9/go.mod h1:S2oDrQGGwySpoQPVqRShND87VCbxmc6bL1Yd2oYrm6k=", "golang.org/x/net v0.29.0 h1:5ORfpBpCs4HzDYoodCDBbwHzdR5UrLBZ3sOnUJmFoHo=", "golang.org/x/net v0.29.0/go.mod h1:gLkgy8jTGERgjzMic6DS9+SP0ajcu6Xu3Orq/SpETg0=", "golang.org/x/sys v0.5.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=", "golang.org/x/sys v0.6.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=", "golang.org/x/sys v0.25.0 h1:r+8e+loiHxRqhXVl6ML1nO3l1+oFoWbnlu2Ehimmi34=", "golang.org/x/sys v0.25.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=", "golang.org/x/text v0.18.0 h1:XvMDiNzPAl0jr17s6W9lcaIhGUfUORdGCNsuLmPG224=", "golang.org/x/text v0.18.0/go.mod h1:BuEKDfySbSR4drPmRPG/7iBdf8hvFMuRexcpahXilzY=", "google.golang.org/protobuf v1.34.2 h1:6xV6lTsCfpGD21XK49h7MhtcApnLqkfYgPcdHftf6hg=", "google.golang.org/protobuf v1.34.2/go.mod h1:qYOHts0dSfpeUzUFpOMr/WGzszTmLH+DiWniOlNbLDw=", "gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=", "gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15 h1:YR8cESwS4TdDjEe65xsg0ogRM/Nc3DYOhEAlW+xobZo=", "gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=", "gopkg.in/ini.v1 v1.67.0 h1:Dgnx+6+nfE+IfzjUEISNeydPJh9AXNNsWbGP9KzCsOA=", "gopkg.in/ini.v1 v1.67.0/go.mod h1:pNLf8WUiyNEtQjuu5G5vTm06TEv9tsIgeAvK8hOrP4k=", "gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=", "gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=", "gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=", "nullprogram.com/x/optparse v1.0.0/go.mod h1:KdyPE+Igbe0jQUrVfMqDMeJQIJZEuyV7pjYmp6pbG50=", "rsc.io/pdf v0.1.1/go.mod h1:n8OzWcQ6Sp37PL01nO98y4iUCRdTGarVfzxY20ICaU4="], "file_path": "internal/database/database.go"}
{"Link_to_commit": "https://github.com/tom125813/BreezyAI/commit/f4bf11674d3646e42fcaedc28d4e8dbdcd22722a", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 20, "n_files_impacted": 7, "longest_chunk": ["// Close menu when clicking a link", "navLinks.querySelectorAll('a').forEach(link => {", "  link.addEventListener('click', () => {", "    menuToggle.classList.remove('active');", "    navLinks.classList.remove('show');", "    mobileFooter.classList.remove('show');", "    body.classList.remove('menu-open');", "  });", "});", "", "// Reset mobile menu state on window resize", "window.addEventListener('resize', () => {", "  if (window.innerWidth > 768) {", "    menuToggle.classList.remove('active');", "    navLinks.classList.remove('show');", "    mobileFooter.classList.remove('show');", "    body.classList.remove('menu-open');", "  }", "});", ""], "file_path": "marketing/script.js"}
{"Link_to_commit": "https://github.com/officialkrunalkumar/010-Churn-Ticket-Analysis-using-OpenAi/commit/b292de36ea7d83d24b0658a460babf4b3ea73c8c", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 32, "n_files_impacted": 1, "longest_chunk": ["import os, requests", "", "def main(event):", "  token = os.getenv(\"ChatGPT\")", "  notes = event.get('inputFields').get('notes')", "  meetings = event.get('inputFields').get('meeting_notes')", "  preText = \"I have extracted following ticket information i.e. ticket notes and meetings notes. I want you to summarise why this is churn?\"", "  postText = \" Just write the summary and do not enter anything like here's the summary. go straight into summary.\"", "  finalText = preText + \"Ticket Notes ->\" + notes + \"Meeting Notes ->\" + meetings + postText", "  openai_endpoint = 'https://api.openai.com/v1/chat/completions'", "  headers = {", "        'Authorization': f'Bearer {token}',", "        'Content-Type': 'application/json'", "  }", "  data = {", "    'model': 'gpt-4',", "    'messages': [", "      {", "        'role': 'user',", "        'content': finalText", "      }", "    ],", "    'max_tokens': 500", "  }", "  response = requests.post(openai_endpoint, headers=headers, json=data)", "  print(response.status_code)", "  conclusion = response.json().get('choices', [{}])[0].get('message', {}).get('content', '')", "  return {", "    \"outputFields\": {", "      \"conclusion\": conclusion", "    }", "  }"], "file_path": "Generate Summary.py"}
{"Link_to_commit": "https://github.com/eduardosand/intracranial_ephys_utils/commit/ae744b662c07c70a7154999a08b5d365fdd74d3d", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 6, "n_files_impacted": 1, "longest_chunk": ["                interp_data_t = np.linspace(data_t[missing_samples_start_ind], data_t[missing_samples_start_ind+1],", "                                            missing_samples)", "", "                # full_data_t = np.linspace(data_t[missing_samples_start_ind], data_t[missing_samples_end_ind],", "                #                           missing_samples)", "                data_x_interp = cs(interp_data_t)"], "file_path": "intracranial_ephys_utils/load_data.py"}
{"Link_to_commit": "https://github.com/sjsu-interconnect/ourhexgame/commit/43906b1acddb6e16e7b08e41c04fa4060bd57e3f", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 70, "n_files_impacted": 2, "longest_chunk": ["", "", "class UnionFind:", "    \"\"\"", "    A Union-Find (Disjoint-Set) data structure that supports efficient operations", "    to find the root of a set and unite two sets. This implementation includes", "    path compression and rank optimization to keep the tree structures shallow.", "", "    Attributes:", "        parent (List[int]): Parent list where parent[i] is the parent of element i.", "                            If parent[i] == i, then i is the root of its set.", "        rank (List[int]): Rank list to track the depth of the tree rooted at each element.", "    \"\"\"", "", "    def __init__(self, n):", "        \"\"\"", "        Initializes the Union-Find data structure with `n` elements.", "", "        Each element is initially its own parent, representing `n` individual sets.", "        The rank of all elements is initialized to 0.", "", "        Args:", "            n (int): The number of elements in the set, indexed from 0 to n-1.", "        \"\"\"", "        self.parent = list(range(n))", "        self.rank = [0] * n", "", "    def find(self, x):", "        \"\"\"", "        Finds the root of the set containing the element `x` with path compression.", "", "        Path compression ensures that all elements on the path from `x` to the root", "        point directly to the root, optimizing future operations.", "", "        Args:", "            x (int): The element whose set root is to be found.", "", "        Returns:", "            int: The root of the set containing `x`.", "        \"\"\"", "        if self.parent[x] != x:", "            self.parent[x] = self.find(self.parent[x])  # Path compression", "        return self.parent[x]", "", "    def union(self, x, y):", "        \"\"\"", "        Unites the sets containing elements `x` and `y` using rank optimization.", "", "        The root of one set becomes the parent of the root of the other set based", "        on the rank of the roots. This helps keep the tree structures shallow.", "", "        Args:", "            x (int): An element in the first set.", "            y (int): An element in the second set.", "", "        Returns:", "            None", "        \"\"\"", "        rootX = self.find(x)", "        rootY = self.find(y)", "", "        if rootX != rootY:", "            # Union by rank", "            if self.rank[rootX] > self.rank[rootY]:", "                self.parent[rootY] = rootX", "            elif self.rank[rootX] < self.rank[rootY]:", "                self.parent[rootX] = rootY", "            else:", "                self.parent[rootY] = rootX", "                self.rank[rootX] += 1"], "file_path": "ourhexenv.py"}
{"Link_to_commit": "https://github.com/Hunter-Hadi/ext/commit/4d397bd04dd4042b82cba01fe60406228d6e8872", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 5, "n_files_impacted": 5, "longest_chunk": ["    '',", "    '',", "    navigator.language,", "    navigator.languages.join(','),", "    0,"], "file_path": "src/features/chatgpt/core/generateSentinelChatRequirementsToken.ts"}
{"Link_to_commit": "https://github.com/Hakansabol/ACSL-practice-problem-generator/commit/1e84efe32275046d74dd041cf3abf8bfb1f4920b", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 130, "n_files_impacted": 1, "longest_chunk": ["", "    public static void main(String[] args) {", "        int numberOfProblems = 5; // Number of problems to generate", "        for (int i = 0; i < numberOfProblems; i++) {", "            generateProblem();", "        }", "    }", "", "    private static void generateProblem() {", "        int problemType = random.nextInt(3); // 0: Conversion, 1: Operation, 2: Mixed", "", "        switch (problemType) {", "            case 0:", "                generateConversionProblem();", "                break;", "            case 1:", "                generateOperationProblem();", "                break;", "            case 2:", "                generateMixedProblem();", "                break;", "        }", "    }", "", "    private static void generateConversionProblem() {", "        // Generate a random number in decimal", "        int decimalNumber = random.nextInt(100); // Range from 0 to 99", "        String targetBase = getBaseString(random.nextInt(3)); // 0: Binary, 1: Octal, 2: Hexadecimal", "", "        System.out.printf(\"Convert the decimal number %d to %s:\\n\", decimalNumber, targetBase);", "        System.out.print(\"Your answer: \");", "        String answer = scanner.nextLine();", "        String correctAnswer = convertDecimal(decimalNumber, targetBase);", "", "        if (answer.equalsIgnoreCase(correctAnswer)) {", "            System.out.println(\"Correct!\\n\");", "        } else {", "            System.out.printf(\"Incorrect. The correct answer is %s.\\n\\n\", correctAnswer);", "        }", "    }", "", "    private static void generateOperationProblem() {", "        // Generate two random numbers in different bases", "        int base1 = getRandomBase();", "        int base2 = getRandomBase();", "", "        int number1 = random.nextInt(50); // Generate random number for the first base", "        int number2 = random.nextInt(50); // Generate random number for the second base", "", "        String base1Str = convertToBase(number1, base1);", "        String base2Str = convertToBase(number2, base2);", "", "        System.out.printf(\"What is %s (%s) + %s (%s)?\\n\", base1Str, getBaseString(base1), base2Str, getBaseString(base2));", "        System.out.print(\"Your answer (in decimal): \");", "        String answer = scanner.nextLine();", "", "        int correctAnswer = convertToDecimal(base1Str, base1) + convertToDecimal(base2Str, base2);", "", "        if (Integer.parseInt(answer) == correctAnswer) {", "            System.out.println(\"Correct!\\n\");", "        } else {", "            System.out.printf(\"Incorrect. The correct answer is %d.\\n\\n\", correctAnswer);", "        }", "    }", "", "    private static void generateMixedProblem() {", "        // Generate a conversion problem followed by an operation", "        int decimalNumber = random.nextInt(100);", "        String targetBase = getBaseString(random.nextInt(3));", "        System.out.printf(\"Convert the decimal number %d to %s:\\n\", decimalNumber, targetBase);", "        System.out.print(\"Your answer: \");", "        String answer = scanner.nextLine();", "        String correctAnswer = convertDecimal(decimalNumber, targetBase);", "", "        if (answer.equalsIgnoreCase(correctAnswer)) {", "            System.out.println(\"Correct!\\n\");", "        } else {", "            System.out.printf(\"Incorrect. The correct answer is %s.\\n\\n\", correctAnswer);", "        }", "", "        // Now generate an operation problem", "        generateOperationProblem();", "    }", "", "    private static String convertDecimal(int number, String targetBase) {", "        switch (targetBase) {", "            case \"Binary\":", "                return Integer.toBinaryString(number);", "            case \"Octal\":", "                return Integer.toOctalString(number);", "            case \"Hexadecimal\":", "                return Integer.toHexString(number).toUpperCase();", "            default:", "                return String.valueOf(number);", "        }", "    }", "", "    private static int convertToDecimal(String number, int base) {", "        return Integer.parseInt(number, base);", "    }", "", "    private static String convertToBase(int number, int base) {", "        switch (base) {", "            case 2:", "                return Integer.toBinaryString(number);", "            case 8:", "                return Integer.toOctalString(number);", "            case 16:", "                return Integer.toHexString(number).toUpperCase();", "            default:", "                return String.valueOf(number);", "        }", "    }", "", "    private static String getBaseString(int baseIndex) {", "        switch (baseIndex) {", "            case 0:", "                return \"Binary\";", "            case 1:", "                return \"Octal\";", "            case 2:", "                return \"Hexadecimal\";", "            default:", "                return \"Decimal\";", "        }", "    }", "", "    private static int getRandomBase() {", "        return random.nextInt(3) + 2; // Return bases 2 (Binary), 8 (Octal), 16 (Hexadecimal)", "    }"], "file_path": "src/computer_numbering_systems.java"}
{"Link_to_commit": "https://github.com/samsiso/siso-agency-onboarding-app-main/commit/3588742c139653b2769a7949078f1624ac83fc61", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 85, "n_files_impacted": 6, "longest_chunk": ["    console.log(\"Using OpenAI for analysis\");", "    ", "    // [Analysis] Prepare prompt for article analysis with meaningful structure", "    const prompt = `", "      Analyze this AI news article and provide insights for AI agency professionals:", "      ", "      Title: ${title}", "      ", "      Content: ${content || \"\"}", "      ", "      Source: ${source || \"Unknown\"}", "      ", "      Category: ${category || \"AI Technology\"}", "      ", "      Please respond with JSON containing the following keys:", "      - market_impact: A detailed paragraph analyzing market implications for AI agencies", "      - technical_predictions: Array of 3-5 bullet points with technical predictions", "      - related_technologies: Array of relevant technologies mentioned or implied in the article", "      - business_implications: A paragraph explaining strategic business implications for AI agencies", "    `;", "    ", "    // [Analysis] Call OpenAI API for enhanced analysis", "    const openAIResponse = await fetch(\"https://api.openai.com/v1/chat/completions\", {", "      method: \"POST\",", "      headers: {", "        \"Content-Type\": \"application/json\",", "        \"Authorization\": `Bearer ${openAIKey}`,", "      },", "      body: JSON.stringify({", "        model: \"gpt-4o-mini\", // [Analysis] Using an available model from OpenAI", "        messages: [", "          {", "            role: \"system\",", "            content: \"You are an AI technology analyst specialized in extracting business insights from news articles for AI agencies. Provide detailed, actionable analysis in JSON format.\"", "          },", "          {", "            role: \"user\",", "            content: prompt", "          }", "        ],", "        temperature: 0.3, // [Analysis] Lower temperature for more consistent, analytical responses", "      }),", "    });", "    ", "    if (!openAIResponse.ok) {", "      const errorData = await openAIResponse.text();", "      console.error(\"OpenAI API error:\", errorData);", "      throw new Error(`OpenAI API error: ${errorData}`);", "    }", "    ", "    // [Analysis] Process and parse the OpenAI response", "    const openAIData = await openAIResponse.json();", "    console.log(\"Received response from OpenAI\");", "    ", "    let analysis;", "    try {", "      // [Analysis] Extract JSON from OpenAI response", "      const responseContent = openAIData.choices[0].message.content;", "      ", "      // [Framework] Parse JSON, handling potential formatting issues", "      if (responseContent.includes(\"{\") && responseContent.includes(\"}\")) {", "        const jsonStart = responseContent.indexOf(\"{\");", "        const jsonEnd = responseContent.lastIndexOf(\"}\") + 1;", "        const jsonStr = responseContent.substring(jsonStart, jsonEnd);", "        analysis = JSON.parse(jsonStr);", "      } else {", "        // [Analysis] Fallback to text parsing if JSON structure is missing", "        analysis = {", "          market_impact: \"Analysis of market impact unavailable at this time.\",", "          technical_predictions: [\"Prediction data could not be extracted\"],", "          related_technologies: [\"AI\"],", "          business_implications: \"Business implications analysis unavailable at this time.\"", "        };", "      }", "    } catch (parseError) {", "      console.error(\"Error parsing OpenAI response:\", parseError);", "      ", "      // [Analysis] Provide fallback analysis when parsing fails", "      analysis = {", "        market_impact: \"Unable to parse analysis results.\",", "        technical_predictions: [\"Analysis results unavailable\"],", "        related_technologies: [\"AI\"],", "        business_implications: \"Unable to extract business implications at this time.\"", "      };", "    }"], "file_path": "supabase/functions/analyze-article/index.ts"}
{"Link_to_commit": "https://github.com/Lordsisodia/siso-resource-hub/commit/3588742c139653b2769a7949078f1624ac83fc61", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 85, "n_files_impacted": 6, "longest_chunk": ["    console.log(\"Using OpenAI for analysis\");", "    ", "    // [Analysis] Prepare prompt for article analysis with meaningful structure", "    const prompt = `", "      Analyze this AI news article and provide insights for AI agency professionals:", "      ", "      Title: ${title}", "      ", "      Content: ${content || \"\"}", "      ", "      Source: ${source || \"Unknown\"}", "      ", "      Category: ${category || \"AI Technology\"}", "      ", "      Please respond with JSON containing the following keys:", "      - market_impact: A detailed paragraph analyzing market implications for AI agencies", "      - technical_predictions: Array of 3-5 bullet points with technical predictions", "      - related_technologies: Array of relevant technologies mentioned or implied in the article", "      - business_implications: A paragraph explaining strategic business implications for AI agencies", "    `;", "    ", "    // [Analysis] Call OpenAI API for enhanced analysis", "    const openAIResponse = await fetch(\"https://api.openai.com/v1/chat/completions\", {", "      method: \"POST\",", "      headers: {", "        \"Content-Type\": \"application/json\",", "        \"Authorization\": `Bearer ${openAIKey}`,", "      },", "      body: JSON.stringify({", "        model: \"gpt-4o-mini\", // [Analysis] Using an available model from OpenAI", "        messages: [", "          {", "            role: \"system\",", "            content: \"You are an AI technology analyst specialized in extracting business insights from news articles for AI agencies. Provide detailed, actionable analysis in JSON format.\"", "          },", "          {", "            role: \"user\",", "            content: prompt", "          }", "        ],", "        temperature: 0.3, // [Analysis] Lower temperature for more consistent, analytical responses", "      }),", "    });", "    ", "    if (!openAIResponse.ok) {", "      const errorData = await openAIResponse.text();", "      console.error(\"OpenAI API error:\", errorData);", "      throw new Error(`OpenAI API error: ${errorData}`);", "    }", "    ", "    // [Analysis] Process and parse the OpenAI response", "    const openAIData = await openAIResponse.json();", "    console.log(\"Received response from OpenAI\");", "    ", "    let analysis;", "    try {", "      // [Analysis] Extract JSON from OpenAI response", "      const responseContent = openAIData.choices[0].message.content;", "      ", "      // [Framework] Parse JSON, handling potential formatting issues", "      if (responseContent.includes(\"{\") && responseContent.includes(\"}\")) {", "        const jsonStart = responseContent.indexOf(\"{\");", "        const jsonEnd = responseContent.lastIndexOf(\"}\") + 1;", "        const jsonStr = responseContent.substring(jsonStart, jsonEnd);", "        analysis = JSON.parse(jsonStr);", "      } else {", "        // [Analysis] Fallback to text parsing if JSON structure is missing", "        analysis = {", "          market_impact: \"Analysis of market impact unavailable at this time.\",", "          technical_predictions: [\"Prediction data could not be extracted\"],", "          related_technologies: [\"AI\"],", "          business_implications: \"Business implications analysis unavailable at this time.\"", "        };", "      }", "    } catch (parseError) {", "      console.error(\"Error parsing OpenAI response:\", parseError);", "      ", "      // [Analysis] Provide fallback analysis when parsing fails", "      analysis = {", "        market_impact: \"Unable to parse analysis results.\",", "        technical_predictions: [\"Analysis results unavailable\"],", "        related_technologies: [\"AI\"],", "        business_implications: \"Unable to extract business implications at this time.\"", "      };", "    }"], "file_path": "supabase/functions/analyze-article/index.ts"}
{"Link_to_commit": "https://github.com/ashleycheng/InsTrip/commit/3e4fe3feefd12664f8284eb50f7fadbe20e060a1", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 16, "n_files_impacted": 6, "longest_chunk": ["", "", "def trip_analysis(request, country_name):", "    top_city_counter = get_top_city(country_name, 5)", "    labels = [i[0] for i in top_city_counter]", "    data = [i[1] for i in top_city_counter]", "", "    info = CountryInfo.objects.get(country_name=country_name)", "", "    return render(request, 'analysis.html', {", "        'country_name': country_name,", "        'country_name_ch': info.country_name_ch,", "        'analysis': info.analysis,", "        \"labels\": labels,", "        \"data\": data,", "    })"], "file_path": "tripsite/urls.py"}
{"Link_to_commit": "https://github.com/meharklair/brazmas/commit/664b3a87941e73a5242ff4ba1cc0338e985f5417", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 16, "n_files_impacted": 15, "longest_chunk": ["package com.CMPUT301W24T32.brazmascheckin.controllers;", "", "public interface GetFailureListener {", "    /**", "     * Listener interface for handling the failure of retrieving objects from the database.", "     */", "    public interface FailureListener {", "", "        /**", "         * Called when an error occurs during object retrieval.", "         *", "         * @param e the exception representing the error.", "         */", "        void onFailure(Exception e);", "    }", "}"], "file_path": "app/src/main/java/com/CMPUT301W24T32/brazmascheckin/controllers/GetSuccessListener.java"}
{"Link_to_commit": "https://github.com/samsiso/siso-agency-onboarding-app-30/commit/3588742c139653b2769a7949078f1624ac83fc61", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 85, "n_files_impacted": 6, "longest_chunk": ["    console.log(\"Using OpenAI for analysis\");", "    ", "    // [Analysis] Prepare prompt for article analysis with meaningful structure", "    const prompt = `", "      Analyze this AI news article and provide insights for AI agency professionals:", "      ", "      Title: ${title}", "      ", "      Content: ${content || \"\"}", "      ", "      Source: ${source || \"Unknown\"}", "      ", "      Category: ${category || \"AI Technology\"}", "      ", "      Please respond with JSON containing the following keys:", "      - market_impact: A detailed paragraph analyzing market implications for AI agencies", "      - technical_predictions: Array of 3-5 bullet points with technical predictions", "      - related_technologies: Array of relevant technologies mentioned or implied in the article", "      - business_implications: A paragraph explaining strategic business implications for AI agencies", "    `;", "    ", "    // [Analysis] Call OpenAI API for enhanced analysis", "    const openAIResponse = await fetch(\"https://api.openai.com/v1/chat/completions\", {", "      method: \"POST\",", "      headers: {", "        \"Content-Type\": \"application/json\",", "        \"Authorization\": `Bearer ${openAIKey}`,", "      },", "      body: JSON.stringify({", "        model: \"gpt-4o-mini\", // [Analysis] Using an available model from OpenAI", "        messages: [", "          {", "            role: \"system\",", "            content: \"You are an AI technology analyst specialized in extracting business insights from news articles for AI agencies. Provide detailed, actionable analysis in JSON format.\"", "          },", "          {", "            role: \"user\",", "            content: prompt", "          }", "        ],", "        temperature: 0.3, // [Analysis] Lower temperature for more consistent, analytical responses", "      }),", "    });", "    ", "    if (!openAIResponse.ok) {", "      const errorData = await openAIResponse.text();", "      console.error(\"OpenAI API error:\", errorData);", "      throw new Error(`OpenAI API error: ${errorData}`);", "    }", "    ", "    // [Analysis] Process and parse the OpenAI response", "    const openAIData = await openAIResponse.json();", "    console.log(\"Received response from OpenAI\");", "    ", "    let analysis;", "    try {", "      // [Analysis] Extract JSON from OpenAI response", "      const responseContent = openAIData.choices[0].message.content;", "      ", "      // [Framework] Parse JSON, handling potential formatting issues", "      if (responseContent.includes(\"{\") && responseContent.includes(\"}\")) {", "        const jsonStart = responseContent.indexOf(\"{\");", "        const jsonEnd = responseContent.lastIndexOf(\"}\") + 1;", "        const jsonStr = responseContent.substring(jsonStart, jsonEnd);", "        analysis = JSON.parse(jsonStr);", "      } else {", "        // [Analysis] Fallback to text parsing if JSON structure is missing", "        analysis = {", "          market_impact: \"Analysis of market impact unavailable at this time.\",", "          technical_predictions: [\"Prediction data could not be extracted\"],", "          related_technologies: [\"AI\"],", "          business_implications: \"Business implications analysis unavailable at this time.\"", "        };", "      }", "    } catch (parseError) {", "      console.error(\"Error parsing OpenAI response:\", parseError);", "      ", "      // [Analysis] Provide fallback analysis when parsing fails", "      analysis = {", "        market_impact: \"Unable to parse analysis results.\",", "        technical_predictions: [\"Analysis results unavailable\"],", "        related_technologies: [\"AI\"],", "        business_implications: \"Unable to extract business implications at this time.\"", "      };", "    }"], "file_path": "supabase/functions/analyze-article/index.ts"}
{"Link_to_commit": "https://github.com/Bbop311/WeeMo/commit/3c436009e95dee108fc1947aab61e304f7cc1cd5", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 126, "n_files_impacted": 1, "longest_chunk": ["<?php", "", "namespace App\\DataFixtures;", "", "use App\\Entity\\Listing;", "use Doctrine\\Bundle\\FixturesBundle\\Fixture;", "use Doctrine\\Persistence\\ObjectManager;", "use Doctrine\\Common\\DataFixtures\\DependentFixtureInterface;", "use DateTime;", "use DateTimeImmutable;", "", "class ListingFixtures extends Fixture implements DependentFixtureInterface", "{", "    public function load(ObjectManager $manager) : void", "    {", "        //Code written by ChatGPT to generate an array of listings", "        $listings = [];", "        $titles = [", "            'Luxurious Apartment in Paris',", "            'Charming Studio in the Heart of Paris',", "            'Spacious Flat with Eiffel Tower View',", "            'Modern Loft Near Champs-\u00c9lys\u00e9es',", "            'Cozy 1-Bedroom Apartment in Montmartre',", "            'Elegant 2-Bedroom Apartment in Le Marais',", "            'Sunny Apartment with Balcony in Paris',", "            'Stylish Duplex in the Latin Quarter',", "            'Renovated Apartment in Historical Building',", "            'Contemporary Apartment near Seine River',", "            'Beautiful Apartment with Garden View',", "            'Luxury Penthouse in Central Paris',", "            'Bright and Airy Apartment in Paris',", "            'Quaint Studio Near Sacr\u00e9-C\u0153ur',", "            'Exclusive Apartment in Saint-Germain',", "            'Classic Parisian Apartment with High Ceilings',", "            'Designer Loft in the Marais District',", "            'Chic Apartment with Modern Amenities',", "            'Top Floor Apartment with City Views',", "            'Charming Flat in Parisian Style',", "            'Cozy Studio in Paris 7th Arrondissement',", "            'Spacious Apartment with Terrace',", "            'Elegant Flat in the Golden Triangle',", "            'Bright Apartment in Paris 6th Arrondissement',", "            'Charming 2-Bedroom Flat in Paris 5th',", "            'Luxury Studio in Paris 1st Arrondissement',", "            'Modern Apartment in Paris 8th Arrondissement',", "            'Beautiful Apartment in Paris 15th Arrondissement',", "            'Gorgeous Flat in Paris 16th Arrondissement',", "            'Prestigious Apartment in Paris 2nd Arrondissement',", "        ];", "        ", "        $descriptions = [", "            'A luxurious apartment with stunning views of Paris.',", "            'A charming studio located in the heart of Paris.',", "            'A spacious flat offering a breathtaking view of the Eiffel Tower.',", "            'A modern loft located near the famous Champs-\u00c9lys\u00e9es.',", "            'A cozy 1-bedroom apartment situated in Montmartre.',", "            'An elegant 2-bedroom apartment located in Le Marais.',", "            'A sunny apartment with a balcony, perfect for enjoying Paris.',", "            'A stylish duplex in the vibrant Latin Quarter.',", "            'A renovated apartment in a historical building.',", "            'A contemporary apartment located near the Seine River.',", "            'A beautiful apartment with a garden view.',", "            'A luxury penthouse located in the center of Paris.',", "            'A bright and airy apartment perfect for Parisian living.',", "            'A quaint studio near the famous Sacr\u00e9-C\u0153ur.',", "            'An exclusive apartment located in Saint-Germain.',", "            'A classic Parisian apartment with high ceilings.',", "            'A designer loft in the trendy Marais District.',", "            'A chic apartment with modern amenities.',", "            'A top-floor apartment offering stunning city views.',", "            'A charming flat in true Parisian style.',", "            'A cozy studio in the 7th Arrondissement of Paris.',", "            'A spacious apartment with a terrace.',", "            'An elegant flat located in the Golden Triangle.',", "            'A bright apartment in the 6th Arrondissement of Paris.',", "            'A charming 2-bedroom flat in the 5th Arrondissement.',", "            'A luxury studio in the 1st Arrondissement of Paris.',", "            'A modern apartment in the 8th Arrondissement.',", "            'A beautiful apartment in the 15th Arrondissement of Paris.',", "            'A gorgeous flat in the 16th Arrondissement of Paris.',", "            'A prestigious apartment in the 2nd Arrondissement of Paris.',", "        ];", "        ", "        $statuses = ['active', 'inactive', 'expired', 'suspended'];", "        $propertyIds = range(1, 31);", "        unset($propertyIds[array_search(29, $propertyIds)]);", "        $propertyIds = array_values($propertyIds);", "        shuffle($propertyIds);", "        ", "        for ($i = 0; $i < 30; $i++) {", "            $startDate = new DateTimeImmutable();", "            $endDate = (clone $startDate)->modify('+'.mt_rand(1, 8).' weeks');", "        ", "            $listings[] = [", "                'property_id' => $propertyIds[$i],", "                'listing_title' => $titles[array_rand($titles)],", "                'listing_description' => $descriptions[array_rand($descriptions)],", "                'start_date' => $startDate,", "                'end_date' => $endDate,", "                'status' => $statuses[array_rand($statuses)]", "            ];", "        }", "        //end of the code generated by ChatGPT", "        ", "        foreach ($listings as $listingData) {", "            $listing = new Listing;", "", "            $listing->setProperty($this->getReference('property_' . $listingData['property_id']));", "            $listing->setListingTitle($listingData['listing_title']);", "            $listing->setListingDescription($listingData['listing_description']);", "            $listing->setStartDate($listingData['start_date']);", "            $listing->setEndDate($listingData['end_date']);", "            $listing->setStatus($listingData['status']);", "            ", "            $manager->persist($listing);", "        }", "        $manager->flush();", "    }", "", "    public function getDependencies()", "    {", "        return [", "            PropertyFixtures::class,", "        ];", "    }", "}"], "file_path": "src/DataFixtures/ListingFixtures.php"}
{"Link_to_commit": "https://github.com/hkw1831/obsidian-action-status-updater/commit/a5e5482a767ffe0500a068dac2de3ab11f347367", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 30, "n_files_impacted": 1, "longest_chunk": ["\t\tthis.addChatGPTPromptForGeneratingSummaryToClipboard();", "\t\tthis.addCommand({", "\t\t\tid: \"chatgpt-prompt-for-generating-summary-to-clipboard\",", "\t\t\tname: \"ChatGPT prompt for generating summary to clipboard\",", "\t\t\ticon: `chatgpt-prompt-for-generating-summary-to-clipboard`,", "\t\t\teditorCallback: (editor: Editor, view: MarkdownView) => {", "\t\t\t\tconst prompt = \"\u8acb\u5c07\u4ee5\u4e0b\u7684\u6587\u7ae0\u7bc0\u9304\u7e2e\u77ed\u6210\u7d04150\u5b57\u7684\u4e2d\u6587\u6458\u8981\uff0c\u78ba\u4fdd\u6458\u8981\u5167\u5bb9\u7cbe\u7149\u4e14\u7a81\u51fa\u91cd\u9ede\u3002\u4f60\u9700\u8981\u6ce8\u610f\u4ee5\u4e0b\u5e7e\u9ede\uff1a\\n\" +", "\t\t\t\t\t\t\t   \"\\n\" +", "\t\t\t\t\t\t\t   \"1. \u5c07\u9577\u7bc7\u5927\u8ad6\u7e2e\u77ed\uff0c\u53ea\u4fdd\u7559\u6700\u91cd\u8981\u7684\u8a0a\u606f\u548c\u4e3b\u984c\u3002\\n\" +", "\t\t\t\t\t\t\t   \"2. \u53bb\u9664\u975e\u5fc5\u8981\u7684\u8a73\u7d30\u8a0a\u606f\uff0c\u4e26\u907f\u514d\u4f7f\u7528\u904e\u65bc\u7e41\u8907\u6216\u4e0d\u5fc5\u8981\u7684\u8a9e\u8a00\u3002\\n\" +", "\t\t\t\t\t\t\t   \"3. \u4fdd\u7559\u6587\u7ae0\u4e2d\u6700\u91cd\u8981\u7684\u4e3b\u984c\u548c\u8a0a\u606f\uff0c\u4e26\u78ba\u4fdd\u9019\u4e9b\u8a0a\u606f\u5728\u6458\u8981\u4e2d\u6e05\u695a\u5730\u8868\u9054\u51fa\u4f86\u3002\\n\" +", "\t\t\t\t\t\t\t   \"4. \u4f7f\u7528\u7cbe\u7149\u4e14\u76f4\u63a5\u7684\u8a9e\u8a00\uff0c\u4ee5\u5438\u5f15\u4eba\u7684\u65b9\u5f0f\u8868\u9054\u4f5c\u8005\u5c07\u5728\u6587\u7ae0\u4e2d\u6df1\u5165\u5206\u4eab\u9019\u4e9b\u4e3b\u984c\u7684\u610f\u5716\u3002\\n\" +", "\t\t\t\t\t\t\t   \"5. \u4f7f\u7528\u300c\u6211\u300d\u4f86\u6307\u7a31\u300c\u4f5c\u8005\u300d\uff0c\u300c\u4f60\u300d\u4f86\u6307\u7a31\u8b80\u8005\u3002\\n\" +", "\t\t\t\t\t\t\t   \"\\n\" +", "\t\t\t\t\t\t\t   \"\u5177\u9ad4\u4f86\u8aaa\uff0c\u4f60\u9700\u8981\u78ba\u4fdd\u4ee5\u4e0b\u91cd\u9ede\u8a0a\u606f\u88ab\u5305\u542b\u5176\u4e2d\uff1a\\n\" +", "\t\t\t\t\t\t\t   \"1. \u6587\u7ae0\u7684\u4e3b\u8981\u4e3b\u984c\u6216\u91cd\u9ede\u8a0e\u8ad6\u3002\\n\" +", "\t\t\t\t\t\t\t   \"2. \u4f5c\u8005\u63d0\u51fa\u7684\u5efa\u8b70\u3001\u7b56\u7565\u6216\u91cd\u8981\u89c0\u9ede\u3002\\n\" +", "\t\t\t\t\t\t\t   \"3. \u9019\u4e9b\u5efa\u8b70\u6216\u7b56\u7565\u7684\u5177\u9ad4\u6548\u76ca\u6216\u7d50\u679c\u3002\\n\" +", "\t\t\t\t\t\t\t   \"\\n\" +", "\t\t\t\t\t\t\t   \"\u6700\u5f8c\uff0c\u4ee5\u5438\u5f15\u4e26\u9f13\u52f5\u8b80\u8005\u9032\u884c\u4e0b\u4e00\u6b65\u884c\u52d5\u7684\u65b9\u5f0f\u7de8\u5beb\u6458\u8981\uff0c\u4e26\u8868\u9054\u51fa\u6587\u7ae0\u4e2d\u66f4\u591a\u6df1\u5165\u7684\u5167\u5bb9\u7b49\u5f85\u8b80\u8005\u53bb\u63a2\u7d22\u3002\\n\" +", "\t\t\t\t\t\t\t   \"\u8acb\u5beb\u51fa3\u500b\u7248\u672c\u3002\\n\\n\" + editor.getValue();", "", "\t\t\t\tnavigator.clipboard.writeText(prompt).then(function () {", "\t\t\t\t\tnew Notice(`Copied prompt for generate summary to clipboard!`);", "\t\t\t\t}, function (error) {", "\t\t\t\t\tnew Notice(`error when copy to clipboard!`);", "\t\t\t\t});", "\t\t\t}", "\t\t});", ""], "file_path": "main.ts"}
{"Link_to_commit": "https://github.com/lnxgod/GameSynth/commit/17d6de9e6dcc8417a76a0ec307df3acb06accd03", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 92, "n_files_impacted": 3, "longest_chunk": ["  // New endpoint for game design assistance", "  app.post(\"/api/design/chat\", async (req, res) => {", "    try {", "      const { message, sessionId } = req.body;", "", "      // Get or create conversation history", "      if (!designConversations.has(sessionId)) {", "        designConversations.set(sessionId, []);", "      }", "      const history = designConversations.get(sessionId)!;", "", "      // Add user message to history", "      history.push({ role: 'user', content: message });", "", "      logApi(\"Design chat request received\", { message, sessionId });", "", "      const response = await openai.chat.completions.create({", "        model: \"gpt-4o\",", "        messages: [", "          {", "            role: \"system\",", "            content: DESIGN_ASSISTANT_PROMPT", "          },", "          ...history", "        ],", "        temperature: 0.7", "      });", "", "      const assistantMessage = response.choices[0].message.content || \"\";", "", "      // Add assistant response to history", "      history.push({ role: 'assistant', content: assistantMessage });", "", "      logApi(\"Design chat response\", { message }, { response: assistantMessage });", "", "      res.json({ ", "        message: assistantMessage,", "        history: history ", "      });", "    } catch (error: any) {", "      logApi(\"Error in design chat\", req.body, { error: error.message });", "      res.status(500).json({ error: error.message });", "    }", "  });", "", "  // Generate game code based on design conversation", "  app.post(\"/api/design/generate\", async (req, res) => {", "    try {", "      const { sessionId } = req.body;", "      const history = designConversations.get(sessionId);", "", "      if (!history) {", "        throw new Error(\"No design conversation found\");", "      }", "", "      logApi(\"Game generation request\", { sessionId });", "", "      // Compile the conversation into a detailed game specification", "      const response = await openai.chat.completions.create({", "        model: \"gpt-4o\",", "        messages: [", "          {", "            role: \"system\",", "            content: SYSTEM_PROMPT", "          },", "          {", "            role: \"user\",", "            content: `Based on the following conversation, create a complete game implementation:\\n\\n${", "              history.map(msg => `${msg.role}: ${msg.content}`).join('\\n')", "            }`", "          }", "        ],", "        temperature: 0.7,", "        max_tokens: 16000", "      });", "", "      const content = response.choices[0].message.content || \"\";", "      const code = extractGameCode(content);", "", "      const result = {", "        code,", "        response: content", "      };", "", "      logApi(\"Game code generated\", { sessionId }, result);", "      res.json(result);", "    } catch (error: any) {", "      logApi(\"Error generating game\", req.body, { error: error.message });", "      res.status(500).json({ error: error.message });", "    }", "  });", ""], "file_path": "server/routes.ts"}
{"Link_to_commit": "https://github.com/SileNce5k/discord_bot/commit/6172814a2e76b5e8866ffc81de9f4a82a45e419b", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 6, "n_files_impacted": 1, "longest_chunk": ["", "        \"monthly\": \"1month\",", "        \"month\": \"1month\",", "        \"m\": \"1month\",", "", "        \"quarterly\": \"3month\","], "file_path": "util/lastfm/getTopTracks.js"}
{"Link_to_commit": "https://github.com/xwd0418/TripPlanningBackend/commit/f95668655e6b5c106f4b8584acd03bda605ef3c7", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 105, "n_files_impacted": 3, "longest_chunk": ["package service", "", "import (", "\t\"bytes\"", "\t\"encoding/json\"", "\t\"fmt\"", "\t\"net/http\"", "\t\"io/ioutil\"", "\t\"tripPlanning/constants\"", ")", "", "// TravelPlannerService handles travel planning logic", "type TravelPlannerService struct {", "\topenAIKey string", "}", "", "type OpenAIResponse struct {", "    Choices []struct {", "        Message struct {", "            Content string `json:\"content\"`", "        } `json:\"message\"`", "    } `json:\"choices\"`", "}", "", "// NewTravelPlannerService creates a new instance of TravelPlannerService", "func NewTravelPlannerService(openAIKey string) *TravelPlannerService {", "\treturn &TravelPlannerService{", "\t\topenAIKey: constants.Openai_key,", "\t}", "}", "", "// AiGeneratedPlan generates a plan using ChatGPT API", "func (s *TravelPlannerService) AiGeneratedPlan(city, startDay, endDay string) (string, error) {", "    // Set up OpenAI API client", "    apiEndpoint := constants.OpenaiEndpoint", "    ", "    fmt.Println(\"successful connect to gpt\")", "    ", "    // Construct the input prompt for ChatGPT", "    promptMessage := fmt.Sprintf(\"Give me a trip plan to travel to %s from %s to %s with the most famous places of interest. Just list the places of interest for each day\", city, startDay, endDay)", "", "    // Construct the request payload", "    requestPayload := map[string]interface{}{", "        \"model\":     \"gpt-3.5-turbo\",", "        \"messages\":  []map[string]string{", "            {", "                \"role\":    \"system\",", "                \"content\": \"You are a helpful assistant capable of generating travel plans.\",", "            },", "            {", "                \"role\":    \"user\",", "                \"content\": promptMessage,", "            },", "        },", "        \"max_tokens\": 200,", "    }", "", "    // Convert payload to JSON", "    payloadBytes, err := json.Marshal(requestPayload)", "    if err != nil {", "        return \"\", fmt.Errorf(\"error encoding JSON: %v\", err)", "    }", "", "", "\t// Create HTTP request", "\treq, err := http.NewRequest(\"POST\", apiEndpoint, bytes.NewBuffer(payloadBytes))", "\tif err != nil {", "\t\treturn \"\", fmt.Errorf(\"error creating request: %v\", err)", "\t}", "", "\t// Set API key header", "\treq.Header.Set(\"Authorization\", \"Bearer \"+constants.Openai_key)", "\treq.Header.Set(\"Content-Type\", \"application/json\")", "", "\t// Make the request", "\tclient := &http.Client{}", "\tresp, err := client.Do(req)", "\tif err != nil {", "\t\treturn \"\", fmt.Errorf(\"error making request: %v\", err)", "\t}", "\tdefer resp.Body.Close()", "", "\t// Read and parse the response", "    responseBody, err := ioutil.ReadAll(resp.Body)", "    if err != nil {", "        return \"\", fmt.Errorf(\"error reading response body: %v\", err)", "    }", "", "    // Unmarshal JSON response into OpenAIResponse struct", "    var response OpenAIResponse", "    err = json.Unmarshal(responseBody, &response)", "    if err != nil {", "        return \"\", fmt.Errorf(\"error unmarshaling JSON: %v\", err)", "    }", "", "    // Check if response contains content", "    if len(response.Choices) > 0 && len(response.Choices[0].Message.Content) > 0 {", "        // Return the content of the first choice", "        return response.Choices[0].Message.Content, nil", "    } else {", "        return \"\", fmt.Errorf(\"no content found in response\")", "    }", "}", "", ""], "file_path": "go/src/tripPlanning/service/plan_ai.go"}
{"Link_to_commit": "https://github.com/Barszaszabolcs/movie-ticket-booking-webapp/commit/88dad12fd7526675e0d9b1ae1aa7cfd40cfac2e2", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 35, "n_files_impacted": 5, "longest_chunk": ["", "  async generateSummary() {", "    this.inProgress = true;", "    try {", "      const openai = new OpenAI({", "        apiKey: environment.apiKey,", "        dangerouslyAllowBrowser: true", "      });", "  ", "      let completion = await openai.chat.completions.create({", "        messages: [{ role: 'user', content: 'Gener\u00e1lj nekem egy maximum 600 karakteres \u00f6sszegz\u00e9st a: ' + (this.filmForm.get('title')?.value as string) + ' c\u00edm\u0171 filmhez'}],", "        model: 'gpt-3.5-turbo',", "        temperature: 0.95,", "        max_tokens: 300,", "        top_p: 1.0,", "        frequency_penalty: 0.0,", "        presence_penalty: 0.0,", "      }).then(response => {", "        this.summary = response.choices[0].message.content as string;", "        this.filmForm.get('summary')?.setValue(this.summary);", "        this.canGenerate = false;", "        this.isGenerated = true;", "        this.inProgress = false;", "      }).catch(error => {", "        this.isGenerated = false;", "        this.inProgress = false;", "      });", "      ", "    } catch (error) {", "      console.error(error);", "      this.isGenerated = false;", "      this.inProgress = false;", "    }", "  }", "}"], "file_path": "src/app/pages/film-create/film-create.component.ts"}
{"Link_to_commit": "https://github.com/markmansour/marks-chess/commit/d1f65a50c13a446f13b62b859e1a1a81df7291c6", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 56, "n_files_impacted": 3, "longest_chunk": ["", "    // Testing edge cases for location to square conversion", "    @Test", "    public void locationToSquareEdgeCases() {", "        assertThatThrownBy(() -> FenString.locationToSquare(64)).isInstanceOf(IllegalArgumentException.class);", "        assertThatThrownBy(() -> FenString.locationToSquare(-2)).isInstanceOf(IllegalArgumentException.class);", "    }", "", "    // Testing edge cases for square to location conversion with additional characters", "    @Test", "    public void squareToLocationCheckAndCheckmate() {", "        assertThat(FenString.squareToLocation(\"e2+\")).isEqualTo(12);", "        assertThat(FenString.squareToLocation(\"e2#\")).isEqualTo(12);", "        assertThatThrownBy(() -> FenString.squareToLocation(\"k9+\")).isInstanceOf(IllegalArgumentException.class);", "    }", "", "    // Testing invalid piece placements", "    @Test", "    public void invalidPiecePlacements() {", "        assertThatThrownBy(() -> new FenString(\"8/8/8/8/8/8/8/9 w KQkq -\")).isInstanceOf(IllegalArgumentException.class);", "        assertThatThrownBy(() -> new FenString(\"rnbqkbnr/pppppppp/7/8/8/8/PPPPPPPP/RNBQKBNR w KQkq -\")).isInstanceOf(IllegalArgumentException.class);", "    }", "", "    // Testing invalid castling rights scenarios", "    @Test", "    public void invalidCastlingRights() {", "        assertThatThrownBy(() -> new FenString(FenString.INITIAL_BOARD.replace(\"KQkq\", \"KQRkq\"))).isInstanceOf(IllegalArgumentException.class);", "        assertThatThrownBy(() -> new FenString(FenString.INITIAL_BOARD.replace(\"KQkq\", \"QQkk\"))).isInstanceOf(IllegalArgumentException.class);", "    }", "", "    // Testing en passant target with invalid ranks", "    @Test", "    public void enPassantTargetInvalidRanks() {", "        assertThatThrownBy(() -> new FenString(\"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq b2\")).isInstanceOf(IllegalArgumentException.class);", "        assertThatThrownBy(() -> new FenString(\"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq g7\")).isInstanceOf(IllegalArgumentException.class);", "    }", "", "    // Testing move counters with invalid values", "    @Test", "    public void moveCountersInvalidValues() {", "        assertThatThrownBy(() -> new FenString(\"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - -1 1\")).isInstanceOf(NumberFormatException.class);", "        assertThatThrownBy(() -> new FenString(\"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 -1\")).isInstanceOf(NumberFormatException.class);", "    }", "", "    // Testing valid scenarios with minimum and maximum values for move counters", "    @Test", "    public void moveCountersValidExtremes() {", "        FenString fsMin = new FenString(\"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\");", "        assertThat(fsMin.getHalfmoveClock()).isEqualTo(0);", "        assertThat(fsMin.getFullmoveCounter()).isEqualTo(1);", "", "        // Assuming 999 is a valid maximum for the sake of this example", "        FenString fsMax = new FenString(\"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 100 999\");", "        assertThat(fsMax.getHalfmoveClock()).isEqualTo(100);", "        assertThat(fsMax.getFullmoveCounter()).isEqualTo(999);", "    }"], "file_path": "src/test/java/com/stateofflux/chess/model/FenStringTest.java"}
{"Link_to_commit": "https://github.com/aryaman1989/auctionify/commit/b3dad52081f359dac0857ccdda9f059237106752", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 8, "n_files_impacted": 13, "longest_chunk": ["    primaryImage:", "      \"./img/items/DALL\u00b7E 2023-12-17 17.21.27 - A whimsical cat wearing a tiny wizard hat and cape, standing on its hind legs, with a background of twinkling stars and a crescent moon.png\",", "    title: \"The Wizard Cat\",", "    subtitle: \"A Magical Evening\",", "    detail:", "      \"A whimsical cat donning a wizard's attire, standing on hind legs amidst a starry night sky and a crescent moon, casting a spell of charm.\",", "    secondaryImage:", "      \"./img/items/DALL\u00b7E 2023-12-17 17.21.27 - A whimsical cat wearing a tiny wizard hat and cape, standing on its hind legs, with a background of twinkling stars and a crescent moon.png\","], "file_path": "js/items.js"}
{"Link_to_commit": "https://github.com/pump-factory/accountability-bot/commit/db937106c09683e627b1c1f4d527c5643e6cbe5d", "n-gram matched": "chatgpt to generate", "n_lines_longer_change": 17, "n_files_impacted": 1, "longest_chunk": ["\t\tconst chatUsers = await findUsersInChat.run({ chatId }, client)", "\t\tconst userNames = chatUsers.map((user) => user.name).join(', ')", "", "\t\tconst userMessage: ChatCompletionMessageParam = {", "\t\t\trole: 'user',", "\t\t\tcontent: `Good morning, ${userNames}! Today is a new opportunity to continue building positive habits. Whether it's ${habitStr} or any other personal goals, remember that each small step is a part of your journey towards success and well-being. Stay focused, stay motivated, and embrace the day with enthusiasm! You've got this!`,", "\t\t}", "", "\t\tlet chatMessage: string", "\t\ttry {", "\t\t\tchatMessage = await generateChatMessage([userMessage])", "\t\t} catch (error) {", "\t\t\tchatMessage = `Good morning, accountability champions! \ud83c\udf1e Today is a brand new opportunity to find your inner peace and clarity through meditation. Take a deep breath, commit to your practice, and let's make today another successful day on our journey to mindfulness and well-being. \ud83e\uddd8\u200d\u2640\ufe0f\ud83e\uddd8\u200d\u2642\ufe0f #MeditationMasters`", "\t\t\tconsole.error('Failed to generate chat message', error)", "\t\t}", "", "\t\tawait bot.telegram.sendMessage(chatId, chatMessage)"], "file_path": "src/cron.ts"}
{"Link_to_commit": "https://github.com/FlawlessByte/flawlessbyte.github.io/commit/620fa41a59f063bf80cb04e32a3d772ac47c7416", "n-gram matched": "co-authored by chatgpt", "n_lines_longer_change": 78, "n_files_impacted": 3, "longest_chunk": ["<!DOCTYPE html>", "<html lang=\"en\">", "<head>", "    <meta charset=\"UTF-8\">", "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">", "    <title>Retro Portfolio</title>", "    <link rel=\"stylesheet\" href=\"styles.css\">", "</head>", "<body>", "    <div class=\"container\">", "        <header>", "            <h1 id=\"welcome-text\">Hi there! Welcome to my Portfolio.</h1>", "        </header>", "        <nav>", "            <ul>", "                <li><a href=\"#about\">About Me</a></li>", "                <li><a href=\"#experience\">Experience</a></li>", "                <li><a href=\"#contact\">Contact</a></li>", "            </ul>", "        </nav>", "        <main>", "            <section id=\"about\">", "                <h2>About Me</h2>", "                <p>Hi, I'm Jimmy Jose, a passionate software engineer dedicated to automating and crafting robust systems for seamless operations and scale.</p>", "            </section>", "            <section id=\"experience\">", "                <h2>Work Experience</h2>", "                <ul>", "                    <li>", "                        <p>Worked on critical projects involving the migration of legacy application systems to Kubernetes architecture, minimizing downtime and ensuring seamless transition.</p>", "                    </li>", "                    <li>", "                        <p>Executed migration from AMD64-based compute to ARM64-based compute, resulting in a 30% reduction in infrastructure costs.</p>", "                    </li>", "                    <li>", "                        <p>Revamped Docker image build processes to support multiple OS architectures using Docker buildx, enhancing project efficiency and compatibility across diverse host environments.</p>", "                    </li>", "                    <li>", "                        <p>Facilitated collaboration between development and operations teams to streamline project deployment and maintain operational excellence.</p>", "                    </li>", "                    <li>", "                        <p>Performed capacity planning and system optimization, achieving cost savings and notable performance enhancements.</p>", "                    </li>", "                    <li>", "                        <p>Oversaw cluster bootstrapping of Kubernetes components utilizing Helmfile and custom scripts.</p>", "                    </li>", "                    <li>", "                        <p>Utilized Kubernetes Operators and Custom Resources to streamline application and resource provisioning.</p>", "                    </li>", "                    <li>", "                        <p>Architected and implemented scalable Kubernetes infrastructure utilizing kOps, EKS, and cluster components for hosting microservices.</p>", "                    </li>", "                    <li>", "                        <p>Automated customer migration projects across various internal stacks using Ansible and Python, resulting in a drastic reduction of time and effort.</p>", "                    </li>", "                    <li>", "                        <p>Initiated the development of an incident management bot and deployment-as-a-service initiative, significantly automating processes and reducing manual intervention.</p>", "                    </li>", "                    <li>", "                        <p>Implemented robust automation pipelines leveraging Jenkins, Buildkite, and ArgoCD, effectively reducing operational overhead and ensuring reliability.</p>", "                    </li>", "                    <li>", "                        <p>Optimized Docker image sizes by approximately 95% through the adoption of multi-stage builds and scratch images.</p>", "                    </li>", "                </ul>", "            </section>", "            <section id=\"contact\">", "                <h2>Contact</h2>", "                <p>Email: jimmyjose009@gmail.com</p>", "            </section>", "        </main>", "        <footer>", "            <p>&copy; 2024 Jimmy Jose</p>", "        </footer>", "    </div>", "    <script src=\"scripts.js\"></script>", "</body>", "</html>"], "file_path": "scripts.js"}
{"Link_to_commit": "https://github.com/crowdsecurity/crowdsec-wasm-playground/commit/a701de1f201deb8c96b3b1d1f5ddfb6e590b984e", "n-gram matched": "co-authored by chatgpt", "n_lines_longer_change": 21, "n_files_impacted": 1, "longest_chunk": ["\t\t\t\t\t\t<div>", "\t\t\t\t\t\t\t<TableContainer component={Paper}>", "\t\t\t\t\t\t\t\t<Table aria-label=\"simple table\" size=\"small\">", "\t\t\t\t\t\t\t\t\t<TableHead>", "\t\t\t\t\t\t\t\t\t\t<TableRow>", "\t\t\t\t\t\t\t\t\t\t\t{columns.map((column) => (<TableCell>{column.title}</TableCell>))}", "\t\t\t\t\t\t\t\t\t\t</TableRow>", "\t\t\t\t\t\t\t\t\t</TableHead>", "\t\t\t\t\t\t\t\t\t<TableBody>", "", "\t\t\t\t\t\t\t\t\t\t{outputDictValue.filter(", "\t\t\t\t\t\t\t\t\t\t\t(key) => {", "\t\t\t\t\t\t\t\t\t\t\t\treturn key.value !== \"\";", "\t\t\t\t\t\t\t\t\t\t\t}).map((row) => (<TableRow key={row.pattern}>", "\t\t\t\t\t\t\t\t\t\t\t\t<CustomTableCell color={row.color} align=\"right\">{row.pattern}</CustomTableCell>", "\t\t\t\t\t\t\t\t\t\t\t\t<CustomTableCell color={row.color} align=\"right\">{row.value}</CustomTableCell>", "\t\t\t\t\t\t\t\t\t\t\t</TableRow>))}", "\t\t\t\t\t\t\t\t\t</TableBody>", "\t\t\t\t\t\t\t\t</Table>", "\t\t\t\t\t\t\t</TableContainer>", "\t\t\t\t\t\t</div>"], "file_path": "crowdsec-playground/src/components/grokDebugger/grokDebugger.js"}
{"Link_to_commit": "https://github.com/blocktorch-xyz/data-collections/commit/ed618b44c94fc3fa642c428b3381f0a47ac104de", "n-gram matched": "co-authored by chatgpt", "n_lines_longer_change": 40, "n_files_impacted": 4, "longest_chunk": ["import fs from 'fs';", "", "function base64ToHex(input: any): any {", "  // skip numbers", "  if (typeof input === 'string' && /^[0-9]+$/.test(input)) {", "    return parseInt(input);", "  }", "", "  const base64Regex = /^[A-Za-z0-9+/=]+$/; // regex to match base64 string", "  if (typeof input === 'string' && base64Regex.test(input)) {", "    const hex = '0x' + Buffer.from(input, 'base64').toString('hex');", "    return hex;", "  }", "", "  if (Array.isArray(input)) {", "    return input.map((val) => base64ToHex(val));", "  }", "", "  if (input !== null && typeof input === 'object') {", "    const newObj: { [key: string]: any } = {};", "    for (const prop in input) {", "      newObj[prop] = base64ToHex(input[prop]);", "    }", "    return newObj;", "  }", "", "  return input;", "}", "", "// sample usage", "const jsonString = fs.readFileSync('./encoded/pool_calls64.json', 'utf-8');", "const jsonObject = JSON.parse(jsonString);", "const result = base64ToHex(jsonObject);", "fs.writeFile(\"../aave/pool_calls.json\", JSON.stringify(result, null, 2), 'utf8', function (fsErr: any) {", "  if (fsErr) {", "      console.log(\"An error occured while writing JSON Object to File.\");", "      return console.log(fsErr);", "  }", "  console.log(\"JSON file has been saved.\");", "});"], "file_path": "utils/decode64.ts"}
{"Link_to_commit": "https://github.com/alex-donaldson/alarm-clock/commit/4cae4de72281c3507da0ca7040558aac8578094b", "n-gram matched": "help of copilot", "n_lines_longer_change": 175, "n_files_impacted": 6, "longest_chunk": ["#!/usr/bin/python3", "", "import urllib.request", "import json", "from datetime import datetime, timezone", "", "# Constants", "AQI_URL = 'http://api.openweathermap.org/data/2.5/air_pollution/forecast?lat={lat}&lon={lon}&appid={key}'", "KEY_FILE = '/private/keys/openweather.txt'", "OUT_TIME_FORMAT = '%Y-%m-%d %H:%M:%S'", "", "# AQI Categories", "AQI_GOOD = 1", "AQI_FAIR = 2", "AQI_MODERATE = 3", "AQI_POOR = 4", "AQI_VERY_POOR = 5", "", "AQI_CATEGORY_MAP = {", "    AQI_GOOD: \"Good\",", "    AQI_FAIR: \"Fair\",", "    AQI_MODERATE: \"Moderate\",", "    AQI_POOR: \"Poor\",", "    AQI_VERY_POOR: \"Very Poor\"", "}", "", "class RemoteAQI:", "    \"\"\"", "    A class to fetch and process AQI data from the OpenWeatherMap API.", "    \"\"\"", "", "    def __init__(self, lat, lon, key):", "        \"\"\"", "        Initialize the RemoteAQI class with latitude, longitude, and API key.", "        \"\"\"", "        self.lat = lat", "        self.lon = lon", "        self.key = key", "        self.forecast_url = AQI_URL.format(lat=self.lat, lon=self.lon, key=self.key)", "", "    def get_raw_forecast_data(self):", "        \"\"\"", "        Fetch raw AQI forecast data from the API.", "        \"\"\"", "        response = urllib.request.urlopen(self.forecast_url)", "        return json.loads(response.read().decode('utf8'))", "", "    def get_forecast(self):", "        \"\"\"", "        Get a detailed AQI forecast including local timestamps, AQI values, and categories.", "        \"\"\"", "        raw_data = self.get_raw_forecast_data()", "        forecast = []", "", "        for entry in raw_data['list']:", "            timestamp = entry['dt']", "            # Convert UTC timestamp to local time", "            date_time = convert_timestamp_to_local(timestamp)", "", "            aqi = entry['main']['aqi']", "            category = AQI_CATEGORY_MAP.get(aqi, \"Unknown\")", "            components = entry['components']", "", "            forecast.append({", "                \"timestamp\": date_time,", "                \"aqi\": aqi,", "                \"category\": category,", "                \"components\": components", "            })", "", "        return forecast", "", "    def get_hourly_aqi_forecast(self):", "        \"\"\"", "        Get the hourly AQI forecast for the next 24 hours.", "        \"\"\"", "        forecast = self.get_forecast()", "        hourly_aqi = []", "", "        for entry in forecast[:24]:", "            timestamp = entry[\"timestamp\"]", "            aqi = entry[\"aqi\"]", "            category = entry[\"category\"]", "            components = entry[\"components\"]", "", "            hourly_aqi.append({", "                \"timestamp\": timestamp,", "                \"aqi\": aqi,", "                \"category\": category", "            })", "        return hourly_aqi", "    ", "    def get_detailed_current_aqi(self):", "        \"\"\"", "        Get the current AQI and its components.", "        \"\"\"", "        forecast = self.get_forecast()", "        current_aqi = forecast[0]", "        timestamp = current_aqi[\"timestamp\"]", "        aqi = current_aqi[\"aqi\"]", "        category = current_aqi[\"category\"]", "        components = current_aqi[\"components\"]", "        # Convert components to a human-readable format", "        components = {k: f\"{v} \u03bcg/m\u00b3\" for k, v in components.items()}", "        ", "        return {", "            \"timestamp\": timestamp,", "            \"aqi\": aqi,", "            \"category\": category,", "            \"components\": components", "        }", "    ", "    def get_daily_aqi_forecast(self):", "        \"\"\"", "        Get the daily AQI forecast by calculating the maximum AQI for each day.", "        \"\"\"", "        forecast = self.get_forecast()", "        daily_aqi = {}", "", "        for entry in forecast:", "            date = entry[\"timestamp\"].split(\" \")[0]  # Extract the date (YYYY-MM-DD)", "            aqi = entry[\"aqi\"]", "            category = entry[\"category\"]", "", "            if date not in daily_aqi or aqi > daily_aqi[date][\"aqi\"]:", "                daily_aqi[date] = {", "                    \"date\": date,", "                    \"aqi\": aqi,", "                    \"category\": category", "                }", "", "        # Convert the dictionary to a sorted list of daily AQI forecasts", "        return [daily_aqi[date] for date in sorted(daily_aqi.keys())]", "", "def convert_timestamp_to_local(timestamp): ", "    \"\"\"", "    Convert a UTC timestamp to local time.", "    \"\"\"", "    utc_time = datetime.fromtimestamp(timestamp, timezone.utc)", "    local_time = utc_time.astimezone()  # Converts to local time zone", "    return local_time.strftime(OUT_TIME_FORMAT)", "", "def main():", "    \"\"\"", "    Main function to fetch and display AQI data.", "    \"\"\"", "    with open(KEY_FILE, encoding=\"utf-8\") as f:", "        key = f.read().strip()", "", "    # Example coordinates for testing", "    lat, lon = 47.697, -122.3222", "    ra = RemoteAQI(lat, lon, key)", "    daily_aqi_forecast = ra.get_daily_aqi_forecast()", "", "    # Print the current AQI", "    current_aqi = ra.get_detailed_current_aqi()", "    print(f\"[Current AQI] {current_aqi['timestamp']}\")", "    print(f\"  AQI: {current_aqi['aqi']} ({current_aqi['category']})\")", "    print(f\"  Components: {current_aqi['components']}\")", "    print()", "    # Print the hourly AQI forecast", "    hourly_aqi_forecast = ra.get_hourly_aqi_forecast()", "    print(\"[Hourly AQI Forecast]\")", "    for hour in hourly_aqi_forecast:", "        print(f\"Timestamp: {hour['timestamp']}\")", "        print(f\"  AQI: {hour['aqi']} ({hour['category']})\")", "        print()   ", "    print(\"[7-Day AQI Forecast]\")", "    for day in daily_aqi_forecast:", "        print(f\"Date: {day['date']}\")", "        print(f\"  AQI: {day['aqi']} ({day['category']})\")", "        print()", "", "if __name__ == '__main__':", "    main()"], "file_path": "openweatheraqi.py"}
{"Link_to_commit": "https://github.com/Antvirf/stui/commit/c9fb6105fa833c9524b0481486e52cde8abe040d", "n-gram matched": "help of copilot", "n_lines_longer_change": 321, "n_files_impacted": 6, "longest_chunk": ["package model", "", "import (", "\t\"bufio\"", "\t\"encoding/gob\"", "\t\"fmt\"", "\t\"os\"", "\t\"path\"", "\t\"runtime\"", "\t\"sync\"", "\t\"sync/atomic\"", "\t\"testing\"", "\t\"time\"", "", "\t\"github.com/antvirf/stui/internal/config\"", "\t\"github.com/stretchr/testify/assert\"", "\t\"github.com/stretchr/testify/require\"", ")", "", "func init() {", "\t// Register types for gob", "\tgob.Register(&[]config.ColumnConfig{})", "\tgob.Register(TableData{})", "}", "", "// setupTestCache creates a temporary cache for testing", "func setupTestCache(t *testing.T) (*SacctCache, string) {", "\t// Create a temporary directory for test", "\ttempDir, err := os.MkdirTemp(\"\", \"sacct_cache_test\")", "\trequire.NoError(t, err)", "", "\t// Create cache file path", "\tfilePath := path.Join(tempDir, \"sacct_cache.gob\")", "", "\t// Create a test cache", "\tfile, err := os.OpenFile(filePath, os.O_RDWR|os.O_CREATE|os.O_APPEND, 0600)", "\trequire.NoError(t, err)", "", "\tcache := &SacctCache{", "\t\tfile:   file,", "\t\twriter: nil, // will be set after initialization", "\t\treader: nil, // will be set after initialization", "\t}", "", "\t// Set up reader/writer", "\tcache.writer = bufio.NewWriter(file)", "\tcache.reader = bufio.NewReader(file)", "", "\treturn cache, tempDir", "}", "", "// createTestTableData creates a TableData for testing", "func createTestTableData(t *testing.T, rows int) *TableData {", "\theaders := []config.ColumnConfig{", "\t\t{Name: \"JobIDRaw\"},", "\t\t{Name: \"JobID\"},", "\t\t{Name: \"JobName\"},", "\t\t{Name: \"Partition\"},", "\t\t{Name: \"State\"},", "\t}", "", "\ttableData := &TableData{", "\t\tHeaders: &headers,", "\t\tRows:    make([][]string, rows),", "\t}", "", "\tfor i := 0; i < rows; i++ {", "\t\ttableData.Rows[i] = []string{", "\t\t\tfmt.Sprintf(\"job_%d\", i),      // JobIDRaw", "\t\t\tfmt.Sprintf(\"%d\", i),          // JobID", "\t\t\tfmt.Sprintf(\"test_job_%d\", i), // JobName", "\t\t\t\"test_partition\",              // Partition", "\t\t\t\"RUNNING\",                     // State", "\t\t}", "\t}", "", "\treturn tableData", "}", "", "// cleanupTestCache removes the temporary directory and files", "func cleanupTestCache(t *testing.T, tempDir string) {", "\terr := os.RemoveAll(tempDir)", "\trequire.NoError(t, err)", "}", "", "// TestBasicCacheOperations tests basic writing and reading operations", "func TestBasicCacheOperations(t *testing.T) {", "\tcache, tempDir := setupTestCache(t)", "\tdefer cleanupTestCache(t, tempDir)", "", "\t// Create test data", "\ttestData := createTestTableData(t, 10)", "\tstartTime := time.Now().Add(-24 * time.Hour)", "\tendTime := time.Now()", "", "\t// Test writing to cache", "\terr := cache.WriteToCache(testData, startTime, endTime, true)", "\tassert.NoError(t, err)", "", "\t// Test reading from cache", "\treadData, err := cache.GetFromCache()", "\tassert.NoError(t, err)", "\tassert.True(t, cache.IsUsable)", "", "\t// Verify data integrity", "\tassert.Equal(t, len(testData.Rows), len(readData.Rows))", "\tassert.Equal(t, (*testData.Headers)[0].Name, (*readData.Headers)[0].Name)", "", "\t// Check content field is updated", "\tassert.Equal(t, startTime.Unix(), cache.Content.StartTime.Unix()) // Compare unix timestamps to avoid precision issues", "\tassert.Equal(t, endTime.Unix(), cache.Content.EndTime.Unix())", "}", "", "// TestCorruptedCache tests recovery from corrupted cache file", "func TestCorruptedCache(t *testing.T) {", "\tcache, tempDir := setupTestCache(t)", "\tdefer cleanupTestCache(t, tempDir)", "", "\t// Write invalid data to the cache file", "\t_, err := cache.file.Write([]byte(\"This is not valid gob data\"))", "\trequire.NoError(t, err)", "\trequire.NoError(t, cache.file.Sync())", "", "\t// Try to read, should fail but not panic", "\t_, err = cache.GetFromCache()", "\tassert.Error(t, err)", "\tassert.False(t, cache.IsUsable)", "", "\t// Now write valid data, should work", "\ttestData := createTestTableData(t, 5)", "\terr = cache.WriteToCache(testData, time.Now(), time.Now(), true)", "\tassert.NoError(t, err)", "", "\t// Try to read again, should succeed", "\treadData, err := cache.GetFromCache()", "\tassert.NoError(t, err)", "\tassert.True(t, cache.IsUsable)", "\tassert.Equal(t, len(testData.Rows), len(readData.Rows))", "}", "", "// TestEmptyCache tests handling of empty cache file", "func TestEmptyCache(t *testing.T) {", "\tcache, tempDir := setupTestCache(t)", "\tdefer cleanupTestCache(t, tempDir)", "", "\t// Try to read from empty cache", "\tdata, err := cache.GetFromCache()", "\tassert.Error(t, err)", "\tassert.False(t, cache.IsUsable)", "\tassert.Nil(t, data)", "", "\t// Write empty data", "\temptyData := createTestTableData(t, 0)", "\terr = cache.WriteToCache(emptyData, time.Now(), time.Now(), true)", "\tassert.NoError(t, err)", "", "\t// Read empty data", "\treadData, err := cache.GetFromCache()", "\tassert.NoError(t, err)", "\tassert.Equal(t, 0, len(readData.Rows))", "}", "", "// TestCacheMerging tests the merging functionality", "func TestCacheMerging(t *testing.T) {", "\tcache, tempDir := setupTestCache(t)", "\tdefer cleanupTestCache(t, tempDir)", "", "\t// Create initial data", "\tinitialData := createTestTableData(t, 10)", "\tstartTime := time.Now().Add(-48 * time.Hour)", "\tmidTime := time.Now().Add(-24 * time.Hour)", "", "\t// Write initial data", "\terr := cache.WriteToCache(initialData, startTime, midTime, true)", "\tassert.NoError(t, err)", "", "\t// Create new data (with some overlap to test merging)", "\tnewData := createTestTableData(t, 15) // More rows", "\tfor i := 0; i < 5; i++ {", "\t\t// Modify first 5 rows to test replacement", "\t\tnewData.Rows[i][4] = \"COMPLETED\" // Change state", "\t}", "\tendTime := time.Now()", "", "\t// Write new data with merge", "\terr = cache.WriteToCache(newData, midTime, endTime, false)", "\tassert.NoError(t, err)", "", "\t// Read merged data", "\tmergedData, err := cache.GetFromCache()", "\tassert.NoError(t, err)", "", "\t// Verify merged data", "\tassert.GreaterOrEqual(t, len(mergedData.Rows), 15) // Should have at least 15 rows", "", "\t// Check if state of first 5 jobs was updated", "\tfoundCompleted := false", "\tfor _, row := range mergedData.Rows {", "\t\tif row[0] == \"job_0\" && row[4] == \"COMPLETED\" {", "\t\t\tfoundCompleted = true", "\t\t\tbreak", "\t\t}", "\t}", "\tassert.True(t, foundCompleted, \"Modified data should be present in merged result\")", "}", "", "// TestConcurrentAccess tests thread safety of the cache", "func TestConcurrentAccess(t *testing.T) {", "\tcache, tempDir := setupTestCache(t)", "\tdefer cleanupTestCache(t, tempDir)", "", "\t// Write initial data", "\tinitialData := createTestTableData(t, 10)", "\terr := cache.WriteToCache(initialData, time.Now().Add(-24*time.Hour), time.Now(), true)", "\tassert.NoError(t, err)", "", "\tvar wg sync.WaitGroup", "\tconcurrentReaders := 10", "\tconcurrentWriters := 2", "\tsuccessfulReads := int32(0)", "\tsuccessfulWrites := int32(0)", "", "\t// Start concurrent readers", "\tfor i := 0; i < concurrentReaders; i++ {", "\t\twg.Add(1)", "\t\tgo func(id int) {", "\t\t\tdefer wg.Done()", "\t\t\tdata, err := cache.GetFromCache()", "\t\t\tif err == nil && len(data.Rows) > 0 {", "\t\t\t\tatomic.AddInt32(&successfulReads, 1)", "\t\t\t}", "\t\t}(i)", "\t}", "", "\t// Start concurrent writers", "\tfor i := 0; i < concurrentWriters; i++ {", "\t\twg.Add(1)", "\t\tgo func(id int) {", "\t\t\tdefer wg.Done()", "\t\t\tdata := createTestTableData(t, 5+id)", "\t\t\terr := cache.WriteToCache(data, time.Now().Add(-12*time.Hour), time.Now(), false)", "\t\t\tif err == nil {", "\t\t\t\tatomic.AddInt32(&successfulWrites, 1)", "\t\t\t}", "\t\t}(i)", "\t}", "", "\twg.Wait()", "", "\t// Verify that operations completed without deadlock", "\tassert.Greater(t, int(successfulReads), 0)", "\tassert.Greater(t, int(successfulWrites), 0)", "}", "", "// TestCacheFilePermissions tests handling of permission errors", "func TestCacheFilePermissions(t *testing.T) {", "\t// Skip on Windows since permission testing works differently", "\tif runtime.GOOS == \"windows\" {", "\t\tt.Skip(\"Skipping permission test on Windows\")", "\t}", "", "\ttempDir, err := os.MkdirTemp(\"\", \"sacct_cache_test\")", "\trequire.NoError(t, err)", "\tdefer os.RemoveAll(tempDir)", "", "\tfilePath := path.Join(tempDir, \"readonly_cache.gob\")", "", "\t// Create file with read-only permissions", "\tfile, err := os.OpenFile(filePath, os.O_RDWR|os.O_CREATE, 0400)", "\trequire.NoError(t, err)", "\tfile.Close()", "", "\t// Try to open the cache (should fail on write)", "\tfile, err = os.OpenFile(filePath, os.O_RDWR, 0400)", "\tif err == nil {", "\t\t// Some platforms may allow opening but fail on write", "\t\tcache := &SacctCache{", "\t\t\tfile:   file,", "\t\t\twriter: bufio.NewWriter(file),", "\t\t\treader: bufio.NewReader(file),", "\t\t}", "", "\t\ttestData := createTestTableData(t, 5)", "\t\terr = cache.WriteToCache(testData, time.Now(), time.Now(), true)", "\t\tassert.Error(t, err)", "\t}", "}", "", "// TestCacheReinitialization tests reinitialization from disk", "func TestCacheReinitialization(t *testing.T) {", "\tcache1, tempDir := setupTestCache(t)", "\tdefer cleanupTestCache(t, tempDir)", "", "\t// Write data with first instance", "\ttestData := createTestTableData(t, 10)", "\tstartTime := time.Now().Add(-24 * time.Hour)", "\tendTime := time.Now()", "", "\terr := cache1.WriteToCache(testData, startTime, endTime, true)", "\tassert.NoError(t, err)", "", "\t// Close first instance", "\tcache1.file.Close()", "", "\t// Create second instance pointing to same file", "\tfilePath := cache1.file.Name()", "\tfile2, err := os.OpenFile(filePath, os.O_RDWR, 0600)", "\trequire.NoError(t, err)", "", "\tcache2 := &SacctCache{", "\t\tfile:   file2,", "\t\twriter: bufio.NewWriter(file2),", "\t\treader: bufio.NewReader(file2),", "\t}", "", "\t// Read with second instance", "\treadData, err := cache2.GetFromCache()", "\tassert.NoError(t, err)", "\tassert.True(t, cache2.IsUsable)", "\tassert.Equal(t, len(testData.Rows), len(readData.Rows))", "}"], "file_path": "internal/model/sacct_fetchers.go"}
{"Link_to_commit": "https://github.com/AthaDeokar19/jasper-ai-clone/commit/83f6e51864ed82cc18e729591d592753b114cade", "n-gram matched": "help of copilot", "n_lines_longer_change": 29, "n_files_impacted": 7, "longest_chunk": ["const carouselNext = document.querySelector('.carousel-next');", "const carouselPrev = document.querySelector('.carousel-prev');", "", "if (carouselNext) {", "  carouselNext.addEventListener('click', () => {", "    currentTestimonial = (currentTestimonial + 1) % testimonials.length;", "    showTestimonial(currentTestimonial);", "  });", "}", "", "if (carouselPrev) {", "  carouselPrev.addEventListener('click', () => {", "    currentTestimonial = (currentTestimonial - 1 + testimonials.length) % testimonials.length;", "    showTestimonial(currentTestimonial);", "  });", "if (loginForm) {", "  loginForm.addEventListener('submit', (e) => {", "    e.preventDefault();", "    const email = document.getElementById('email').value;", "    if (!email.includes('@')) {", "      const emailError = document.getElementById('email-error');", "      if (emailError) {", "        emailError.textContent = 'Valid email required';", "      }", "      return;", "    }", "    // Proceed with login", "  });", "}"], "file_path": "script.js"}
{"Link_to_commit": "https://github.com/Yassin-info/holbertonschool-simple_shell/commit/30de19c4ce32c518d64972ae0f84a2de26ca372b", "n-gram matched": "help of copilot", "n_lines_longer_change": 30, "n_files_impacted": 2, "longest_chunk": ["#include \"main.h\"", "/**", " * prompt - checks mode and prints prompt if in interactive mode", " * @fd: file stream", " */", "void prompt(int fd)", "{", "    struct stat buf;", "    ", "    if (fstat(fd, &buf) == -1)", "    {", "        return;", "    }", "", "    if (S_ISCHR(buf.st_mode))", "        _puts(PROMPT);", "}", "/**", "* _puts - prints a string without a \\n", "* @str: string to print", "* Return: void", "*/", "void _puts(char *str)", "{", "\tunsigned int length;", "", "\tlength = _strlen(str);", "", "\twrite(STDOUT_FILENO, str, length);", "}"], "file_path": "wich.c"}
{"Link_to_commit": "https://github.com/Hbindra11/Project-8-BE/commit/5c45dcdb261fa36123789352fd62eb4404c2839f", "n-gram matched": "help of copilot", "n_lines_longer_change": 23, "n_files_impacted": 5, "longest_chunk": ["# Project 8 Fullstack Travelblog Backend API", "", "This backend API is designed to support the functionality of the Project 8 travel blog posts application. It provides endpoints for managing data, handling user authentication, and enabling seamless communication between the frontend and the database. Built with Node.js, it follows RESTful principles and ensures secure and efficient data handling.", "", "## Features", "- CRUD operations for managing resources", "- Integration with a database for persistent storage", "- Error handling and validation", "- Scalable and modular architecture", "- API endpoints for the posts resource:", "  - **GET /posts**:  Retrieve all posts.", "  - **GET /posts/:id**:  Retrieve a single post by ID.", "  - **POST /posts**:  Create a new post.", "  - **PUT /posts/:id**:  Update an existing post by ID.", "  - **DELETE /posts/:id**:  Delete a post by ID.", "", "## Technologies Used", "- Node.js", "- Express.js", "- PostgreSQL database", "- Middleware for request validation and error handling", "", ""], "file_path": "controllers/blogPosts.js"}
{"Link_to_commit": "https://github.com/ak1ra-lab/lunar-birthday-ical/commit/a4689abb9a009f34f67f2a06a58e91ac63d1a89d", "n-gram matched": "help of copilot", "n_lines_longer_change": 91, "n_files_impacted": 6, "longest_chunk": ["import datetime", "import zoneinfo", "from pathlib import Path", "", "from icalendar import Calendar, Event, vCalAddress", "", "from lunar_birthday_ical.ical import (", "    add_attendees_to_event,", "    add_event_to_calendar,", "    add_reminders_to_event,", "    create_calendar,", "    get_local_datetime,", "    local_datetime_to_utc_datetime,", ")", "from tests.__init__ import config", "", "", "def test_get_local_datetime():", "    local_date = \"2023-10-01\"", "    local_time = \"12:00:00\"", "    timezone = zoneinfo.ZoneInfo(\"UTC\")", "    result = get_local_datetime(local_date, local_time, timezone)", "    expected = datetime.datetime(2023, 10, 1, 12, 0, tzinfo=timezone)", "    assert result == expected", "", "", "def test_local_datetime_to_utc_datetime():", "    local_datetime = datetime.datetime(", "        2023, 10, 1, 12, 0, tzinfo=zoneinfo.ZoneInfo(\"Asia/Shanghai\")", "    )", "    result = local_datetime_to_utc_datetime(local_datetime)", "    expected = datetime.datetime(2023, 10, 1, 4, 0, tzinfo=zoneinfo.ZoneInfo(\"UTC\"))", "    assert result == expected", "", "", "def test_add_reminders_to_event():", "    event = Event()", "    reminders = [1, 2]", "    summary = \"Test Event\"", "    add_reminders_to_event(event, reminders, summary)", "    assert len(event.subcomponents) == 2", "", "", "def test_add_attendees_to_event_one():", "    event = Event()", "    attendees = [\"test@example.com\"]", "    add_attendees_to_event(event, attendees)", "    assert (", "        len(", "            [event.get(\"ATTENDEE\")]", "            if isinstance(event.get(\"ATTENDEE\"), vCalAddress)", "            else event.get(\"ATTENDEE\")", "        )", "        == 1", "    )", "", "", "def test_add_attendees_to_event_multi():", "    event = Event()", "    attendees = [\"test@example.com\", \"test@example.net\"]", "    add_attendees_to_event(event, attendees)", "    assert (", "        len(", "            [event.get(\"ATTENDEE\")]", "            if isinstance(event.get(\"ATTENDEE\"), vCalAddress)", "            else event.get(\"ATTENDEE\")", "        )", "        == 2", "    )", "", "", "def test_add_event_to_calendar():", "    calendar = Calendar()", "    dtstart = datetime.datetime(2023, 10, 1, 12, 0, tzinfo=zoneinfo.ZoneInfo(\"UTC\"))", "    dtend = dtstart + datetime.timedelta(hours=1)", "    summary = \"Test Event\"", "    reminders = [1]", "    attendees = [\"test@example.com\"]", "    add_event_to_calendar(calendar, dtstart, dtend, summary, reminders, attendees)", "    assert len(calendar.subcomponents) == 1", "", "", "def test_create_calendar(tmp_path: Path):", "    output = tmp_path / \"test.ics\"", "    create_calendar(config, output)", "    assert output.exists()", "    with output.open(\"rb\") as f:", "        calendar_data = f.read()", "    calendar = Calendar.from_ical(calendar_data)", "    assert len(calendar.subcomponents) > 0", "    assert calendar.get(\"X-WR-CALNAME\") == \"Test Calendar\""], "file_path": "tests/test_lunar.py"}
{"Link_to_commit": "https://github.com/BenSlabbert/tx-manager/commit/17acf90374e9457f37443e90151239b9d74d1794", "n-gram matched": "help of copilot", "n_lines_longer_change": 12, "n_files_impacted": 2, "longest_chunk": ["        .intercept(Advice.withCustomMapping()", "            .bind(Transactional.class, new AnnotationDescription.Loadable<Transactional>() {", "                @Override", "                public Class<? extends Annotation> getAnnotationType() {", "                    return Transactional.class;", "                }", "", "                @Override", "                public Transactional load() {", "                    return annotationDescription.prepare(Transactional.class).load();", "                }", "            }).to(RequiresNewAdvice.class))"], "file_path": "plugin/src/main/java/github/benslabbert/txmanager/plugin/TransactionalAdvicePlugin.java"}
{"Link_to_commit": "https://github.com/radkesvat/WaterWall/commit/63b6cdc31f964eec3f1bb906e1c68b454765e060", "n-gram matched": "help of copilot", "n_lines_longer_change": 27, "n_files_impacted": 9, "longest_chunk": ["", "/*", "    @brief Destroys a buffer queue and releases its resources.", "    @param self A pointer to the buffer queue to be destroyed.", "*/", "void bufferqueueDestory(buffer_queue_t *self);", "", "/*", "    @brief Pushes an sbuf_t pointer onto the back of the queue.", "    @param self A pointer to the buffer queue.", "    @param b A pointer to the sbuf_t to be added to the queue.", "*/", "void bufferqueuePush(buffer_queue_t *self, sbuf_t *b);", "", "/*", "    @brief Pops an sbuf_t pointer from the front of the queue.", "    @param self A pointer to the buffer queue.", "    @return A pointer to the sbuf_t at the front of the queue, or NULL if the queue is empty.", "*/", "sbuf_t *bufferqueuePop(buffer_queue_t *self);", "", "/*", "    @brief Gets the number of elements in the queue.", "    @param self A pointer to the buffer queue.", "    @return The number of sbuf_t pointers currently in the queue.", "*/", "size_t bufferqueueLen(buffer_queue_t *self);"], "file_path": "ww/bufio/context_queue.c"}
{"Link_to_commit": "https://github.com/Zlatimir/SoftUni-QA-Front-End/commit/d58ac8c8ff2c1be3ab2d4bb17a403d5d7b7522b0", "n-gram matched": "help of copilot", "n_lines_longer_change": 39, "n_files_impacted": 3, "longest_chunk": ["async function fetchData() {", "    try {", "        const response = await fetch('https://swapi.dev/api/people/');", "        const data = await response.json();", "        createTable(data.results);", "    } catch (error) {", "        console.error('Error fetching data:', error);", "    }", "}", "", "function createTable(data) {", "    const table = document.createElement('table');", "    const headerRow = document.createElement('tr');", "", "    // Create table headers", "    const headers = ['Name', 'Height', 'Mass', 'Hair Color', 'Skin Color', 'Eye Color', 'Birth Year', 'Gender'];", "    headers.forEach(headerText => {", "        const header = document.createElement('th');", "        header.textContent = headerText;", "        headerRow.appendChild(header);", "    });", "    table.appendChild(headerRow);", "", "    // Create table rows", "    data.forEach(item => {", "        const row = document.createElement('tr');", "        Object.values(item).slice(0, 8).forEach(text => {", "            const cell = document.createElement('td');", "            cell.textContent = text;", "            row.appendChild(cell);", "        });", "        table.appendChild(row);", "    });", "", "    // Append table to the div", "    const dataTable = document.getElementById('data-table');", "    dataTable.innerHTML = ''; // Clear any existing content", "    dataTable.appendChild(table);", "}"], "file_path": "11.JS-Async-Exercises-2/01-Network-Request-Vizualization-Experiments/app.js"}
{"Link_to_commit": "https://github.com/manulea/Construction/commit/3189188768201dfee19a942d86770d1f7b46446d", "n-gram matched": "help of copilot", "n_lines_longer_change": 88, "n_files_impacted": 3, "longest_chunk": ["});", "", "// Add dynamic background gradient for the photo wall", "let fadeTimeout;", "photoWall.addEventListener(\"mousemove\", (event) => {", "    const { clientX, clientY, currentTarget } = event;", "    const { width, height, left, top } = currentTarget.getBoundingClientRect();", "    const xPercent = ((clientX - left) / width) * 100;", "    const yPercent = ((clientY - top) / height) * 100;", "", "    // Update the background gradient based on the cursor's position", "    photoWall.style.background = `radial-gradient(circle at ${xPercent}% ${yPercent}%, #ff7eb3, #ff758c, #ff6a6a)`;", "", "    // Clear any existing fade timeout", "    clearTimeout(fadeTimeout);", "", "    // Set a timeout to fade the gradient when the cursor stops moving", "    fadeTimeout = setTimeout(() => {", "        photoWall.style.transition = \"background 0.8s ease\"; // Smooth fade transition", "        photoWall.style.background = `radial-gradient(circle at ${xPercent}% ${yPercent}%, white, #ff758c, #ff7eb3)`;", "    }, 300); // 300ms delay before fading", "});", "", "// Function to randomly swap two images in the active set", "function swapRandomImages() {", "    // Select the active set", "    const activeSet = document.querySelector(\".photo-wall.set.active\");", "", "    if (!activeSet) return; // Ensure there is an active set", "", "    // Select all images within the active set", "    const images = activeSet.querySelectorAll(\"img\");", "", "    if (images.length < 2) return; // Ensure there are at least two images to swap", "", "    // Select two random images", "    const firstIndex = Math.floor(Math.random() * images.length);", "    let secondIndex;", "    do {", "        secondIndex = Math.floor(Math.random() * images.length);", "    } while (secondIndex === firstIndex); // Ensure the two indices are different", "", "    const firstImage = images[firstIndex];", "    const secondImage = images[secondIndex];", "", "    // Add fade-out effect", "    firstImage.style.transition = \"opacity 0.5s ease\";", "    secondImage.style.transition = \"opacity 0.5s ease\";", "    firstImage.style.opacity = \"0\";", "    secondImage.style.opacity = \"0\";", "", "    // After fade-out, swap positions and fade back in", "    setTimeout(() => {", "        // Swap the actual DOM positions", "        const parent = firstImage.parentNode;", "        parent.insertBefore(secondImage, firstImage);", "", "        // Fade back in", "        firstImage.style.opacity = \"1\";", "        secondImage.style.opacity = \"1\";", "    }, 500); // Match the duration of the fade-out effect", "}", "", "// Set an interval to swap images every 3-5 seconds in the active set", "setInterval(() => {", "    swapRandomImages();", "}, Math.random() * 2000 + 3000); // Random interval between 3-5 seconds", "", "// Carousel Functionality", "const sets = document.querySelectorAll(\".photo-wall.set\"); // Select all sets", "const rightArrow = document.querySelector(\".carousel-arrow.right-arrow\"); // Select the right arrow button", "", "let currentSetIndex = 0; // Track the current set index", "", "// Function to show the current set", "function showSet(index) {", "    sets.forEach((set, i) => {", "        set.classList.toggle(\"active\", i === index); // Show the active set, hide others", "    });", "}", "", "// Show the first set initially", "showSet(currentSetIndex);", "", "// Handle Right Arrow Click", "rightArrow.addEventListener(\"click\", () => {", "    currentSetIndex = (currentSetIndex + 1) % sets.length; // Move to the next set, loop back to the start", "    showSet(currentSetIndex); // Update the visible set"], "file_path": "js/script.js"}
{"Link_to_commit": "https://github.com/rootulp/exercism/commit/911eb9d71a36746ddf6ca48635f94b743046d40b", "n-gram matched": "help of copilot", "n_lines_longer_change": 29, "n_files_impacted": 1, "longest_chunk": ["\tusedNumbers := make([]bool, 10)", "\tletterToNumber := make(map[string]int)", "", "\tif solveBacktrack(&equation, letters, letterToNumber, usedNumbers, 0) {", "\t\treturn letterToNumber, nil", "\t}", "\treturn nil, errors.New(\"no solution found\")", "}", "", "func solveBacktrack(equation *equation, letters []string, letterToNumber map[string]int, usedNumbers []bool, index int) bool {", "\tfmt.Printf(\"letters %v letterToNumber %v usedNumbers %v index %v\\n\", letters, letterToNumber, usedNumbers, index) // Debug statement", "\tif index == len(letters) {", "\t\treturn equation.evaluate(letterToNumber)", "\t}", "", "\tfor num := 0; num <= 9; num++ {", "\t\tif !usedNumbers[num] {", "\t\t\tletterToNumber[letters[index]] = num", "\t\t\tusedNumbers[num] = true", "", "\t\t\tfmt.Printf(\"Trying %s = %d\\n\", letters[index], num) // Debug statement", "\t\t\tfmt.Printf(\"Current map: %v\\n\", letterToNumber)     // Debug statement", "", "\t\t\tif !equation.isLeadingZero(letterToNumber) && solveBacktrack(equation, letters, letterToNumber, usedNumbers, index+1) {", "\t\t\t\treturn true", "\t\t\t}", "", "\t\t\tusedNumbers[num] = false", "\t\t\tdelete(letterToNumber, letters[index])"], "file_path": "go/alphametics/alphametics.go"}
{"Link_to_commit": "https://github.com/MoserElias/aoc24/commit/a3446c12003afda48b27570699dae3ced36c9435", "n-gram matched": "help of copilot", "n_lines_longer_change": 139, "n_files_impacted": 4, "longest_chunk": ["import * as fs8 from \"fs\";", "", "class Day8Execution {", "", "    private readonly inputFilePath: string;", "    private readonly data: string;", "    private readonly inputMap: string[][];", "    private posAntennas: Map<string, string>;", "", "    constructor() {", "        this.inputFilePath = \"real-input.txt\";", "        this.data = fs8.readFileSync(this.inputFilePath, \"utf8\");", "        this.inputMap = this.data.split(\"\\n\").map((line) => line.replace(\"\\r\", \"\").split(\" \"));", "", "        // parse the stringLine of every row to a string array ", "        this.inputMap.forEach((stringLine, index) => {", "            this.inputMap[index] = stringLine[0].split(\"\");", "        });", "        // console.log(this.inputMap);", "    }", "", "    // search for antennas in inputMap (could be letters or digits) and save the position in posAntennas ", "    // (key: position (unique), value: antenna char)", "    searchAntennas(): void {", "        this.posAntennas = new Map<string, string>();", "        for (let i = 0; i < this.inputMap.length; i++) {", "            for (let j = 0; j < this.inputMap[i].length; j++) {", "                if (RegExp(/[a-zA-Z0-9]/).exec(this.inputMap[i][j])) {", "                    this.posAntennas.set(i + \",\" + j, this.inputMap[i][j]);", "                }", "            }", "        }", "", "        // print the antennas and their positions ", "        // this.posAntennas.forEach((value, key) => {", "        //     console.log(key + \" -> \" + value);", "        // });", "    }", "", "    // calculate antinode positions and return the total count", "    calculateAnitnodePositions(): number {", "        const antinodePositions = new Set<string>();", "    ", "        const antennasByFreq = this.groupAntennasByFrequency();", "        antennasByFreq.forEach((positions, freq) => {", "            for (let i = 0; i < positions.length; i++) {", "                for (let j = i + 1; j < positions.length; j++) {", "                    const [x1, y1] = positions[i];", "                    const [x2, y2] = positions[j];", "    ", "                    // Skip if antennas are at the same position", "                    if (x1 === x2 && y1 === y2) {", "                        continue;", "                    }", "    ", "                    // Consider both ratios: 1:2 and 2:1", "                    const ratios = [", "                        { m: 1, n: 2 },", "                        { m: 2, n: 1 },", "                    ];", "    ", "                    ratios.forEach(({ m, n }) => {", "                        // Calculate internal and external division points", "                        const internalPoint = this.dividePoints(x1, y1, x2, y2, m, n, true);", "                        const externalPoint = this.dividePoints(x1, y1, x2, y2, m, n, false);", "    ", "                        // Validate and add positions", "                        [internalPoint, externalPoint].forEach(point => {", "                            if (this.isValidPosition(point[0], point[1])) {", "                                antinodePositions.add(`${point[0]},${point[1]}`);", "                            }", "                        });", "                    });", "                }", "            }", "        });", "    ", "        // Return the total count of unique antinode positions", "        return antinodePositions.size;", "    }", "", "    // group antennas by frequency, return new map with frequency as key and positions of antennas as value", "    private groupAntennasByFrequency(): Map<string, Array<[number, number]>> {", "        const antennasByFreq = new Map<string, Array<[number, number]>>();", "        this.posAntennas.forEach((freq, pos) => {", "            const [row, col] = pos.split(',').map(Number);", "            if (!antennasByFreq.has(freq)) {", "                antennasByFreq.set(freq, []);", "            }", "            antennasByFreq.get(freq)!.push([row, col]);", "        });", "        return antennasByFreq;", "    }", "", "    private dividePoints(", "        x1: number,", "        y1: number,", "        x2: number,", "        y2: number,", "        m: number,", "        n: number,", "        isInternal: boolean", "    ): [number, number] {", "        let x: number;", "        let y: number;", "        if (isInternal) {", "            x = (n * x1 + m * x2) / (m + n);", "            y = (n * y1 + m * y2) / (m + n);", "        } else {", "            x = (n * x1 - m * x2) / (n - m);", "            y = (n * y1 - m * y2) / (n - m);", "        }", "        return [x, y];", "    }", "", "    private isValidPosition(x: number, y: number): boolean {", "        return (", "            Number.isInteger(x) &&", "            Number.isInteger(y) &&", "            x >= 0 &&", "            y >= 0 &&", "            x < this.inputMap.length &&", "            y < this.inputMap[x].length ", "        );", "    }", "", "    getInputString(): string[][] {", "        return this.inputMap;", "    }", "}", "", "function main8() {", "    let execution = new Day8Execution();", "    execution.searchAntennas();", "    let res = execution.calculateAnitnodePositions();", "    console.log(`Number of unique antinode positions: ${res}`);", "}", "", "main8();"], "file_path": "Day8/day8.ts"}
{"Link_to_commit": "https://github.com/DeepakRajasekaran/humble_ws/commit/36b87516f7a9535564ca515586e0d74028fef318", "n-gram matched": "help of copilot", "n_lines_longer_change": 23, "n_files_impacted": 1, "longest_chunk": ["    void huntPrayCallback()", "    {", "        std::lock_guard<std::mutex> lock(mutex_);", "        if (!pose_ || !target_){", "            RCLCPP_ERROR(this->get_logger(), \"Error: Pose or target is not available.\");", "            return;", "        }", "        geometry_msgs::msg::Twist msg;", "", "        if (target_distance_ > 0.5)", "        {", "            msg.linear.x  = kp_linear_ * target_distance_;", "            double diff = std::fmod(goal_theta_ - pose_->theta + M_PI, 2 * M_PI) - M_PI;", "            msg.angular.z = kp_angular_ * diff;", "        }", "        else", "        {", "            msg.linear.x  = 0.0;", "            msg.angular.z = 0.0;", "            threads_.emplace_back(&HunterNode::sendKillRequest, this, target_->name);", "        }", "", "        cmd_vel_publisher_->publish(msg);"], "file_path": "study_ws/src/turtle_hunter_cpp/src/hunter_.cpp"}
{"Link_to_commit": "https://github.com/hainguyen2757/UdderBot/commit/19eb18df80bf7d7caf9385ed7214db361d5a239d", "n-gram matched": "help of copilot", "n_lines_longer_change": 19, "n_files_impacted": 5, "longest_chunk": ["", "//------------------ CONTROLLERS ------------------", "let isBillionare = false;", "let isWaiting = false;", "let isBettingOff = false;", "let isAutoGambaOn = false;", "let isAutoGamba2On = false;", "let isBroke = false;", "let isWinstreak = false;", "let rebirthed = true;", "let riskUnder50 = 5; //if has less than 50m, risk 5m", "let riskUnder100 = 10; //if has less than 100m, risk 10m", "let riskUnder200 = 20; //if has less than 200m, risk 20m", "let riskOver200 = 30; //if has more than 200m, risk 30m", "let setAutoGamba2Risk=2; //set it to \"2k\" or \"200k\" or \"2\"", "let isAutoSpinOn = true;", "", "//------------------ CONTROLLERS ------------------", ""], "file_path": "UdderBot.js"}
{"Link_to_commit": "https://github.com/deors/deors-demos-github-action-dorelease-copilot/commit/a069aa5bac1865d4ebd8f8579bafdf13b27d5ac4", "n-gram matched": "help of copilot", "n_lines_longer_change": 9, "n_files_impacted": 8, "longest_chunk": ["    expect(setOutputMock).toHaveBeenNthCalledWith(", "      2,", "      'release-status',", "      'success'", "    )", "    expect(setOutputMock).toHaveBeenNthCalledWith(", "      3,", "      'target-url',", "      'https://example.com'"], "file_path": "__tests__/main.test.ts"}
{"Link_to_commit": "https://github.com/jaluebbe/FlightRoutes/commit/5680ee09471b1e69e3811adfecdb03f4bbf747b2", "n-gram matched": "help of copilot", "n_lines_longer_change": 29, "n_files_impacted": 1, "longest_chunk": ["        if (", "            _row[\"gps_code\"] in duplicate_icaos", "            and _row[\"ident\"] != _row[\"gps_code\"]", "        ):", "            logger.info(", "                f\"ignoring duplicate entry {_row['ident']} for \"", "                f\"{_row['gps_code']} / {_row['iata_code']}.\"", "            )", "            continue", "        _longitude = float(_row[\"longitude_deg\"])", "        _latitude = float(_row[\"latitude_deg\"])", "        _timezone = tf.timezone_at(lng=_longitude, lat=_latitude)", "        _country = countries[_row[\"iso_country\"]]", "        if _timezone is None:", "            logger.warning(\"timezone info unknown: {}\".format(_row[\"gps_code\"]))", "        _cursor.execute(", "            \"REPLACE INTO airports(Name, City, Country, IATA, ICAO, Latitude, \"", "            \"Longitude, Altitude, Timezone) VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?)\",", "            (", "                _row[\"name\"],", "                _row[\"municipality\"],", "                _country,", "                _row[\"iata_code\"],", "                _row[\"gps_code\"],", "                _latitude,", "                _longitude,", "                _row[\"elevation_ft\"],", "                _timezone,", "            ),"], "file_path": "prepare_airport_data.py"}
{"Link_to_commit": "https://github.com/chaosbooy/Projeckt-Space-Invading/commit/a71c7ea3ff6ca08e17bb04e4106ccf408a5170ff", "n-gram matched": "help of copilot", "n_lines_longer_change": 122, "n_files_impacted": 5, "longest_chunk": ["", "        private void SetupGame()", "        {", "            player = new Rectangle { Width = 50, Height = 20, Fill = Brushes.Blue };", "            Canvas.SetLeft(player, (MainCanvas.Width - player.Width) / 2);", "            Canvas.SetTop(player, MainCanvas.Height - player.Height - 10);", "            MainCanvas.Children.Add(player);", "", "            for (int i = 0; i < 5; i++)", "            {", "                Rectangle block = new() { Width = 60, Height = 30, Fill = Brushes.Red };", "                Canvas.SetLeft(block, i * 70 + 20);", "                Canvas.SetTop(block, 20);", "                MainCanvas.Children.Add(block);", "                blocks.Add(block);", "            }", "        }", "", "        private void GameLoop(object sender, EventArgs e)", "        {", "            if (playerLeft && Canvas.GetLeft(player) > 0)", "            {", "                Canvas.SetLeft(player, Canvas.GetLeft(player) - playerSpeed);", "            }", "            if (playerRight && Canvas.GetLeft(player) < MainCanvas.Width - player.Width)", "            {", "                Canvas.SetLeft(player, Canvas.GetLeft(player) + playerSpeed);", "            }", "            if (playerAttack == KeyState.Pressed)", "            {", "                playerAttack = KeyState.Down;", "                Shoot();", "            }", "", "            foreach (var bullet in bullets.ToArray())", "            {", "                Canvas.SetTop(bullet, Canvas.GetTop(bullet) - bulletSpeed);", "                if (Canvas.GetTop(bullet) < 0)", "                {", "                    MainCanvas.Children.Remove(bullet);", "                    bullets.Remove(bullet);", "                }", "            }", "", "            foreach (var block in blocks.ToArray())", "            {", "                foreach (var bullet in bullets.ToArray())", "                {", "                    if (IsColliding(bullet, block))", "                    {", "                        MainCanvas.Children.Remove(bullet);", "                        MainCanvas.Children.Remove(block);", "                        bullets.Remove(bullet);", "                        blocks.Remove(block);", "                        break;", "                    }", "                }", "            }", "        }", "", "        private bool IsColliding(Rectangle a, Rectangle b)", "        {", "            double aX = Canvas.GetLeft(a);", "            double aY = Canvas.GetTop(a);", "            double bX = Canvas.GetLeft(b);", "            double bY = Canvas.GetTop(b);", "            return aX < bX + b.Width && aX + a.Width > bX && aY < bY + b.Height && aY + a.Height > bY;", "        }", "", "        private void Window_KeyDown(object sender, KeyEventArgs e)", "        {", "            switch(e.Key)", "            {", "                case Key.A:", "                    playerLeft = true;", "                    break;", "                case Key.D:", "                    playerRight = true;", "                    break;", "                case Key.Space:", "                    if (playerAttack == KeyState.Pressed || playerAttack == KeyState.Down)", "                        playerAttack = KeyState.Down;", "                    else", "                        playerAttack = KeyState.Pressed;", "                    break;", "            }", "        }", "", "        private void Window_KeyUp(object sender, KeyEventArgs e)", "        {", "            switch(e.Key)", "            {", "                case Key.A:", "                    playerLeft = false;", "                    break;", "                case Key.D:", "                    playerRight = false;", "                    break;", "                case Key.Space:", "                    playerAttack = KeyState.Released;", "                    break;", "            }", "        }", "", "        private void Shoot()", "        {", "            Rectangle bullet = new Rectangle { Width = 5, Height = 15, Fill = Brushes.Black };", "            double x = Canvas.GetLeft(player) + player.Width / 2 - bullet.Width / 2;", "            double y = Canvas.GetTop(player) - bullet.Height;", "            Canvas.SetLeft(bullet, x);", "            Canvas.SetTop(bullet, y);", "            MainCanvas.Children.Add(bullet);", "            bullets.Add(bullet);", "        }", "    }", "", "    enum KeyState", "    {", "        Down,", "        Pressed,", "        Up,", "        Released"], "file_path": "SpaceInvading/Resources/Pages/Game.xaml.cs"}
{"Link_to_commit": "https://github.com/nageshwar-mehta/DSA-in-C-/commit/7f8970d117e4975f13698df67f746e11f0aded70", "n-gram matched": "help of copilot", "n_lines_longer_change": 42, "n_files_impacted": 1, "longest_chunk": ["#include <bits/stdc++.h>\r", "using namespace std;\r", "\r", "// Graph using Adjacency Matrix\r", "\r", "// Function to print the graph represented by an adjacency matrix\r", "void PrintGraph(vector<vector<bool>> v, int vertex) {\r", "    cout << \"Graph: \" << endl;\r", "    // Loop through each vertex\r", "    for (int i = 0; i < vertex; i++) {\r", "        // Loop through each edge\r", "        for (int j = 0; j < vertex; j++) {\r", "            // Print the value of the adjacency matrix at position (i, j)\r", "            cout << v[i][j] << \" \";\r", "        }\r", "        // Print a new line after each row of the matrix\r", "        cout << endl;\r", "    }\r", "}\r", "\r", "int main() {\r", "    int vertex, edge;\r", "    // Prompt the user to enter the number of vertices\r", "    cout << \"Vertex: \";\r", "    cin >> vertex;\r", "    // Prompt the user to enter the number of edges\r", "    cout << \"Edges: \";\r", "    cin >> edge;\r", "    // Undirected, unweighted graph\r", "    vector<vector<bool>> AdjMat(vertex, vector<bool>(vertex, 0));\r", "    int u, v;\r", "    // Read the edges and update the adjacency matrix\r", "    for (int i = 0; i < edge; i++) {\r", "        cin >> u >> v;\r", "        AdjMat[u][v] = 1;\r", "        AdjMat[v][u] = 1;\r", "    }\r", "    // Print the graph\r", "    PrintGraph(AdjMat, vertex);\r", "\r", "    return 0;\r", "}"], "file_path": "graph_1.cpp"}
{"Link_to_commit": "https://github.com/emsqrd/tic-tac-toe/commit/e926097b3e597cab96fc1d3873a9025e2a3dc480", "n-gram matched": "help of copilot", "n_lines_longer_change": 55, "n_files_impacted": 6, "longest_chunk": ["  const initialState: GameState = {", "    gameBoard: Array(9).fill({ gamePiece: '', isWinner: false }),", "    player1: {", "      name: 'Player 1',", "      piece: 'X',", "      wins: 0,", "    },", "    player2: {", "      name: 'Player 2',", "      piece: 'O',", "      wins: 0,", "    },", "    currentPlayer: {", "      name: 'Player 1',", "      piece: 'X',", "      wins: 0,", "    },", "    winner: null,", "    isDraw: false,", "    draws: 0,", "  };", "", "  it('should select the game board', () => {", "    const result = selectGameBoard.projector(initialState);", "    expect(result).toEqual(initialState.gameBoard);", "  });", "", "  it('should select the current player', () => {", "    const result = selectCurrentPlayer.projector(initialState);", "    expect(result).toEqual(initialState.currentPlayer);", "  });", "", "  it('should select the winner', () => {", "    const result = selectWinner.projector(initialState);", "    expect(result).toEqual(initialState.winner);", "  });", "", "  it('should select player 1', () => {", "    const result = selectPlayer1.projector(initialState);", "    expect(result).toEqual(initialState.player1);", "  });", "", "  it('should select player 2', () => {", "    const result = selectPlayer2.projector(initialState);", "    expect(result).toEqual(initialState.player2);", "  });", "", "  it('should select the number of draws', () => {", "    const result = selectDraws.projector(initialState);", "    expect(result).toEqual(initialState.draws);", "  });", "", "  it('should select if the game is a draw', () => {", "    const result = selectIsDraw.projector(initialState);", "    expect(result).toEqual(initialState.isDraw);"], "file_path": "src/app/store/game/game.selectors.spec.ts"}
{"Link_to_commit": "https://github.com/umergulkaleem/hackathon2-my/commit/973f2a6825bf3ed888aac207ffc216854868fa85", "n-gram matched": "help of copilot", "n_lines_longer_change": 53, "n_files_impacted": 49, "longest_chunk": ["  \textend: {", "  \t\tbackgroundImage: {", "  \t\t\t'gradient-radial': 'radial-gradient(var(--tw-gradient-stops))',", "  \t\t\t'gradient-conic': 'conic-gradient(from 180deg at 50% 50%, var(--tw-gradient-stops))'", "  \t\t},", "  \t\tborderRadius: {", "  \t\t\tlg: 'var(--radius)',", "  \t\t\tmd: 'calc(var(--radius) - 2px)',", "  \t\t\tsm: 'calc(var(--radius) - 4px)'", "  \t\t},", "  \t\tcolors: {", "  \t\t\tbackground: 'hsl(var(--background))',", "  \t\t\tforeground: 'hsl(var(--foreground))',", "  \t\t\tcard: {", "  \t\t\t\tDEFAULT: 'hsl(var(--card))',", "  \t\t\t\tforeground: 'hsl(var(--card-foreground))'", "  \t\t\t},", "  \t\t\tpopover: {", "  \t\t\t\tDEFAULT: 'hsl(var(--popover))',", "  \t\t\t\tforeground: 'hsl(var(--popover-foreground))'", "  \t\t\t},", "  \t\t\tprimary: {", "  \t\t\t\tDEFAULT: 'hsl(var(--primary))',", "  \t\t\t\tforeground: 'hsl(var(--primary-foreground))'", "  \t\t\t},", "  \t\t\tsecondary: {", "  \t\t\t\tDEFAULT: 'hsl(var(--secondary))',", "  \t\t\t\tforeground: 'hsl(var(--secondary-foreground))'", "  \t\t\t},", "  \t\t\tmuted: {", "  \t\t\t\tDEFAULT: 'hsl(var(--muted))',", "  \t\t\t\tforeground: 'hsl(var(--muted-foreground))'", "  \t\t\t},", "  \t\t\taccent: {", "  \t\t\t\tDEFAULT: 'hsl(var(--accent))',", "  \t\t\t\tforeground: 'hsl(var(--accent-foreground))'", "  \t\t\t},", "  \t\t\tdestructive: {", "  \t\t\t\tDEFAULT: 'hsl(var(--destructive))',", "  \t\t\t\tforeground: 'hsl(var(--destructive-foreground))'", "  \t\t\t},", "  \t\t\tborder: 'hsl(var(--border))',", "  \t\t\tinput: 'hsl(var(--input))',", "  \t\t\tring: 'hsl(var(--ring))',", "  \t\t\tchart: {", "  \t\t\t\t'1': 'hsl(var(--chart-1))',", "  \t\t\t\t'2': 'hsl(var(--chart-2))',", "  \t\t\t\t'3': 'hsl(var(--chart-3))',", "  \t\t\t\t'4': 'hsl(var(--chart-4))',", "  \t\t\t\t'5': 'hsl(var(--chart-5))'", "  \t\t\t}", "  \t\t}", "  \t}"], "file_path": "tailwind.config.ts"}
{"Link_to_commit": "https://github.com/equalizedigital/accessibility-checker/commit/b3d3ab88fb013f730c884a8644dd057bdfe48d7a", "n-gram matched": "help of copilot", "n_lines_longer_change": 109, "n_files_impacted": 1, "longest_chunk": ["<?php", "/**", " * Test class for FixesManager.", " *", " * @package accessibility-checker", " */", "", "use PHPUnit\\Framework\\TestCase;", "use EqualizeDigital\\AccessibilityChecker\\Fixes\\FixesManager;", "use EqualizeDigital\\AccessibilityChecker\\Fixes\\FixInterface;", "", "/**", " * Unit tests for the FixesManager class.", " */", "class FixesManagerTest extends TestCase {", "", "\t/**", "\t * Setup the test environment by resetting the instance before each test.", "\t *", "\t * @return void", "\t */", "\tpublic function setUp(): void {", "\t\tparent::setUp();", "\t\t// Reset the instance before each test.", "\t\t$reflection = new ReflectionClass( FixesManager::class );", "\t\t$instance   = $reflection->getProperty( 'instance' );", "\t\t$instance->setAccessible( true );", "\t\t$instance->setValue( null, null );", "\t}", "", "\t/**", "\t * Test that the instance retuns an empty array when no fixes are registered.", "\t *", "\t * @return void", "\t */", "\tpublic function test_get_fixes_settings_returns_empty_array_when_no_fixes() {", "\t\t$fixes_manager = FixesManager::get_instance();", "\t\t$this->assertEmpty( $fixes_manager->get_fixes_settings() );", "\t}", "", "\t/**", "\t * Test that the instance returns the correct structure when fixes are registered.", "\t *", "\t * @return void", "\t */", "\tpublic function test_get_fixes_settings_returns_correct_structure() {", "\t\t$fix_mock = $this->createMock( FixInterface::class );", "\t\t$fix_mock->method( 'get_fields_array' )->willReturn(", "\t\t\t[", "\t\t\t\t'field1' => [ 'default' => 'value1' ],", "\t\t\t\t'field2' => [ 'default' => 'value2' ],", "\t\t\t]", "\t\t);", "\t\t$fix_mock->method( 'get_slug' )->willReturn( 'mock_fix' );", "\t\t$fix_mock->is_pro = true;", "", "\t\t$fixes_manager  = FixesManager::get_instance();", "\t\t$reflection     = new ReflectionClass( $fixes_manager );", "\t\t$fixes_property = $reflection->getProperty( 'fixes' );", "\t\t$fixes_property->setAccessible( true );", "\t\t$fixes_property->setValue( $fixes_manager, [ 'mock_fix' => $fix_mock ] );", "", "\t\t$expected = [", "\t\t\t'mock_fix' => [", "\t\t\t\t'fields' => [", "\t\t\t\t\t'field1' => 'value1',", "\t\t\t\t\t'field2' => 'value2',", "\t\t\t\t],", "\t\t\t\t'is_pro' => true,", "\t\t\t],", "\t\t];", "", "\t\t$this->assertEquals( $expected, $fixes_manager->get_fixes_settings() );", "\t}", "", "\t/**", "\t * Test that the instance returns the default values when options aren't set.", "\t *", "\t * @return void", "\t */", "\tpublic function test_get_fixes_settings_uses_default_values() {", "\t\t$fix_mock = $this->createMock( FixInterface::class );", "\t\t$fix_mock->method( 'get_fields_array' )->willReturn(", "\t\t\t[", "\t\t\t\t'field1' => [ 'default' => 'default_value1' ],", "\t\t\t\t'field2' => [ 'default' => 'default_value2' ],", "\t\t\t]", "\t\t);", "\t\t$fix_mock->method( 'get_slug' )->willReturn( 'mock_fix' );", "", "\t\t$fixes_manager = FixesManager::get_instance();", "\t\t$reflection    = new ReflectionClass( $fixes_manager );", "\t\t$fixes_propert = $reflection->getProperty( 'fixes' );", "\t\t$fixes_propert->setAccessible( true );", "\t\t$fixes_propert->setValue( $fixes_manager, [ 'mock_fix' => $fix_mock ] );", "", "\t\t$expected = [", "\t\t\t'mock_fix' => [", "\t\t\t\t'fields' => [", "\t\t\t\t\t'field1' => 'default_value1',", "\t\t\t\t\t'field2' => 'default_value2',", "\t\t\t\t],", "\t\t\t\t'is_pro' => false,", "\t\t\t],", "\t\t];", "", "\t\t$this->assertEquals( $expected, $fixes_manager->get_fixes_settings() );", "\t}", "}"], "file_path": "tests/phpunit/includes/classes/Fixes/FixesManagerTest.php"}
{"Link_to_commit": "https://github.com/jlessler/CVBuilder/commit/4f6180c95e8d710eb68d779c741d4eaeab8556e6", "n-gram matched": "help of copilot", "n_lines_longer_change": 113, "n_files_impacted": 7, "longest_chunk": ["from flask import Flask, render_template, request, redirect, url_for, jsonify", "import sqlite3", "from crossref.restful import Works", "", "app = Flask(__name__, template_folder=\"../templates\")", "DB_FILE = \"./mydata/refs.db\"", "", "# Helper function to connect to the database", "def get_db_connection():", "    conn = sqlite3.connect(DB_FILE)", "    conn.row_factory = sqlite3.Row", "    return conn", "", "# Home route to display all references", "@app.route(\"/\")", "def index():", "    conn = get_db_connection()", "    refs = conn.execute(\"\"\"", "        SELECT refs.ref_id, refs.type,", "               (SELECT GROUP_CONCAT(author_name, ', ') ", "                FROM authors ", "                WHERE authors.ref_id = refs.ref_id) AS authors,", "               MAX(CASE WHEN ref_dat.key = 'title' THEN ref_dat.value END) AS title,", "               MAX(CASE WHEN ref_dat.key = 'journal' THEN ref_dat.value END) AS journal,", "               MAX(CASE WHEN ref_dat.key = 'year' THEN ref_dat.value END) AS year,", "               MAX(CASE WHEN ref_dat.key = 'doi' THEN ref_dat.value END) AS doi", "        FROM refs", "        LEFT JOIN ref_dat ON refs.ref_id = ref_dat.ref_id", "        GROUP BY refs.ref_id", "    \"\"\").fetchall()", "    conn.close()", "    return render_template(\"index.html\", refs=refs)", "", "# Route to view details of a specific reference", "@app.route(\"/ref/<int:ref_id>\")", "def view_ref(ref_id):", "    conn = get_db_connection()", "    ref = conn.execute(\"SELECT * FROM refs WHERE ref_id = ?\", (ref_id,)).fetchone()", "    authors = conn.execute(\"SELECT author_name FROM authors WHERE ref_id = ?\", (ref_id,)).fetchall()", "    ref_data = conn.execute(\"SELECT key, value FROM ref_dat WHERE ref_id = ?\", (ref_id,)).fetchall()", "    conn.close()", "    return render_template(\"view_ref.html\", ref=ref, authors=authors, ref_data=ref_data)", "", "# Route to add a new reference", "@app.route(\"/add/<ref_type>\", methods=[\"GET\", \"POST\"])", "def add_ref(ref_type):", "    if request.method == \"POST\":", "        authors = request.form.getlist(\"authors\")", "        ref_data = {key: value for key, value in request.form.items() if key not in [\"authors\", \"submit\"]}", "", "        conn = get_db_connection()", "        cursor = conn.cursor()", "", "        # Insert into refs table", "        cursor.execute(\"INSERT INTO refs (type) VALUES (?)\", (ref_type,))", "        ref_id = cursor.lastrowid", "", "        # Insert authors", "        for author in authors:", "            if author.strip():", "                cursor.execute(\"INSERT INTO authors (ref_id, author_name) VALUES (?, ?)\", (ref_id, author.strip()))", "", "        # Insert ref details", "        for key, value in ref_data.items():", "            if value.strip():", "                cursor.execute(\"INSERT INTO ref_dat (ref_id, key, value) VALUES (?, ?, ?)\", (ref_id, key.strip(), value.strip()))", "", "        conn.commit()", "        conn.close()", "        return redirect(url_for(\"index\"))", "", "    return render_template(\"add_ref.html\", ref_type=ref_type)", "", "# Route to delete a reference", "@app.route(\"/delete/<int:ref_id>\", methods=[\"POST\"])", "def delete_ref(ref_id):", "    conn = get_db_connection()", "    conn.execute(\"DELETE FROM refs WHERE ref_id = ?\", (ref_id,))", "    conn.execute(\"DELETE FROM authors WHERE ref_id = ?\", (ref_id,))", "    conn.execute(\"DELETE FROM ref_dat WHERE ref_id = ?\", (ref_id,))", "    conn.commit()", "    conn.close()", "    return redirect(url_for(\"index\"))", "", "# Route to fetch DOI details", "@app.route(\"/fetch_doi\", methods=[\"POST\"])", "def fetch_doi():", "    doi = request.json.get(\"doi\")", "    if not doi:", "        return jsonify({\"error\": \"DOI is required\"}), 400", "", "    works = Works()", "    try:", "        ref = works.doi(doi)", "        if not ref:", "            return jsonify({\"error\": \"DOI not found\"}), 404", "", "        # Extract relevant fields", "        data = {", "            \"title\": ref.get(\"title\", [\"\"])[0],", "            \"year\": ref.get(\"published-print\", {}).get(\"date-parts\", [[None]])[0][0] or ref.get(\"published-online\", {}).get(\"date-parts\", [[None]])[0][0],", "            \"journal\": ref.get(\"container-title\", [\"\"])[0],", "            \"volume\": ref.get(\"volume\", \"\"),", "            \"issue\": ref.get(\"issue\", \"\"),", "            \"pages\": ref.get(\"page\", \"\"),", "            \"authors\": \", \".join([f\"{author['family']} {author['given']}\" for author in ref.get(\"author\", [])]),", "        }", "        return jsonify(data)", "    except Exception as e:", "        return jsonify({\"error\": str(e)}), 500", "", "if __name__ == \"__main__\":", "    app.run(debug=True)"], "file_path": "sql_db/src/app.py"}
{"Link_to_commit": "https://github.com/vishvas-git/ubiquitous-dollop/commit/25bf60446fb03f3fffb5e3c95e21d52f735617b2", "n-gram matched": "help of copilot", "n_lines_longer_change": 15, "n_files_impacted": 1, "longest_chunk": ["    return {'token': string}", "", "", "class Text(BaseModel):", "    text: str", "", "# Create a FastAPI endpoint that accepts a POST request with a JSON body containing a single field called \"text\" and returns a checksum of the text.", "# The checksum should be a SHA-256 hash of the text encoded in base64.", "# The endpoint should return a JSON response with a single field called \"checksum\" containing the base64-encoded checksum.", "@app.post('/checksum')", "def checksum(text: Text):", "    import hashlib", "    checksum = hashlib.sha256(text.text.encode()).digest()", "    return {'checksum': base64.b64encode(checksum).decode()}", ""], "file_path": "webapp/main.py"}
{"Link_to_commit": "https://github.com/Ved0715/ChatTube/commit/420a130b2b4ff38ade2763abc90a9c80402287cb", "n-gram matched": "help of copilot", "n_lines_longer_change": 34, "n_files_impacted": 19, "longest_chunk": ["        const fromUserId = req.id;", "        const { to, subject, message } = req.body;", "", "        if (!to || !subject || !message) return res.status(400).json({", "            message: \"All the information is required\",", "            success: false,", "        });", "", "        const toUser = await User.findOne({ email: to });", "        if (!toUser) return res.status(404).json({", "            message: \"Recipient user not found\",", "            success: false,", "        });", "", "        const email = await Email.create({", "            to: toUser.email,", "            subject,", "            message,", "            userId: fromUserId", "        });", "", "        // Save the email for the recipient user as well", "        await Email.create({", "            to: toUser.email,", "            subject,", "            message,", "            userId: toUser._id", "        });", "", "        return res.status(201).json({", "            email,", "            message: \"Message sent successfully\",", "            success: true,", "        });"], "file_path": "backend/controllers/email.controller.js"}
{"Link_to_commit": "https://github.com/vick2592/LeetCodePython/commit/60a975d3e8b1e194426b10a8dde78758f4b20c89", "n-gram matched": "help of copilot", "n_lines_longer_change": 25, "n_files_impacted": 1, "longest_chunk": ["class Solution:", "    def numMovesStonesII(stones):", "        stones.sort()", "        i, n, low = 0, len(stones), len(stones)", "        total = len(stones)", "        sSum = stones[-1] - stones[0] + 1 - total", "        high = sSum - min(stones[1] - stones[0] -1, stones[-1] - stones[-2] - 1)", "        if (stones[total-2] - stones[0] + 1) == total - 1:", "            low = min(stones[total-1] - stones[total-2] - 1, 2)", "            return [high, low]", "        if (stones[total-1] - stones[1] + 1) == total - 1:", "            low = min(stones[1] - stones[0] - 1, 2)", "            return [high, low]", "        ", "        for j in range(n):", "            while stones[j] - stones[i] > n:", "                i += 1", "            low = min(low, n - (j - i + 1))", "        return [low, high]", "", "stones = [7,4,9]", "ans = Solution.numMovesStonesII(stones)", "print(ans)  # Output: [1,2]", "    ", "    "], "file_path": "LeetCodeProblems/1040MovingStonesUntilConsecutiveTwo.py"}
{"Link_to_commit": "https://github.com/17Swagat/SmallAPI/commit/cfbe4954646f4b7d2edf1779c96124215317e48e", "n-gram matched": "help of copilot", "n_lines_longer_change": 12, "n_files_impacted": 1, "longest_chunk": ["# ERROR:\r", "# 'becoz of `post`'\r", "# @app.put('/posts/update/{id}')\r", "# def update_post(id: int, post: Post):\r", "#     for index, post in enumerate(my_posts):\r", "#         if post['id'] == id:\r", "#             my_posts[index]['title'] = post.title\r", "#             return {'all-post': my_posts}\r", "#     return {'message': 'post not found'}\r", "\r", "# Works!! (Copilot):\r", "# solved it via changing `post` -> `updated_post`\r"], "file_path": "main.py"}
{"Link_to_commit": "https://github.com/dgomie/good-picks/commit/a310daba09391413d34625c6a806b8feaff52988", "n-gram matched": "help of copilot", "n_lines_longer_change": 78, "n_files_impacted": 1, "longest_chunk": ["const router = require('express').Router();", "const { Rating } = require('../../models');", "", "// these aren't definite routes, just a starting point based off what I think we need.", "// route to get all ratings", "router.get(\"/\", async (req, res) => {", "  try {", "    const ratingData = await Rating.findAll();", "    res.status(200).json(ratingData);", "  } catch (err) {", "    res.status(500).json(err);", "  }", "});", "", "// route to get one rating", "router.get(\"/:id\", async (req, res) => {", "  try {", "    const ratingData = await Rating.findByPk(req.params.id);", "    if (!ratingData) {", "      res.status(404).json({ message: \"No rating found with this id!\" });", "      return;", "    }", "    res.status(200).json(ratingData);", "  } catch (err) {", "    res.status(500).json(err);", "  }", "});", "", "// router to get all ratings by user", "router.get(\"/user/:user_id\", async (req, res) => {", "  try {", "    const ratingData = await Rating.findAll({ where: { user_id: req.params.user_id } });", "    if (!ratingData) {", "      res.status(404).json({ message: \"No ratings found with this user!\" });", "      return;", "    }", "    res.status(200).json(ratingData);", "  } catch (err) {", "    res.status(500).json(err);", "  }", "});", "", "// route to get one rating by user", "router.get(\"/user/:user_id/:id\", async (req, res) => {", "  try {", "    const ratingData = await Rating.findOne({ where: { user_id: req.params.user_id, id: req.params.id } });", "    if (!ratingData) {", "      res.status(404).json({ message: \"No rating found with this user!\" });", "      return;", "    }", "    res.status(200).json(ratingData);", "  } catch (err) {", "    res.status(500).json(err);", "  }", "});", "", "", "// copilot mumbo jumbo, don't know if we need, but can be used as a reference for the future", "// route to get average rating for a song", "router.get(\"/average/:music_id\", async (req, res) => {", "  try {", "    const ratingData = await Rating.findAll({ where: { music_id: req.params.music_id } });", "    if (!ratingData) {", "      res.status(404).json({ message: \"No ratings found for this song!\" });", "      return;", "    }", "    let total = 0;", "    for (let i = 0; i < ratingData.length; i++) {", "      total += ratingData[i].rating;", "    }", "    const average = total / ratingData.length;", "    res.status(200).json(average);", "  } catch (err) {", "    res.status(500).json(err);", "  }", "});", "", "module.exports = router;"], "file_path": "controllers/api/rating-routes.js"}
{"Link_to_commit": "https://github.com/dborowiec194073/prj3/commit/593a2ae07417538e7bb36c049a3f6108be276f7b", "n-gram matched": "help of copilot", "n_lines_longer_change": 6, "n_files_impacted": 4, "longest_chunk": ["        for (String word : sensationalWords) {", "            if (headline.toLowerCase().contains(word.toLowerCase())) {", "                return \"Sensational\";", "            }", "        }", "        return \"Not Sensational\";"], "file_path": "src/main/java/com/codedotorg/HonestHeadlines.java"}
{"Link_to_commit": "https://github.com/bnjn/music-link-converter/commit/9a3948c50788837f110166489be807e38efedff4", "n-gram matched": "help of copilot", "n_lines_longer_change": 32, "n_files_impacted": 8, "longest_chunk": ["import re", "", "def extract_song_artist(string):", "    string = re.sub(r'\\[.*?\\]', '', string)  # Remove anything in square brackets", "    pattern = re.compile(r'^(.+) - (.+)')", "    match = pattern.match(string)", "    if match:", "        return match.groups()", "    else:", "        return None", "    ", "def url_sanitiser(url):", "    tidal_link_pattern = re.compile(r'https?://(?:www\\.)?tidal\\.com/browse/track/\\d+')", "    spotify_link_pattern = re.compile(r'https://open\\.spotify\\.com/track/[a-zA-Z0-9]+')", "    youtube_link_pattern = re.compile(r'(https?://)?(www\\.)?(youtube\\.com|youtu\\.?be)/.+')", "", "    tidal_link = re.search(tidal_link_pattern, url)", "    spotify_link = re.search(spotify_link_pattern, url)", "    youtube_link= re.search(youtube_link_pattern, url)", "", "    if youtube_link:", "      return { 'source_platform': 'youtube', 'id': url }", "        ", "    if tidal_link:", "      track_id = re.search(r'\\d+', url)", "      return { 'source_platform': 'tidal', 'id': track_id.group(0)}", "", "    if spotify_link:", "      track_id = re.search(r'https://open\\.spotify\\.com/track/([a-zA-Z0-9]+)', url)", "      return { 'source_platform': 'spotify', 'id': track_id.group(1) }", "", "    return None"], "file_path": "music_services/utils.py"}
{"Link_to_commit": "https://github.com/weirdball7/odinProjectRestaurantProject/commit/a891ecb6a8bf835247a03f0d937cd0a3796e73eb", "n-gram matched": "help of copilot", "n_lines_longer_change": 25, "n_files_impacted": 7, "longest_chunk": ["#homeTab {", "    width: 80%;", "    height:80%;", "    background-color: aquamarine;", "", "};", "", "#descriptionContainer {", "    width: 20%;", "    height: 20%;", "    background-color: rgb(9, 49, 36);", "};", "", "#hoursContainer {", "    width: 20%;", "    height: 20%;", "    background-color: rgb(23, 77, 59);", "}", "", "#locationContainer {", "    width: 20%;", "    height: 20%;", "    background-color: rgb(3, 129, 87);", "}", ""], "file_path": "webpack.config.js"}
{"Link_to_commit": "https://github.com/cognominal/vscode-ext-helper-shortcuts/commit/5c375e80ffe3a884780c16c66515a5c8c166cf52", "n-gram matched": "help of copilot", "n_lines_longer_change": 30, "n_files_impacted": 1, "longest_chunk": ["\t\t\"Cut selection. Cut line on empty selection\": { key: \"\u2318X\", wkey: \"Ctrl+X\", id: \"editor.action.clipboardCutAction\" },", "\t\t\"Copy Selection\": { key: \"\u2318C\", wkey: \"Ctrl+C\", id: \"editor.action.clipboardCopyAction\" },", "\t\t\"Copy line (empty selection\": { key: \"\u2318C\", wkey: \"Ctrl+C\", id: \"editor.action.clipboardCopyAction\" },", "\t\t\"Move line down\": { key: \"\u2325\u2193\", wkey: \"Alt+Down\", id: \"editor.action.moveLinesDownAction\" },", "\t\t\"Move line up\": { key: \"\u2325\u2191\", wkey: \"Alt+Up\", id: \"editor.action.moveLinesUpAction\" },", "\t\t\"Copy line down\": { key: \"\u21e7\u2325\u2193\", wkey: \"Shift+Alt+Down\", id: \"editor.action.copyLinesDownAction\" },", "\t\t\"Copy line up\": { key: \"\u21e7\u2325\u2191\", wkey: \"Shift+Alt+Up\", id: \"editor.action.copyLinesUpAction\" },", "\t\t\"Delete line\": { key: \"\u21e7\u2318K\", wkey: \"Shift+Ctrl+K\", id: \"editor.action.deleteLines\" },", "\t\t\"Insert line below\": { key: \"\u2318Enter\", wkey: \"Ctrl+Enter\", id: \"editor.action.insertLineAfter\" },", "\t\t\"Insert line above\": { key: \"\u21e7\u2318Enter\", wkey: \"Shift+Ctrl+Enter\", id: \"editor.action.insertLineBefore\" },", "\t\t\"Jump to matching bracket\": { key: \"\u21e7\u2318\\\\\", wkey: \"Shift+Ctrl+\\\\\", id: \"editor.action.jumpToBracket\" },", "\t\t\"Indent line\": { key: \"\u2318]\", wkey: \"Ctrl+]\", id: \"editor.action.indentLines\" },", "\t\t\"outdent line\": { key: \"\u2318[\", wkey: \"Ctrl+[\", id: \"editor.action.outdentLines\" },", "\t\t\"Go to beginning of line\": { key: \"Home\", wkey: \"Home\", id: \"cursorHome\" },", "\t\t\"Go end of line\": { key: \"End\", wkey: \"End\", id: \"cursorEnd\" },", "\t\t\"Go to beginning of file\": { key: \"\u2318\u2191\", wkey: \"Ctrl+Up\", id: \"cursorTop\" },", "\t\t\"Go to end of file\": { key: \"\u2318\u2193\", wkey: \"Ctrl+Down\", id: \"cursorBottom\" },", "\t\t\"Scroll line up\": { key: \"\u2303PgUp\", wkey: \"Ctrl+PgUp\", id: \"scrollLineUp\" },", "\t\t\"Scroll page down\": { key: \"\u2318PgDown\", wkey: \"Ctrl+PgDown\", id: \"scrollPageDown\" },", "\t\t\"Fold region\": { key: \"\u2325\u2318[\", wkey: \"Alt+Ctrl+[\", id: \"editor.fold\" },", "\t\t\"Unfold region\": { key: \"\u2325\u2318]\", wkey: \"Alt+Ctrl+]\", id: \"editor.unfold\" },", "\t\t\"Fold all subregions\": { key: \"\u2318K \u2318[\", wkey: \"Ctrl+K Ctrl+[\", id: \"editor.foldRecursively\" },", "\t\t\"Unfold all subregions\": { key: \"\u2318K \u2318]\", wkey: \"Ctrl+K Ctrl+]\", id: \"editor.unfoldRecursively\" },", "\t\t\"Fold all regions\": { key: \"\u2318K \u23180\", wkey: \"Ctrl+K Ctrl+0\", id: \"editor.foldAll\" },", "\t\t\"Unfold all regions\": { key: \"\u2318K \u2318J\", wkey: \"Ctrl+K Ctrl+J\", id: \"editor.unfoldAll\" },", "\t\t\"Add line comment\": { key: \"\u2318/\", wkey: \"Ctrl+/\", id: \"editor.action.addCommentLine\" },", "\t\t\"Remove line comment\": { key: \"\u2318K \u2318U\", wkey: \"Ctrl+K Ctrl+U\", id: \"editor.action.removeCommentLine\" },", "\t\t\"Toggle line comment\": { key: \"\u2318/\", wkey: \"Ctrl+/\", id: \"editor.action.commentLine\" },", "\t\t\"Toggle block comment\": { key: \"\u21e7\u2325A\", wkey: \"Shift+Alt+A\", id: \"editor.action.blockComment\" },", "\t\t\"Toggle word wrap\": { key: \"\u2325Z\", wkey: \"Alt+Z\", id: \"editor.action.toggleWordWrap\" },\t\t"], "file_path": "src/helper.ts"}
{"Link_to_commit": "https://github.com/shcodingtoday/Macro-ni/commit/cab308c9d420e903e194590adc54c3a99e117cb8", "n-gram matched": "help of copilot", "n_lines_longer_change": 35, "n_files_impacted": 5, "longest_chunk": ["    print(hotkeys)", "    return hotkeys", "", "", "", "# hhh = loadHotkeys()", "# hhh[\"<ctrl>+<alt>+p\"]()", "# print(hhh)", "def addHotkeys(hotkeyDict):", "", "    with keyboard.GlobalHotKeys(", "    #     {", "    #     \"<ctrl>+<alt>+p\": lambda: callUserHotkey(\"python3 /Users/sh/DEV/password-generator/generate-password.py\"),", "        ", "    #     \"<ctrl>+<alt>+<cmd>+o\": lambda: callUserHotkey(\"python3 /Users/sh/DEV/macroni/scripts/reload.py\")", "    # }", "    hotkeyDict", "    ) as h:", "        h.join()", "", "addHotkeys(loadHotkeys())", "# Attempting to initialise multiple global hotkeys with small dict to fix lamda dict issue", "# does not work, only the first executes", "def initHotkeys():", "    f = open(\"/Users/sh/DEV/macroni/hotkeys.json\", \"rt\")", "    hotkey_string_commands = json.loads(f.read())", "    # hotkeys = {}", "    for key, value in hotkey_string_commands.items():", "", "        addHotkeys({key: lambda: callUserHotkey(hotkey_string_commands[key])})", "    # print(hotkeys)", "    f.close()", "    # print(hotkeys)", "    # return hotkeys", "# initHotkeys()"], "file_path": "listen.py"}
{"Link_to_commit": "https://github.com/sams258/Dice-game-Pig/commit/3262406305c2a6ef3fc62b802e5e0f6fc9570a73", "n-gram matched": "help of copilot", "n_lines_longer_change": 29, "n_files_impacted": 1, "longest_chunk": ["class Highscore:\r", "\r", "    def __init__(self, name, score):\r", "        self.name = name\r", "        self.score = score\r", "\r", "    def __lessthan__(self, other):\r", "        return self.score < other.score\r", "\r", "    def __greaterthan__(self, other):\r", "        return self.score > other.score\r", "\r", "    def __equal__(self, other):\r", "        return self.score == other.score\r", "\r", "    def __lessthanorequal__(self, other):\r", "        return self.score <= other.score\r", "\r", "    def __greaterthanorequal__(self, other):\r", "        return self.score >= other.score\r", "\r", "    def __notqual__(self, other):\r", "        return self.score != other.score\r", "\r", "    def __str__(self):\r", "        return self.name + \" \" + str(self.score)\r", "\r", "    def __repr__(self):\r", "        return self.name + \" \" + str(self.score)\r"], "file_path": "Highscore.py"}
{"Link_to_commit": "https://github.com/AlhuwaidiNora/AMANID/commit/e619be6f054cc3b09e2e171ab48bf6d10ae436d2", "n-gram matched": "help of copilot", "n_lines_longer_change": 26, "n_files_impacted": 1, "longest_chunk": ["    }", "        hint_answer_edit_text = findViewById(R.id.hint_answer_edit_text);", "        button9 = findViewById(R.id.button9);", "        button9.setOnClickListener( new View.OnClickListener(){", "            public void onClick(View v) {", "                database = FirebaseDatabase.getInstance();", "                reference = database.getReference(\"users\");", "                String idnum = editTextid_signup.getText().toString();", "                String pass = editTextpass.getText().toString();", "                String pass2 = editTextpass2.getText().toString();", "                String qhint = editTextid_qhint.getText().toString();", "                HelperClass helperClass = new HelperClass(idnum , pass , pass2 ,qhint);", "                reference.child(idnum).setValue(helperClass);", "                Toast.makeText(signup_page.this, \"you have signup successfully!\",Toast.LENGTH_LONG).show();", "                Intent intent = new Intent(signup_page.this,done_page9.class);", "", "            }", "        });", "", "        btn.setOnClickListener(new View.OnClickListener() {", "            @Override", "            public void onClick(View v) {", "                startActivity(new Intent(signup_page.this,know_more_about_qhint.class));", "            }", "        });", ""], "file_path": "app/src/main/java/com/example/amanid/signup_page.java"}
{"Link_to_commit": "https://github.com/internationaltouch/fit_tournaments/commit/e4ce630633c39ed52ed131277d20fa6599317d65", "n-gram matched": "help of copilot", "n_lines_longer_change": 13, "n_files_impacted": 3, "longest_chunk": ["", "", "class UtilityTests(TestCase):", "    def test_get_age_edge_cases(self):", "        \"Ensure that get_age() handles edge cases correctly.\"", "        player = PlayerFactory.create(date_of_birth=date(2014, 2, 8))", "        for census_date, expected_age in [", "            (date(2022, 2, 7), 7),", "            (date(2022, 2, 8), 8),", "            (date(2022, 2, 9), 8),", "        ]:", "            with self.subTest(census_date=census_date):", "                self.assertEqual(get_age(player, census_date), expected_age)"], "file_path": "eligibility/utils.py"}
{"Link_to_commit": "https://github.com/Krish4523/Aarogyam/commit/d8e0a2c304cd62504af9557692b7692c9af7acc6", "n-gram matched": "help of copilot", "n_lines_longer_change": 63, "n_files_impacted": 12, "longest_chunk": ["import { PrismaClient, Token, TokenType } from \"@prisma/client\";", "", "const tokenClient = new PrismaClient().token;", "", "/**", " * Creates a new token (either verification or reset password) for a user.", " *", " * @param userId - The ID of the user.", " * @param token - The token string.", " * @param type - The type of the token (VERIFICATION or RESET_PASSWORD).", " * @returns A promise that resolves to the created token.", " */", "export const createToken = async (", "  userId: number,", "  token: string,", "  type: TokenType", "): Promise<Token> => {", "  return tokenClient.create({", "    data: {", "      userId,", "      token,", "      type,", "    },", "  });", "};", "", "/**", " * Deletes tokens by user ID and token type.", " *", " * @param userId - The ID of the user whose tokens are to be deleted.", " * @param type - The type of the token (VERIFICATION or RESET_PASSWORD).", " * @returns A promise that resolves to the result of the delete operation.", " */", "export const deleteTokensByUserId = async (userId: number, type: TokenType) => {", "  return tokenClient.deleteMany({", "    where: {", "      userId,", "      type,", "    },", "  });", "};", "", "/**", " * Retrieves a token by its token string and type.", " *", " * @param token - The token string.", " * @param type - The type of the token (VERIFICATION or RESET_PASSWORD).", " * @returns A promise that resolves to the found token, including the associated user.", " */", "export const getTokenByTokenString = async (", "  token: string,", "  type: TokenType", "): Promise<Token | null> => {", "  return tokenClient.findFirst({", "    where: {", "      token,", "      type,", "    },", "    include: {", "      user: true,", "    },", "  });", "};"], "file_path": "server/aarogyam-server/src/dao/user.dao.ts"}
{"Link_to_commit": "https://github.com/s-Radu/Odin-Project-TicTacToe/commit/947a369194721b5ac3b274ed885d0ac30cde6610", "n-gram matched": "help of copilot", "n_lines_longer_change": 36, "n_files_impacted": 7, "longest_chunk": ["  layout.innerHTML = ` <div class=\"container bg-gray-300 rounded-2xl mx-auto flex flex-col justify-between m-3\">", "  <div class=\"flex flex-col justify-between mx-auto m-3 items-center\">", "      <h1 class=\"font-josefin text-2xl\">Player?</h1>", "", "      <div class=\"\">", "          <button", "              class=\"player bg-transparent w-20 border-2 border-black rounded-xl m-3 hover:bg-gray-200 ease-in duration-300 shadow-btn\">X</button>", "          <button", "              class=\"player bg-transparent w-20 border-2 border-black rounded-xl m-3 hover:bg-gray-200 ease-in duration-300 shadow-btn\">O</button>", "      </div>", "", "  </div>", "", "  <div class=\"container mx-auto w-2/4 p-3 mt-10 h-3/4 flex items-center justify-center\">", "      <div class=\"grid grid-cols-3 place-items-center font-cursive\">", "          <button", "              class=\"gameCard relative border-4 border-l-0 border-t-0 text-5xl md:text-9xl min-h-16 md:min-h-36 min-w-16 md:min-w-32 after:absolute after:-translate-x-2/4 after:-translate-y-2/4 after:left-2/4 after:top-2/4 after:text-5xl after:md:text-9xl after:text-gray-400 \"></button>", "          <button", "              class=\"gameCard relative border-4 border-l-0 border-r-0 border-t-0 text-5xl md:text-9xl min-h-16 md:min-h-36 min-w-16 md:min-w-32 after:absolute after:-translate-x-2/4 after:-translate-y-2/4 after:left-2/4 after:top-2/4 after:text-5xl after:md:text-9xl after:text-gray-400 \"></button>", "          <button", "              class=\"gameCard relative border-4 border-r-0 border-t-0 text-5xl md:text-9xl min-h-16 md:min-h-36 min-w-16 md:min-w-32 after:absolute after:-translate-x-2/4 after:-translate-y-2/4 after:left-2/4 after:top-2/4 after:text-5xl after:md:text-9xl after:text-gray-400 \"></button>", "          <button", "              class=\"gameCard relative border-4 border-l-0 text-5xl md:text-9xl min-h-16 md:min-h-36 min-w-16 md:min-w-32 after:absolute after:-translate-x-2/4 after:-translate-y-2/4 after:left-2/4 after:top-2/4 after:text-5xl after:md:text-9xl after:text-gray-400 \"></button>", "          <button", "              class=\"gameCard relative border-4 border-l-0 border-r-0 text-5xl md:text-9xl min-h-16 md:min-h-36 min-w-16 md:min-w-32 after:absolute after:-translate-x-2/4 after:-translate-y-2/4 after:left-2/4 after:top-2/4 after:text-5xl after:md:text-9xl after:text-gray-400 \"></button>", "          <button", "              class=\"gameCard relative border-4 border-r-0 text-5xl md:text-9xl min-h-16 md:min-h-36 min-w-16 md:min-w-32 after:absolute after:-translate-x-2/4 after:-translate-y-2/4 after:left-2/4 after:top-2/4 after:text-5xl after:md:text-9xl after:text-gray-400 \"></button>", "          <button", "              class=\"gameCard relative border-4 border-l-0 border-b-0 text-5xl md:text-9xl min-h-16 md:min-h-36 min-w-16 md:min-w-32 after:absolute after:-translate-x-2/4 after:-translate-y-2/4 after:left-2/4 after:top-2/4 after:text-5xl after:md:text-9xl after:text-gray-400 \"></button>", "          <button", "              class=\"gameCard relative border-4 border-l-0 border-r-0 border-b-0 text-5xl md:text-9xl min-h-16 md:min-h-36 min-w-16 md:min-w-32 after:absolute after:-translate-x-2/4 after:-translate-y-2/4 after:left-2/4 after:top-2/4 after:text-5xl after:md:text-9xl after:text-gray-400 \"></button>", "          <button", "              class=\"gameCard relative border-4 border-r-0 border-b-0 text-5xl md:text-9xl min-h-16 md:min-h-36 min-w-16 md:min-w-32 after:absolute after:-translate-x-2/4 after:-translate-y-2/4 after:left-2/4 after:top-2/4 after:text-5xl after:md:text-9xl after:text-gray-400 \"></button>", "      </div>", "  </div>", "</div>`;"], "file_path": "assets/layout.js"}
{"Link_to_commit": "https://github.com/PixeledLuaWriter/nextjs-portfolio/commit/390fe92d604223062e4509f60ac517c6f1b15198", "n-gram matched": "help of copilot", "n_lines_longer_change": 87, "n_files_impacted": 3, "longest_chunk": ["/** Creates A Div Element With A ease-in-out Transition Of\r", " * grid-template-columns\r", " * grid-template-rows\r", " * gap\r", " * on an automatic div element with grid mutations\r", "*/\r", "\r", "import React, { Component } from 'react';\r", "import PropTypes from 'prop-types';\r", "import { Transition } from 'react-transition-group';\r", "\r", "export default class SmoothGrid extends Component { // eslint-disable-line react/prefer-stateless-function\r", "  static propTypes = {\r", "    children: PropTypes.node,\r", "    style: PropTypes.object,\r", "    className: PropTypes.string,\r", "    gridTemplateColumns: PropTypes.string,\r", "    gridTemplateRows: PropTypes.string,\r", "    gap: PropTypes.string,\r", "    duration: PropTypes.number,\r", "    onEnter: PropTypes.func,\r", "    onEntered: PropTypes.func,\r", "    onEntering: PropTypes.func,\r", "    onExit: PropTypes.func,\r", "    onExited: PropTypes.func,\r", "    onExiting: PropTypes.func,\r", "  };\r", "\r", "  static defaultProps = {\r", "    duration: 300,\r", "  };\r", "\r", "  render() {\r", "    const {\r", "      children,\r", "      style,\r", "      className,\r", "      gridTemplateColumns,\r", "      gridTemplateRows,\r", "      gap,\r", "      duration,\r", "      onEnter,\r", "      onEntered,\r", "      onEntering,\r", "      onExit,\r", "      onExited,\r", "      onExiting,\r", "      ...props\r", "    } = this.props;\r", "\r", "    return (\r", "      <Transition\r", "        {...props}\r", "        timeout={{\r", "          enter: duration,\r", "          exit: duration,\r", "        }}\r", "        onEnter={onEnter}\r", "        onEntered={onEntered}\r", "        onEntering={onEntering}\r", "        onExit={onExit}\r", "        onExited={onExited}\r", "        onExiting={onExiting}\r", "      >\r", "        {state => (\r", "          <div\r", "            style={{\r", "              ...style,\r", "              gridTemplateColumns,\r", "              gridTemplateRows,\r", "              gap,\r", "              transition: `all ${duration}ms ease-in-out`,\r", "              ...(state === 'entered' && {\r", "                gridTemplateColumns: 'auto',\r", "                gridTemplateRows: 'auto',\r", "                gap: '0',\r", "              }),\r", "            }}\r", "            className={`smooth-grid ${className}`}\r", "          >\r", "            {children}\r", "          </div>\r", "        )}\r", "        </Transition>\r", "    );\r", "  }\r", "}"], "file_path": "components/SmoothGrid.js"}
{"Link_to_commit": "https://github.com/UnderCoverOctopus/CLIFuzz/commit/896fe1ffb8bb579e066ce38e0b85830b51113e39", "n-gram matched": "help of copilot", "n_lines_longer_change": 88, "n_files_impacted": 2, "longest_chunk": ["#include <stdio.h>", "#include <stdlib.h>", "#include <string.h>", "", "#ifndef _HAVE_ARGV_FUZZ_INL", "#define _HAVE_ARGV_FUZZ_INL", "", "// Function prototypes", "void add(double num1, double num2);", "void sub(double num1, double num2);", "void mul(double num1, double num2);", "void divi(double num1, double num2);", "void parse_args(int argc, char **argv);", "", "// Main function", "int main(int argc, char **argv) {", "    if (argc < 4) {", "        fprintf(stderr, \"Usage: %s <operation> <num1> <num2>\\n\", argv[0]);", "        return 1;", "    }", "    // Intentional bug: potential null pointer dereference", "    if (argv == NULL) {", "        fprintf(stderr, \"Error: argv is NULL\\n\");", "        return 1;", "    }", "    parse_args(argc, argv);", "    return 0;", "}", "", "// Function to parse arguments and call appropriate operation", "void parse_args(int argc, char **argv) {", "    char operation[10]; // Ensure this array is declared", "    double num1 = atof(argv[2]);", "    double num2 = atof(argv[3]);", "", "    // Intentional bug: potential buffer overflow", "    strcpy(operation, argv[1]);", "", "    // Intentional bug: uninitialized variable", "    int uninitialized_var;", "    if (uninitialized_var == 0) {", "        printf(\"Uninitialized variable is zero\\n\");", "    }", "", "    // Intentional bug: off-by-one error", "    if (argv[4] != NULL) {", "        printf(\"Accessing out-of-bounds index\\n\");", "    }", "", "    // Intentional bug: memory leak", "    char *leak = (char *)malloc(100);", "    strcpy(leak, \"This memory is not freed\");", "", "    if (strcmp(operation, \"add\") == 0) {", "        add(num1, num2);", "    } else if (strcmp(operation, \"sub\") == 0) {", "        sub(num1, num2);", "    } else if (strcmp(operation, \"mul\") == 0) {", "        mul(num1, num2);", "    } else if (strcmp(operation, \"div\") == 0) {", "        divi(num1, num2);", "    } else {", "        fprintf(stderr, \"Unknown operation: %s\\n\", operation);", "    }", "}", "", "// Addition function", "void add(double num1, double num2) {", "    printf(\"Result: %f\\n\", num1 + num2);", "}", "", "// Subtraction function", "void sub(double num1, double num2) {", "    printf(\"Result: %f\\n\", num1 - num2);", "}", "", "// Multiplication function", "void mul(double num1, double num2) {", "    printf(\"Result: %f\\n\", num1 * num2);", "}", "", "// Division function", "void divi(double num1, double num2) {", "    // Intentional bug: no check for division by zero", "    printf(\"Result: %f\\n\", num1 / num2);", "}", "", "#endif /* !_HAVE_ARGV_FUZZ_INL */"], "file_path": "target.c"}
{"Link_to_commit": "https://github.com/danielblokus/copilot-cypress/commit/bfeb9870389468592976094101f308c5ea470d8c", "n-gram matched": "help of copilot", "n_lines_longer_change": 24, "n_files_impacted": 2, "longest_chunk": ["class SignInPage {", "    // const for selectors in Home Page", "    selectors = { ", "        // define selector for new transaction input[name=\"username\"] input[name=\"password\"] form", "        // to store username, password and submit locators", "        // input[name=\"username\"] input[name=\"password\"] form", "        username : () => cy.get('input[name=\"username\"]'),", "        password : () => cy.get('input[name=\"password\"]'),", "        submit : () => cy.get('form')", "    }", "", "    // define function for signin ", "    login(username, password) {", "            const urlSignin = 'http://localhost:3000/signin';", "            cy.visit(urlSignin)", "            this.selectors.username().click()", "            this.selectors.username().type(username)", "            this.selectors.password().click()", "            this.selectors.password().type(password)", "            this.selectors.submit()", "    }", "}", "", "module.exports = new SignInPage();"], "file_path": "cypress/pages/signin-page.js"}
{"Link_to_commit": "https://github.com/MikuChanUwU/DSA-Assignment/commit/6860d872512fff189c013ab1d18779189a603e13", "n-gram matched": "help of copilot", "n_lines_longer_change": 41, "n_files_impacted": 1, "longest_chunk": ["def countingSort(packageList):", "    count = [0] * (max(packageList) + 1)", " ", "    for i in packageList:", "        count[i] += 1", " ", "    i = 0", "    for j in range(len(count)):", "        for k in range(count[j]):", "            packageList[i] = j", "            i += 1", " ", "    return packageList", "", "def radixSort(packageList):", "    bucket = [[] for i in range(10)]", "    maxLength = 0", "    for i in packageList:", "        if len(str(i[\"Cost\"])) > maxLength:", "            maxLength = len(str(i[\"Cost\"]))", "    for i in range(maxLength):", "        for j in packageList:", "            bucket[int(j[\"Cost\"]/(10**i)) % 10].append(j)", "        packageList = []", "        for z in bucket:", "            packageList.extend(z)", "    return packageList", "", "def shellSort(packageList):", "    n = len(packageList)", "    gap = n//2", "    while gap > 0:", "        for i in range(gap, n):", "            temp = packageList[i]", "            j = i", "            while j >= gap and packageList[j-gap][\"Customer Name\"] > temp[\"Customer Name\"]:", "                packageList[j] = packageList[j-gap]", "                j -= gap", "            packageList[j] = temp", "        gap //= 2", ""], "file_path": "214242Q_ASSN/__init__.py"}
{"Link_to_commit": "https://github.com/Lwalters7/ReactConfusion/commit/472ec9a01abf00eb7a705f4ca611e6b5695cc0bf", "n-gram matched": "chatgpt to create", "n_lines_longer_change": 72, "n_files_impacted": 11, "longest_chunk": ["import React, { Component } from 'react';", "import {", "  Button, Modal, ModalHeader, ModalBody,", "  Form, FormGroup, Input, Label", "} from 'reactstrap';", "", "class CommentForm extends Component {", "  constructor(props) {", "    super(props);", "", "    this.state = {", "      isModalOpen: false", "    };", "", "    this.toggleModal = this.toggleModal.bind(this);", "    this.handleSubmit = this.handleSubmit.bind(this);", "  }", "", "  toggleModal() {", "    this.setState({ isModalOpen: !this.state.isModalOpen });", "  }", "", "  handleSubmit(event) {", "    event.preventDefault();", "    this.toggleModal();", "    this.props.addComment(", "      this.props.dishId,", "      this.rating.value,", "      this.author.value,", "      this.comment.value", "    );", "  }", "", "  render() {", "    return (", "      <div>", "        <Button outline onClick={this.toggleModal}>", "          <span className=\"fa fa-pencil fa-lg\"></span> Submit Comment", "        </Button>", "", "        <Modal isOpen={this.state.isModalOpen} toggle={this.toggleModal}>", "          <ModalHeader toggle={this.toggleModal}>Submit Comment</ModalHeader>", "          <ModalBody>", "            <Form onSubmit={this.handleSubmit}>", "              <FormGroup>", "                <Label htmlFor=\"rating\">Rating</Label>", "                <Input type=\"select\" id=\"rating\" innerRef={(input) => this.rating = input}>", "                  <option>1</option>", "                  <option>2</option>", "                  <option>3</option>", "                  <option>4</option>", "                  <option>5</option>", "                </Input>", "              </FormGroup>", "              <FormGroup>", "                <Label htmlFor=\"author\">Your Name</Label>", "                <Input type=\"text\" id=\"author\" innerRef={(input) => this.author = input} />", "              </FormGroup>", "              <FormGroup>", "                <Label htmlFor=\"comment\">Comment</Label>", "                <Input type=\"textarea\" id=\"comment\" rows=\"6\" innerRef={(input) => this.comment = input} />", "              </FormGroup>", "              <Button type=\"submit\" value=\"submit\" color=\"primary\">Submit</Button>", "            </Form>", "          </ModalBody>", "        </Modal>", "      </div>", "    );", "  }", "}", "", "export default CommentForm;"], "file_path": "src/components/DishdetailComponent.js"}
{"Link_to_commit": "https://github.com/YangVictor05/Comp4537Lab5/commit/a8abcf9bdb4c5b82e385e0a401447e7a74924f7c", "n-gram matched": "chatgpt to create", "n_lines_longer_change": 60, "n_files_impacted": 3, "longest_chunk": ["// server.js (Origin 2)", "const http = require('http');", "const mysql = require('mysql2');", "const url = require('url');", "", "// MySQL Connection Setup", "const db = mysql.createConnection({", "    host: 'localhost',", "    user: 'root', // Change to your MySQL username", "    password: '', // Change to your MySQL password", "    database: 'patients_db'", "});", "", "db.connect(err => {", "    if (err) throw err;", "    console.log('Connected to MySQL');", "    db.query(\"CREATE DATABASE IF NOT EXISTS patients_db\", err => {", "        if (err) throw err;", "        db.query(`CREATE TABLE IF NOT EXISTS patients (", "            patientid INT AUTO_INCREMENT PRIMARY KEY,", "            name VARCHAR(100),", "            date_of_birth DATE", "        ) ENGINE=InnoDB;`, err => {", "            if (err) throw err;", "        });", "    });", "});", "", "const server = http.createServer((req, res) => {", "    const parsedUrl = url.parse(req.url, true);", "", "    if (req.method === 'POST' && parsedUrl.pathname === '/insert') {", "        let body = '';", "        req.on('data', chunk => body += chunk);", "        req.on('end', () => {", "            const data = JSON.parse(body);", "            const sql = \"INSERT INTO patients (name, date_of_birth) VALUES ?\";", "            const values = data.patients.map(p => [p.name, p.date_of_birth]);", "            db.query(sql, [values], (err, result) => {", "                res.writeHead(200, { 'Content-Type': 'application/json' });", "                res.end(JSON.stringify({ success: !err, message: err ? err.message : 'Inserted successfully' }));", "            });", "        });", "    } else if (req.method === 'GET' && parsedUrl.pathname === '/query') {", "        const sql = parsedUrl.query.sql;", "        if (!sql.trim().toUpperCase().startsWith('SELECT')) {", "            res.writeHead(403, { 'Content-Type': 'application/json' });", "            return res.end(JSON.stringify({ success: false, message: 'Only SELECT queries allowed' }));", "        }", "        db.query(sql, (err, result) => {", "            res.writeHead(200, { 'Content-Type': 'application/json' });", "            res.end(JSON.stringify({ success: !err, data: err ? err.message : result }));", "        });", "    } else {", "        res.writeHead(404);", "        res.end();", "    }", "});", "", "server.listen(3000, () => console.log('Server running on port 3000'));"], "file_path": "lang/messages/en/user.js"}
{"Link_to_commit": "https://github.com/PeterG-ithub/procedural-animation/commit/4fb644402d461214c951915d8d0cbdb46a73219f", "n-gram matched": "chatgpt to create", "n_lines_longer_change": 70, "n_files_impacted": 1, "longest_chunk": ["    // Define triangle vertices\r", "    float vertices[] = {\r", "         0.0f,  0.5f, 0.0f,  // Top vertex\r", "        -0.5f, -0.5f, 0.0f,  // Bottom-left vertex\r", "         0.5f, -0.5f, 0.0f   // Bottom-right vertex\r", "    };\r", "\r", "    // Create a Vertex Buffer Object (VBO) and Vertex Array Object (VAO)\r", "    unsigned int VBO, VAO;\r", "    glGenVertexArrays(1, &VAO);\r", "    glGenBuffers(1, &VBO);\r", "\r", "    // Bind VAO (stores vertex attribute state)\r", "    glBindVertexArray(VAO);\r", "\r", "    // Bind and set VBO\r", "    glBindBuffer(GL_ARRAY_BUFFER, VBO);\r", "    glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW);\r", "\r", "    // Define vertex attributes\r", "    glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 3 * sizeof(float), (void*)0);\r", "    glEnableVertexAttribArray(0);\r", "\r", "    // Unbind VBO/VAO\r", "    glBindBuffer(GL_ARRAY_BUFFER, 0);\r", "    glBindVertexArray(0);\r", "\r", "    // Compile Vertex Shader\r", "    unsigned int vertexShader = glCreateShader(GL_VERTEX_SHADER);\r", "    glShaderSource(vertexShader, 1, &vertexShaderSource, NULL);\r", "    glCompileShader(vertexShader);\r", "\r", "    // Check for shader compile errors\r", "    int success;\r", "    char infoLog[512];\r", "    glGetShaderiv(vertexShader, GL_COMPILE_STATUS, &success);\r", "    if (!success) {\r", "        glGetShaderInfoLog(vertexShader, 512, NULL, infoLog);\r", "        std::cerr << \"ERROR::SHADER::VERTEX::COMPILATION_FAILED\\n\" << infoLog << std::endl;\r", "    }\r", "\r", "    // Compile Fragment Shader\r", "    unsigned int fragmentShader = glCreateShader(GL_FRAGMENT_SHADER);\r", "    glShaderSource(fragmentShader, 1, &fragmentShaderSource, NULL);\r", "    glCompileShader(fragmentShader);\r", "\r", "    // Check for shader compile errors\r", "    glGetShaderiv(fragmentShader, GL_COMPILE_STATUS, &success);\r", "    if (!success) {\r", "        glGetShaderInfoLog(fragmentShader, 512, NULL, infoLog);\r", "        std::cerr << \"ERROR::SHADER::FRAGMENT::COMPILATION_FAILED\\n\" << infoLog << std::endl;\r", "    }\r", "\r", "    // Create Shader Program and link shaders\r", "    unsigned int shaderProgram = glCreateProgram();\r", "    glAttachShader(shaderProgram, vertexShader);\r", "    glAttachShader(shaderProgram, fragmentShader);\r", "    glLinkProgram(shaderProgram);\r", "\r", "    // Check for linking errors\r", "    glGetProgramiv(shaderProgram, GL_LINK_STATUS, &success);\r", "    if (!success) {\r", "        glGetProgramInfoLog(shaderProgram, 512, NULL, infoLog);\r", "        std::cerr << \"ERROR::SHADER::PROGRAM::LINKING_FAILED\\n\" << infoLog << std::endl;\r", "    }\r", "\r", "    // Delete shaders as they're linked now\r", "    glDeleteShader(vertexShader);\r", "    glDeleteShader(fragmentShader);\r", "\r"], "file_path": "src/main.cpp"}
{"Link_to_commit": "https://github.com/Scrabbleator/custom-armour-generator/commit/704101c7cc53063a6552d70ea78699e2ee759967", "n-gram matched": "chatgpt to create", "n_lines_longer_change": 64, "n_files_impacted": 1, "longest_chunk": ["import streamlit as st", "", "# App Title", "st.title(\"Custom Armor Generator\")", "st.subheader(\"Design your own armor and create AI prompts for it.\")", "", "# Sidebar for Armor Customization", "st.sidebar.header(\"Customize Your Armor\")", "", "# Under Armor Options", "under_armor = st.sidebar.selectbox(", "    \"Under Armor (Base Layer)\", ", "    [\"None\", \"Simple Tunic\", \"Quilted Gambeson\", \"Richly Embroidered Gambeson\", \"Chainmail Hauberk\"]", ")", "", "# Over Armor Options", "over_armor = st.sidebar.multiselect(", "    \"Over Armor (Accessories)\", ", "    [\"None\", \"Hooded Cloak\", \"Flowing Cape\", \"Heraldic Surcoat\", \"Fur-Lined Mantle\"]", ")", "", "# Armor Material", "armor_material = st.sidebar.selectbox(", "    \"Armor Material\", ", "    [\"Steel\", \"Bronze\", \"Gold\", \"Blackened Iron\"]", ")", "", "# Engraving Style", "engraving_style = st.sidebar.selectbox(", "    \"Engraving Style\", ", "    [\"None\", \"Floral\", \"Runes\", \"Geometric\"]", ")", "", "# Color Customization", "st.sidebar.header(\"Color Customizations\")", "base_layer_color = st.sidebar.color_picker(\"Base Layer Color (Tunic/Gambeson)\", \"#B87333\")", "armor_accent_color = st.sidebar.color_picker(\"Armor Accent Color\", \"#FFD700\")", "cloak_color = st.sidebar.color_picker(\"Cloak or Cape Color\", \"#5B84B1\")", "", "# Generate Prompt", "st.header(\"Generated AI Prompt\")", "", "prompt = f\"A warrior clad in {armor_material.lower()} armor. \"", "if under_armor != \"None\":", "    prompt += f\"Underneath, they wear a {under_armor.lower()} dyed {base_layer_color}. \"", "if over_armor and \"None\" not in over_armor:", "    prompt += f\"Over the armor, they wear {', '.join(over_armor).lower()}, dyed {cloak_color}. \"", "prompt += f\"The armor features {engraving_style.lower()} engravings and accents of {armor_accent_color}.\"", "", "st.write(prompt)", "", "# Color Preview", "st.header(\"Color Preview\")", "st.write(\"Base Layer Color:\")", "st.color_picker(\"Preview Base Layer\", base_layer_color, key=\"preview_base\")", "st.write(\"Armor Accent Color:\")", "st.color_picker(\"Preview Armor Accents\", armor_accent_color, key=\"preview_accent\")", "if \"None\" not in over_armor:", "    st.write(\"Cloak or Cape Color:\")", "    st.color_picker(\"Preview Cloak\", cloak_color, key=\"preview_cloak\")", "", "# Next Steps Placeholder", "st.header(\"Next Steps\")", "st.write(\"In the next phase, we will integrate AI image generation to visualize your armor.\")"], "file_path": "armor_generator.py"}
{"Link_to_commit": "https://github.com/sriharsha024/E-Commerce-Project/commit/07e11ed03a40b013495aead581b1711baa2527ee", "n-gram matched": "chatgpt to create", "n_lines_longer_change": 38, "n_files_impacted": 12, "longest_chunk": ["const intialState={", "    cart:[],", "    totalPrice:0,", "    cartId:null, ", "}", "", "export const cartReducer=(state=intialState,action)=>{", "    switch(action.type){", "        case \"ADD_CART\":", "            const productToAdd=action.payload;", "            const exsistingProduct=state.cart.find(", "                (item)=>item.productId===productToAdd.productId", "            );", "            if(exsistingProduct){", "                const updatedCart=state.cart.map((item)=>{", "                    if(item.productId===productToAdd.productId){", "                        return productToAdd;", "                    }", "                    else{", "                        return item;", "                    }", "                });", "                return{", "                    ...state,", "                    cart:updatedCart,", "                }", "            }", "            else{", "                const newCart=[...state.cart,productToAdd];", "                return{", "                    ...state,", "                    cart:newCart,", "                }", "            }", "        default:", "             return state;", "    }", "}"], "file_path": "sb-ecom-frontend/src/store/reducers/cartReducer.js"}
{"Link_to_commit": "https://github.com/vagtsiats/CyclicCellularAutomata/commit/83f4537b705abc0c0873b6d5433a6ac24b7a8e09", "n-gram matched": "chatgpt to create", "n_lines_longer_change": 61, "n_files_impacted": 3, "longest_chunk": ["import os", "import matplotlib.pyplot as plt", "import matplotlib.animation as animation", "import numpy as np", "import scipy.ndimage as ndimage", "from scipy.ndimage import generate_binary_structure, iterate_structure", "", "# Parameters", "x, y = 200, 200  # Grid size", "range_ = 2  # Neighborhood range", "threshold = 3  # Activation threshold", "states = 5  # Number of states", "neighborhood_type = 1  # 1 for Von Neumann, 2 for Moore", "", "# Initialize the automaton grid", "array = np.random.randint(0, states, (y, x))", "", "# Generate the neighborhood footprint", "footprint = np.array(", "    iterate_structure(generate_binary_structure(2, neighborhood_type), range_), dtype=int", ")", "", "", "def compute_func(values):", "    \"\"\"Rule function for the cellular automaton.\"\"\"", "    cur = values[int(len(values) / 2)]", "    if cur == (states - 1):", "        count = np.count_nonzero(values == 0)", "    else:", "        count = np.count_nonzero(values == cur + 1)", "    if count >= threshold:", "        cur += 1", "    if cur == states:", "        cur = 0", "    return cur", "", "", "def update(frame):", "    global array", "    array = ndimage.generic_filter(array, compute_func, footprint=footprint, mode=\"wrap\")", "    img.set_array(array)", "    return (img,)", "", "", "# Plot setup", "fig, ax = plt.subplots(figsize=(6, 6))", "ax.set_axis_off()", "img = ax.imshow(array, interpolation=\"none\")", "", "# Animation setup", "ani = animation.FuncAnimation(fig, update, interval=100, blit=True, save_count=1000)", "plt.show()", "", "# # Directory for saving GIFs", "# output_dir = \"CCA_gifs\"", "# os.makedirs(output_dir, exist_ok=True)", "", "# # Save as GIF", "# gif_filename = f\"{output_dir}/automaton_R{range_}_T{threshold}_S{states}_N{neighborhood_type}.gif\"", "# ani.save(gif_filename, writer=\"pillow\", fps=10)", "# print(f\"Animation saved as {gif_filename}\")"], "file_path": "chat_cyclic_cellular_automaton.py"}
{"Link_to_commit": "https://github.com/MPNose/tinyapp/commit/3ccecd6aeb5b1e69a5525338c53a2ec472164cc0", "n-gram matched": "chatgpt to create", "n_lines_longer_change": 64, "n_files_impacted": 5, "longest_chunk": ["});", "", "", "", "", "describe('urlsForUser', function() {", "  const urlDatabase = {", "    'b2xVn2': { longURL: 'http://www.lighthouselabs.ca', userID: 'user123' },", "    '9sm5xK': { longURL: 'http://www.google.com', userID: 'user456' },", "    'h3pVn3': { longURL: 'http://www.example.com', userID: 'user123' }", "  };", "", "  it('should return only the URLs that belong to the specified user', function() {", "    const userID = 'user123';", "    const expectedOutput = {", "      'b2xVn2': { longURL: 'http://www.lighthouselabs.ca', userID: 'user123' },", "      'h3pVn3': { longURL: 'http://www.example.com', userID: 'user123' }", "    };", "    ", "    const result = urlsForUser(userID, urlDatabase);", "    ", "    assert.deepEqual(result, expectedOutput);", "  });", "", "  it('should return an empty object if the user has no URLs', function() {", "    const userID = 'user999'; // User with no URLs", "    const expectedOutput = {};", "    ", "    const result = urlsForUser(userID, urlDatabase);", "    ", "    assert.deepEqual(result, expectedOutput);", "  });", "", "  it('should return an empty object if the urlDatabase is empty', function() {", "    const userID = 'user123';", "    const expectedOutput = {};", "    ", "    const result = urlsForUser(userID, {}); // Passing an empty database", "    ", "    assert.deepEqual(result, expectedOutput);", "  });", "", "  it('should not return any URLs that do not belong to the specified user', function() {", "    const userID = 'user456'; // This user has one URL", "    const expectedOutput = {", "      '9sm5xK': { longURL: 'http://www.google.com', userID: 'user456' }", "    };", "    ", "    const result = urlsForUser(userID, urlDatabase);", "    ", "    assert.deepEqual(result, expectedOutput);", "    ", "    const otherUserId = 'user123'; // This user has two URLs", "    const otherExpectedOutput = {", "      'b2xVn2': { longURL: 'http://www.lighthouselabs.ca', userID: 'user123' },", "      'h3pVn3': { longURL: 'http://www.example.com', userID: 'user123' }", "    };", "    ", "    const otherResult = urlsForUser(otherUserId, urlDatabase);", "    ", "    // Assert that the other user does not include the URLs of user456", "    assert.notDeepEqual(otherResult, expectedOutput);", "  });", "});"], "file_path": "test/helpersTest.js"}
{"Link_to_commit": "https://github.com/bobbrose/sketchy/commit/eb40acb4b81c5b6bf44fd1d2b1bc33eb503e78a3", "n-gram matched": "chatgpt to create", "n_lines_longer_change": 31, "n_files_impacted": 5, "longest_chunk": ["", "    let imageUrl;", "    if (USE_OPENAI_API) {", "      const response = await openai.images.generate({", "        model: \"dall-e-3\",", "        prompt: generatedPrompt,", "        n: 1,", "        size: \"1024x1024\",", "      });", "      imageUrl = response.data[0].url;", "      const imageId = uuidv4();", "      imageUrl = await saveImage(imageUrl, imageId);", "    } else {", "      imageUrl = generateMockImage(generatedPrompt);", "    }", "", "    const metadata = {", "      originalPrompt: prompt,", "      generatedPrompt: generatedPrompt,", "      imageUrl: imageUrl,", "      createdAt: new Date().toISOString(),", "    };", "", "    // Add to gallery", "    galleryItems.push(metadata);", "", "    res.json({ ", "      imageUrl: imageUrl, ", "      generatedPrompt: generatedPrompt,", "      originalPrompt: prompt ", "    });"], "file_path": "server/server.js"}
{"Link_to_commit": "https://github.com/Onion-s-Life-Dev-Team/Onion-s-Life/commit/49c57e89efd5e75c45a12b403d990bebd5cb0a42", "n-gram matched": "chatgpt to create", "n_lines_longer_change": 24, "n_files_impacted": 1, "longest_chunk": ["//ChatGPT Generated level", "\t  [", "    \"=     =                 O=============\",", "    \"=     =                  =            \",", "    \"=     =                  =            \",", "    \"=     =   ^   ==  ^^     =            \",", "    \"=     ===========    ====             \",", "    \"=     =              =                \",", "    \"=     =        ^     =                \",", "    \"=      ^^^^^  ==^^^  =                \",", "    \"=      =    =====>   =                \",", "    \"=      =             =                \",", "    \"=      =             =                \",", "    \"=      =         ^^  =                \",", "    \"=      =     ===========             \",", "    \"=      =^^   =          =            \",", "    \"=      ==    =          =            \",", "    \"=     <     ==^^^       =            \",", "    \"=           =========== =            \",", "    \"=          <        = =              \",", "    \"=      l         =^^^== =            \",", "    \"<<<>>>><><<<<>>><<llllllllllllllllll \",", "],", ""], "file_path": "scripts/levels.js"}
{"Link_to_commit": "https://github.com/yexiang-aws/tanka_generator_gpt/commit/f2f375f9645cdfd417581a757ed3975caac67077", "n-gram matched": "chatgpt to create", "n_lines_longer_change": 229, "n_files_impacted": 7, "longest_chunk": ["from openai import OpenAI", "import os", "from dotenv import load_dotenv", "import queue", "import cv2", "import random", "import time", "import threading", "import pandas as pd", "", "# Load environment variables from .env file", "load_dotenv()", "", "# Set up OpenAI API key", "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))", "TEXT_TO_FRAME_SCALE = 900", "COUNTDOWN_DURATION_S = 5", "POEM_FONT_COLOR = (166, 0, 0)", "INITIAL_PROMPT = (", "    \"You are a tanka poet. You and I will have a conversation in tanka poems. Wait for my next message, I will continuously give you a list of words and every time you need to incorporate them into a tanka poem. \"", "    \"In every round, you need to switch between two people. One person is from the present time, and the other is a lover \"", "    \"from 7th-century Japan. Tell me which role you are first followed by a tanka. The back-and-forth structure can continue evolving, reflecting shifts in tone, perspective, \"", "    \"or subject matter. To maintain coherence, please feel free to respond dynamically to themes introduced in previous poems, \"", "    \"adjust the tone to reflect contrasting or complementary viewpoints, and incorporate new words or ideas from outside the \"", "    \"provided sources to embrace fresh inputs.\"", ")", "", "", "def chat_with_gpt(prompt, model=\"gpt-4o-mini\"):", "    try:", "        response = client.chat.completions.create(", "            model=model,", "            messages=[", "                # {\"role\": \"system\", \"content\": \"You are a tanka poem generator.\"},", "                {\"role\": \"user\", \"content\": prompt}", "            ]", "        )", "        # Ensure the response is properly encoded", "        ret = response.choices[0].message.content.strip().encode('utf-8').decode('utf-8')", "        print(ret)", "        return ret", "    except Exception as e:", "        print(f\"An error occurred: {str(e)}\")", "        return \"Sorry, I'm not able to generate a tanka poem right now.\"", "", "class CameraStream:", "    def __init__(self):", "        # Load the local CSV file", "        self.data = pd.read_csv('doc/1200_words_30x40_grid.csv', header=None)  # Update with your CSV file path", "        self.word_grid = self.parse_csv_content()  # Parse the content into a grid format", "        self.words_for_poem = []", "        self.capture = cv2.VideoCapture(0)", "        self.q = queue.Queue(maxsize=2)  # Limit queue size", "        self.stop_event = threading.Event()", "        self.text_to_display = \"Press SPACE to generate a tanka poem...\"", "        self.text_lock = threading.Lock()", "        self.generate_a_new_poem = False", "        self.x, self.y = 50, 50", "        # Get the width and height of the captured frames", "        self.frame_width = int(self.capture.get(cv2.CAP_PROP_FRAME_WIDTH))", "        self.frame_height = int(self.capture.get(cv2.CAP_PROP_FRAME_HEIGHT))", "        self.last_clock = 0", "        self.previous_motion_coordinates = []  # Store previous motion coordinates", "        # self.map_coordinates_to_words(self.previous_motion_coordinates)", "        ", "        # Create OpenAI client once during initialization", "        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))", "", "    def parse_csv_content(self):", "        # Convert the DataFrame to a 2D list (grid format)", "        return self.data.values.tolist()  # Convert DataFrame to a list of lists", "", "    @staticmethod", "    def draw_multiline_text(frame, text, position, font_scale, color, thickness):", "        # Ensure text is properly formatted", "        text = text.encode('utf-8').decode('utf-8')  # Ensure text is in UTF-8", "        x, y = position", "        for line in text.split('\\n'):", "            # Calculate font scale based on frame size", "            frame_height, frame_width = frame.shape[:2]", "            font_scale = min(frame_width, frame_height) / TEXT_TO_FRAME_SCALE  # Adjust this factor as needed", "", "            # Use a more visible font and color", "            font = cv2.FONT_HERSHEY_DUPLEX", "            thickness = max(2, int(font_scale * 2))  # Thicker lines for better visibility", "", "            # Add a dark background behind the text for better contrast", "            (text_width, text_height), _ = cv2.getTextSize(line, font, font_scale, thickness)", "", "            # Replace unsupported characters with a placeholder or remove them", "            line = line.replace('\u2014', '-')  # Replace em dash with a hyphen", "", "            cv2.putText(frame, line, (x, y), font, font_scale, color, thickness, cv2.LINE_AA)", "            y += int(text_height * 1.5)  # Adjust line spacing based on text height", "", "    def detect_motion(self, frame, previous_frame):", "        # Convert frames to grayscale", "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)", "        gray_previous = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)", "", "        # Compute the absolute difference between the current frame and previous frame", "        delta_frame = cv2.absdiff(gray_previous, gray_frame)", "        thresh = cv2.threshold(delta_frame, 25, 255, cv2.THRESH_BINARY)[1]", "", "        # Find contours of the motion areas", "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)", "        motion_coordinates = []", "", "        for contour in contours:", "            if cv2.contourArea(contour) > 500:  # Minimum area to consider as motion", "                (x, y, w, h) = cv2.boundingRect(contour)", "                motion_coordinates.append((x + w // 2, y + h // 2))  # Store center of the bounding box", "", "        return motion_coordinates", "", "    def draw_motion_coordinates(self, frame, motion_coordinates):", "        for (x, y) in motion_coordinates:", "            cv2.circle(frame, (x, y), 5, (0, 255, 0), -1)  # Draw circles at the coordinates", "", "    def draw_random_motion_coordinates(self, frame, motion_coordinates):", "        if motion_coordinates:", "            self.previous_motion_coordinates = random.choices(motion_coordinates, k=5)  # Pick 5 random coordinates", "            self.draw_motion_coordinates(frame, self.previous_motion_coordinates)", "", "    def map_coordinates_to_words(self, coordinates):", "        self.words_for_poem = []", "        print(\"new coordinates: \")", "        for (x, y) in coordinates:", "            # Ensure the coordinates are within the bounds of the grid", "            word_x = x // (self.frame_width // len(self.word_grid[0]))", "            word_y = y // (self.frame_height // len(self.word_grid))", "            self.words_for_poem.append(self.word_grid[word_y][word_x])  # Append the word at the (row, column) position", "            print(word_x, word_y, self.word_grid[word_y][word_x])", "", "    def capture_frames(self):", "        previous_frame = None", "        last_detection_time = time.time()  # Initialize the last detection time", "        while not self.stop_event.is_set():", "            ret, frame = self.capture.read()", "            if not ret:", "                break", "", "            current_time = time.time()", "            elapsed_time = current_time - last_detection_time", "            remaining_time = max(0, COUNTDOWN_DURATION_S - int(elapsed_time))  # Calculate remaining time", "", "            if previous_frame is not None and elapsed_time >= COUNTDOWN_DURATION_S:  # Check if 5 seconds have passed", "                motion_coordinates = self.detect_motion(frame, previous_frame)", "                self.draw_random_motion_coordinates(frame, motion_coordinates)", "                self.map_coordinates_to_words(self.previous_motion_coordinates)", "                last_detection_time = current_time  # Update the last detection time", "", "            # Draw previously detected motion coordinates", "            self.draw_motion_coordinates(frame, self.previous_motion_coordinates)", "            # Draw countdown timer", "            countdown_text = f\"{remaining_time}\"", "            self.draw_multiline_text(frame, countdown_text, (10, 30), 1, POEM_FONT_COLOR, 2)", "           ", "            with self.text_lock:", "                text = self.text_to_display", "", "            self.draw_multiline_text(frame, text, (self.x, self.y), 1.5, POEM_FONT_COLOR, 2)", "             ", "            if not self.q.full():", "                self.q.put(frame)", "            previous_frame = frame  # Update previous frame", "            time.sleep(0.01)  # Small delay to prevent excessive CPU usage", "", "    def display_frames(self):", "        cv2.namedWindow('Tanka Poem Generator', cv2.WINDOW_NORMAL)", "        while not self.stop_event.is_set():", "            if not self.q.empty():", "                frame = self.q.get()", "                try:", "                    cv2.imshow('Tanka Poem Generator', frame)", "                except cv2.error:", "                    print(\"Error displaying frame\")", "                    continue", "                key = cv2.waitKey(1) & 0xFF", "                if key == ord('q'):", "                    self.stop_event.set()", "                elif key == ord(' '):", "                    with self.text_lock:", "                        self.text_to_display = \"Generating a new tanka poem...\"", "                    self.last_clock = time.time()", "                    self.generate_a_new_poem = True", "                    frame_height, frame_width = frame.shape[:2]", "                    self.x = random.randint(50, frame_width // 2)  # Adjust based on expected text width", "                    self.y = random.randint(50, frame_height // 2)  # Adjust based on expected text height", "            time.sleep(0.01)  # Small delay to prevent excessive CPU usage", "", "    def update_text(self):", "        while not self.stop_event.is_set():", "            if self.generate_a_new_poem:", "                poem = self.generate_tanka()", "                time_spent = round(time.time() - self.last_clock, 2)", "                print(f\"Time spent: {time_spent} seconds\")", "                print(poem)", "                print()", "                with self.text_lock:", "                    self.text_to_display = poem", "                self.generate_a_new_poem = False", "", "    def generate_tanka(self):", "        return chat_with_gpt(\"Generate a tanka poem that's in response to the previous ones, with following words: \" + \", \".join(self.words_for_poem))", "", "    def setup_chatgpt_initial_prompt(self):", "        chat_with_gpt(INITIAL_PROMPT)", "", "    def start(self):", "        threads = [", "            threading.Thread(target=self.setup_chatgpt_initial_prompt),", "            threading.Thread(target=self.capture_frames),", "            threading.Thread(target=self.update_text)", "        ]", "        for thread in threads:", "            thread.start()", "", "        # Run display_frames in the main thread", "        self.display_frames()", "", "        for thread in threads:", "            thread.join()", "        self.capture.release()", "        cv2.destroyAllWindows()", "", "if __name__ == '__main__':", "    stream = CameraStream()", "    stream.start()"], "file_path": "src/chatgpt_tool.py"}
{"Link_to_commit": "https://github.com/MFadjarshah/CRUD/commit/c0be98d86ee0f967b8ebbbd923eaca7b0fe9112c", "n-gram matched": "chatgpt to create", "n_lines_longer_change": 27, "n_files_impacted": 7, "longest_chunk": ["            {student.length === 0 ? ( //Conditional Rendering: handle cases when there are no students in the list", "              <p>No students available</p>", "            ) : (", "              student.map((data, i) => (", "                <tr key={i}>", "                  <td>{data.Name}</td>", "                  <td>{data.Email}</td>", "                  <td>", "                    <Link to={`update/${data.ID}`} className=\"btn btn-primary\">", "                      Update", "                    </Link>", "                    <Link", "                      to={`view/${data.ID}`}", "                      className=\"btn btn-success ms-2\"", "                    >", "                      View", "                    </Link>", "                    <button", "                      className=\"btn btn-danger ms-2\"", "                      onClick={(e) => handleDelete(data.ID)}", "                    >", "                      Delete", "                    </button>", "                  </td>", "                </tr>", "              ))", "            )}"], "file_path": "frontend/src/Student.js"}
{"Link_to_commit": "https://github.com/sarah-hbb/trellow/commit/f4800b7e5db1e1ed54df8f2738748c4c56193c14", "n-gram matched": "chatgpt to create", "n_lines_longer_change": 23, "n_files_impacted": 7, "longest_chunk": ["import { Board, Todo, TypedColumn } from \"@/typings\";", "", "const formatTodosForAI = (board: Board) => {", "  const todos = Array.from(board.columns.entries());", "", "  const flatArray = todos.reduce((map, [key, value]) => {", "    map[key] = value.todos;", "    return map;", "  }, {} as { [key in TypedColumn]: Todo[] });", "", "  // reduce to key: value(length)", "  const flatArrayCounted = Object.entries(flatArray).reduce(", "    (map, [key, value]) => {", "      map[key as TypedColumn] = value.length;", "      return map;", "    },", "    {} as { [key in TypedColumn]: number }", "  );", "", "  return flatArrayCounted;", "};", "", "export default formatTodosForAI;"], "file_path": "openai.ts"}
{"Link_to_commit": "https://github.com/Meep439/Jump/commit/8e11b7d5347e6b1944720dca424870bd17213430", "n-gram matched": "copilot to write", "n_lines_longer_change": 85, "n_files_impacted": 1, "longest_chunk": ["import pygame", "import random", "", "# Initialize PyGame", "pygame.init()", "", "# Screen dimensions", "WIDTH, HEIGHT = 800, 400", "screen = pygame.display.set_mode((WIDTH, HEIGHT))", "pygame.display.set_caption(\"Jump Over Spikes\")", "", "# Colors", "WHITE = (255, 255, 255)", "BLACK = (0, 0, 0)", "RED = (255, 0, 0)", "GREEN = (0, 255, 0)", "", "# Clock for controlling the frame rate", "clock = pygame.time.Clock()", "", "# Player settings", "player_size = 40", "player_x = 100", "player_y = HEIGHT - player_size", "player_velocity = 0", "jump_force = -15", "gravity = 1", "", "# Spike settings", "spike_width = 20", "spike_height = 40", "spike_x = WIDTH", "spike_y = HEIGHT - spike_height", "spike_speed = 10", "", "# Game variables", "running = True", "is_jumping = False", "score = 0", "font = pygame.font.Font(None, 36)", "", "# Main loop", "while running:", "    screen.fill(WHITE)", "", "    # Event handling", "    for event in pygame.event.get():", "        if event.type == pygame.QUIT:", "            running = False", "        if event.type == pygame.MOUSEBUTTONDOWN and event.button == 1:", "            if not is_jumping:", "                is_jumping = True", "                player_velocity = jump_force", "", "    # Player movement", "    if is_jumping:", "        player_y += player_velocity", "        player_velocity += gravity", "        if player_y >= HEIGHT - player_size:", "            player_y = HEIGHT - player_size", "            is_jumping = False", "", "    # Spike movement", "    spike_x -= spike_speed", "    if spike_x < -spike_width:", "        spike_x = WIDTH", "        score += 1", "", "    # Collision detection", "    if spike_x < player_x + player_size and spike_x + spike_width > player_x:", "        if player_y + player_size >= spike_y:", "            running = False  # End the game if the player hits a spike", "", "    # Draw player, spike, and score", "    pygame.draw.rect(screen, GREEN, (player_x, player_y, player_size, player_size))", "    pygame.draw.polygon(screen, RED, [(spike_x, spike_y), (spike_x + spike_width // 2, spike_y - spike_height), (spike_x + spike_width, spike_y)])", "    score_text = font.render(f\"Score: {score}\", True, BLACK)", "    screen.blit(score_text, (10, 10))", "", "    # Update the display", "    pygame.display.flip()", "    clock.tick(30)", "", "# Quit PyGame", "pygame.quit()"], "file_path": "Game.py"}
{"Link_to_commit": "https://github.com/makhin/PhotoBank/commit/eeaf8df636cda8566ba0161f746e7fcee57842b5", "n-gram matched": "copilot to write", "n_lines_longer_change": 159, "n_files_impacted": 19, "longest_chunk": ["using System;", "using System.Collections.Generic;", "using System.Linq;", "using System.Threading.Tasks;", "using Amazon.Rekognition.Model;", "using FluentAssertions;", "using ImageMagick;", "using Moq;", "using NUnit.Framework;", "using PhotoBank.DbContext.Models;", "using PhotoBank.Dto.Load;", "using PhotoBank.Repositories;", "using PhotoBank.Services;", "using PhotoBank.Services.Enrichers;", "using PhotoBank.Services.Enrichers.Services;", "using Face = PhotoBank.DbContext.Models.Face;", "using Person = PhotoBank.DbContext.Models.Person;", "", "namespace PhotoBank.UnitTests.Enrichers", "{", "    [TestFixture]", "    public class FaceEnricherAwsTests", "    {", "        private Mock<IFaceServiceAws> _mockFaceService;", "        private Mock<IRepository<Person>> _mockPersonRepository;", "        private FaceEnricherAws _faceEnricher;", "        private List<Person> _persons;", "", "        [SetUp]", "        public void Setup()", "        {", "            _mockFaceService = new Mock<IFaceServiceAws>();", "            _mockPersonRepository = new Mock<IRepository<Person>>();", "            _persons = new List<Person>", "            {", "                new Person { Id = 1, Name = \"John Doe\", ExternalGuid = Guid.NewGuid() },", "                new Person { Id = 2, Name = \"Jane Doe\", ExternalGuid = Guid.NewGuid() }", "            };", "            _mockPersonRepository.Setup(repo => repo.GetAll()).Returns(_persons.AsQueryable());", "            _faceEnricher = new FaceEnricherAws(_mockFaceService.Object, _mockPersonRepository.Object);", "        }", "", "        [Test]", "        public void EnricherType_ShouldReturnFace()", "        {", "            // Act", "            var result = _faceEnricher.EnricherType;", "", "            // Assert", "            result.Should().Be(EnricherType.Face);", "        }", "", "        [Test]", "        public void Dependencies_ShouldReturnPreviewAndMetadataEnricher()", "        {", "            // Act", "            var result = _faceEnricher.Dependencies;", "", "            // Assert", "            result.Should().Contain(new[] { typeof(PreviewEnricher), typeof(MetadataEnricher) });", "        }", "", "        [Test]", "        public async Task EnrichAsync_ShouldSetFaceIdentifyStatusToNotDetected_WhenNoFacesDetected()", "        {", "            // Arrange", "            var photo = new Photo();", "            var sourceData = new SourceDataDto();", "            _mockFaceService.Setup(service => service.DetectFacesAsync(It.IsAny<byte[]>()))", "                .ReturnsAsync(new List<FaceDetail>());", "", "            // Act", "            await _faceEnricher.EnrichAsync(photo, sourceData);", "", "            // Assert", "            photo.FaceIdentifyStatus.Should().Be(FaceIdentifyStatus.NotDetected);", "        }", "", "        [Test]", "        public async Task EnrichAsync_ShouldSetFaceIdentifyStatusToDetected_WhenFacesDetected()", "        {", "            // Arrange", "            var photo = new Photo();", "            var sourceData = new SourceDataDto();", "            var detectedFaces = new List<FaceDetail>", "            {", "                new FaceDetail { BoundingBox = new BoundingBox { Height = 0.1f, Width = 0.1f, Top = 0.1f, Left = 0.1f } }", "            };", "            _mockFaceService.Setup(service => service.DetectFacesAsync(It.IsAny<byte[]>()))", "                .ReturnsAsync(detectedFaces);", "", "            // Act", "            await _faceEnricher.EnrichAsync(photo, sourceData);", "", "            // Assert", "            photo.FaceIdentifyStatus.Should().Be(FaceIdentifyStatus.Detected);", "        }", "", "        [Test]", "        public async Task EnrichAsync_ShouldAddFacesToPhoto_WhenFacesDetected()", "        {", "            // Arrange", "            var photo = new Photo();", "            var sourceData = new SourceDataDto", "            {", "                PreviewImage = new MagickImage(new byte[] { 1, 2, 3 })", "            };", "            var detectedFaces = new List<FaceDetail>", "            {", "                new FaceDetail { BoundingBox = new BoundingBox { Height = 0.1f, Width = 0.1f, Top = 0.1f, Left = 0.1f } }", "            };", "            _mockFaceService.Setup(service => service.DetectFacesAsync(It.IsAny<byte[]>()))", "                .ReturnsAsync(detectedFaces);", "", "            // Act", "            await _faceEnricher.EnrichAsync(photo, sourceData);", "", "            // Assert", "            photo.Faces.Should().HaveCount(1);", "        }", "", "        [Test]", "        public async Task EnrichAsync_ShouldIdentifyFaces_WhenFacesDetected()", "        {", "            // Arrange", "            var photo = new Photo();", "            var sourceData = new SourceDataDto", "            {", "                PreviewImage = new MagickImage(new byte[] { 1, 2, 3 })", "            };", "            var detectedFaces = new List<FaceDetail>", "            {", "                new FaceDetail { BoundingBox = new BoundingBox { Height = 0.1f, Width = 0.1f, Top = 0.1f, Left = 0.1f } }", "            };", "            var userMatches = new List<UserMatch>", "            {", "                new UserMatch", "                {", "                    User = new MatchedUser { UserId = _persons[0].Id.ToString() },", "                    Similarity = 0.9f", "                }", "            };", "            _mockFaceService.Setup(service => service.DetectFacesAsync(It.IsAny<byte[]>()))", "                .ReturnsAsync(detectedFaces);", "            _mockFaceService.Setup(service => service.SearchUsersByImageAsync(It.IsAny<byte[]>()))", "                .ReturnsAsync(userMatches);", "", "            // Act", "            await _faceEnricher.EnrichAsync(photo, sourceData);", "", "            // Assert", "            photo.Faces.Should().HaveCount(1);", "            photo.Faces[0].IdentityStatus.Should().Be(IdentityStatus.Identified);", "            photo.Faces[0].IdentifiedWithConfidence.Should().Be(0.9f);", "            photo.Faces[0].Person.Should().Be(_persons[0]);", "        }", "    }", "}", ""], "file_path": "PhotoBank.UnitTests/Enrichers/FaceEnricherTests.cs"}
{"Link_to_commit": "https://github.com/Kbouti/Python_Sandbox/commit/71c71a067c92550b15422192c25473aef81abcab", "n-gram matched": "copilot to write", "n_lines_longer_change": 44, "n_files_impacted": 1, "longest_chunk": ["def print_board(board):", "    for row in board:", "        print(\" | \".join(row))", "        print(\"-\" * 9)", "", "def check_winner(board, player):", "    # Check rows", "    for row in board:", "        if all([cell == player for cell in row]):", "            return True", "    # Check columns", "    for col in range(3):", "        if all([board[row][col] == player for row in range(3)]):", "            return True", "    # Check diagonals", "    if all([board[i][i] == player for i in range(3)]) or all([board[i][2 - i] == player for i in range(3)]):", "        return True", "    return False", "", "def tic_tac_toe():", "    board = [[\" \" for _ in range(3)] for _ in range(3)]", "    current_player = \"X\"", "    for _ in range(9):", "        print_board(board)", "        try:", "            row = int(input(f\"Player {current_player}, enter the row (0, 1, 2): \"))", "            col = int(input(f\"Player {current_player}, enter the column (0, 1, 2): \"))", "            if row not in [0, 1, 2] or col not in [0, 1, 2]:", "                raise ValueError", "            if board[row][col] == \" \":", "                board[row][col] = current_player", "                if check_winner(board, current_player):", "                    print_board(board)", "                    print(f\"Player {current_player} wins!\")", "                    return", "                current_player = \"O\" if current_player == \"X\" else \"X\"", "            else:", "                print(\"Cell already taken, try again.\")", "        except ValueError:", "            print(\"Invalid input, please enter numbers 0, 1, or 2.\")", "    print_board(board)", "    print(\"It's a tie!\")", "", "tic_tac_toe()"], "file_path": "ticTacToe.py"}
{"Link_to_commit": "https://github.com/F3Nation-Community/weaselbot/commit/156e2f226f17abdbbc1025c38384da297a7c92ff", "n-gram matched": "copilot to write", "n_lines_longer_change": 12, "n_files_impacted": 1, "longest_chunk": ["def create_table(name, columns, metadata, schema):", "    return Table(", "        name,", "        metadata,", "        *columns,", "        mysql_engine=MYSQL_ENGINE,", "        mysql_charset=MYSQL_CHARSET,", "        mysql_collate=MYSQL_COLLATE,", "        schema=schema,", "    )", "", "achievements_list_columns = ["], "file_path": "achievement_tables.py"}
{"Link_to_commit": "https://github.com/EdiWang/Edi.AspNetCore.Jwt/commit/3a02fb5de345336281810488b29e4f6b28afd3eb", "n-gram matched": "copilot to write", "n_lines_longer_change": 23, "n_files_impacted": 1, "longest_chunk": ["        EnsureRefreshTokenTableCreated();", "", "        var tokens = new List<KeyValuePair<string, RefreshToken>>();", "", "        await using var command = _connection.CreateCommand();", "        command.CommandText = \"SELECT Id, UserIdentifier, TokenString, ExpireAt FROM RefreshTokens WHERE ExpireAt < @ExpireAt\";", "        command.AddParameter(\"@ExpireAt\", DbType.DateTime, time);", "        await command.ExecuteNonQueryAsync();", "", "        await using var reader = await command.ExecuteReaderAsync();", "        while (await reader.ReadAsync())", "        {", "            tokens.Add(new KeyValuePair<string, RefreshToken>(", "                reader.GetString(0),", "                new RefreshToken", "                {", "                    UserIdentifier = reader.GetString(1),", "                    TokenString = reader.GetString(2),", "                    ExpireAt = reader.GetDateTime(3)", "                }));", "        }", "", "        return tokens;"], "file_path": "src/Edi.AspNetCore.Jwt.SqlServer/SqlServerRefreshTokenStore.cs"}
{"Link_to_commit": "https://github.com/insomniac807/DetectDuplicateStringInput/commit/018efda16620eb1c548c1b3f48d62f6a5b0049ca", "n-gram matched": "copilot to write", "n_lines_longer_change": 7, "n_files_impacted": 1, "longest_chunk": ["}", "", "int main()", "{", "\tdetectDuplicateWords();", "}", ""], "file_path": "DetectDuplicateStringInput/ChkStringDuplicates.cpp"}
{"Link_to_commit": "https://github.com/splunk/stef/commit/be6e476c9a8638a09ee9f4950a50b28a992c54ef", "n-gram matched": "copilot to write", "n_lines_longer_change": 84, "n_files_impacted": 15, "longest_chunk": ["// Copyright The OpenTelemetry Authors", "// SPDX-License-Identifier: Apache-2.0", "", "package stefreceiver", "", "import (", "\t\"context\"", "\t\"testing\"", "", "\t\"github.com/stretchr/testify/assert\"", "\t\"github.com/stretchr/testify/require\"", "\t\"go.opentelemetry.io/collector/component/componenttest\"", "\t\"go.opentelemetry.io/collector/config/configgrpc\"", "\t\"go.opentelemetry.io/collector/config/confignet\"", "\t\"go.opentelemetry.io/collector/consumer\"", "\t\"go.opentelemetry.io/collector/consumer/consumertest\"", "\t\"go.opentelemetry.io/collector/receiver/receivertest\"", ")", "", "func TestCreateDefaultConfig(t *testing.T) {", "\tfactory := NewFactory()", "\tcfg := factory.CreateDefaultConfig()", "\tassert.NotNil(t, cfg, \"failed to create default config\")", "\tassert.NoError(t, componenttest.CheckConfigStruct(cfg))", "}", "", "func TestCreateMetric(t *testing.T) {", "\tfactory := NewFactory()", "\tdefaultGRPCSettings := &configgrpc.ServerConfig{", "\t\tNetAddr: confignet.AddrConfig{", "\t\t\tEndpoint:  \"127.0.0.1:0\",", "\t\t\tTransport: confignet.TransportTypeTCP,", "\t\t},", "\t}", "", "\ttests := []struct {", "\t\tname         string", "\t\tcfg          *Config", "\t\twantStartErr bool", "\t\twantErr      bool", "\t\tsink         consumer.Metrics", "\t}{", "\t\t{", "\t\t\tname: \"default\",", "\t\t\tcfg: &Config{", "\t\t\t\tServerConfig: *defaultGRPCSettings,", "\t\t\t},", "\t\t\tsink: consumertest.NewNop(),", "\t\t},", "\t\t{", "\t\t\tname: \"invalid_grpc_address\",", "\t\t\tcfg: &Config{", "\t\t\t\tServerConfig: configgrpc.ServerConfig{", "\t\t\t\t\tNetAddr: confignet.AddrConfig{", "\t\t\t\t\t\tEndpoint:  \"327.0.0.1:1122\",", "\t\t\t\t\t\tTransport: confignet.TransportTypeTCP,", "\t\t\t\t\t},", "\t\t\t\t},", "\t\t\t},", "\t\t\twantStartErr: true,", "\t\t\tsink:         consumertest.NewNop(),", "\t\t},", "\t}", "\tctx := context.Background()", "\tcreationSet := receivertest.NewNopSettings()", "\tfor _, tt := range tests {", "\t\tt.Run(", "\t\t\ttt.name, func(t *testing.T) {", "\t\t\t\tmr, err := factory.CreateMetrics(ctx, creationSet, tt.cfg, tt.sink)", "\t\t\t\tif tt.wantErr {", "\t\t\t\t\tassert.Error(t, err)", "\t\t\t\t\treturn", "\t\t\t\t}", "\t\t\t\trequire.NoError(t, err)", "\t\t\t\tif tt.wantStartErr {", "\t\t\t\t\tassert.Error(t, mr.Start(context.Background(), componenttest.NewNopHost()))", "\t\t\t\t} else {", "\t\t\t\t\trequire.NoError(t, mr.Start(context.Background(), componenttest.NewNopHost()))", "\t\t\t\t\tassert.NoError(t, mr.Shutdown(context.Background()))", "\t\t\t\t}", "\t\t\t},", "\t\t)", "\t}", "}"], "file_path": "otelcol/internal/stefreceiver/stef.go"}
{"Link_to_commit": "https://github.com/AzzAnAq/Chemistry-Quiz/commit/59af40d9550e814b480c7bdc7e1ee7ad5948d4c0", "n-gram matched": "copilot to write", "n_lines_longer_change": 150, "n_files_impacted": 3, "longest_chunk": ["import random", "", "names = [", "    'hydrogen', 'helium', 'lithium', 'beryllium', 'boron', 'carbon', 'nitrogen', 'oxygen', 'fluorine', 'neon', 'sodium', 'magnesium', 'aluminium', 'silicon', ", "    'phosphorus', 'sulfur', 'chlorine', 'argon', 'potassium', 'calcium', 'scandium', 'titanium', 'vanadium', 'chromium', 'manganese', 'iron', 'cobalt', 'nickel', ", "    'copper', 'zinc', 'gallium', 'germanium', 'arsenic', 'selenium', 'bromine', 'krypton', 'rubidium', 'strontium', 'yttrium', 'zirconium', 'niobium', ", "    'molybdenum', 'technetium', 'ruthenium', 'rhodium', 'palladium', 'silver', 'cadmium', 'indium', 'tin', 'antimony', 'tellurium', 'iodine', 'xenon', ", "    'cesium', 'barium', 'lanthanum', 'cerium', 'praseodymium', 'neodymium', 'promethium', 'samarium', 'europium', 'gadolinium', 'terbium', 'dysprosium', ", "    'holmium', 'erbium', 'thulium', 'ytterbium', 'lutetium', 'hafnium', 'tantalum', 'tungsten', 'rhenium', 'osmium', 'iridium', 'platinum', 'gold', 'mercury', ", "    'thallium', 'lead', 'bismuth', 'polonium', 'astatine', 'radon', 'francium', 'radium', 'actinium', 'thorium', 'protactinium', 'uranium', 'neptunium', ", "    'plutonium', 'americium', 'curium', 'berkelium', 'californium', 'einsteinium', 'fermium', 'mendelevium', 'nobelium', 'lawrencium', 'rutherfordium', ", "    'dubnium', 'seaborgium', 'bohrium', 'hassium', 'meitnerium', 'darmstadtium', 'roentgenium', 'copernicium', 'nihonium', 'flerovium', 'moscovium', ", "    'livermorium', 'tennessine', 'oganesson'", "]", "symbols = [", "    'h', 'he', 'li', 'be', 'b', 'c', 'n', 'o', 'f', 'ne', 'na', 'mg', 'al', 'si','p', 's', 'cl', 'ar', 'k', 'ca', 'sc', 'ti', 'v', 'cr', 'mn', 'fe', 'co', 'ni', ", "    'cu', 'zn', 'ga', 'ge', 'as', 'se', 'br', 'kr', 'rb', 'sr', 'y', 'zr', 'nb','mo', 'tc', 'ru', 'rh', 'pd', 'ag', 'cd', 'in', 'sn', 'sb', 'te', 'i', 'xe', ", "    'cs', 'ba', 'la', 'ce', 'pr', 'nd', 'pm', 'sm', 'eu', 'gd', 'tb', 'dy', 'ho','er', 'tm', 'yb', 'lu', 'hf', 'ta', 'w', 're', 'os', 'ir', 'pt', 'au', 'hg', ", "    'tl', 'pb', 'bi', 'po', 'at', 'rn', 'fr', 'ra', 'ac', 'th', 'pa', 'u', 'np','pu', 'am', 'cm', 'bk', 'cf', 'es', 'fm', 'md', 'no', 'lr', 'rf', 'db', 'sg', ", "    'bh', 'hs', 'mt', 'ds', 'rg', 'cn', 'nh', 'fl', 'mc', 'lv', 'ts', 'og'", "]", "ato_num = [", "    1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, ", "    43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, ", "    83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, ", "    118", "]", "groups = [", "    1, 18, 1, 2, 13, 14, 15, 16, 17, 18, 1, 2, 13, 14, 15, 16, 17, 18, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1, 2, 3, 4, 5, 6, ", "    7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, ", "    1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18", "]", "periods = [", "    1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, ", "    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, ", "    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7", "]", "try:", "    print('This is a chem quiz')", "    print('Input range of the quiz, in atomic number (e.g helium-boron is min=2, max=5)')", "    min = int(input('Minimum atomic number is: '))", "    max = int(input('Maximum atomic number is: '))", "    num_questions = int(input('How long should the quiz be: '))", "    print('Choose your quiz question (symbol, atomic number, name, group, period, random):')", "    print('If chosen symbol, then name is shown and the symbol is the question')", "    print('If chosen atomic number, then name is shown and the atomic numbers is the question')", "    print('If chosen group, then name is shown and the group is the question')", "    print('If chosen period, then name is shown and the period is the question')", "    print('If chosen name, then symbol is shown and the name is the question')", "    print('If chosen random, each question will have the aforemensioned parts randomised')", "    question = str(input('What is your choice:'))", "    i=0", "    for i in range(0,num_questions):", "        ranint = random.randint(min,max)", "        ranint = ranint-1 #for list as list begins at zero", "        if question.lower() == 'name':", "            print(symbols[ranint])", "            answer1 = str(input('The name is? '))", "            if answer1.lower() == names[ranint]:", "                print('Correct!')", "            else:", "                print('Incorrect')", "                print(names[ranint])", "        elif question.lower() == 'symbol':", "            print(names[ranint])", "            answer1 = str(input('The symbols is? '))", "            if answer1.lower() == symbols[ranint]:", "                print('Correct!')", "            else:", "                print('Incorrect')", "                print(symbols[ranint]) ", "        elif question.lower() == 'atomic number':", "            print(names[ranint])", "            answer1 = int(input('The atomic number is? '))", "            if answer1 == ato_num[ranint]:", "                print('Correct!')", "            else:", "                print('Incorrect')", "                print(ato_num[ranint]) ", "        elif question.lower() == 'group':", "            print(names[ranint])", "            answer1 = int(input('The group is? '))", "            if answer1 == groups[ranint]:", "                print('Correct!')", "            else:", "                print('Incorrect')", "                print(groups[ranint]) ", "        elif question.lower() == 'period':", "            print(names[ranint])", "            answer1 = int(input('The period is? '))", "            if answer1 == periods[ranint]:", "                print('Correct!')", "            else:", "                print('Incorrect')", "                print(periods[ranint])", "        elif question.lower() == 'random':", "            ranrand = random.randint(1,5) #1 = names, 2=symbols,3=atomic num, 4=groups, 5=periods,", "            if ranrand > 1 or ranrand <= 5:", "                print(names[ranint])", "                if ranrand == 2:", "                    answer1 = str(input('The symbols is? '))", "                    if answer1.lower() == symbols[ranint]:", "                        print('Correct!')", "                    else:", "                        print('Incorrect')", "                        print(symbols[ranint]) ", "                elif ranrand == 3:", "                    answer1 = int(input('The atomic number is? '))", "                    if answer1 == ato_num[ranint]:", "                        print('Correct!')", "                    else:", "                        print('Incorrect')", "                        print(ato_num[ranint]) ", "                elif ranrand == 4:", "                    answer1 = str(input('The group is? '))", "                    if answer1 == groups[ranint]:", "                        print('Correct!')", "                    else:", "                        print('Incorrect')", "                        print(groups[ranint])", "                elif ranrand == 5:", "                    answer1 = int(input('The period is? '))", "                    if answer1 == periods[ranint]:", "                        print('Correct!')", "                    else:", "                        print('Incorrect')", "                        print(periods[ranint])", "            elif ranrand == 1:", "                print(symbols[ranint])", "                answer1 = int(input('The name is? '))", "                if answer1.lower() == names[ranint]:", "                    print('Correct!')", "                else:", "                    print('Incorrect')", "                    print(names[ranint])", "", "        else:", "            print('Incorrect input') ", "            break", "", "except ValueError:", "    print('There has been a value error')", "", "", "", "", "", "", "", ""], "file_path": "functional chem quiz code.py"}
{"Link_to_commit": "https://github.com/cleardusk/test-copilot/commit/ad9351e099b24cdbffe353a9dd78aba9dd564be1", "n-gram matched": "copilot to write", "n_lines_longer_change": 203, "n_files_impacted": 4, "longest_chunk": ["import pygame", "import numpy as np", "import random", "", "# Define the board size", "board_size = 9", "# Define the global range of numbers to try for each empty cell", "num_range = list(range(1, 10))", "# Shuffle the list of numbers to try", "random.shuffle(num_range)", "", "", "", "# Define the function to print the board", "def print_board(board):", "    for i in range(board_size):", "        for j in range(board_size):", "            print(board[i][j], end=\" \")", "        print()", "", "# Define the function to check if a move is valid", "def is_valid_move(board, row, col, num):", "    # Check row", "    for i in range(board_size):", "        if board[row][i] == num:", "            return False", "", "    # Check column", "    for i in range(board_size):", "        if board[i][col] == num:", "            return False", "", "    # Check 3x3 box", "    box_row = (row // 3) * 3", "    box_col = (col // 3) * 3", "    for i in range(box_row, box_row + 3):", "        for j in range(box_col, box_col + 3):", "            if board[i][j] == num:", "                return False", "", "    # Move is valid", "    return True", "", "# Define the function to solve the board", "def solve_board(board):", "    # Find the next empty cell", "    for i in range(board_size):", "        for j in range(board_size):", "            if board[i][j] == 0:", "                # Try each number from 1 to 9", "                for num in num_range:", "                    if is_valid_move(board, i, j, num):", "                        # Make the move", "                        board[i][j] = num", "", "                        # Recursively solve the rest of the board", "                        if solve_board(board):", "                            return True", "", "                        # Undo the move", "                        board[i][j] = 0", "", "                # No valid move found", "                return False", "", "    # Board is solved", "    return True", "", "# Define the function to generate a random Sudoku puzzle", "def generate_puzzle():", "    # Start with a solved board", "    board = np.zeros((board_size, board_size), dtype=int)", "    solve_board(board)", "", "    # Remove some numbers to create the puzzle", "    num_removed = 0", "    while num_removed < 50:", "        row = random.randint(0, board_size - 1)", "        col = random.randint(0, board_size - 1)", "        if board[row][col] != 0:", "            temp = board[row][col]", "            board[row][col] = 0", "            temp_board = np.copy(board)", "            if solve_board(temp_board) and np.count_nonzero(temp_board) == board_size * board_size:", "                num_removed += 1", "            else:", "                board[row][col] = temp", "", "    # Return the puzzle", "    return board", "", "# Initialize Pygame", "pygame.init()", "", "# Set up the display", "screen_width = 540", "screen_height = 600", "screen = pygame.display.set_mode((screen_width, screen_height))", "pygame.display.set_caption(\"Sudoku\")", "", "# Set up the font", "font = pygame.font.SysFont(None, 40)", "", "# Set up the colors", "white = (255, 255, 255)", "black = (0, 0, 0)", "gray = (128, 128, 128)", "light_gray = (192, 192, 192)", "red = (255, 0, 0)", "", "# Set up the game variables", "selected_row = -1", "selected_col = -1", "game_over = False", "solving = False", "new_game = False", "board = generate_puzzle()", "board_copy = board.copy()", "", "# Set up the game loop", "while not game_over:", "    # Handle events", "    for event in pygame.event.get():", "        if event.type == pygame.QUIT:", "            game_over = True", "        elif event.type == pygame.MOUSEBUTTONDOWN:", "            # Get the mouse position", "            pos = pygame.mouse.get_pos()", "", "            # Check if the mouse is on the board", "            if pos[0] >= 20 and pos[0] < 500 and pos[1] >= 20 and pos[1] < 500:", "                # Get the row and column of the clicked cell", "                selected_row = (pos[1] - 20) // 50", "                selected_col = (pos[0] - 20) // 50", "            else:", "                selected_row = -1", "                selected_col = -1", "", "            # Check if the mouse is on the solve button", "            if pos[0] >= 20 and pos[0] < 140 and pos[1] >= 540 and pos[1] < 580:", "                solve_board(board)", "                solving = True", "", "            # Check if the mouse is on the new game button", "            if pos[0] >= 160 and pos[0] < 340 and pos[1] >= 540 and pos[1] < 580:", "                random.shuffle(num_range)", "                board = generate_puzzle()", "                board_copy = board.copy()", "                solving = False", "", "            # Check if the mouse is on the quit button", "            if pos[0] >= 380 and pos[0] < 500 and pos[1] >= 540 and pos[1] < 580:", "                game_over = True", "", "    # Clear the screen", "    screen.fill(white)", "", "    # Draw the board", "    for i in range(board_size):", "        for j in range(board_size):", "            # Draw the cell", "            cell_rect = pygame.Rect(j * 50 + 20, i * 50 + 20, 50, 50)", "            pygame.draw.rect(screen, black, cell_rect, 1)", "", "            # Draw the number", "            if board[i][j] != 0:", "                if solving and board[i][j] != board_copy[i][j]:  # Check if solving and not an original number", "                    number_color = (0, 128, 0)  # Set solved numbers to green", "                else:", "                    number_color = black", "                number_text = font.render(str(board[i][j]), True, number_color)", "                number_rect = number_text.get_rect(center=cell_rect.center)", "                screen.blit(number_text, number_rect)", "            # Draw the selection", "            if i == selected_row and j == selected_col:", "                pygame.draw.rect(screen, red, cell_rect, 3)", "", "    # Draw the solve button", "    solve_button_rect = pygame.Rect(20, 540, 120, 40)", "    pygame.draw.rect(screen, gray, solve_button_rect)", "    solve_button_text = font.render(\"Solve\", True, black)", "    solve_button_text_rect = solve_button_text.get_rect(center=solve_button_rect.center)", "    screen.blit(solve_button_text, solve_button_text_rect)", "", "    # Draw the new game button", "    new_game_button_rect = pygame.Rect(160, 540, 180, 40)  # Make the button wider", "    pygame.draw.rect(screen, gray, new_game_button_rect)", "    new_game_button_text = font.render(\"New Game\", True, black)", "    new_game_button_text_rect = new_game_button_text.get_rect(center=new_game_button_rect.center)", "    screen.blit(new_game_button_text, new_game_button_text_rect)", "", "    # Draw the quit button", "    quit_button_rect = pygame.Rect(380, 540, 120, 40)", "    pygame.draw.rect(screen, gray, quit_button_rect)", "    quit_button_text = font.render(\"Quit\", True, black)", "    quit_button_text_rect = quit_button_text.get_rect(center=quit_button_rect.center)", "    screen.blit(quit_button_text, quit_button_text_rect)", "", "    # Update the display", "    pygame.display.update()", "", "# Quit Pygame", "pygame.quit()"], "file_path": "suduko.py"}
{"Link_to_commit": "https://github.com/EdiWang/Edi.CacheAside.InMemory/commit/761ca8be0033364632270f27eac6bccabe0fddd0", "n-gram matched": "copilot to write", "n_lines_longer_change": 170, "n_files_impacted": 4, "longest_chunk": ["using Microsoft.Extensions.Caching.Memory;", "using Moq;", "", "namespace Edi.CacheAside.InMemory.Tests", "{", "    [TestFixture]", "    public class MemoryCacheAsideTests", "    {", "        [Test]", "        public void GetOrCreate_NullKey_ReturnsDefault()", "        {", "            // Arrange", "            var mockCache = new Mock<IMemoryCache>();", "            var cacheAside = new MemoryCacheAside(mockCache.Object);", "", "            // Act", "            var result = cacheAside.GetOrCreate<string>(\"testPartition\", null, (_) => \"testValue\");", "", "            // Assert", "            Assert.IsNull(result);", "        }", "", "        [Test]", "        public async Task GetOrCreateAsync_NullKey_ReturnsDefault()", "        {", "            // Arrange", "            var mockCache = new Mock<IMemoryCache>();", "            var cacheAside = new MemoryCacheAside(mockCache.Object);", "", "            // Act", "            var result = await cacheAside.GetOrCreateAsync<string>(\"testPartition\", null, (_) => Task.FromResult(\"testValue\"));", "", "            // Assert", "            Assert.IsNull(result);", "        }", "", "        //[Test]", "        //public void GetOrCreate_PartitionAndKey_AddsToCachePartition()", "        //{", "        //    // Arrange", "        //    var mockCacheEntry = new Mock<ICacheEntry>();", "        //    var mockCache = new Mock<IMemoryCache>();", "        //    mockCache.Setup(m => m.GetOrCreate(", "        //            It.IsAny<object>(),", "        //            It.IsAny<Func<ICacheEntry, string>>()))", "        //        .Returns(\"testValue\")", "        //        .Callback<object, Func<ICacheEntry, string>>((_, factory) => factory(mockCacheEntry.Object));", "        //    var cacheAside = new MemoryCacheAside(mockCache.Object);", "", "        //    // Act", "        //    var result = cacheAside.GetOrCreate<string>(\"testPartition\", \"testKey\", (_) => \"testValue\");", "", "        //    // Assert", "        //    Assert.AreEqual(\"testValue\", result);", "        //    Assert.IsTrue(cacheAside.CachePartitions.ContainsKey(\"testPartition\"));", "        //    Assert.IsTrue(cacheAside.CachePartitions[\"testPartition\"].Contains(\"testKey\"));", "        //}", "", "        //[Test]", "        //public async Task GetOrCreateAsync_PartitionAndKey_AddsToCachePartition()", "        //{", "        //    // Arrange", "        //    var mockCacheEntry = new Mock<ICacheEntry>();", "        //    var mockCache = new Mock<IMemoryCache>();", "        //    mockCache.Setup(m => m.GetOrCreateAsync(", "        //            It.IsAny<object>(),", "        //            It.IsAny<Func<ICacheEntry, Task<string>>>()))", "        //        .ReturnsAsync(\"testValue\")", "        //        .Callback<object, Func<ICacheEntry, Task<string>>>((_, factory) => factory(mockCacheEntry.Object));", "        //    var cacheAside = new MemoryCacheAside(mockCache.Object);", "", "        //    // Act", "        //    var result = await cacheAside.GetOrCreateAsync<string>(\"testPartition\", \"testKey\", (_) => Task.FromResult(\"testValue\"));", "", "        //    // Assert", "        //    Assert.AreEqual(\"testValue\", result);", "        //    Assert.IsTrue(cacheAside.CachePartitions.ContainsKey(\"testPartition\"));", "        //    Assert.IsTrue(cacheAside.CachePartitions[\"testPartition\"].Contains(\"testKey\"));", "        //}", "", "        [Test]", "        public void Clear_CachesExist_RemovesAllFromCache()", "        {", "            // Arrange", "            var mockCache = new Mock<IMemoryCache>();", "            var cacheAside = new MemoryCacheAside(mockCache.Object);", "            cacheAside.CachePartitions[\"testPartition1\"] = new[] { \"testKey1\", \"testKey2\" }.ToList();", "            cacheAside.CachePartitions[\"testPartition2\"] = new[] { \"testKey3\" }.ToList();", "", "            mockCache.Setup(m => m.Remove(It.IsAny<object>()))", "                .Callback<object>((_) => { });", "", "            mockCache.Setup(m => m.Remove(It.Is<object>(key => key.ToString() == \"testPartition1-testKey1\")))", "                .Callback<object>((_) => { });", "", "            mockCache.Setup(m => m.Remove(It.Is<object>(key => key.ToString() == \"testPartition1-testKey2\")))", "                .Callback<object>((_) => { });", "", "            mockCache.Setup(m => m.Remove(It.Is<object>(key => key.ToString() == \"testPartition2-testKey3\")))", "                .Callback<object>((_) => { });", "", "            // Act            ", "            cacheAside.Clear();", "", "            // Assert", "            mockCache.Verify(m => m.Remove(It.Is<object>(key => key.ToString() == \"testPartition1-testKey1\")), Times.Once);", "            mockCache.Verify(m => m.Remove(It.Is<object>(key => key.ToString() == \"testPartition1-testKey2\")), Times.Once);", "            mockCache.Verify(m => m.Remove(It.Is<object>(key => key.ToString() == \"testPartition2-testKey3\")), Times.Once);", "            mockCache.Verify(m => m.Remove(It.IsAny<object>()), Times.Exactly(3));", "        }", "", "        [Test]", "        public void Remove_PartitionNotExist_NothingHappens()", "        {", "            // Arrange", "            var mockCache = new Mock<IMemoryCache>();", "            var cacheAside = new MemoryCacheAside(mockCache.Object);", "", "            // Act            ", "            Assert.DoesNotThrow(() => cacheAside.Remove(\"testPartitionNotExist\"));", "        }", "", "        [Test]", "        public void Remove_CacheKeysExist_RemovesAllFromCache()", "        {", "            // Arrange", "            var mockCache = new Mock<IMemoryCache>();", "            var cacheAside = new MemoryCacheAside(mockCache.Object);", "            cacheAside.CachePartitions[\"testPartition1\"] = new[] { \"testKey1\", \"testKey2\" }.ToList();", "", "            mockCache.Setup(m => m.Remove(\"testPartition1-testKey1\"));", "            mockCache.Setup(m => m.Remove(\"testPartition1-testKey2\"));", "", "            // Act            ", "            cacheAside.Remove(\"testPartition1\");", "", "            // Assert", "            mockCache.Verify(m => m.Remove(\"testPartition1-testKey1\"), Times.Once);", "            mockCache.Verify(m => m.Remove(\"testPartition1-testKey2\"), Times.Once);", "        }", "", "        [Test]", "        public void Remove_KeyNotExist_NothingHappens()", "        {", "            // Arrange", "            var mockCache = new Mock<IMemoryCache>();", "            var cacheAside = new MemoryCacheAside(mockCache.Object);", "", "            // Act            ", "            Assert.DoesNotThrow(() => cacheAside.Remove(\"testPartition\", \"testKeyNotExist\"));", "        }", "", "        [Test]", "        public void Remove_KeyExist_RemovesFromCache()", "        {", "            // Arrange", "            var mockCache = new Mock<IMemoryCache>();", "            var cacheAside = new MemoryCacheAside(mockCache.Object);", "            cacheAside.CachePartitions[\"testPartition\"] = new[] { \"testKey\", \"testKey2\" }.ToList();", "", "            mockCache.Setup(m => m.Remove(\"testPartition-testKey\"));", "", "            // Act            ", "            cacheAside.Remove(\"testPartition\", \"testKey\");", "", "            // Assert", "            mockCache.Verify(m => m.Remove(\"testPartition-testKey\"), Times.Once);", "        }", "    }", "}"], "file_path": "src/Edi.CacheAside.InMemory.Tests/MemoryCacheAsideTests.cs"}
{"Link_to_commit": "https://github.com/RomanRudin/bfu_ads_labs/commit/f4a8698378daa5f42234b89380f08e76f9cc3080", "n-gram matched": "copilot to write", "n_lines_longer_change": 228, "n_files_impacted": 31, "longest_chunk": ["from typing import Any, LiteralString", "", "", "class Node:", "    def __init__(self, value):", "        self.value = value", "        self.left = None", "        self.right = None", "        self.children = [self.left, self.right]", "", "", "", "", "class BST:", "    def __init__(self):", "        self.root = None", "", "", "    def __insert(self, current, value) -> None:", "        if value < current.value:", "            if not current.left:", "                current.left = Node(value)", "            else:", "                self.__insert(current.left, value)", "        elif value > current.value:", "            if not current.right:", "                current.right = Node(value)", "            else:", "                self.__insert(current.right, value)", "        else:", "            raise Exception(f\"{value} already exist!!\")", "", "    def insert(self, value) -> None:", "        if not self.root:", "            self.root = Node(value)", "        else:", "            self.__insert(self.root, value)", "", "", "    def __search(self, value, parent, current) -> tuple | tuple[None, None] | Any | None:", "        if value == current.value:", "            return (parent, current)", "        elif value < current.value:", "            if not current.left:", "                return (None, None)", "            return self.__search(value, current, current.left)", "        elif value > current.value:", "            if not current.right:", "                return (None, None)", "            return self.__search(value, current, current.right)", "        ", "", "    def search(self, value) -> tuple[None, None] | tuple | Any | None:", "        if not self.root:", "            return (None, None)", "        return self.__search(value, parent=self.root, current=self.root)", "", "    def __minValueNode(self, node) -> Any:", "        current = node", "        while current.left:", "            current = current.left", "        return current", "    ", "        ", "    def __remove(self, root, value) -> Any | None:", "        if not root:", "            return None", "", "        if value < root.value:", "            root.left = self.__remove(root.left, value)", "        elif value > root.value:", "            root.right = self.__remove(root.right, value)", "        else:", "            if not root.left:", "                temp = root.right", "                root = None", "                return temp", "", "            elif not root.right:", "                temp = root.left", "                root = None", "                return temp", "            ", "            successor = self.__minValueNode(root.right)", "            root.value = successor.value", "            root.right = self.__remove(root.right, successor.value)", "        return root", "", "    def remove(self, value) -> None:", "        was_removed = self.__remove(self.root, value)", "", "", "    def __traversePostOrder(self, current) -> None:", "        print(current.value, end=' -- ')", "        if current.left:", "            self.__traversePostOrder(current.left)", "        if current.right:", "            self.__traversePostOrder(current.right)", "", "    def traversePostOrder(self) -> None:", "        self.__traversePostOrder(self.root)", "", "    ", "    def __traverseInOrder(self, current) -> None:", "        if current.left:", "            self.__traverseInOrder(current.left)", "        print(current.value, end=' -- ')", "        if current.right:", "            self.__traverseInOrder(current.right)", "", "    def traverseInOrder(self) -> None:", "        self.__traverseInOrder(self.root)", "", "", "    def __traversePreOrder(self, current) -> None:", "        if current.left:", "            self.__traversePreOrder(current.left)", "        if current.right:", "            self.__traversePreOrder(current.right)", "        print(current.value, end=' -- ')", "", "", "    def traversePreOrder(self) -> None:", "        self.__traversePreOrder(self.root) ", "", "        ", "    def traverseNonRecursive(self) -> None:", "        from queue import LifoQueue", "        stack = LifoQueue()", "        stack.put(self.root)", "        while not stack.empty():", "            current = stack.get()", "            while current:", "                print(current.value, end=' -- ')", "                if current.left:", "                    stack.put(current.left)", "                current = current.right", "", "", "    def draw(self) -> None:", "        self.root.display()", "", "", "    def inputBracketNotation(self, string: str) -> None:", "        def find_right_subtree(string: str, start: int, end: int):", "            bracket_counter = -1", "            while True:", "                if (start >= end): return -1", "                if ((string[start] == ',') and (bracket_counter == 0)): return start + 1", "                if string[start] == '(': bracket_counter += 1", "                if string[start] == ')': bracket_counter -= 1", "                start += 1", "", "        def create_subtree(string: str, start: int, end: int) -> Node:", "            while string[start] == ' ' or string[start] == '(': start += 1", "            if (start >= end): return", "", "            number = ''", "            while string[start] in '1234567890':", "                number += string[start]", "                start += 1", "                if start >= end: return Node(int(number))", "            node = Node(int(number))", "", "            right_subtree_index = find_right_subtree(string, start, end) - 1", "", "            if right_subtree_index == -1:", "                raise Exception(\"Wrong bracket notation string!\")", "", "            if right_subtree_index :", "                node.left = create_subtree(string, start+1, right_subtree_index)", "                node.right = create_subtree(string, right_subtree_index+1, end - 1)", "            return node", "", "        self.root = create_subtree(string, 0, len(string))", "", "", "", "", "if __name__ == \"__main__\":", "    bst = BST()", "    while True:", "        print(''' Enter the number of command you want to execute:", "    1) insert the vertexes by values", "    2) search the vertexes by values", "    3) remove the vertexes by values", "    4) traverse Pre-Order", "    5) traverse In-Order", "    6) traverse Post-Order", "    7) traverse Non-Recursive", "    8) Enter tree through bracket notation (changes the root element and doesn't assert, that the string is going to be BST)", "    0) Exit''')", "        command = input()", "        match command:", "            case '1':", "                values = list(map(int, input('Please, enter the values you want to inswrt: ').strip().replace(',', ' ').split()))", "                for value in values:", "                    bst.insert(value)", "            case '2':", "                values = list(map(int, input('Please, enter the values of the vertex you are searching: ').strip().replace(',', ' ').split()))", "                for value in values:", "                    parent, current = bst.search(value); ", "                    print(f'Parent value is {parent.value}, searched value is {current.value}')", "            case '3':", "                values = list(map(int, input('Please, enter the value of the vertex you are searching: ').strip().replace(',', ' ').split()))", "                for value in values:", "                    bst.remove(value)", "            case '4':", "                bst.traversePreOrder()", "                print()", "            case '5':", "                bst.traverseInOrder()", "                print()", "            case '6':", "                bst.traversePostOrder()", "                print()", "            case '7':", "                bst.traverseNonRecursive()", "                print()", "            case '8':", "                bst.inputBracketNotation(input('Please, enter the bracket notation string: '))", "            case '0':", "                break", "            case _:", "                continue", "    bst.traverseInOrder()", "    ", "        "], "file_path": "Semester_1/Labs/Lab17/lab_17.py"}
{"Link_to_commit": "https://github.com/pramnora/python/commit/5092c0a8550a6dbc9b90c1da0164b10f1478d8d5", "n-gram matched": "copilot to write", "n_lines_longer_change": 29, "n_files_impacted": 1, "longest_chunk": ["def get_number(uppercase_letter):", "    # Define the mapping of letters to digits", "    char_numbers = [", "        ('ABC', 2), ('DEF', 3), ('GHI', 4), ('JKL', 5),", "        ('MNO', 6), ('PQRS', 7), ('TUV', 8), ('WXYZ', 9)", "    ]", "    ", "    # Convert uppercase letter to its corresponding digit", "    for chars, digit in char_numbers:", "        if uppercase_letter in chars:", "            return digit", "    ", "    # If the letter is not in the mapping, return the original letter", "    return uppercase_letter", "", "def translate_number():", "    phone_number = input(\"Enter a string: \")", "    translated_number = \"\"", "", "    for char in phone_number.upper():", "        if char.isalpha():", "            translated_number += str(get_number(char))", "        else:", "            translated_number += char", "", "    print(\"Translated number:\", translated_number)", "", "# Call the function to translate the user input", "translate_number()"], "file_path": "programs/encryption/ai/ai-01.py"}
{"Link_to_commit": "https://github.com/thecjharries/r-daily-programmer/commit/9935bf336abfbd67dbc1d0be043d34b19d7c6591", "n-gram matched": "copilot to write", "n_lines_longer_change": 12, "n_files_impacted": 1, "longest_chunk": ["func roll(dice string) int {", "\tmatches := dicePattern.FindStringSubmatch(dice)", "\tif len(matches) != 3 {", "\t\treturn 0", "\t}", "\tcount, _ := strconv.Atoi(matches[1])", "\tsides, _ := strconv.Atoi(matches[2])", "\tsum := 0", "\tfor i := 0; i < count; i++ {", "\t\tsum += 1 + rand.Intn(sides)", "\t}", "\treturn sum"], "file_path": "easy/364/go/main.go"}
{"Link_to_commit": "https://github.com/namuan/bin-utils/commit/820a6f08d1cf77494ee41c56091a593c8e3b39d6", "n-gram matched": "copilot to write", "n_lines_longer_change": 41, "n_files_impacted": 1, "longest_chunk": ["#!/usr/bin/env python", "", "import git", "import sqlite3", "import os", "import sys", "import datetime", "", "# check if the directory is a git repo", "if not os.path.isdir('.git'):", "    print('Not a git repo')", "    sys.exit(1)", "", "# connect to the database", "conn = sqlite3.connect('git_log.db')", "c = conn.cursor()", "", "# create the table", "c.execute('''CREATE TABLE IF NOT EXISTS commits", "                 (id INTEGER PRIMARY KEY,", "                 date TEXT,", "                 author TEXT,", "                 message TEXT)''')", "", "# get the log of commits", "repo = git.Repo('./')", "for commit in repo.iter_commits():", "    # get the date of the commit", "    date = datetime.datetime.fromtimestamp(commit.committed_date).strftime('%Y-%m-%d %H:%M:%S')", "    # get the author of the commit", "    author = commit.author.name", "    # get the message of the commit", "    message = commit.message", "    # insert the commit into the table", "    c.execute(\"INSERT INTO commits VALUES (NULL, ?, ?, ?)\", (date, author, message))", "", "# commit the changes", "conn.commit()", "", "# close the connection", "conn.close()"], "file_path": "git_log_to_sqlite.py"}
{"Link_to_commit": "https://github.com/ferDMS/Roborregos-2023/commit/44026df5ed8447b28297907ec8c761b429384254", "n-gram matched": "copilot to write", "n_lines_longer_change": 102, "n_files_impacted": 9, "longest_chunk": ["#include <iostream>", "#include \"../../general/Graph.hpp\"", "", "void testVertex() {", "    // Test default constructor", "    Vertex v1;", "    assert(v1.x == -1 && v1.y == -1 && v1.color == \"\");", "", "    // Test constructor with two arguments", "    Vertex v2(1, 2);", "    assert(v2.x == 1 && v2.y == 2 && v2.color == \"\");", "", "    // Test constructor with three arguments", "    Vertex v3(3, 4, \"red\");", "    assert(v3.x == 3 && v3.y == 4 && v3.color == \"red\");", "", "    // Test up() method", "    Vertex v4 = v2.up();", "    assert(v4.x == 1 && v4.y == 3 && v4.color == \"\");", "", "    // Test down() method", "    Vertex v5 = v2.down();", "    assert(v5.x == 1 && v5.y == 1 && v5.color == \"\");", "", "    // Test left() method", "    Vertex v6 = v2.left();", "    assert(v6.x == 0 && v6.y == 2 && v6.color == \"\");", "", "    // Test right() method", "    Vertex v7 = v2.right();", "    assert(v7.x == 2 && v7.y == 2 && v7.color == \"\");", "", "    // Test distance() method", "    unsigned int d1 = v2.distance(v3);", "    assert(d1 == 4);", "", "    unsigned int d2 = v3.distance(v2);", "    assert(d2 == 4);", "", "    // Test operator== method", "    assert(v2 == v2);", "    assert(!(v2 == v3));", "}", "", "void testGraph() {", "    // Test constructor with no arguments", "    Graph g1;", "    assert(g1.size() == 0);", "", "    // Test constructor with one argument", "    Graph g2(5);", "    assert(g2.size() == 5);", "", "    // Test constructor with vector argument (vector of vertices)", "    std::vector<Vertex> vertices;", "    vertices.push_back(Vertex(1, 2));", "    vertices.push_back(Vertex(3, 4));", "    Graph g3(vertices);", "    assert(g3.size() == 2);", "", "    // Test constructor with unordered_map argument (map of max_edges)", "    std::unordered_map<Vertex, int> max_edges;", "    max_edges[Vertex(1, 2)] = 3;", "    max_edges[Vertex(3, 4)] = 5;", "    Graph g4(max_edges);", "    assert(g4.size() == 2);", "", "    // Test constructor with unordered_map argument (map of edges)", "    std::unordered_map<Vertex, std::vector<Edge> > adj;", "    adj[Vertex(1, 2)] = std::vector<Edge>();", "    adj[Vertex(3, 4)] = std::vector<Edge>();", "    Graph g5(adj);", "    assert(g5.size() == 2);", "", "    // Test addVertex() method", "    g1.addVertex(Vertex(1, 2));", "    assert(g1.size() == 1);", "", "    // Test addDirEdge() and getEdges() methods", "    g1.addDirEdge(Edge(Vertex(1, 2), Vertex(3, 4)));", "    assert(g1.getEdges(Vertex(1, 2)).size() == 1);", "", "    // Test addUndirEdge() method", "    g1.addUndirEdge(Edge(Vertex(1, 2), Vertex(5, 6)));", "    assert(g1.getEdges(Vertex(1, 2)).size() == 2);", "    assert(g1.getEdges(Vertex(5, 6)).size() == 1);", "", "    // Test setAsVisited() and getVisitedStatus() methods", "    g1.setAsVisited(Vertex(1, 2));", "    assert(g1.getVisitedStatus(Vertex(1, 2)) == true);", "", "    // Test setMaxEdges() and getMaxEdges() method", "    g1.setMaxEdges(Vertex(1, 2), 3);", "    assert(g1.getMaxEdges(Vertex(1, 2)) == 3);", "}", "", "int main() {", "    testVertex();", "    testGraph();", "    std::cout << \"Tests passed\" << std::endl;", "    return 0;", "}"], "file_path": "tests/Graph/Graph_Test.cpp"}
{"Link_to_commit": "https://github.com/uva-spin/ANN-NMR/commit/635d9fde6d2ecf9a93c430848e11076dfb2e22ac", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 41, "n_files_impacted": 7, "longest_chunk": ["        print(\"\\nERROR DURING SIGNAL GENERATION:\")", "        print(\"-\" * 60)", "        ", "        import traceback", "        print(f\"Error type: {type(e).__name__}\")", "        print(f\"Error message: {str(e)}\")", "        print(\"\\nTraceback:\")", "        traceback.print_exc()", "        ", "        print(\"\\nDIAGNOSTIC INFORMATION:\")", "        try:", "            print(f\"TensorFlow imported: {'tf' in globals()}\")", "            if 'tf' in globals():", "                print(f\"TensorFlow version: {tf.__version__}\")", "            ", "            # Check input parameters", "            print(f\"\\nInput parameters:\")", "            print(f\"  Mode: {args.mode}\")", "            print(f\"  Num samples: {args.num_samples}\")", "            print(f\"  Add noise: {args.add_noise}\")", "            print(f\"  Oversampling: {args.oversampling}\")", "            ", "            # Check if frequency range is properly defined", "            print(f\"\\nFrequency configuration:\")", "            if hasattr(generator, 'center_freq'):", "                print(f\"  Center frequency: {generator.center_freq} MHz\")", "            ", "            # Check for file system issues", "            print(f\"\\nOutput directory:\")", "            print(f\"  Path: {args.output_dir}\")", "            print(f\"  Exists: {os.path.exists(args.output_dir)}\")", "            print(f\"  Writable: {os.access(args.output_dir, os.W_OK) if os.path.exists(args.output_dir) else 'N/A'}\")", "            ", "            # import psutil   ", "            # process = psutil.Process(os.getpid())", "            # print(f\"\\nMemory usage: {process.memory_info().rss / (1024 * 1024):.2f} MB\")", "            ", "        except Exception as diag_error:", "            print(f\"Error during diagnostics: {diag_error}\")", "        ", "        print(\"-\" * 60)"], "file_path": "NM4_Fermilab/Data_Creation/Create_Training_Data.py"}
{"Link_to_commit": "https://github.com/Michelle-Watson/LC/commit/c158904b2bded1159696049147b6034504cc29a3", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 34, "n_files_impacted": 1, "longest_chunk": ["class Solution(object):", "    def maxSubArray(self, nums):", "        \"\"\"", "        :type nums: List[int]", "        :rtype: int", "        \"\"\"", "        return 2  # Placeholder for you to implement the solution", "", "", "# Test cases", "test_cases = [", "    # Example 1", "    ([-2, 1, -3, 4, -1, 2, 1, -5, 4], 6),", "", "    # Example 2", "    ([1], 1),", "", "    # Example 3", "    ([5, 4, -1, 7, 8], 23),", "", "    # Additional test cases", "    ([-1, -2, -3, -4], -1),  # Single negative number", "    ([2, 1, 3, 4], 10),  # All positive numbers", "    ([0, 0, 0, 0], 0),  # All zeros", "    ([-2, -1, -3, 4, -1, 2, 1, -5, 4], 6),  # Subarray with both positive and negative values", "    ([-2, 1, -3, 4, -1, 2, 1], 6)  # Another example of mix of positive and negative numbers", "]", "", "# Running the test cases", "sol = Solution()", "for i, (nums, expected) in enumerate(test_cases):", "    result = sol.maxSubArray(nums)", "    print(f\"Test case {i + 1}: {'Pass' if result == expected else 'Fail'}\")", "    print(f\"Expected: {expected}, Got: {result}\")"], "file_path": "arr/LC53_maxSubarr_kadanesAlg.py"}
{"Link_to_commit": "https://github.com/sgrammas10/CareerMatch-AI/commit/ddb90db63e38a80404d6303d750b8a1c8e208fe5", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 36, "n_files_impacted": 1, "longest_chunk": ["import fetch from \"node-fetch\"", "", "async function fetchTournamentData() {", "    try {", "      const response = await fetch(\"https://www.itftennis.com/tennis/api/TournamentApi/GetCalendar?circuitCode=MT&searchString=&skip=0&take=100&nationCodes=&zoneCodes=&dateFrom=2025-01-01&dateTo=2025-01-31&indoorOutdoor=&categories=&isOrderAscending=true&orderField=startDate&surfaceCodes=\", {", "        \"headers\": {", "          \"accept\": \"*/*\",", "          \"accept-language\": \"en-US,en;q=0.9\",", "          \"if-modified-since\": \"Fri, 24 Jan 2025 19:57:13 GMT\",", "          \"priority\": \"u=1, i\",", "          \"sec-ch-ua\": \"\\\"Google Chrome\\\";v=\\\"131\\\", \\\"Chromium\\\";v=\\\"131\\\", \\\"Not_A Brand\\\";v=\\\"24\\\"\",", "          \"sec-ch-ua-mobile\": \"?0\",", "          \"sec-ch-ua-platform\": \"\\\"macOS\\\"\",", "          \"sec-fetch-dest\": \"empty\",", "          \"sec-fetch-mode\": \"cors\",", "          \"sec-fetch-site\": \"same-origin\",", "          \"cookie\": \"ARRAffinity=1a7aff82bc21373b03d8fda86d009014a254fb43661cd4068b45b28f7aa56160; ARRAffinitySameSite=1a7aff82bc21373b03d8fda86d009014a254fb43661cd4068b45b28f7aa56160; nlbi_178373=Ran5N41HZQkbS5zEtoSRdQAAAABom3lJWWsIWcVYqOPhRl1C; visid_incap_178373=7jzkGB/oTpq6/Jc/BRvGgbVdCGcAAAAAQUIPAAAAAADUwUyZ73WafusZyKj+p7zW; OptanonAlertBoxClosed=2024-10-10T23:05:34.074Z; incap_ses_182_178373=as+rMSL0V37QcjIjQZiGAtEBlGcAAAAAORKY7tNtvmc9UA8GrvugGw==; OptanonConsent=isGpcEnabled=0&datestamp=Fri+Jan+24+2025+16%3A11%3A15+GMT-0500+(Eastern+Standard+Time)&version=6.23.0&isIABGlobal=false&hosts=&consentId=a7629d4b-c3f6-4ed2-a3d9-6d1b9259adc2&interactionCount=1&landingPath=NotLandingPage&groups=C0001%3A1%2CC0002%3A1%2CC0003%3A1%2CC0004%3A1%2CC0005%3A1&geolocation=US%3BNY&AwaitingReconsent=false\"", "        },", "        \"referrerPolicy\": \"no-referrer\",", "        \"body\": null,", "        \"method\": \"GET\"", "      });", "  ", "      if (!response.ok) {", "        throw new Error(`HTTP error! Status: ${response.status}`);", "      }", "  ", "      const data = await response.json();", "      return data;", "    } catch (error) {", "      console.error(\"Failed to fetch tournament data:\", error);", "      throw error;", "    }", "}", "let dataOut = await fetchTournamentData()", "console.log(dataOut)"], "file_path": "Scripts/temp.js"}
{"Link_to_commit": "https://github.com/ElementsProject/lightning/commit/71eb04064c06c9530c937c5fcdaea6499e6ad276", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 194, "n_files_impacted": 8, "longest_chunk": ["#include \"config.h\"", "#include \"../shutdown_scriptpubkey.c\"", "#include <assert.h>", "#include <ccan/array_size/array_size.h>", "#include <ccan/err/err.h>", "#include <common/amount.h>", "#include <common/setup.h>", "#include <common/utils.h>", "#include <stdio.h>", "#include <wire/wire.h>", "", "/* AUTOGENERATED MOCKS START */", "/* Generated stub for amount_asset_is_main */", "bool amount_asset_is_main(struct amount_asset *asset UNNEEDED)", "{ fprintf(stderr, \"amount_asset_is_main called!\\n\"); abort(); }", "/* Generated stub for amount_asset_to_sat */", "struct amount_sat amount_asset_to_sat(struct amount_asset *asset UNNEEDED)", "{ fprintf(stderr, \"amount_asset_to_sat called!\\n\"); abort(); }", "/* Generated stub for amount_feerate */", " bool amount_feerate(u32 *feerate UNNEEDED, struct amount_sat fee UNNEEDED, size_t weight UNNEEDED)", "{ fprintf(stderr, \"amount_feerate called!\\n\"); abort(); }", "/* Generated stub for amount_sat */", "struct amount_sat amount_sat(u64 satoshis UNNEEDED)", "{ fprintf(stderr, \"amount_sat called!\\n\"); abort(); }", "/* Generated stub for amount_sat_add */", " bool amount_sat_add(struct amount_sat *val UNNEEDED,", "\t\t\t\t       struct amount_sat a UNNEEDED,", "\t\t\t\t       struct amount_sat b UNNEEDED)", "{ fprintf(stderr, \"amount_sat_add called!\\n\"); abort(); }", "/* Generated stub for amount_sat_eq */", "bool amount_sat_eq(struct amount_sat a UNNEEDED, struct amount_sat b UNNEEDED)", "{ fprintf(stderr, \"amount_sat_eq called!\\n\"); abort(); }", "/* Generated stub for amount_sat_greater_eq */", "bool amount_sat_greater_eq(struct amount_sat a UNNEEDED, struct amount_sat b UNNEEDED)", "{ fprintf(stderr, \"amount_sat_greater_eq called!\\n\"); abort(); }", "/* Generated stub for amount_sat_sub */", " bool amount_sat_sub(struct amount_sat *val UNNEEDED,", "\t\t\t\t       struct amount_sat a UNNEEDED,", "\t\t\t\t       struct amount_sat b UNNEEDED)", "{ fprintf(stderr, \"amount_sat_sub called!\\n\"); abort(); }", "/* Generated stub for amount_sat_to_asset */", "struct amount_asset amount_sat_to_asset(struct amount_sat *sat UNNEEDED, const u8 *asset UNNEEDED)", "{ fprintf(stderr, \"amount_sat_to_asset called!\\n\"); abort(); }", "/* Generated stub for amount_tx_fee */", "struct amount_sat amount_tx_fee(u32 fee_per_kw UNNEEDED, size_t weight UNNEEDED)", "{ fprintf(stderr, \"amount_tx_fee called!\\n\"); abort(); }", "/* AUTOGENERATED MOCKS END */", "", "/* Thanks ChatGPT! */", "static const u8 *construct_script(const tal_t *ctx,", "\t\t\t\t  const u8 *data, size_t data_len)", "{", "\tu8 *script = tal_arr(ctx, u8, 0);", "\ttowire_u8(&script, OP_RETURN);", "\tif (data_len >= 6 && data_len <= 75) {", "\t\ttowire_u8(&script, data_len);", "\t\ttowire(&script, data, data_len);", "\t} else if (data_len >= 76 && data_len <= 80) {", "\t\ttowire_u8(&script, 76);", "\t\ttowire_u8(&script, data_len);", "\t\ttowire(&script, data, data_len);", "\t} else {", "\t\treturn tal_free(script); // Invalid case", "\t}", "\treturn script;", "}", "", "static void test_valid_op_returns(void)", "{", "\tu8 data[80];", "\tconst u8 *script;", "", "\tfor (size_t i = 6; i <= 80; i++) {", "\t\tmemset(data, i, sizeof(data));", "\t\tscript = construct_script(tmpctx, data, i);", "\t\tassert(is_valid_op_return(script, tal_bytelen(script)));", "\t}", "}", "", "static void test_invalid_op_return_too_short(void)", "{", "\tu8 data[80];", "\tconst u8 *script;", "", "\tfor (size_t i = 0; i < 6; i++) {", "\t\tmemset(data, i, sizeof(data));", "\t\tscript = construct_script(tmpctx, data, i);", "\t\tassert(!is_valid_op_return(script, tal_bytelen(script)));", "\t}", "}", "", "static void test_invalid_op_return_too_long(void)", "{", "\tu8 data[100];", "\tconst u8 *script;", "", "\tfor (size_t i = 81; i < sizeof(data); i++) {", "\t\tmemset(data, i, sizeof(data));", "\t\tscript = construct_script(tmpctx, data, i);", "\t\tassert(!is_valid_op_return(script, tal_bytelen(script)));", "\t}", "}", "", "// Test case: Invalid OP_RETURN with incorrect push length (e.g., 77 bytes using wrong prefix)", "static void test_invalid_op_return_wrong_push_length(void)", "{", "\tu8 script[90] = {0x6a, 0x76, 0x77}; // Invalid push of 77 bytes", "\tassert(!is_valid_op_return(script, 3 + 77));", "}", "", "// Test case: Invalid OP_RETURN with incorrect OP_RETURN opcode", "static void test_invalid_op_return_wrong_opcode(void)", "{", "\tu8 script[10] = {0x00, 0x06, 1, 2, 3, 4, 5, 6}; // 0x00 instead of 0x6a", "\tassert(!is_valid_op_return(script, 8));", "}", "", "// Test case: Invalid OP_RETURN with no data", "static void test_invalid_op_return_empty(void)", "{", "\tu8 script[1] = {0x6a}; // Only OP_RETURN, no data", "\tassert(!is_valid_op_return(script, 1));", "}", "", "static const u8 *construct_witness_script(const tal_t *ctx, u8 version,", "                                          const u8 *data, size_t data_len)", "{", "\tu8 *script = tal_arr(ctx, u8, 0);", "", "\tif (version < OP_1 || version > OP_16 || data_len < 2 || data_len > 40)", "\t\treturn tal_free(script); // Invalid case", "", "\ttowire_u8(&script, version);  // OP_1 to OP_16", "\ttowire_u8(&script, data_len); // Push length", "\ttowire(&script, data, data_len); // Data", "", "\treturn script;", "}", "", "static void test_valid_witnessprogs(void)", "{", "\tu8 data[40];", "\tconst u8 *script;", "", "\tfor (u8 version = OP_1; version <= OP_16; version++) {", "\t\tfor (size_t i = 2; i <= 40; i++) {", "\t\t\tmemset(data, i, sizeof(data));", "\t\t\tscript = construct_witness_script(tmpctx, version, data, i);", "\t\t\tassert(is_valid_witnessprog(script, tal_bytelen(script)));", "\t\t}", "\t}", "}", "", "static void test_invalid_witnessprogs(void)", "{", "\tu8 data[41];", "\tconst u8 *script;", "", "\t// Test: Invalid versions (0 and >16)", "\tmemset(data, 0xAA, sizeof(data));", "\tscript = construct_witness_script(tmpctx, OP_0, data, 20);", "\tassert(!is_valid_witnessprog(script, tal_bytelen(script)));", "", "\tscript = construct_witness_script(tmpctx, OP_16 + 1, data, 20);", "\tassert(!is_valid_witnessprog(script, tal_bytelen(script)));", "", "\t// Test: Invalid data lengths (1 byte and >40 bytes)", "\tscript = construct_witness_script(tmpctx, OP_1, data, 1);", "\tassert(!is_valid_witnessprog(script, tal_bytelen(script)));", "", "\tscript = construct_witness_script(tmpctx, OP_2, data, 41);", "\tassert(!is_valid_witnessprog(script, tal_bytelen(script)));", "", "\t// Test: Completely empty script (invalid)", "\tscript = tal_arr(tmpctx, u8, 0);", "\tassert(!is_valid_witnessprog(script, tal_bytelen(script)));", "}", "", "// Test runner", "int main(int argc, char *argv[])", "{", "\tcommon_setup(argv[0]);", "", "\ttest_valid_op_returns();", "\ttest_invalid_op_return_too_short();", "\ttest_invalid_op_return_too_long();", "\ttest_invalid_op_return_wrong_push_length();", "\ttest_invalid_op_return_wrong_opcode();", "\ttest_invalid_op_return_empty();", "\ttest_valid_witnessprogs();", "\ttest_invalid_witnessprogs();", "\tcommon_shutdown();", "\treturn 0;", "}"], "file_path": "lightningd/channel_control.c"}
{"Link_to_commit": "https://github.com/Bittium1/Lightning/commit/71eb04064c06c9530c937c5fcdaea6499e6ad276", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 194, "n_files_impacted": 8, "longest_chunk": ["#include \"config.h\"", "#include \"../shutdown_scriptpubkey.c\"", "#include <assert.h>", "#include <ccan/array_size/array_size.h>", "#include <ccan/err/err.h>", "#include <common/amount.h>", "#include <common/setup.h>", "#include <common/utils.h>", "#include <stdio.h>", "#include <wire/wire.h>", "", "/* AUTOGENERATED MOCKS START */", "/* Generated stub for amount_asset_is_main */", "bool amount_asset_is_main(struct amount_asset *asset UNNEEDED)", "{ fprintf(stderr, \"amount_asset_is_main called!\\n\"); abort(); }", "/* Generated stub for amount_asset_to_sat */", "struct amount_sat amount_asset_to_sat(struct amount_asset *asset UNNEEDED)", "{ fprintf(stderr, \"amount_asset_to_sat called!\\n\"); abort(); }", "/* Generated stub for amount_feerate */", " bool amount_feerate(u32 *feerate UNNEEDED, struct amount_sat fee UNNEEDED, size_t weight UNNEEDED)", "{ fprintf(stderr, \"amount_feerate called!\\n\"); abort(); }", "/* Generated stub for amount_sat */", "struct amount_sat amount_sat(u64 satoshis UNNEEDED)", "{ fprintf(stderr, \"amount_sat called!\\n\"); abort(); }", "/* Generated stub for amount_sat_add */", " bool amount_sat_add(struct amount_sat *val UNNEEDED,", "\t\t\t\t       struct amount_sat a UNNEEDED,", "\t\t\t\t       struct amount_sat b UNNEEDED)", "{ fprintf(stderr, \"amount_sat_add called!\\n\"); abort(); }", "/* Generated stub for amount_sat_eq */", "bool amount_sat_eq(struct amount_sat a UNNEEDED, struct amount_sat b UNNEEDED)", "{ fprintf(stderr, \"amount_sat_eq called!\\n\"); abort(); }", "/* Generated stub for amount_sat_greater_eq */", "bool amount_sat_greater_eq(struct amount_sat a UNNEEDED, struct amount_sat b UNNEEDED)", "{ fprintf(stderr, \"amount_sat_greater_eq called!\\n\"); abort(); }", "/* Generated stub for amount_sat_sub */", " bool amount_sat_sub(struct amount_sat *val UNNEEDED,", "\t\t\t\t       struct amount_sat a UNNEEDED,", "\t\t\t\t       struct amount_sat b UNNEEDED)", "{ fprintf(stderr, \"amount_sat_sub called!\\n\"); abort(); }", "/* Generated stub for amount_sat_to_asset */", "struct amount_asset amount_sat_to_asset(struct amount_sat *sat UNNEEDED, const u8 *asset UNNEEDED)", "{ fprintf(stderr, \"amount_sat_to_asset called!\\n\"); abort(); }", "/* Generated stub for amount_tx_fee */", "struct amount_sat amount_tx_fee(u32 fee_per_kw UNNEEDED, size_t weight UNNEEDED)", "{ fprintf(stderr, \"amount_tx_fee called!\\n\"); abort(); }", "/* AUTOGENERATED MOCKS END */", "", "/* Thanks ChatGPT! */", "static const u8 *construct_script(const tal_t *ctx,", "\t\t\t\t  const u8 *data, size_t data_len)", "{", "\tu8 *script = tal_arr(ctx, u8, 0);", "\ttowire_u8(&script, OP_RETURN);", "\tif (data_len >= 6 && data_len <= 75) {", "\t\ttowire_u8(&script, data_len);", "\t\ttowire(&script, data, data_len);", "\t} else if (data_len >= 76 && data_len <= 80) {", "\t\ttowire_u8(&script, 76);", "\t\ttowire_u8(&script, data_len);", "\t\ttowire(&script, data, data_len);", "\t} else {", "\t\treturn tal_free(script); // Invalid case", "\t}", "\treturn script;", "}", "", "static void test_valid_op_returns(void)", "{", "\tu8 data[80];", "\tconst u8 *script;", "", "\tfor (size_t i = 6; i <= 80; i++) {", "\t\tmemset(data, i, sizeof(data));", "\t\tscript = construct_script(tmpctx, data, i);", "\t\tassert(is_valid_op_return(script, tal_bytelen(script)));", "\t}", "}", "", "static void test_invalid_op_return_too_short(void)", "{", "\tu8 data[80];", "\tconst u8 *script;", "", "\tfor (size_t i = 0; i < 6; i++) {", "\t\tmemset(data, i, sizeof(data));", "\t\tscript = construct_script(tmpctx, data, i);", "\t\tassert(!is_valid_op_return(script, tal_bytelen(script)));", "\t}", "}", "", "static void test_invalid_op_return_too_long(void)", "{", "\tu8 data[100];", "\tconst u8 *script;", "", "\tfor (size_t i = 81; i < sizeof(data); i++) {", "\t\tmemset(data, i, sizeof(data));", "\t\tscript = construct_script(tmpctx, data, i);", "\t\tassert(!is_valid_op_return(script, tal_bytelen(script)));", "\t}", "}", "", "// Test case: Invalid OP_RETURN with incorrect push length (e.g., 77 bytes using wrong prefix)", "static void test_invalid_op_return_wrong_push_length(void)", "{", "\tu8 script[90] = {0x6a, 0x76, 0x77}; // Invalid push of 77 bytes", "\tassert(!is_valid_op_return(script, 3 + 77));", "}", "", "// Test case: Invalid OP_RETURN with incorrect OP_RETURN opcode", "static void test_invalid_op_return_wrong_opcode(void)", "{", "\tu8 script[10] = {0x00, 0x06, 1, 2, 3, 4, 5, 6}; // 0x00 instead of 0x6a", "\tassert(!is_valid_op_return(script, 8));", "}", "", "// Test case: Invalid OP_RETURN with no data", "static void test_invalid_op_return_empty(void)", "{", "\tu8 script[1] = {0x6a}; // Only OP_RETURN, no data", "\tassert(!is_valid_op_return(script, 1));", "}", "", "static const u8 *construct_witness_script(const tal_t *ctx, u8 version,", "                                          const u8 *data, size_t data_len)", "{", "\tu8 *script = tal_arr(ctx, u8, 0);", "", "\tif (version < OP_1 || version > OP_16 || data_len < 2 || data_len > 40)", "\t\treturn tal_free(script); // Invalid case", "", "\ttowire_u8(&script, version);  // OP_1 to OP_16", "\ttowire_u8(&script, data_len); // Push length", "\ttowire(&script, data, data_len); // Data", "", "\treturn script;", "}", "", "static void test_valid_witnessprogs(void)", "{", "\tu8 data[40];", "\tconst u8 *script;", "", "\tfor (u8 version = OP_1; version <= OP_16; version++) {", "\t\tfor (size_t i = 2; i <= 40; i++) {", "\t\t\tmemset(data, i, sizeof(data));", "\t\t\tscript = construct_witness_script(tmpctx, version, data, i);", "\t\t\tassert(is_valid_witnessprog(script, tal_bytelen(script)));", "\t\t}", "\t}", "}", "", "static void test_invalid_witnessprogs(void)", "{", "\tu8 data[41];", "\tconst u8 *script;", "", "\t// Test: Invalid versions (0 and >16)", "\tmemset(data, 0xAA, sizeof(data));", "\tscript = construct_witness_script(tmpctx, OP_0, data, 20);", "\tassert(!is_valid_witnessprog(script, tal_bytelen(script)));", "", "\tscript = construct_witness_script(tmpctx, OP_16 + 1, data, 20);", "\tassert(!is_valid_witnessprog(script, tal_bytelen(script)));", "", "\t// Test: Invalid data lengths (1 byte and >40 bytes)", "\tscript = construct_witness_script(tmpctx, OP_1, data, 1);", "\tassert(!is_valid_witnessprog(script, tal_bytelen(script)));", "", "\tscript = construct_witness_script(tmpctx, OP_2, data, 41);", "\tassert(!is_valid_witnessprog(script, tal_bytelen(script)));", "", "\t// Test: Completely empty script (invalid)", "\tscript = tal_arr(tmpctx, u8, 0);", "\tassert(!is_valid_witnessprog(script, tal_bytelen(script)));", "}", "", "// Test runner", "int main(int argc, char *argv[])", "{", "\tcommon_setup(argv[0]);", "", "\ttest_valid_op_returns();", "\ttest_invalid_op_return_too_short();", "\ttest_invalid_op_return_too_long();", "\ttest_invalid_op_return_wrong_push_length();", "\ttest_invalid_op_return_wrong_opcode();", "\ttest_invalid_op_return_empty();", "\ttest_valid_witnessprogs();", "\ttest_invalid_witnessprogs();", "\tcommon_shutdown();", "\treturn 0;", "}"], "file_path": "lightningd/channel_control.c"}
{"Link_to_commit": "https://github.com/dustinma324/DataValidationScripts/commit/745cfb6f6e8fa6a7fbf707503bd57fba4c3ffcf8", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 63, "n_files_impacted": 1, "longest_chunk": ["import pandas as pd", "import matplotlib.pyplot as plt", "import numpy as np", "from scipy.integrate import simps", "", "# Define the file path", "file_path = \"/home/tma/Desktop/DataValidationScripts/chan395/profiles/chan395.means\"", "", "# Number of header rows to skip", "header_lines_to_skip = 24", "", "# Read the file into a pandas DataFrame, skipping the header lines and setting column names", "try:", "    df = pd.read_csv(file_path, delimiter='\\s+', skiprows=header_lines_to_skip, header=None, names=[\"y\", \"y+\", \"Umean\", \"col4\", \"col5\", \"col6\", \"col7\"])  # Use '\\s+' for any whitespace delimiter", "    ", "    # Extracting the required columns and converting to floats", "    y = df[\"y\"].iloc[1:].reset_index(drop=True).astype(float)", "    Umean = df[\"Umean\"].iloc[1:].reset_index(drop=True).astype(float)", "    ", "    # Calculate Ubulk (normalized integral of Umean with respect to y)", "    H = 1.0  # Assuming H is the height and normalizing factor", "    Ubulk = simps(Umean, y) / H", "    ", "    # Print Ubulk value", "    print(f\"Ubulk: {Ubulk}\")", "    ", "    # Plotting", "    plt.figure(figsize=(12, 10))", "    plt.plot(Umean, y, marker='o', linestyle='-', color='b', label=r\"Moser1999, $Re_{\\tau}=395$\")", "    ", "    # Labels", "    plt.xlabel(r'$\\overline{U}$', fontsize=14, fontname='serif')", "    plt.ylabel(r'$y/H$', fontsize=14, fontname='serif')", "    ", "    # Set y-axis range", "    plt.ylim(0.0, 1.0)", "    ", "    # Turn off grid", "    plt.grid(False)", "    ", "    # Legend", "    plt.legend(fontsize=12)", "    ", "    # Use LaTeX interpreter for the figure", "    plt.rcParams.update({'mathtext.default': 'regular'})", "    ", "    # Set font to serif for all text elements", "    plt.rcParams['font.family'] = 'serif'", "    ", "    # Set global font size", "    plt.rcParams.update({'font.size': 14})", "    ", "    # Show plot", "    plt.tight_layout()", "    plt.show()", "    ", "except FileNotFoundError:", "    print(f\"The file at {file_path} does not exist.\")", "except pd.errors.EmptyDataError:", "    print(\"The file is empty.\")", "except pd.errors.ParserError:", "    print(\"Error parsing the file.\")", ""], "file_path": "src/plotProfiles.py"}
{"Link_to_commit": "https://github.com/ottomann1/leaguebuilder/commit/e87b6856874d8b3615a6c5003fc715d269cb2461", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 43, "n_files_impacted": 11, "longest_chunk": ["import { db } from '.';", "import { champions, items } from './schema';", "import { eq, inArray } from 'drizzle-orm';", "", "export async function insertMissingChampionsAndItems(ddChampions: DDChampion[], ddItems: DDItem[]) {", "  await db.transaction(async (tx) => {", "    // Fetch all existing champions and items in one go", "    const existingChampions = await tx.select().from(champions);", "    const existingItems = await tx.select().from(items);", "", "    // Create a set of existing champion names and item IDs for quick lookup", "    const existingChampionNames = new Set(existingChampions.map((champ) => champ.name));", "    const existingItemIds = new Set(existingItems.map((item) => item.id));", "", "    // Filter out the champions and items that already exist", "    const newChampions = ddChampions.filter((ddChampion) => !existingChampionNames.has(ddChampion.name));", "    const newItems = ddItems.filter((ddItem) => !existingItemIds.has(ddItem.id));", "", "    // Insert new champions and items in bulk", "    if (newChampions.length > 0) {", "      await tx.insert(champions).values(", "        newChampions.map((ddChampion) => ({", "          name: ddChampion.name,", "        }))", "      );", "      console.log(`Inserted champions: ${newChampions.map((c) => c.name).join(', ')}`);", "    } else {", "      console.log('No new champions to insert.');", "    }", "", "    if (newItems.length > 0) {", "      await tx.insert(items).values(", "        newItems.map((ddItem) => ({", "          id: ddItem.id,", "          name: ddItem.name,", "        }))", "      );", "      console.log(`Inserted items: ${newItems.map((i) => i.name).join(', ')}`);", "    } else {", "      console.log('No new items to insert.');", "    }", "  });", "}"], "file_path": "src/server/db/index.ts"}
{"Link_to_commit": "https://github.com/Code-Fellows-School-Work/odd-duck/commit/d3b76894dcb6f65dcc629b799040cffabc26c37d", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 10, "n_files_impacted": 3, "longest_chunk": ["  const resultsContainer = document.getElementById('report');", "  resultsContainer.innerHTML = ''; // Clear previous results", "", "  // Loop through allPictures and display results", "  state.allPictures.forEach((picture) => {", "    const resultItem = document.createElement('p');", "    resultItem.textContent = `${picture.name}: Votes - ${picture.votes}, Seen - ${picture.views}`;", "    resultsContainer.appendChild(resultItem);", "  });", ""], "file_path": "js/apps.js"}
{"Link_to_commit": "https://github.com/Viralore/LearnJFS/commit/7e954d7174b90162d88af9de735aa6dd3e460db1", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 27, "n_files_impacted": 7, "longest_chunk": ["package com.lpu.lib.config;", "", "import org.springframework.context.annotation.Bean;", "import org.springframework.context.annotation.Configuration;", "import org.springframework.web.servlet.config.annotation.CorsRegistry;", "import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;", "", "@Configuration", "public class WebConfig implements WebMvcConfigurer ", "{", "", "    @Bean", "    public WebMvcConfigurer corsConfigurer() ", "    {", "        return new WebMvcConfigurer() ", "        {", "            @Override", "            public void addCorsMappings(CorsRegistry registry) ", "            {", "                registry.addMapping(\"/**\")", "                        .allowedOrigins(\"http://127.0.0.1:5500\")", "                        .allowedMethods(\"GET\", \"POST\", \"PUT\", \"DELETE\", \"OPTIONS\")", "                        .allowedHeaders(\"*\");", "            }", "        };", "    }", "}"], "file_path": "libboot/src/main/java/com/lpu/lib/config/WebConfig.java"}
{"Link_to_commit": "https://github.com/Harlan-Howe/ArrowMazeForAISolution/commit/23b05fb441736acb296eb4d170ee70b855e56d70", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 81, "n_files_impacted": 1, "longest_chunk": ["", "    public boolean noBlackArrowsRemain()", "    {", "        for (int r=0; r<NUM_ROWS; r++)", "            for (int c=0; c<NUM_COLS; c++)", "                if (myGrid[r][c].getMyColor() == Color.BLACK)", "                    return false;", "        return true;", "    }", "", "    public ArrowCell targetOfArrowCell(ArrowCell cell)", "    {", "        switch (cell.getDirection())", "        {", "            case ArrowCell.RIGHT:", "                if (cell.getMyCol() == NUM_COLS-1)", "                    return null;", "                return myGrid[cell.getMyRow()][cell.getMyCol()+1];", "            case ArrowCell.LEFT:", "                if (cell.getMyCol() == 0)", "                    return null;", "                return myGrid[cell.getMyRow()][cell.getMyCol()-1];", "            case ArrowCell.DOWN:", "                if (cell.getMyRow() == NUM_ROWS-1)", "                    return null;", "                return myGrid[cell.getMyRow()+1][cell.getMyCol()];", "            case ArrowCell.UP:", "                if (cell.getMyRow() == 0)", "                    return null;", "                return myGrid[cell.getMyRow()-1][cell.getMyCol()];", "", "            default:", "                return null;", "        }", "", "    }", "", "    public void setColorForPath(Color c, ArrayList<ArrowCell> path)", "    {", "        for (ArrowCell cell:path)", "        {", "            cell.setMyColor(c);", "        }", "        repaint();", "    }", "", "    public void colorPathStartingAt(int r, int c)", "    {", "        ArrowCell startCell = myGrid[r][c];", "        if (startCell.getMyColor() != Color.BLACK)", "            return;", "        ArrayList<ArrowCell> path = new ArrayList<>();", "        path.add(startCell);", "        while(true)", "        {", "            ArrowCell nextCell = targetOfArrowCell(path.get(path.size()-1));", "            if (nextCell == null || path.contains(nextCell))", "            {", "                if (nextCell != null)", "                    path.add(nextCell);", "                setColorForPath(new Color((int)(Math.random()*128)+64,", "                                (int)(Math.random()*128)+64,", "                                (int)(Math.random()*128)+64),", "                        path);", "                return;", "            }", "            if (nextCell.getMyColor() != Color.BLACK)", "            {", "                setColorForPath(nextCell.getMyColor(), path);", "                return;", "            }", "            path.add(nextCell);", "        }", "    }", "", "    public void execute()", "    {", "        for (int cellNum = 0; cellNum<NUM_ROWS*NUM_COLS; cellNum++)", "            colorPathStartingAt(cellNum/NUM_COLS, cellNum%NUM_COLS);", "", "    }"], "file_path": "src/ArrowMazePanel.java"}
{"Link_to_commit": "https://github.com/ZhenyingZhu/StudyNotes/commit/2368ffcb5f3dc17981f3a889cb3fa3eb91f59cb2", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 78, "n_files_impacted": 1, "longest_chunk": ["string subscriptionId = \"YOUR_SUBSCRIPTION_ID\";", "string clientId = \"YOUR_CLIENT_ID\";", "            string clientSecret = \"YOUR_CLIENT_SECRET\";", "            string tenantId = \"YOUR_TENANT_ID\";", "            string resourceGroupName = \"YOUR_RESOURCE_GROUP_NAME\";", "            string vmName = \"YOUR_VM_NAME\";", "            string location = \"YOUR_VM_LOCATION\"; // e.g., \"eastus\"", "", "            // Authenticate using a service principal", "            var serviceClientCredentials = ApplicationTokenProvider.LoginSilentAsync(", "                tenantId, clientId, clientSecret).Result;", "", "            // Create a Resource Management client", "            var resourceManagementClient = new ResourceManagementClient(serviceClientCredentials)", "            {", "                SubscriptionId = subscriptionId", "            };", "", "            // Create the resource group", "            var resourceGroup = new ResourceGroup", "            {", "                Location = location", "            };", "", "            resourceManagementClient.ResourceGroups.CreateOrUpdate(resourceGroupName, resourceGroup);", "", "            // Create the VM", "            var vmClient = new ComputeManagementClient(serviceClientCredentials)", "            {", "                SubscriptionId = subscriptionId", "            };", "            var vmParams = new VirtualMachine", "            {", "                Location = location,", "                OsProfile = new OSProfile", "                {", "                    ComputerName = vmName,", "                    AdminUsername = \"YOUR_ADMIN_USERNAME\", // Replace with your desired admin username", "                    AdminPassword = \"YOUR_ADMIN_PASSWORD\"  // Replace with your desired admin password", "                },", "                HardwareProfile = new HardwareProfile", "                {", "                    VmSize = \"Standard_D2s_v3\" // Replace with the desired VM size", "                },", "                StorageProfile = new StorageProfile", "                {", "                    ImageReference = new ImageReference", "                    {", "                        Publisher = \"MicrosoftWindowsServer\",", "                        Offer = \"WindowsServer\",", "                        Sku = \"2022-datacenter\",", "                        Version = \"latest\"", "                    },", "                    OsDisk = new OSDisk", "                    {", "                        CreateOption = DiskCreateOptionTypes.FromImage,", "                        ManagedDisk = new ManagedDiskParameters", "                        {", "                            StorageAccountType = StorageAccountTypes.StandardLRS", "                        }", "                    }", "                },", "                NetworkProfile = new NetworkProfile", "                {", "                    NetworkInterfaces = new[]", "                    {", "                        new NetworkInterfaceReference", "                        {", "                            Id = \"/subscriptions/YOUR_SUBSCRIPTION_ID/resourceGroups/YOUR_RESOURCE_GROUP/providers/Microsoft.Network/networkInterfaces/YOUR_NETWORK_INTERFACE_NAME\"", "                            // Replace with the actual network interface ID", "                        }", "                    }", "                }", "            };", "            vmClient.VirtualMachines.CreateOrUpdate(resourceGroupName, vmName, vmParams);", "", "            Console.WriteLine(\"Virtual Machine created successfully!\");", ""], "file_path": "csharp-example/AzureManagementTest/Program.cs"}
{"Link_to_commit": "https://github.com/theonlytechnohead/TouchFaders/commit/4b429b7f705547efe36b546705d4a1f07625e183", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 105, "n_files_impacted": 9, "longest_chunk": ["", "    public class RCPAddress {", "        // parameters: X, Y, min, max, default, unit, type, UI?, r/w, scale", "        // get: X, Y", "        // set: X, Y, value, textValue", "", "", "        public static string ToString (object instance) {", "            Type currentType = instance.GetType();", "            string result = string.Empty;", "", "            while (currentType != null && currentType != typeof(object)) {", "                if (currentType.DeclaringType == typeof(Console)) {", "                    result = $\"{currentType.Name}:{result}\";", "                    currentType = null;", "                } else {", "                    result = $\"{currentType.Name}/{result}\";", "                    currentType = currentType.DeclaringType;", "                }", "            }", "", "            return result.TrimEnd('/') + \" \" + instance;", "        }", "", "        public class Console {", "            public class MIXER {", "", "            }", "", "            public class CL {", "", "            }", "", "            public class QL {", "", "                public class Current {", "", "                    public class CustomFaderBank {", "", "                        public class SourceCh {", "                            // 4 32 0 11 \"NO ASSIGN\" \"\" string any rw 1", "                        }", "                        public class Master {", "                            public class SourceCh {", "                                // 4 2 0 11 \"NO ASSIGN\" \"\" string any rw 1", "                            }", "                        }", "                    }", "                    public class FaderBank {", "                        public enum FaderBanks {", "                            I,", "                            Dont,", "                            Know", "                        }", "                        public class Select {", "                            // 1 1 0 1 0 \"\" integer any rw 1", "                            // TODO: what does it do?", "                            public enum Selects {", "                                BankA,", "                                BankB", "                            }", "                            private readonly Selects? bank = null;", "                            public Select () { }", "                            public Select (Selects bank) { this.bank = bank; }", "                            public override string ToString () {", "                                return \"0 0\" + bank != null ? \" \" + bank.Value.ToString() : string.Empty;", "                            }", "                        }", "                        public class Bank {", "                            public enum Banks {", "                                Input1,", "                                Input2,", "                                StInDCA,", "                                MixMatrix,", "                                B1,", "                                B2,", "                                B3,", "                                B4", "                            }", "                            public class Recall {", "                                // 1 3 0 8 0 \"\" integer any rw 1", "                                // TODO: 3 banks, each with 8 options", "                                public FaderBanks? faderBank = null;", "                                public Banks? bank = null;", "                                public Recall (FaderBanks faderBank) {", "                                    this.faderBank = faderBank;", "                                }", "                                public Recall (FaderBanks faderBank, Banks bank) {", "                                    this.faderBank = faderBank;", "                                    this.bank = bank;", "                                }", "                                public override string ToString () {", "                                    return \"0 \" + (int)faderBank.Value + (bank != null ? ' ' + ((int)bank.Value).ToString() : string.Empty);", "                                }", "                            }", "                            public class Toggle {", "                                // 1 1 0 8 0 \"\" integer any rw 1", "                                // TODO: what does it do?", "                            }", "                        }", "                    }", "                }", "            }", "        }", "    }"], "file_path": "TouchFaders/RCPParser.cs"}
{"Link_to_commit": "https://github.com/cse110-sp23-group20/fortune-teller/commit/ca20f25881b40e042a688af4768b0da9b4d0d9c4", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 119, "n_files_impacted": 1, "longest_chunk": ["/* global global */", "", "import { mod, pick, wait, timeoutId } from \"../source/utils.js\";", "", "// Yes. I generated these tests with ChatGPT. \ud83d\ude0e", "", "describe(\"pick\", () => {", "  // Test with an array of numbers", "  test(\"picks a number from the given array\", () => {", "    const options = [1, 2, 3, 4, 5];", "    const result = pick(options);", "    expect(options).toContain(result);", "  });", "", "  // Test with an array of strings", "  test(\"picks a string from the given array\", () => {", "    const options = [\"apple\", \"banana\", \"cherry\", \"date\"];", "    const result = pick(options);", "    expect(options).toContain(result);", "  });", "", "  // Test with an array of objects", "  test(\"picks an object from the given array\", () => {", "    const options = [", "      { id: 1, name: \"Alice\" },", "      { id: 2, name: \"Bob\" },", "      { id: 3, name: \"Charlie\" },", "    ];", "    const result = pick(options);", "    expect(options).toContain(result);", "  });", "", "  // Test with an empty array", "  test(\"returns undefined for an empty array\", () => {", "    const options = [];", "    const result = pick(options);", "    expect(result).toBeUndefined();", "  });", "", "  // Test with a single-element array", "  test(\"returns the only option for a single-element array\", () => {", "    const options = [\"only\"];", "    const result = pick(options);", "    expect(result).toEqual(\"only\");", "  });", "", "  // Test with a large array", "  test(\"picks an element from a large array\", () => {", "    const options = Array.from({ length: 1000 }, (_, index) => index);", "    const result = pick(options);", "    expect(options).toContain(result);", "  });", "});", "", "describe(\"mod\", () => {", "  // Test with positive numbers", "  test(\"returns the correct modulus for positive numbers\", () => {", "    expect(mod(10, 3)).toBe(1);", "    expect(mod(15, 6)).toBe(3);", "    expect(mod(20, 7)).toBe(6);", "  });", "", "  // Test with negative numbers", "  test(\"returns the correct modulus for negative numbers\", () => {", "    expect(mod(-10, 3)).toBe(2);", "    expect(mod(-15, 6)).toBe(3);", "    expect(mod(-20, 7)).toBe(1);", "  });", "", "  // Test with zero divisor", "  test(\"returns NaN when the divisor is zero\", () => {", "    expect(mod(10, 0)).toBeNaN();", "    expect(mod(-10, 0)).toBeNaN();", "    expect(mod(0, 0)).toBeNaN();", "  });", "", "  // Test with large numbers", "  test(\"returns the correct modulus for large numbers\", () => {", "    expect(mod(987654321, 123456789)).toBe(9);", "    expect(mod(123456789, 987654321)).toBe(123456789);", "  });", "});", "", "jest.useFakeTimers();", "", "describe(\"wait\", () => {", "  let setTimeoutMock;", "", "  beforeEach(() => {", "    setTimeoutMock = jest.spyOn(global, \"setTimeout\");", "  });", "", "  afterEach(() => {", "    jest.clearAllTimers();", "    setTimeoutMock.mockRestore();", "  });", "", "  test(\"resolves after the specified delay\", () => {", "    const delay = 1000;", "    const promise = wait(delay);", "    expect(setTimeoutMock).toHaveBeenCalledTimes(1);", "    expect(setTimeoutMock).toHaveBeenCalledWith(expect.any(Function), delay);", "    jest.advanceTimersByTime(delay);", "    return expect(promise).resolves.toBeUndefined();", "  });", "", "  test(\"does not resolve if clearTimeout is called\", () => {", "    const delay = 1000;", "    const promise = wait(delay);", "    expect(setTimeoutMock).toHaveBeenCalledTimes(1);", "    expect(setTimeoutMock).toHaveBeenCalledWith(expect.any(Function), delay);", "    clearTimeout(timeoutId);", "    jest.advanceTimersByTime(delay);", "    const passSymbol = Symbol();", "    return expect(", "      Promise.race([promise, Promise.resolve(passSymbol)])", "    ).resolves.toBe(passSymbol);", "  });", "});"], "file_path": "__tests__/utils.unit.test.js"}
{"Link_to_commit": "https://github.com/pieroapretto/ChatGPT-Code-Mentor/commit/152f328a671446696291aaaff91f033924e52402", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 32, "n_files_impacted": 3, "longest_chunk": ["const tectalicOpenai = require('@tectalic/openai').default;", "const config = require('dotenv').config();", "", "async function cypressTestsGenerator(input) {", "  try {", "    const res = await tectalicOpenai(process.env.OPENAI_API_KEY)", "    .completions.create({", "      model: 'text-davinci-003',", "      prompt: 'Write Cypress tests for the following JavaScript code: ' + input,", "    })", "", "    const cypress_test_recommendations = res.data.choices[0].text.trim();", "", "    console.log(`\\n${cypress_test_recommendations}`);", "", "    return `Cypress tests suggestion for this pull request:\\n\\`\\`\\`diff\\n${cypress_test_recommendations}\\n\\`\\`\\``;", "", "  } catch (err) {", "    if (err?.response) {", "      const { status = null, statusText = '' } = err.response;", "      console.error(`${status} - ${statusText}`);", "      return err?.response;", "    } else {", "      console.error(err);", "      return err;", "    }", "  }", "}", "", "module.exports = {", "  cypressTestsGenerator", "};"], "file_path": "methods/cypressTestsGenerator.js"}
{"Link_to_commit": "https://github.com/willemverbuyst/bookworm/commit/c0a55671cf8e2b8d29d7fdf3e9c418f3f325b04e", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 23, "n_files_impacted": 1, "longest_chunk": ["import { getColorIndex } from \"./getColorIndex\";", "", "describe(\"getColorIndex function\", () => {", "  it(\"should return 0 when index is 0\", () => {", "    const result = getColorIndex(0);", "    expect(result).toBe(0);", "  });", "", "  it(\"should return 0 when index is 1\", () => {", "    const result = getColorIndex(1);", "    expect(result).toBe(0);", "  });", "", "  it(\"should return index - 1 when index is even\", () => {", "    const result = getColorIndex(4);", "    expect(result).toBe(3);", "  });", "", "  it(\"should return index - 2 when index is odd\", () => {", "    const result = getColorIndex(5);", "    expect(result).toBe(3);", "  });", "});"], "file_path": "frontend/src/business/functions/getColorIndex.test.ts"}
{"Link_to_commit": "https://github.com/willemverbuyst/bookworm/commit/4e1f447bb00cba1b0c8f877dbcfe11539027fca0", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 45, "n_files_impacted": 1, "longest_chunk": ["import { genericSearch } from \"./genericSearch\";", "", "describe(\"genericSearch function\", () => {", "  const obj1 = { name: \"Alice\", age: 30, address: \"123 Main St\" };", "  const obj2 = { name: \"Bob\", age: 40, address: \"456 Second Ave\" };", "  const obj3 = { name: \"Charlie\", age: 50, address: \"789 Third St\" };", "  const objs = [obj1, obj2, obj3];", "", "  it(\"should return true when query is empty string\", () => {", "    const result = genericSearch(obj1, [\"name\"], \"\");", "    expect(result).toBe(true);", "  });", "", "  it(\"should return true when query matches property value\", () => {", "    const result = genericSearch(obj1, [\"name\"], \"Alice\");", "    expect(result).toBe(true);", "  });", "", "  it(\"should return false when query does not match any property value\", () => {", "    const result = genericSearch(obj1, [\"name\"], \"Bob\");", "    expect(result).toBe(false);", "  });", "", "  it(\"should return true when query matches any property value\", () => {", "    const result = genericSearch(obj1, [\"name\", \"age\", \"address\"], \"123\");", "    expect(result).toBe(true);", "  });", "", "  it(\"should handle case-sensitive searches when specified\", () => {", "    const result = genericSearch(obj1, [\"name\"], \"alice\", true);", "    expect(result).toBe(false);", "  });", "", "  it(\"should handle case-insensitive searches when specified\", () => {", "    const result = genericSearch(obj1, [\"name\"], \"alice\", false);", "    expect(result).toBe(true);", "  });", "", "  it(\"should return true when query matches any property value in an array of objects\", () => {", "    const result = objs.some((obj) =>", "      genericSearch(obj, [\"name\", \"age\", \"address\"], \"third\")", "    );", "    expect(result).toBe(true);", "  });", "});"], "file_path": "frontend/src/business/functions/genericSearch.test.ts"}
{"Link_to_commit": "https://github.com/willemverbuyst/bookworm/commit/77e5f5b8d578fedf7ef87991b8f84f2c3840244a", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 24, "n_files_impacted": 1, "longest_chunk": ["import { compare } from \"./compare\";", "", "describe(\"compare function\", () => {", "  it(\"should return -1 if a[key] < b[key]\", () => {", "    const obj1 = { name: \"Alice\", age: 30 };", "    const obj2 = { name: \"Bob\", age: 40 };", "    const result = compare(\"age\")(obj1, obj2);", "    expect(result).toBe(-1);", "  });", "", "  it(\"should return 1 if a[key] > b[key]\", () => {", "    const obj1 = { name: \"Alice\", age: 40 };", "    const obj2 = { name: \"Bob\", age: 30 };", "    const result = compare(\"age\")(obj1, obj2);", "    expect(result).toBe(1);", "  });", "", "  it(\"should return 0 if a[key] === b[key]\", () => {", "    const obj1 = { name: \"Alice\", age: 30 };", "    const obj2 = { name: \"Bob\", age: 30 };", "    const result = compare(\"age\")(obj1, obj2);", "    expect(result).toBe(0);", "  });", "});"], "file_path": "frontend/src/business/functions/compare.test.ts"}
{"Link_to_commit": "https://github.com/rxa254/livespec/commit/3a7645806e3488eb34cb5721298ea8b0defc9654", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 44, "n_files_impacted": 1, "longest_chunk": ["import pyaudio", "import numpy as np", "import matplotlib.pyplot as plt", "", "p = pyaudio.PyAudio()", "", "# Parameters for the spectrogram", "NFFT = 1024  # Number of points for the FFT", "Fs = 44100   # Sampling frequency", "", "# Initialize the plot", "fig, ax = plt.subplots()", "img = ax.imshow(np.zeros((NFFT // 2 + 1, 100)), extent=[0, 1, 0, Fs/2], cmap='inferno', aspect='auto')", "", "# Function to update the spectrogram", "def update_specgram(in_data, frame_count, time_info, status):", "    # Compute the spectrogram", "    spec = np.abs(np.fft.rfft(np.frombuffer(in_data, dtype=np.int16), n=NFFT))", "    spec = 20 * np.log10(spec)", "    ", "    # Update the plot data", "    img.set_data(np.hstack([img.get_array()[:, 1:], np.atleast_2d(spec).T]))", "", "    #img.setImage(img_array, autoLevels=True)", "    # Redraw the plot", "    fig.canvas.draw()", "", "# Start the microphone input stream and update the spectrogram", "", "stream = p.open(format=pyaudio.paInt16, channels=1, rate=Fs, input=True, frames_per_buffer=NFFT, stream_callback=update_specgram)", "", "stream.start_stream()", "", "# Continuous update loop", "plt.show(block=False)", "while stream.is_active():", "    plt.pause(0.01)", "", "# Stop the stream and close the audio device", "stream.stop_stream()", "stream.close()", "p.terminate()", "", ""], "file_path": "GPTspec.py"}
{"Link_to_commit": "https://github.com/Canadadry/1brc-2024/commit/0ed97d40f74ccdd632a897fb9b529037302f1afa", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 68, "n_files_impacted": 3, "longest_chunk": ["package reader", "", "import (", "\t\"bytes\"", "\t\"strings\"", "\t\"testing\"", ")", "", "func TestR1(t *testing.T) {", "\ttests := map[string]struct {", "\t\tinput          string", "\t\texpectedOutput string", "\t}{", "\t\t\"single station\": {", "\t\t\tinput:          \"StationA;20.5\\nStationA;22.5\\n\",", "\t\t\texpectedOutput: \"{StationA=20.5/21.5/22.5}\\n\",", "\t\t},", "\t\t\"multiple stations\": {", "\t\t\tinput:          \"StationA;20.5\\nStationB;25.5\\nStationA;22.5\\nStationB;27.5\\n\",", "\t\t\texpectedOutput: \"{StationA=20.5/21.5/22.5, StationB=25.5/26.5/27.5}\\n\",", "\t\t},", "\t\t\"invalid lines ignored\": {", "\t\t\tinput:          \"StationA;20.5\\nInvalidLine\\nStationA;22.5\\n\",", "\t\t\texpectedOutput: \"{StationA=20.5/21.5/22.5}\\n\",", "\t\t},", "\t\t\"alphabetical order\": {", "\t\t\tinput:          \"StationC;5.0\\nStationA;3.5\\nStationB;4.2\\nStationA;2.5\\nStationC;5.2\\n\",", "\t\t\texpectedOutput: \"{StationA=2.5/3.0/3.5, StationB=4.2/4.2/4.2, StationC=5.0/5.1/5.2}\\n\",", "\t\t},", "\t}", "", "\tfor name, tc := range tests {", "\t\tt.Run(name, func(t *testing.T) {", "\t\t\tinput := strings.NewReader(tc.input)", "\t\t\tvar output bytes.Buffer", "", "\t\t\terr := R1(input, &output)", "\t\t\tif err != nil {", "\t\t\t\tt.Fatalf(\"unexpected error: %v\", err)", "\t\t\t}", "", "\t\t\tif gotOutput := output.String(); gotOutput != tc.expectedOutput {", "\t\t\t\tt.Errorf(\"expected output %q, got %q\", tc.expectedOutput, gotOutput)", "\t\t\t}", "\t\t})", "\t}", "}", "", "func BenchmarkR1(b *testing.B) {", "\t// Example input for the benchmarking", "\tinputData := \"StationA;1.1\\nStationB;2.2\\nStationC;3.3\\nStationA;4.4\\nStationB;5.5\\n\"", "\tfor i := 0; i < 1000; i++ {", "\t\tinputData += \"StationA;1.1\\nStationB;2.2\\nStationC;3.3\\nStationA;4.4\\nStationB;5.5\\n\"", "\t}", "\tinput := strings.NewReader(inputData)", "", "\tfor i := 0; i < b.N; i++ {", "\t\t// Reset the input reader to the beginning before each run", "\t\tinput.Seek(0, 0)", "\t\tvar output bytes.Buffer", "", "\t\t// Run the function being benchmarked", "\t\terr := R1(input, &output)", "\t\tif err != nil {", "\t\t\tb.Fatalf(\"Benchmark failed: %v\", err)", "\t\t}", "\t}", "}"], "file_path": "reader/reader_test.go"}
{"Link_to_commit": "https://github.com/hugorila/hangman-with-chatgpt/commit/7142684ddd8f015e8cccbb1cb498f0436fe78c2e", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 63, "n_files_impacted": 1, "longest_chunk": ["import random", "", "# List of possible words for the game", "words = ['apple', 'banana', 'cherry', 'dragonfruit', 'elderberry', 'fig', 'grapefruit', 'honeydew']", "", "def select_word():", "    \"\"\"Selects a random word from the list of possible words.\"\"\"", "    return random.choice(words)", "", "def display_word(word, guessed_letters):", "    \"\"\"Displays the current state of the word with guessed letters filled in.\"\"\"", "    display = ''", "    for letter in word:", "        if letter in guessed_letters:", "            display += letter", "        else:", "            display += '_'", "    print(display)", "", "def make_guess(guessed_letters):", "    \"\"\"Asks the player to guess a letter and adds it to the list of guessed letters.\"\"\"", "    while True:", "        guess = input('Guess a letter: ').lower()", "        if len(guess) == 1 and guess.isalpha() and guess not in guessed_letters:", "            guessed_letters.append(guess)", "            return guess", "        else:", "            print('Please enter a single letter that has not been guessed yet.')", "", "def check_guess(word, guess, guessed_letters):", "    \"\"\"Checks whether the guessed letter is in the word and updates the list of guessed letters.\"\"\"", "    if guess in word:", "        print('Correct!')", "    else:", "        print('Incorrect.')", "    display_word(word, guessed_letters)", "", "def play_game():", "    \"\"\"Plays a game of Hangman.\"\"\"", "    word = select_word()", "    guessed_letters = []", "    guesses_left = 6", "", "    print('Welcome to Hangman!')", "    display_word(word, guessed_letters)", "", "    while True:", "        guess = make_guess(guessed_letters)", "        check_guess(word, guess, guessed_letters)", "", "        if guess not in word:", "            guesses_left -= 1", "            print(f'You have {guesses_left} guesses left.')", "            if guesses_left == 0:", "                print(f'Sorry, you lost. The word was \"{word}\".')", "                break", "", "        if set(word).issubset(set(guessed_letters)):", "            print(f'Congratulations, you won! The word was \"{word}\".')", "            break", "", "if __name__ == '__main__':", "    play_game()"], "file_path": "hangman.py"}
{"Link_to_commit": "https://github.com/pappavis/EasyLab-retro-synth-SN76489/commit/8bb96a23ed018c1849989a0ea1ed8a7e4d8fd824", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 39, "n_files_impacted": 1, "longest_chunk": ["# Speelt Mary had a Little Lamb via I2C op een ESP8266 met SN76489", "# ChatGPT --> Please write a program in micropython which plays mary Had a Little Lamb using a ESP8266 and SN76489 via I2C. Also, generate a 4Mhz clock signal on pin A0.", "# https://chat.openai.com/chat/ede913fa-4fcb-44ae-81d8-b8f546e91327", "", "import machine", "import time", "", "# Define clock frequency", "clock_freq = 4000000", "", "# Set up I2C interface", "i2c = machine.I2C(scl=machine.Pin(5), sda=machine.Pin(4), freq=clock_freq)", "", "# Set up clock signal", "clock_pin = machine.Pin(0, machine.Pin.OUT)", "clock_timer = machine.Timer(0)", "", "# Configure SN76489 for 4-bit volume, 8-bit tone registers", "i2c.writeto(0x40, b'\\x9F')", "", "# Define note frequencies", "C = 0x60", "D = 0x68", "E = 0x70", "F = 0x78", "G = 0x80", "A = 0x88", "B = 0x90", "", "# Play \"Mary Had a Little Lamb\"", "notes = [E, D, C, D, E, E, E, D, D, D, E, G, G, E, D, C, D, E, E, E, E, D, D, E, D, C]", "durations = [4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4]", "", "# Define function to generate clock signal", "def generate_clock(timer):", "    clock_pin.value(not clock_pin.value())", "", "# Set up timer to generate clock signal", "clock_timer.init(period=int"], "file_path": "src/example_simplesong.py"}
{"Link_to_commit": "https://github.com/hg-gong/portfolio/commit/6df9e32de41e788047996635b3446a0ecb429a4c", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 53, "n_files_impacted": 57, "longest_chunk": ["function App() {", "  return (", "    <div>", "      <h1>Welcome to Larry's personal website!</h1>", "      <p>This is the landing page for Larry's personal website. Here, you can learn more about Larry and see some of his work.</p>", "    </div>", "  );", "}", "", "// Create a new About component", "function About() {", "  return (", "    <div>", "      <h1>About Larry</h1>", "      <p>Larry is a software engineer who enjoys building web and mobile applications. He has experience with a variety of programming languages and frameworks, including React, Angular, and Node.js.</p>", "      <p>In his free time, Larry enjoys hiking, reading, and playing video games.</p>", "    </div>", "  );", "}", "", "// Create a new Projects component", "function Projects() {", "  return (", "    <div>", "      <h1>Larry's Projects</h1>", "      <p>Here are some of the projects that Larry has worked on:</p>", "      <ul>", "        <li><a href=\"https://github.com/larry/react-todo-app\">React Todo App</a></li>", "        <li><a href=\"https://github.com/larry/node-chat-app\">Node.js Chat App</a></li>", "        <li><a href=\"https://github.com/larry/angular-blog\">Angular Blog</a></li>", "      </ul>", "    </div>", "  );", "}", "", "// Create a new Contact component", "function Contact() {", "  return (", "    <div>", "      <h1>Contact Larry</h1>", "      <p>You can contact Larry using the following methods:</p>", "      <ul>", "        <li>Email: larry@example.com</li>", "        <li>Twitter: @larry</li>", "        <li>GitHub: larry</li>", "      </ul>", "    </div>", "  );", "}", "", "// Render the About, Projects, and Contact components", "ReactDOM.render(", "  <>"], "file_path": "src/index.js"}
{"Link_to_commit": "https://github.com/Jcros214/QuizBox/commit/b20f51e41dbbc55d78d6c97eded516c946ae5dc2", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 136, "n_files_impacted": 5, "longest_chunk": ["from i2c_display_ME import I2C_Display", "from tlc5947_ME import TLC5947", "", "from machine import Pin, time_pulse_us", "import time", "", "class DebouncedPin(Pin):", "    def __init__(self, pin_id, debounce_time=20, *args, **kwargs):", "        self.pin_id = pin_id", "        super().__init__(pin_id, *args, **kwargs)", "        self.debounce_time = debounce_time", "        self.last_bounce_time = 0", "        self.last_value = self.value()", "        self.irq_handler_rise = None", "        self.irq_handler_fall = None", "", "        self.irq(self.on_rise, self.IRQ_RISING)", "        self.irq(self.on_fall, self.IRQ_FALLING)", "        ", "", "    def value(self, *args, **kwargs):", "        current_time = time_pulse_us(Pin(self.pin_id), 1)", "        if current_time - self.last_bounce_time > self.debounce_time:", "            new_value = super().value(*args, **kwargs)", "            if new_value != self.last_value:", "                self.last_value = new_value", "                if new_value == 1 and self.irq_handler_rise is not None:", "                    self.irq_handler_rise()", "                elif new_value == 0 and self.irq_handler_fall is not None:", "                    self.irq_handler_fall()", "                self.last_bounce_time = current_time", "        return self.last_value", "", "    def irq(self, handler=None, trigger=Pin.IRQ_FALLING):", "        if trigger == Pin.IRQ_RISING:", "            self.irq_handler_rise = handler", "        elif trigger == Pin.IRQ_FALLING:", "            self.irq_handler_fall = handler", "    ", "    def on_rise(self):", "        pass", "    ", "    def on_fall(self):", "        pass", "", "", "", "class QuizBox:", "    boxState = 1", "    ", "    class Quizzer:", "        color = 0xF66733", "        def __init__(self, seat: int = -1, switch: int = -1, num: int = -1) -> None:", "            self.seatpin = Pin(seat, pull=Pin.PULL_UP)", "            self.switchpin = Pin(switch, pull=Pin.PULL_UP)", "            self.num = num", "", "            self.seatval = self.seatpin.value()", "            self.switchval = self.switchpin.value()", "        ", "        @property", "        def seatread(self):", "            self.seatval = not self.seatpin.value()", "            return self.seatval", "", "        @property", "        def switchread(self):", "            self.switchval = not self.switchpin.value()", "            return self.switchval", "", "        @property", "        def bothread(self):", "            return (self.switchread, self.seatread)", "    class Buzzer(Pin):", "        def __init__(self, id = 16):", "            super().__init__(id, Pin.PULL_UP)", "", "        def buzz(self, time: int):", "            ", "", "    class Reset(DebouncedPin):", "        def __init__(self, id = 17):", "            super().__init__(id, Pin.PULL_UP)", "", "        def on_rise(self, pin):", "            pass", "", "        def on_fall(self, pin):", "            QuizBox.boxState = QuizBox.boxState + 1 if QuizBox.boxState < 3 else 0", "", "", "    def __init__(self) -> None:", "        self.display = I2C_Display()", "        self.tlc = TLC5947(24, sclk_pin=2, sdin_pin=3, blank_pin=4, xlat_pin=5)", "", "        self.reset = self.Reset()", "        self.buzzer = self.Buzzer()", "", "        self.quizzers = [", "            self.Quizzer(10, 18, 1),", "            self.Quizzer(11, 19, 2),", "            self.Quizzer(12, 20, 3),", "            self.Quizzer(13, 21, 4),", "            self.Quizzer(14, 22, 5),", "        ]", "", "    def update(self):", "        if self.boxState == 1:", "            for quizzer in self.quizzers:", "                if quizzer.bothread == (True,True):", "                    self.tlc.set_led(quizzer.num - 1, 1)", "                else:", "                    self.tlc.set_led(quizzer.num - 1, 0)", "            self.tlc.update()", "", "        elif self.boxState == 2:", "            for quizzer in self.quizzers:", "                if quizzer.bothread == (True,True):", "                    self.tlc.set_led(quizzer.num - 1, 1)", "                    self.tlc.update()", "                    self.boxState = 3", "                    break", "", "        elif self.boxState == 3:", "            ...", "        else:", "            self.boxState = 1", "            raise ValueError(\"BoxState should be in [1,2,3]. Setting to 1\")", "", "if __name__ == '__main__':", "    box = QuizBox()", "    # tlc = TLC5947(24, sclk_pin=2, sdin_pin=3, blank_pin=4, xlat_pin=5)", "", "    while True:", "        box.update()", "        # time.sleep(1)"], "file_path": "code.py"}
{"Link_to_commit": "https://github.com/gfletcher0311/VolleyBallProject/commit/56adc3835f46e9401ceb604440b421a9ba644d94", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 77, "n_files_impacted": 3, "longest_chunk": ["    continueButton3.addEventListener(\"click\", function() {", "      if(checkNumbers() && stage === 3) {", "        userData = [];", "        step3Container.querySelectorAll(\".skill\").forEach(skill => {", "          currentInput = skill.querySelector(\".skillsInput\")", "          if (!currentInput.disabled) {", "            userData.push(currentInput.value);", "          }", "        });", "        stage = 4; // Change to results stage", "        staggerButtonsReverse(skillGrid, 200);", "        staggerButtonsReverse(document.querySelectorAll(\".skillLabel\"), 250);", "        continueButton3.style.cssText = \"opacity: 0; transform: translateY(-20);\";", "        setTimeout(function() {", "          continueButton3.style.cssText = \"opacity: 0; display: none;\";", "          document.getElementById(\"parent-container\").style.display = \"none\";", "        }, 2000);", "", "", "", "        const sum = userData.reduce((accumulator, currentValue) => accumulator + parseInt(currentValue, 10), 0);", "        const totalPoints = (positionSkills[userPosition].length) * 10 // Take the number of skills * 10(max points)", "        const percentage = Math.round((sum/totalPoints)*100) // Round to nearest whole number for ranking", "        //console.log(percentage + \" therefore rank: \"+ getRank(percentage));", "        var config = {", "          type: 'radar',", "          data: {", "            labels: positionSkills[userPosition],", "            datasets: [{", "              label: \"Skill Results\",", "              backgroundColor: \"rgba(69, 162, 158, 0.6)\",", "              borderColor: \"white\",", "              pointBackgroundColor: \"white\",", "              pointRadius: 5.3,", "              data: userData,", "            }]", "          },", "          options: {", "            legend: {", "              position: 'top',", "              labels: {", "                fontColor: 'white'", "              }", "            },", "            scale: {", "              ticks: {", "                beginAtZero: true,", "                fontColor: 'white', // labels such as 10, 20, etc", "                showLabelBackdrop: false, // hide square behind text", "                suggestedMax: 10,", "                callback: function(value) {", "                  // Show only '10' 5 and 1, hide other values", "                  return value === 10 || value === 5 || value === 1? value : '';", "              }", "              },", "              pointLabels: {", "                fontColor: '#c5c6c7',", "                fontSize: 18,", "              },", "              gridLines: {", "                color: 'grey'", "              },", "              angleLines: {", "                color: 'white' // lines radiating from the center", "              }", "            },", "          }", "        };", "        const drawChart = new Chart(skillChart, config);", "        step4Container.style.display = \"block\";", "        setTimeout(function() {", "          document.getElementById(\"chartContainer\").style.left = \"85%\"", "          document.getElementById(\"continueButton4\").style.cssText = \"transform: translateY(0); opacity: 1;\";", "        }, 2000);", "      } ", "})", ""], "file_path": "script.js"}
{"Link_to_commit": "https://github.com/snowywaffles/golang/commit/ddb91469d7b730bc89df061e525f86214221abd3", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 18, "n_files_impacted": 1, "longest_chunk": ["func findMostCommon(nums []int) []int {", "\tcountMap := make(map[int]int)", "\tmaxCount := 0", "\tfor _, num := range nums {", "\t\tcountMap[num]++", "\t\tif countMap[num] > maxCount {", "\t\t\tmaxCount = countMap[num]", "\t\t}", "\t}", "\tmostCommon := []int{}", "\tfor num, count := range countMap {", "\t\tif count == maxCount {", "\t\t\tmostCommon = append(mostCommon, num)", "\t\t}", "\t}", "\treturn mostCommon", "}", ""], "file_path": "pseudorandom.go"}
{"Link_to_commit": "https://github.com/karthikjavangula/karthikjavangula.github.io/commit/26e148128313d0511a6d1d320a18f9cad69e14eb", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 53, "n_files_impacted": 1, "longest_chunk": ["<!DOCTYPE html>", "<html>", "  <head>", "    <meta charset=\"UTF-8\">", "    <title>My Personal Page</title>", "    <link rel=\"stylesheet\" type=\"text/css\" href=\"style.css\">", "  </head>", "  <body>", "    <header>", "      <h1>Hello, I'm Your Name</h1>", "      <nav>", "        <ul>", "          <li><a href=\"#about\">About Me</a></li>", "          <li><a href=\"#projects\">Projects</a></li>", "          <li><a href=\"#contact\">Contact</a></li>", "        </ul>", "      </nav>", "    </header>", "", "    <main>", "      <section id=\"about\">", "        <h2>About Me</h2>", "        <p>Write a short bio here.</p>", "      </section>", "", "      <section id=\"projects\">", "        <h2>Projects</h2>", "        <ul>", "          <li>Project 1</li>", "          <li>Project 2</li>", "          <li>Project 3</li>", "        </ul>", "      </section>", "", "      <section id=\"contact\">", "        <h2>Contact Me</h2>", "        <form action=\"#\" method=\"post\">", "          <label for=\"email\">Email:</label>", "          <input type=\"email\" id=\"email\" name=\"email\" required>", "          <br><br>", "          <label for=\"message\">Message:</label>", "          <textarea id=\"message\" name=\"message\" required></textarea>", "          <br><br>", "          <input type=\"submit\" value=\"Submit\">", "        </form>", "      </section>", "    </main>", "", "    <footer>", "      <p>Copyright &copy; 2023 Your Name</p>", "    </footer>", "  </body>", "</html>"], "file_path": "index.php"}
{"Link_to_commit": "https://github.com/Aervtas/playground/commit/279e1d3e75c2476df12c5600267b86a2469af257", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 83, "n_files_impacted": 6, "longest_chunk": ["import tkinter as tk", "import json", "", "def load_data():", "    # Load data from the JSON file", "    try:", "        with open(\"data.json\", \"r\") as file:", "            data = json.load(file)", "    except FileNotFoundError:", "        data = {\"column1\": [], \"column2\": []}", "", "    return data", "", "def save_data(data):", "    # Save data to the JSON file", "    with open(\"data.json\", \"w\") as file:", "        json.dump(data, file)", "", "def add_text():", "    text = text_entry.get()", "", "    if selected_column.get() == \"Column 1\":", "        data[\"column1\"].append(text)", "    elif selected_column.get() == \"Column 2\":", "        data[\"column2\"].append(text)", "", "    save_data(data)", "    refresh_lists()", "", "def refresh_lists():", "    # Clear the existing lists", "    listbox1.delete(0, tk.END)", "    listbox2.delete(0, tk.END)", "", "    # Add items from the data to the lists", "    for item in data[\"column1\"]:", "        listbox1.insert(tk.END, item)", "", "    for item in data[\"column2\"]:", "        listbox2.insert(tk.END, item)", "", "# Load initial data from the JSON file", "data = load_data()", "", "# Create the main window", "root = tk.Tk()", "", "# Create a label and pack it", "label = tk.Label(root, text=\"Enter Text:\")", "label.pack()", "", "# Create a text entry widget", "text_entry = tk.Entry(root)", "text_entry.pack()", "", "# Create a radio button to select the column", "selected_column = tk.StringVar(value=\"Column 1\")", "column1_radio = tk.Radiobutton(root, text=\"Column 1\", variable=selected_column, value=\"Column 1\")", "column1_radio.pack(anchor=tk.W)", "column2_radio = tk.Radiobutton(root, text=\"Column 2\", variable=selected_column, value=\"Column 2\")", "column2_radio.pack(anchor=tk.W)", "", "# Create a button to add text to the selected column", "button = tk.Button(root, text=\"Add Text\", command=add_text)", "button.pack()", "", "# Create a frame to hold the listboxes", "frame = tk.Frame(root)", "frame.pack(side=tk.LEFT)", "", "# Create a listbox for Column 1", "listbox1 = tk.Listbox(frame)", "listbox1.pack(side=tk.LEFT)", "", "# Create a listbox for Column 2", "listbox2 = tk.Listbox(frame)", "listbox2.pack(side=tk.LEFT)", "", "# Refresh the lists initially", "refresh_lists()", "", "# Run the main event loop", "root.mainloop()"], "file_path": "webscraping utils/thread_status.py"}
{"Link_to_commit": "https://github.com/pieroapretto/ChatGPT-Code-Mentor/commit/8fca9e55585648173735fe52366b86e1b8147409", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 33, "n_files_impacted": 3, "longest_chunk": ["const tectalicOpenai = require('@tectalic/openai').default;", "const config = require('dotenv').config();", "", "async function cypressTestsGenerator(input) {", "  try {", "    const res = await tectalicOpenai(process.env.OPENAI_API_KEY)", "    .chatCompletions.create({", "      model: 'gpt-3.5-turbo',", "      messages: [{ role: 'user', content: 'Write Cypress tests for the following JavaScript code: ' + input }]", "    });", "", "", "    const cypress_test_recommendations = res.data.choices[0].message.content.trim();", "    ", "    console.log(`\\n${cypress_test_recommendations}`);", "", "    return `Cypress tests suggestion for this pull request:\\n\\`\\`\\`diff\\n${cypress_test_recommendations}\\n\\`\\`\\``;", "", "  } catch (err) {", "    if (err?.response) {", "      const { status = null, statusText = '' } = err.response;", "      console.error(`${status} - ${statusText}`);", "      return err?.response;", "    } else {", "      console.error(err);", "      return err;", "    }", "  }", "}", "", "module.exports = {", "  cypressTestsGenerator", "};"], "file_path": "methods/cypressTestsGenerator.js"}
{"Link_to_commit": "https://github.com/AFinlayV/GPT-tools/commit/1c061d8633070b50a0bca5e0caccbb77d35e0ce4", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 75, "n_files_impacted": 2, "longest_chunk": ["", "    \"\"\"", "the following is written by gptchat. it needs some work to integrate it with this library, ", "I just wanted to copy it here to work on later", "", "it's 3 classes to create identities that can have conversations with each other and remember them. ", "it's a bit of a mess. I'll clean it up later. if you run stuff on it now it just generates an ever lengthening prompt,", "that repeats itself over and over until you reach the token limit. maybe use the GPTtext.get_summary() function to get a ", "summary of the conversation that stays under the token limit?", "", "", "class Memory:", "    def __init__(self):", "        self.data = {}  # Initialize the data attribute as an empty dictionary", "", "    def add_interaction(self, text: str, response: str):", "        # Store the current interaction in the data attribute", "        self.data[time.time()] = {", "            \"input\": text,", "            \"output\": response", "        }", "", "    def generate_summary(self) -> str:", "        # Generate a summary of past interactions by concatenating the input and output of each interaction", "        summary = \"\"", "        for index, interaction in self.data.items():", "            summary += f\"{interaction['input']}\\n{interaction['output']}\\n\"", "        return summary", "", "", "class GPT3Identity:", "    def __init__(self, name: str):", "        self.name = name", "        self.memory = Memory()  # Initialize the memory attribute as an instance of the Memory class", "", "    def generate_response(self, text: str, ai) -> str:", "        # Generate a summary of past interactions", "        summary = self.memory.generate_summary()", "", "        # Use the name attribute and summary of past interactions to specify the prompt for the GPT-3 model", "        prompt = f\"{summary}\\nWho are you?\\n{self.name}\\n{text}\"", "        response = ai.generate_text(prompt=prompt, max_tokens=1024)", "", "        # Store the current interaction in the memory attribute", "        self.memory.add_interaction(text, response)", "        return response", "", "", "class Conversation:", "    def __init__(self, *identities: GPT3Identity, master_prompt: str, ai):", "        self.identities = identities", "        self.master_prompt = master_prompt", "        self.ai = ai", "", "    def run_conversation(self, num_iterations: int):", "        # Create a loop to generate responses between the identities for the specified number of iterations", "        for i in range(num_iterations):", "            for identity in self.identities:", "                # Generate a response from the current identity to the most recent response from all other identities", "                prompt = self.generate_prompt(identity)", "                response = identity.generate_response(prompt, self.ai)", "                print(response)", "", "    def generate_prompt(self, identity: GPT3Identity) -> str:", "        # Generate a summary of past interactions for all identities except the current one", "        summary = \"\"", "        for other_identity in self.identities:", "            if other_identity != identity:", "                summary += other_identity.memory.generate_summary()", "", "        # Use the master prompt and summary of past interactions to specify the prompt for the GPT-3 model", "        prompt = f\"{summary}\\n{self.master_prompt}\\n\"", "        return prompt", "        ", "        \"\"\""], "file_path": "test.py"}
{"Link_to_commit": "https://github.com/SupremeSkriptKiddie/Decoder/commit/ce58789eb663c7a96557101524d715a923bc2c6b", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 37, "n_files_impacted": 1, "longest_chunk": ["import os\r", "import sys\r", "import chardet\r", "\r", "# Define a function to read and decode binary data\r", "def read_binary_file(filename):\r", "    # Open the file in binary mode\r", "    with open(filename, 'rb') as f:\r", "        # Read the binary data from the file\r", "        binary_data = f.read()\r", "        # Determine the file encoding\r", "        encoding = chardet.detect(binary_data)['encoding']\r", "        # Decode the binary data to text\r", "        text_data = binary_data.decode(encoding)\r", "    return text_data\r", "\r", "# Define the main function to parse command line arguments\r", "def main():\r", "    # Get the filename from the command line arguments\r", "    filename = sys.argv[1]\r", "    # Check if the file exists\r", "    if not os.path.isfile(filename):\r", "        print(\"File not found\")\r", "        return\r", "    # Check if the file is a binary file\r", "    if os.path.isfile(filename) and os.access(filename, os.X_OK):\r", "        print(\"This is an executable file\")\r", "        return\r", "    else:\r", "        # Read and decode the file data\r", "        text_data = read_binary_file(filename)\r", "        # Print the decoded data to the console\r", "        print(text_data)\r", "\r", "if __name__ == '__main__':\r", "    main()\r", "\r"], "file_path": "Decoder.py.py"}
{"Link_to_commit": "https://github.com/Khanstore/odoo_enterprise/commit/e001efc750d25d5dfcc681e42adf7a46edc7f37b", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 20, "n_files_impacted": 13, "longest_chunk": ["import { Component, useSubEnv, xml } from \"@odoo/owl\";", "", "/**", " * Loads the given props to the environment of the nested components.", " *", " * Usage:", " * <WithSubEnv prop1=\"value1\" prop2=\"value2\">", " *    <MyComponent/>", " * </WithSubEnv>", " *", " * MyComponent will then be able to access `prop1` and `prop2` from its environment.", " * (i.e: with `this.env.prop1` and `this.env.prop2`)", " */", "export class WithSubEnv extends Component {", "    static template = xml`<t t-slot=\"default\"/>`;", "    static props = [\"*\"];", "    setup() {", "        useSubEnv(this.props);", "    }", "}"], "file_path": "knowledge/static/src/components/wysiwyg_article_helper/wysiwyg_article_helper.js"}
{"Link_to_commit": "https://github.com/MSKCC-Epi-Bio/dcurves/commit/a4fce89e22064b44c3dcf0925a718d443fa30e6b", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 62, "n_files_impacted": 78, "longest_chunk": ["", "", "# import pandas as pd", "# import matplotlib as mpl", "# mpl.use('tkagg')", "# import matplotlib.pyplot as plt", "# import random", "# from typing import Optional", "#", "# def get_colors(n, color_names):", "#     if color_names is None:", "#         return [\"#%06x\" % random.randint(0, 0xFFFFFF) for _ in range(n)]", "#     elif len(color_names) < n:", "#         raise ValueError(", "#             'More predictors than color_names, please enter more color names in color_names list and try again')", "#     return color_names", "#", "# def plot_scores(", "#         plot_df: pd.DataFrame,", "#         score_col: str,", "#         y_limits: list = [-0.05, 0.2],", "#         color_names: Optional[list] = None", "#         ) -> None:", "#", "#     modelnames = plot_df['model'].value_counts().index", "#     colors = get_colors(len(modelnames), color_names)", "#     for modelname, color in zip(modelnames, colors):", "#         single_model_df = plot_df[plot_df['model'] == modelname]", "#         plt.plot(single_model_df['threshold'], single_model_df[score_col], color=color)", "#", "#         plt.ylim(y_limits)", "#         plt.legend(modelnames)", "#         plt.grid(b=True, which='both', axis='both')", "#         plt.xlabel('Threshold Values')", "#         plt.ylabel(f'Calculated {score_col.capitalize().replace(\"_\", \" \")}')", "#     plt.show()", "#", "# def plot_graphs(plot_df: pd.DataFrame,", "#                 graph_type: str = 'net_benefit',", "#                 y_limits: list = [-0.05, 1],", "#                 color_names: Optional[list] = None", "#                 ) -> None:", "#     \"\"\"", "#     Plot either net benefit or interventions avoided per threshold.", "#", "#     Parameters", "#     ----------", "#     plot_df : pd.DataFrame", "#         Data containing threshold values, model columns of net benefit/intervention scores to be plotted", "#     graph_type : str", "#         Type of plot (either 'net_benefit' or 'net_intervention_avoided')", "#     y_limits : list[float]", "#         2 floats, lower and upper bounds for y-axis", "#     color_names", "#         Colors to render each model (if n models supplied, then need n+2 colors, since 'all' and 'none' models will be", "#         included by default", "#", "#     Returns", "#     -------", "#     None", "#     \"\"\"", "#     plot_scores(plot_df, graph_type, y_limits, color_names)"], "file_path": "dcurves/prevalence.py"}
{"Link_to_commit": "https://github.com/GeorgeTait848/ElementaryDataStructures/commit/4d76aa669ea371e9fb2b1a1c3bb1705a2ce24239", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 36, "n_files_impacted": 2, "longest_chunk": ["    def test_push_pop(self):", "        stack = Stack()", "        stack.push(10)", "        stack.push(20)", "        stack.push(30)", "        self.assertFalse(stack.isEmpty())", "        self.assertEqual(stack.pop(), 30)", "        self.assertEqual(stack.pop(), 20)", "        self.assertEqual(stack.pop(), 10)", "        self.assertTrue(stack.isEmpty())", "", "    def test_stack_with_initial_elements(self):", "        elements = [40, 50, 60]", "        stack = Stack(elements)", "        self.assertFalse(stack.isEmpty())", "        for i, element in enumerate(elements):", "            self.assertEqual(stack[i], element)", "        self.assertEqual(len(stack.elements), len(elements))", "", "    def test_setitem_getitem(self):", "        stack = Stack([70, 80, 90])", "        stack[1] = 85", "        self.assertEqual(stack[1], 85)", "", "    def test_equality(self):", "        stack1 = Stack([100, 200, 300])", "        stack2 = Stack([100, 200, 300])", "        stack3 = Stack([400, 500, 600])", "        self.assertEqual(stack1, stack2)", "        self.assertNotEqual(stack1, stack3)", "", "    def test_pop_empty_stack(self):", "        stack = Stack()", "        with self.assertRaises(StackUnderFlow):", "            stack.pop()", ""], "file_path": "dataStructuresTests.py"}
{"Link_to_commit": "https://github.com/ashfordhill/dynamic-integration-tester/commit/4550c60c0e7db78bd5be4dc043767a94309352ed", "n-gram matched": "chatgpt to write", "n_lines_longer_change": 33, "n_files_impacted": 21, "longest_chunk": ["import { createSlice, PayloadAction } from '@reduxjs/toolkit';", "import { RootState } from './store';", "import { ConnectionDetails } from '../types/connection';", "", "interface ConnectionsState {", "  senderConnection: ConnectionDetails | undefined;", "  receiverConnection: ConnectionDetails | undefined;", "}", "", "const initialState: ConnectionsState = {", "  senderConnection: undefined,", "  receiverConnection: undefined", "};", "export const connectionsSliceName = 'connections';", "const connectionsSlice = createSlice({", "  name: connectionsSliceName,", "  initialState,", "  reducers: {", "    setSenderConnection: (state, action: PayloadAction<ConnectionDetails>) => {", "      state.senderConnection = action.payload;", "    },", "    setReceiverConnection: (state, action: PayloadAction<ConnectionDetails>) => {", "      state.receiverConnection = action.payload;", "    },", "  },", "});", "", "export const { setSenderConnection, setReceiverConnection } = connectionsSlice.actions;", "", "export const selectSenderConnection = (state: RootState) => state[connectionsSliceName].senderConnection;", "export const selectReceiverConnection = (state: RootState) => state[connectionsSliceName].receiverConnection;", "", "export default connectionsSlice.reducer;"], "file_path": "ui/src/store/containersSlice.ts"}
{"Link_to_commit": "https://github.com/SimonHallefelt/Multi-Agent-Simulation/commit/fa96516166b17bec68c210b6d7019cee268cecb9", "n-gram matched": "used copilot to", "n_lines_longer_change": 9, "n_files_impacted": 2, "longest_chunk": ["/*", " * Warehouse is a class that extends SimState and represents a simulation of a warehouse.", " * It initializes the simulation with a given seed and file paths,", " * reads the warehouse configuration from a file,", " * and manages the agents, tasks, and the warehouse environment.", " * It also provides methods for moving agents, and checking for walls or occupied spaces.", " * The class also includes methods for scoring and running the simulation.", " * It uses the MASON simulation library for agent-based modeling.", " */"], "file_path": "src/main/java/simulation/Warehouse.java"}
{"Link_to_commit": "https://github.com/Meep439/Jump/commit/8e11b7d5347e6b1944720dca424870bd17213430", "n-gram matched": "used copilot to", "n_lines_longer_change": 85, "n_files_impacted": 1, "longest_chunk": ["import pygame", "import random", "", "# Initialize PyGame", "pygame.init()", "", "# Screen dimensions", "WIDTH, HEIGHT = 800, 400", "screen = pygame.display.set_mode((WIDTH, HEIGHT))", "pygame.display.set_caption(\"Jump Over Spikes\")", "", "# Colors", "WHITE = (255, 255, 255)", "BLACK = (0, 0, 0)", "RED = (255, 0, 0)", "GREEN = (0, 255, 0)", "", "# Clock for controlling the frame rate", "clock = pygame.time.Clock()", "", "# Player settings", "player_size = 40", "player_x = 100", "player_y = HEIGHT - player_size", "player_velocity = 0", "jump_force = -15", "gravity = 1", "", "# Spike settings", "spike_width = 20", "spike_height = 40", "spike_x = WIDTH", "spike_y = HEIGHT - spike_height", "spike_speed = 10", "", "# Game variables", "running = True", "is_jumping = False", "score = 0", "font = pygame.font.Font(None, 36)", "", "# Main loop", "while running:", "    screen.fill(WHITE)", "", "    # Event handling", "    for event in pygame.event.get():", "        if event.type == pygame.QUIT:", "            running = False", "        if event.type == pygame.MOUSEBUTTONDOWN and event.button == 1:", "            if not is_jumping:", "                is_jumping = True", "                player_velocity = jump_force", "", "    # Player movement", "    if is_jumping:", "        player_y += player_velocity", "        player_velocity += gravity", "        if player_y >= HEIGHT - player_size:", "            player_y = HEIGHT - player_size", "            is_jumping = False", "", "    # Spike movement", "    spike_x -= spike_speed", "    if spike_x < -spike_width:", "        spike_x = WIDTH", "        score += 1", "", "    # Collision detection", "    if spike_x < player_x + player_size and spike_x + spike_width > player_x:", "        if player_y + player_size >= spike_y:", "            running = False  # End the game if the player hits a spike", "", "    # Draw player, spike, and score", "    pygame.draw.rect(screen, GREEN, (player_x, player_y, player_size, player_size))", "    pygame.draw.polygon(screen, RED, [(spike_x, spike_y), (spike_x + spike_width // 2, spike_y - spike_height), (spike_x + spike_width, spike_y)])", "    score_text = font.render(f\"Score: {score}\", True, BLACK)", "    screen.blit(score_text, (10, 10))", "", "    # Update the display", "    pygame.display.flip()", "    clock.tick(30)", "", "# Quit PyGame", "pygame.quit()"], "file_path": "Game.py"}
{"Link_to_commit": "https://github.com/danifunker/usbode/commit/2bba015b7a6c24ed075267b2d0e5efdbd8190ea0", "n-gram matched": "used copilot to", "n_lines_longer_change": 54, "n_files_impacted": 1, "longest_chunk": ["    ", "    # Track last update time to periodically check status", "    last_update_time = time.time()", "    update_interval = 5  # Check for updates every 5 seconds", "    ", "    while not exitRequested:", "        current_time = time.time()", "        ", "        # Check button states", "        for i, pin in enumerate(button_pins):", "            current_state = disp.RPI.digital_read(pin)", "            ", "            # Button press detected (transition from 1 to 0)", "            if current_state == 0 and last_button_states[pin] == 1:", "                last_button_states[pin] = 0", "                ", "            # Button release detected (transition from 0 to 1)", "            elif current_state == 1 and last_button_states[pin] == 0:", "                last_button_states[pin] = 1", "                ", "                # Check debounce", "                if current_time - last_press_time[pin] > debounce_time:", "                    last_press_time[pin] = current_time", "                    ", "                    # Handle button actions", "                    if i == 0:  # Mode button", "                        print(\"Changing MODE\")", "                        switch()", "                        updateDisplay(disp)", "                    elif i == 1:  # Advanced menu button", "                        print(\"ADVANCED MENU\")", "                        updateDisplay_Advanced(disp)", "                        updateDisplay(disp)", "                    elif i == 2:  # OK button", "                        print(\"OK\")", "                        changeISO_OLED(disp)", "                        updateDisplay(disp)", "            ", "            # Update button state", "            last_button_states[pin] = current_state", "        ", "        # Handle display updates separately from button presses", "        should_update = False", "        with update_lock:", "            if updateEvent == 1:", "                updateEvent = 0", "                should_update = True", "        ", "        # Check for periodic updates even if no explicit event", "        if current_time - last_update_time > update_interval:", "            last_update_time = current_time", "            should_update = True", "        ", "        if should_update:"], "file_path": "inst/usbode/usbode.py"}
{"Link_to_commit": "https://github.com/frebergguru/Seats-pdo-intl/commit/4828f994509d34cd265dd5123ed2add21ddbba6c", "n-gram matched": "used copilot to", "n_lines_longer_change": 156, "n_files_impacted": 27, "longest_chunk": ["// deepcode ignore PhpSameEvalBinaryExpressionfalse: <please specify a reason of ignoring this>", "if (isset($nickname) && !empty($nickname) && isset($key) && !empty($key) && $pwdchanged != true) {", "\ttry {", "\t\t$pdo = new PDO($dsn, DB_USERNAME, DB_PASSWORD, $db_options);", "\t\tswitch (DB_DRIVER) {", "\t\t\tcase \"mysql\":", "\t\t\t\t$stmt = $pdo->prepare(\"SELECT forgottoken FROM users WHERE nickname = :nickname\");", "\t\t\t\tbreak;", "\t\t\tcase \"pgsql\":", "\t\t\t\t$stmt = $pdo->prepare(\"SELECT forgottoken FROM users WHERE lower(nickname) = :nickname\");", "\t\t\t\tbreak;", "\t\t}", "\t\t$stmt->bindValue(\":nickname\", $nickname);", "\t\t$stmt->execute();", "\t\t$pdo = null;", "\t} catch (PDOException $e) {", "\t\terror_log($langArray['invalid_query'] . ' ' . $e->getMessage() . '\\n' . $langArray['whole_query'] . ' ' . $stmt->queryString, 0);", "\t}", "\t$sqlresults = $stmt->fetch(PDO::FETCH_ASSOC);", "\t$forgottoken = $sqlresults[\"forgottoken\"];", "", "\tif (mb_strtolower($key) === mb_strtolower($forgottoken)) {", "\t\trequire 'includes/header.php';", "\t\tprint '<form class=\"srs-container\" method=\"POST\" action=\"' . $_SERVER[\"PHP_SELF\"] . '?nickname=' . $nickname . '&key=' . $forgottoken . '\">", "<span class=\"srs-header\">' . $langArray['new_password'] . '</span>", "", "<div class=\"srs-content\">", "\t<a href=\"#\" id=\"passwordRequirements\">' . $langArray['password_requirements'] . '</a><br>", "\t<div class=\"bubble-container\">", "\t\t\t<div class=\"bubble\" id=\"bubblePopup\">", "\t\t\t' . $langArray['password_requirements_text'] . '", "\t\t\t<button id=\"closePopup\">' . $langArray['close_btn'] . '</button>", "\t\t\t</div>", "    <label for=\"password\" class=\"srs-lb\">' . $langArray['password'] . '</label><input name=\"password\" id=\"password\" type=\"password\" class=\"srs-tb\"><br>", "    <span id=\"pwstatus\"></span><br>", "\t</div>", "    <label for=\"password2\" class=\"srs-lb\">' . $langArray['repeat_password'] . '</label><input name=\"password2\" id=\"password2\" type=\"password\" class=\"srs-tb\"><br>", "</div>", "<div class=\"srs-footer\">", "\t<div class=\"srs-button-container\">", "<input type=\"submit\" value=\"' . $langArray['change_password_button'] . '\" class=\"srs-btn\">", "</div>", "<div class=\"srs-slope\"></div>", "</div>", "</form>", "<br><br>", "<script src=\"./js/pwdreq.js\"></script>", "<script src=\"./js/pwdcheck.js\"></script>';", "\t\trequire 'includes/footer.php';", "\t} else {", "\t\trequire 'includes/header.php';", "\t\tprint '<span class=\"srs-header\">' . $langArray['forgot_password_heading'] . ' - ' . $langArray['error'] . '</span>", "<div class=\"srs-content\">", "' . $langArray['wrong_nickname_or_verification_key'] . '", "</div><br><br><br>';", "\t\trequire 'includes/footer.php';", "\t\texit();", "\t}", "\t;", "} elseif (!empty($email)) {", "\trequire 'includes/header.php';", "\tprint '<span class=\"srs-header\">' . $langArray['new_password'] . ' - ' . $langArray['email'] . '</span>", "<div class=\"srs-content\">", "' . $langArray['email_sent_instruction_page_text'] . '", "</div><br><br><br>';", "\trequire 'includes/footer.php';", "\ttry {", "\t\t$pdo = new PDO($dsn, DB_USERNAME, DB_PASSWORD, $db_options);", "\t\t$stmt = $pdo->prepare(\"SELECT nickname FROM users WHERE email=:email\");", "\t\t$stmt->bindValue(\":email\", $email);", "\t\t$stmt->execute();", "\t\t$sqlresults = $stmt->fetch(PDO::FETCH_ASSOC);", "\t\tif ($stmt->rowCount() === 1) {", "\t\t\t$nickname = mb_strtolower($sqlresults['nickname']);", "\t\t\t$randomkey = genRandomKey();", "\t\t\t$pdo = null;", "", "\t\t\t$pdo = new PDO($dsn, DB_USERNAME, DB_PASSWORD, $db_options);", "\t\t\tswitch (DB_DRIVER) {", "\t\t\t\tcase \"mysql\":", "\t\t\t\t\t$stmt = $pdo->prepare(\"UPDATE users SET forgottoken=:randomkey WHERE nickname=:nickname\");", "\t\t\t\t\tbreak;", "\t\t\t\tcase \"pgsql\":", "\t\t\t\t\t$stmt = $pdo->prepare(\"UPDATE users SET forgottoken=:randomkey WHERE lower(nickname) = :nickname\");", "\t\t\t\t\tbreak;", "\t\t\t\tdefault:", "\t\t\t\t\tthrow new Exception(\"unsupported_database_driver\");", "\t\t\t}", "\t\t\t$stmt->bindValue(\":randomkey\", $randomkey);", "\t\t\t$stmt->bindValue(\":nickname\", $nickname);", "\t\t\t$stmt->execute();", "\t\t\t$pdo = null;", "\t\t\t$from_name = htmlspecialchars(trim($from_name), ENT_QUOTES, 'UTF-8');", "\t\t\t$from_mail = filter_var($from_mail, FILTER_VALIDATE_EMAIL);", "\t\t\tif (!$from_mail) {", "\t\t\t\terror_log('Invalid sender email address: ' . $from_mail);", "\t\t\t\texit('Invalid sender email address');", "\t\t\t}", "\t\t\t", "\t\t\t// Verify the email exists in the database", "\t\t\t$stmt = $pdo->prepare(\"SELECT COUNT(*) FROM users WHERE email = :email\");", "\t\t\t$stmt->bindValue(':email', $email);", "\t\t\t$stmt->execute();", "\t\t\tif ($stmt->fetchColumn() === 0) {", "\t\t\t\texit('Email address not found');", "\t\t\t}", "\t\t\t", "\t\t\t// Rate limiting to prevent abuse", "\t\t\tif (!isset($_SESSION['email_attempts'])) {", "\t\t\t\t$_SESSION['email_attempts'] = 0;", "\t\t\t}", "\t\t\tif ($_SESSION['email_attempts'] >= 5) {", "\t\t\t\texit('Too many email attempts. Please try again later.');", "\t\t\t}", "\t\t\t$_SESSION['email_attempts']++;", "\t\t\t", "\t\t\t// Ensure HTTPS is used", "\t\t\tif (empty($_SERVER['HTTPS']) || $_SERVER['HTTPS'] !== 'on') {", "\t\t\t\texit('Secure connection required');", "\t\t\t}", "\t\t\t", "\t\t\t$mailheaders = \"From: {$from_name} <{$from_mail}>\\r\\n\";", "\t\t\t$mailheaders .= \"X-Mailer: Seat Reservation/2.0\";", "\t\t\t$linkPath = '/forgot.php';", "\t\t\t$baseUrl = 'https://' . $_SERVER['SERVER_NAME'] . $linkPath;", "\t\t\t$resetLink = $baseUrl . '?nickname=' . urlencode($nickname) . '&key=' . urlencode($randomkey);", "\t\t\t$mailmsg = $langArray['email_change_password_body_hi'] . \" \" . htmlspecialchars($nickname) . \"\\n\\n\" .", "\t\t\t\t$langArray['email_change_password_body_link'] . \"\\n\\n\" .", "\t\t\t\t$resetLink;", "\t\t\t", "\t\t\t// Log email-sending activity for debugging", "\t\t\t// error_log(\"Password reset email sent to: {$email}\");", "\t\t\t", "\t\t\tmail($email, $mail_subject, $mailmsg, $mailheaders);", "\t\t}", "\t} catch (PDOException $e) {", "\t\terror_log($langArray['invalid_query'] . ' ' . $e->getMessage() . '\\n' . $langArray['whole_query'] . ' ' . $stmt->queryString, 0);", "\t}", "} else {", "\tif ($pwdchanged != true) {", "\t\trequire 'includes/header.php';", "\t\tprint '<form class=\"srs-container\" method=\"POST\" action=\"' . htmlspecialchars($_SERVER[\"PHP_SELF\"]); . '\">", "<span class=\"srs-header\">' . $langArray['forgot_password_heading'] . '</span>", "<div class=\"srs-content\">", "\t<label for=\"email\" class=\"srs-lb\">' . $langArray['email'] . '</label><input name=\"email\" value=\"\" id=\"email\" class=\"srs-tb\"><br>", "</div>", "<div class=\"srs-footer\">", "\t<div class=\"srs-button-container\">", "\t\t<input type=\"submit\" class=\"submit\" name=\"regsubmit\" value=\"' . $langArray['continue'] . '\">", "\t</div>", "\t<div class=\"srs-slope\"></div>", "</div>", "</form><br>';", "\t\trequire 'includes/footer.php';", "\t}", "\t;"], "file_path": "forgot.php"}
{"Link_to_commit": "https://github.com/LorealCI/taskmaster/commit/be752e122bb7e92e73381bc75d13bbe0f08dce43", "n-gram matched": "used copilot to", "n_lines_longer_change": 129, "n_files_impacted": 29, "longest_chunk": ["\"\"\"", "Django settings for taskmaster project.", "", "Generated by 'django-admin startproject' using Django 4.2.19.", "", "For more information on this file, see", "https://docs.djangoproject.com/en/4.2/topics/settings/", "", "For the full list of settings and their values, see", "https://docs.djangoproject.com/en/4.2/ref/settings/", "\"\"\"", "", "from pathlib import Path", "import os", "", "env_path = os.path.join(os.path.dirname(__name__), 'env.py')", "if os.path.exists(env_path):", "    import env", "", "# Build paths inside the project like this: BASE_DIR / 'subdir'.", "BASE_DIR = Path(__file__).resolve().parent.parent", "", "", "# Quick-start development settings - unsuitable for production", "# See https://docs.djangoproject.com/en/4.2/howto/deployment/checklist/", "", "# SECURITY WARNING: keep the secret key used in production secret!", "SECRET_KEY = os.getenv(\"SECRET_KEY\")", "", "# SECURITY WARNING: don't run with debug turned on in production!", "DEBUG = True", "", "ALLOWED_HOSTS = []", "", "", "# Application definition", "", "INSTALLED_APPS = [", "    'django.contrib.admin',", "    'django.contrib.auth',", "    'django.contrib.contenttypes',", "    'django.contrib.sessions',", "    'django.contrib.messages',", "    'django.contrib.staticfiles',", "    'tasks',", "]", "", "MIDDLEWARE = [", "    'django.middleware.security.SecurityMiddleware',", "    'django.contrib.sessions.middleware.SessionMiddleware',", "    'django.middleware.common.CommonMiddleware',", "    'django.middleware.csrf.CsrfViewMiddleware',", "    'django.contrib.auth.middleware.AuthenticationMiddleware',", "    'django.contrib.messages.middleware.MessageMiddleware',", "    'django.middleware.clickjacking.XFrameOptionsMiddleware',", "]", "", "ROOT_URLCONF = 'taskmaster.urls'", "", "TEMPLATES = [", "    {", "        'BACKEND': 'django.template.backends.django.DjangoTemplates',", "        'DIRS': [],", "        'APP_DIRS': True,", "        'OPTIONS': {", "            'context_processors': [", "                'django.template.context_processors.debug',", "                'django.template.context_processors.request',", "                'django.contrib.auth.context_processors.auth',", "                'django.contrib.messages.context_processors.messages',", "            ],", "        },", "    },", "]", "", "WSGI_APPLICATION = 'taskmaster.wsgi.application'", "", "", "# Database", "# https://docs.djangoproject.com/en/4.2/ref/settings/#databases", "", "DATABASES = {", "    'default': {", "        'ENGINE': 'django.db.backends.sqlite3',", "        'NAME': BASE_DIR / 'db.sqlite3',", "    }", "}", "", "", "# Password validation", "# https://docs.djangoproject.com/en/4.2/ref/settings/#auth-password-validators", "", "AUTH_PASSWORD_VALIDATORS = [", "    {", "        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',", "    },", "    {", "        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',", "    },", "    {", "        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',", "    },", "    {", "        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',", "    },", "]", "", "", "# Internationalization", "# https://docs.djangoproject.com/en/4.2/topics/i18n/", "", "LANGUAGE_CODE = 'en-us'", "", "TIME_ZONE = 'UTC'", "", "USE_I18N = True", "", "USE_TZ = True", "", "", "# Static files (CSS, JavaScript, Images)", "# https://docs.djangoproject.com/en/4.2/howto/static-files/", "", "STATIC_URL = 'static/'", "", "# Default primary key field type", "# https://docs.djangoproject.com/en/4.2/ref/settings/#default-auto-field", "", "DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'"], "file_path": "taskmaster/urls.py"}
{"Link_to_commit": "https://github.com/harthmajeed/RocketBoosterAgent/commit/538d4e2e52397b8f99be7483bb16df98dfea9bc1", "n-gram matched": "used copilot to", "n_lines_longer_change": 6, "n_files_impacted": 1, "longest_chunk": ["        # Penalize for high speeds when landing\r", "        if didAgentLand(self) and self.y_speed > 2.0:\r", "            score -= 2000\r", "        # Reward for successful landing\r", "        if didAgentLand(self) and self.y_speed <= 2.0:\r", "            score += 1000\r"], "file_path": "project/main.py"}
{"Link_to_commit": "https://github.com/joeyipps04/Lab9/commit/c5eb810b00c8d652cf73a6af640db0be3bf2764b", "n-gram matched": "used copilot to", "n_lines_longer_change": 47, "n_files_impacted": 2, "longest_chunk": ["", "int main() {", "    string filename;", "    cout << \"Enter the filename: \";", "    cin >> filename;", "", "    ifstream userfile(filename);", "    if (!userfile) {", "        cerr << \"Error opening file.\" << endl;", "        return 1;", "    }", "    int N, type;", "    userfile >> N >> type;", "", "    if (type== 0) {", "        Matrix<int> A(N), B(N);", "        A.readfile(userfile);", "        B.readfile(userfile);", "    ", "        cout << \"Matrix A:\\n\"; A.print();", "        cout << \"Matrix B:\\n\"; B.print();", "        cout << \"A + B:\\n\"; (A + B).print();", "        cout << \"A * B:\\n\"; (A * B).print();", "        cout << \"Diagonal sum of A: \" << A.diagonalSum() << endl;", "        cout << \"Diagonal sum of B: \" << B.diagonalSum() << endl;", "        cout << \"Swapping rows 0 and 1 in A:\\n\"; A.swaprows(0, 1); A.print();", "        cout << \"Swapping columns 0 and 1 in A:\\n\"; A.swapcols(0, 1); A.print();", "        cout << \"Changing value at (0, 0) in A to 348:\\n\"; A.chngevalue(0, 0, 348); A.print();", "        }   else if (type == 1) {", "            Matrix<double> A(N), B(N);", "            A.readfile(userfile);", "            B.readfile(userfile);", "", "            cout << \"Matrix A:\\n\"; A.print();", "            cout << \"Matrix B:\\n\"; B.print();", "            cout << \"A + B:\\n\"; (A + B).print();", "            cout << \"A * B:\\n\"; (A * B).print();", "            cout << \"Diagonal sum of A: \" << A.diagonalSum() << endl;", "            cout << \"Diagonal sum of B: \" << B.diagonalSum() << endl;", "            cout << \"Swapping rows 0 and 1 in A:\\n\"; A.swaprows(0, 1); A.print();", "            cout << \"Swapping columns 0 and 1 in A:\\n\"; A.swapcols(0, 1); A.print();", "            cout << \"Changing value at (0, 0) in A to 348.843:\\n\"; A.chngevalue(0, 0, 349.843); A.print();", "", "        }", "        userfile.close(); ", "        return 0;", "    }"], "file_path": "matrix.cpp"}
{"Link_to_commit": "https://github.com/braelynnesandreth/ProjectSandrethF24/commit/0ee74c6ec4c882729279ce303f657551438ce71c", "n-gram matched": "used copilot to", "n_lines_longer_change": 63, "n_files_impacted": 1, "longest_chunk": ["        public IActionResult CreateTestData()", "        {", "            Officer testOfficer7 = new Officer", "            {", "                Id = \"7\",", "                Firstname = \"Test\",", "                Lastname = \"Officer7\",", "                PhoneNumber = \"123-456-7890\",", "                Email = \"testofficer7@example.com\",", "                SupervisorsOfOfficer = new List<Supervises>()", "            };", "", "            Officer testOfficer8 = new Officer", "            {", "                Id = \"8\",", "                Firstname = \"Test\",", "                Lastname = \"Officer8\",", "                PhoneNumber = \"123-456-7891\",", "                Email = \"testofficer8@example.com\",", "                SupervisorsOfOfficer = new List<Supervises>()", "            };", "", "            Supervisor testSupervisor9 = new Supervisor", "            {", "                Id = \"9\",", "                Firstname = \"Test\",", "                Lastname = \"Supervisor9\",", "                PhoneNumber = \"123-456-7892\",", "                Email = \"testsupervisor9@example.com\",", "                OfficersSupervised = new List<Supervises>()", "            };", "", "            Supervisor testSupervisor10 = new Supervisor", "            {", "                Id = \"10\",", "                Firstname = \"Test\",", "                Lastname = \"Supervisor10\",", "                PhoneNumber = \"123-456-7893\",", "                Email = \"testsupervisor10@example.com\",", "                OfficersSupervised = new List<Supervises>()", "            };", "", "            // Establish the relationship between Officer 7 and Supervisor 9", "            Supervises supervises = new Supervises", "            {", "                Officer = testOfficer7,", "                Supervisor = testSupervisor9,", "                StartDate = DateTime.Now", "            };", "", "            testOfficer7.SupervisorsOfOfficer.Add(supervises);", "            testSupervisor9.OfficersSupervised.Add(supervises);", "", "            // Add the test data to the database", "            _database.Officer.Add(testOfficer7);", "            _database.Officer.Add(testOfficer8);", "            _database.Supervisor.Add(testSupervisor9);", "            _database.Supervisor.Add(testSupervisor10);", "            _database.Supervises.Add(supervises);", "            _database.SaveChanges();", "", "            return RedirectToAction(\"Index\");", "        }"], "file_path": "F24S2DiscussionSolutionSandreth/DiscussionMvcSandreth/Controllers/OfficerController.cs"}
{"Link_to_commit": "https://github.com/abdyer/j-archive-mcp/commit/481b8be710f1ec0ddd310a25837e7d2dc6129bef", "n-gram matched": "copilot to implement", "n_lines_longer_change": 166, "n_files_impacted": 6, "longest_chunk": ["import * as cheerio from \"cheerio\";", "import request from \"request\";", "import _ from \"lodash\";", "", "interface Clue {", "    clue_text: string;", "    correct_response: string;", "    clue_order_number: string;", "}", "", "interface Category {", "    category_name: string;", "    category_comments: string;", "}", "", "interface Round {", "    [key: string]: Clue | Category;", "}", "", "interface GameDetails {", "    finalScores: { contestant: string; score: string }[];", "    coryatScores: { contestant: string; score: string }[];", "    jeopardyScores: { contestant: string; score: string }[];", "    doubleJeopardyScores: { contestant: string; score: string }[];", "}", "", "export const requestIndex = async (url: string): Promise<any> => {", "    try {", "        const html = await new Promise<string>((resolve, reject) => {", "            request(url, (error, response, body) => {", "                if (error) return reject(error);", "                resolve(body);", "            });", "        });", "", "        const $ = cheerio.load(html);", "        const result: { type: \"text\"; text: string }[] = [];", "        $(\"#content table tr\").each(function () {", "            const data = $(this);", "            const row: string[] = [];", "            data.children().each(function (i, element) {", "                if (i === 0) {", "                    let link = $(\"a\", element).first().attr(\"href\") || \"\";", "                    link = link.substring(link.indexOf(\"=\") + 1);", "                    row.push(link);", "                }", "                row.push($(element).text().trim());", "            });", "            const season = _.zipObject([\"id\", \"name\", \"description\", \"note\"], row);", "            result.push({ type: \"text\", text: JSON.stringify(season) });", "        });", "        return { content: [{ type: \"text\", text: JSON.stringify(result) }] };", "    } catch (error) {", "        console.error(\"Error in requestIndex:\", error);", "        throw error;", "    }", "};", "", "export const parseCategories = ($: cheerio.CheerioAPI, context: any, roundPrefix: string): Record<string, any> => {", "    const categories: Record<string, any> = {};", "    const roundTable = $(roundPrefix !== \"FJ\" ? \"table.round\" : \"table.final_round\", context);", "    const firstRowChildren = $(\"tr\", roundTable).first().children();", "", "    firstRowChildren.each(function (i, element) {", "        const data = $(this);", "        categories[`category${roundPrefix}${i + 1}`] = {", "            category_name: data.find(\".category_name\").text(),", "            category_comments: data.find(\".category_comments\").text(),", "        };", "    });", "", "    return categories;", "};", "", "export const parseClues = ($: cheerio.CheerioAPI, context: any): Record<string, any> => {", "    const clues: Record<string, any> = {};", "    const clueTexts = $(\".clue_text\", context).not(\"[id$='_r']\");", "", "    clueTexts.each(function () {", "        const data = $(this);", "        const clueId = data.attr(\"id\");", "        const parent = data.parent();", "        const clueOrder = data.closest(\".clue\").find(\".clue_order_number a\").text();", "        if (clueId) {", "            clues[clueId] = {", "                clue_text: data.text(),", "                correct_response: parent.find(\".correct_response\").text(),", "                clue_order_number: clueOrder,", "            };", "        }", "    });", "", "    return clues;", "};", "", "export const parseRound = ($: cheerio.CheerioAPI, context: any, roundPrefix: string): Round => {", "    const categories = parseCategories($, context, roundPrefix);", "    const clues = parseClues($, context);", "", "    return { ...categories, ...clues } as Round;", "};", "", "export const parseScores = ($: cheerio.CheerioAPI, headerText: string): { contestant: string; score: string }[] => {", "    const scores: { contestant: string; score: string }[] = [];", "    const header = $(`h3:contains('${headerText}')`);", "    const scoresTable = header.next('table');", "", "    if (scoresTable.length === 0) {", "        console.warn(`No table found for header: ${headerText}`);", "        return scores;", "    }", "", "    const nameCells = scoresTable.find('td.score_player_nickname');", "    const scoreCells = scoresTable.find('td.score_positive');", "", "    if (nameCells.length !== scoreCells.length) {", "        console.warn(`Mismatch between number of names and scores for header: ${headerText}`);", "        return scores;", "    }", "", "    nameCells.each((index, element) => {", "        const contestant = $(element).text().trim();", "        const score = $(scoreCells[index]).text().trim();", "        if (contestant && score) {", "            scores.push({ contestant, score });", "        }", "    });", "", "    return scores;", "};", "", "export const parseGameDetails = ($: cheerio.CheerioAPI): GameDetails => {", "    const finalScores = parseScores($, 'Final scores');", "    const coryatScores = parseScores($, 'Coryat scores');", "    const jeopardyScores = parseScores($, 'Scores at the end of the Jeopardy! Round');", "    const doubleJeopardyScores = parseScores($, 'Scores at the end of the Double Jeopardy! Round');", "", "    return { finalScores, coryatScores, jeopardyScores, doubleJeopardyScores };", "};", "", "export const parseGame = ($: cheerio.CheerioAPI): { content: { type: \"text\"; text: string }[] } => {", "    const result: { type: \"text\"; text: string }[] = [];", "", "    const gameTitle = $(\"#game_title\").text();", "    const gameComments = $(\"#game_comments\").text();", "", "    result.push({ type: \"text\", text: `Game Title: ${gameTitle}` });", "    result.push({ type: \"text\", text: `Game Comments: ${gameComments}` });", "", "    const jeopardyRound = parseRound($, $(\"#jeopardy_round\"), \"J\");", "    const doubleJeopardyRound = parseRound($, $(\"#double_jeopardy_round\"), \"DJ\");", "    const finalJeopardyRound = parseRound($, $(\"#final_jeopardy_round\"), \"FJ\");", "", "    result.push({ type: \"text\", text: `Jeopardy Round: ${JSON.stringify(jeopardyRound)}` });", "    result.push({ type: \"text\", text: `Double Jeopardy Round: ${JSON.stringify(doubleJeopardyRound)}` });", "    result.push({ type: \"text\", text: `Final Jeopardy Round: ${JSON.stringify(finalJeopardyRound)}` });", "", "    const gameDetails = parseGameDetails($);", "", "    result.push({ type: \"text\", text: `Jeopardy Scores: ${JSON.stringify(gameDetails.jeopardyScores)}` });", "    result.push({ type: \"text\", text: `Double Jeopardy Scores: ${JSON.stringify(gameDetails.doubleJeopardyScores)}` });", "    result.push({ type: \"text\", text: `Final Scores: ${JSON.stringify(gameDetails.finalScores)}` });", "    result.push({ type: \"text\", text: `Coryat Scores: ${JSON.stringify(gameDetails.coryatScores)}` });", "", "    return { content: result };", "};"], "file_path": "src/parsing.ts"}
